<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Vector Space Representation | Corpus Linguistics</title>
  <meta name="description" content="ENC2036 Course material first edition" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Vector Space Representation | Corpus Linguistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="ENC2036 Course material first edition" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Vector Space Representation | Corpus Linguistics" />
  
  <meta name="twitter:description" content="ENC2036 Course material first edition" />
  

<meta name="author" content="Alvin Chen" />


<meta name="date" content="2020-04-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="xml.html"/>
<link rel="next" href="vector-space-representation-ii.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="libs/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.5/grViz.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://alvinntnu.github.io/NTNU_ENC2036/"><img src="images/alerts/home.svg" height = "20" width = "20"> Course Website</a></li>
<li><a href="./"><img src="images/alerts/book-solid.svg" height = "20" width = "20"> ENC 2036 Corpus Linguistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objective"><i class="fa fa-check"></i>Course Objective</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#textbook"><i class="fa fa-check"></i>Textbook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-website"><i class="fa fa-check"></i>Course Website</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-demo-data"><i class="fa fa-check"></i>Course Demo Data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#necessary-packages"><i class="fa fa-check"></i>Necessary Packages</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html"><i class="fa fa-check"></i><b>1</b> What is Corpus Linguistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#why-do-we-need-corpus-data"><i class="fa fa-check"></i><b>1.1</b> Why do we need corpus data?</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#what-is-corpus"><i class="fa fa-check"></i><b>1.2</b> What is corpus?</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#what-is-a-corpus-linguistic-study"><i class="fa fa-check"></i><b>1.3</b> What is a corpus linguistic study?</a></li>
<li class="chapter" data-level="1.4" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#additional-information-on-cl"><i class="fa fa-check"></i><b>1.4</b> Additional Information on CL</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>2</b> R Fundamentals</a><ul>
<li class="chapter" data-level="" data-path="r-fundamentals.html"><a href="r-fundamentals.html#a-quick-note"><i class="fa fa-check"></i>A Quick Note</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="creating-corpus.html"><a href="creating-corpus.html"><i class="fa fa-check"></i><b>3</b> Creating Corpus</a><ul>
<li class="chapter" data-level="3.1" data-path="creating-corpus.html"><a href="creating-corpus.html#html-structure"><i class="fa fa-check"></i><b>3.1</b> HTML Structure</a><ul>
<li class="chapter" data-level="3.1.1" data-path="creating-corpus.html"><a href="creating-corpus.html#html-syntax"><i class="fa fa-check"></i><b>3.1.1</b> HTML Syntax</a></li>
<li class="chapter" data-level="3.1.2" data-path="creating-corpus.html"><a href="creating-corpus.html#tags-and-attributes"><i class="fa fa-check"></i><b>3.1.2</b> Tags and Attributes</a></li>
<li class="chapter" data-level="3.1.3" data-path="creating-corpus.html"><a href="creating-corpus.html#css"><i class="fa fa-check"></i><b>3.1.3</b> CSS</a></li>
<li class="chapter" data-level="3.1.4" data-path="creating-corpus.html"><a href="creating-corpus.html#html-css-javascript"><i class="fa fa-check"></i><b>3.1.4</b> HTML + CSS ( + JavaScript)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="creating-corpus.html"><a href="creating-corpus.html#web-crawling"><i class="fa fa-check"></i><b>3.2</b> Web Crawling</a></li>
<li class="chapter" data-level="3.3" data-path="creating-corpus.html"><a href="creating-corpus.html#functional-programming"><i class="fa fa-check"></i><b>3.3</b> Functional Programming</a></li>
<li class="chapter" data-level="3.4" data-path="creating-corpus.html"><a href="creating-corpus.html#save-corpus"><i class="fa fa-check"></i><b>3.4</b> Save Corpus</a></li>
<li class="chapter" data-level="3.5" data-path="creating-corpus.html"><a href="creating-corpus.html#additional-resources"><i class="fa fa-check"></i><b>3.5</b> Additional Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html"><i class="fa fa-check"></i><b>4</b> Corpus Analysis: A Start</a><ul>
<li class="chapter" data-level="4.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#installing-quanteda"><i class="fa fa-check"></i><b>4.1</b> Installing <code>quanteda</code></a></li>
<li class="chapter" data-level="4.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#building-a-corpus-from-character-vector"><i class="fa fa-check"></i><b>4.2</b> Building a <code>corpus</code> from character vector</a></li>
<li class="chapter" data-level="4.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#keyword-in-context-kwic"><i class="fa fa-check"></i><b>4.3</b> Keyword-in-Context (KWIC)</a></li>
<li class="chapter" data-level="4.4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#kwic-with-regular-expressions"><i class="fa fa-check"></i><b>4.4</b> KWIC with Regular Expressions</a></li>
<li class="chapter" data-level="4.5" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#tidy-text-format-of-the-corpus"><i class="fa fa-check"></i><b>4.5</b> Tidy Text Format of the Corpus</a></li>
<li class="chapter" data-level="4.6" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#frequency-lists"><i class="fa fa-check"></i><b>4.6</b> Frequency Lists</a><ul>
<li class="chapter" data-level="4.6.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#word-unigram"><i class="fa fa-check"></i><b>4.6.1</b> Word (Unigram)</a></li>
<li class="chapter" data-level="4.6.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#bigrams"><i class="fa fa-check"></i><b>4.6.2</b> Bigrams</a></li>
<li class="chapter" data-level="4.6.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#ngrams-lexical-bundles"><i class="fa fa-check"></i><b>4.6.3</b> Ngrams (Lexical Bundles)</a></li>
<li class="chapter" data-level="4.6.4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#frequency-and-dispersion"><i class="fa fa-check"></i><b>4.6.4</b> Frequency and Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#word-cloud"><i class="fa fa-check"></i><b>4.7</b> Word Cloud</a></li>
<li class="chapter" data-level="4.8" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#collocations"><i class="fa fa-check"></i><b>4.8</b> Collocations</a><ul>
<li class="chapter" data-level="4.8.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#cooccurrence-table-and-observed-frequencies"><i class="fa fa-check"></i><b>4.8.1</b> Cooccurrence Table and Observed Frequencies</a></li>
<li class="chapter" data-level="4.8.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#expected-frequencies"><i class="fa fa-check"></i><b>4.8.2</b> Expected Frequencies</a></li>
<li class="chapter" data-level="4.8.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#association-measures"><i class="fa fa-check"></i><b>4.8.3</b> Association Measures</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#constructions"><i class="fa fa-check"></i><b>4.9</b> Constructions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html"><i class="fa fa-check"></i><b>5</b> Parts-of-Speech Tagging</a><ul>
<li class="chapter" data-level="5.1" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#installing-the-package"><i class="fa fa-check"></i><b>5.1</b> Installing the Package</a></li>
<li class="chapter" data-level="5.2" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#quick-overview"><i class="fa fa-check"></i><b>5.2</b> Quick Overview</a></li>
<li class="chapter" data-level="5.3" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#working-pipeline"><i class="fa fa-check"></i><b>5.3</b> Working Pipeline</a></li>
<li class="chapter" data-level="5.4" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#parsing-your-texts"><i class="fa fa-check"></i><b>5.4</b> Parsing Your Texts</a></li>
<li class="chapter" data-level="5.5" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#metalingusitic-analysis"><i class="fa fa-check"></i><b>5.5</b> Metalingusitic Analysis</a></li>
<li class="chapter" data-level="5.6" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#construction-analysis"><i class="fa fa-check"></i><b>5.6</b> Construction Analysis</a></li>
<li class="chapter" data-level="5.7" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#issues-on-pattern-retrieval"><i class="fa fa-check"></i><b>5.7</b> Issues on Pattern Retrieval</a></li>
<li class="chapter" data-level="5.8" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#saving-pos-tagged-texts"><i class="fa fa-check"></i><b>5.8</b> Saving POS-tagged Texts</a></li>
<li class="chapter" data-level="5.9" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#finalize-spacy"><i class="fa fa-check"></i><b>5.9</b> Finalize spaCy</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="midterm-exam.html"><a href="midterm-exam.html"><i class="fa fa-check"></i><b>6</b> Midterm Exam</a><ul>
<li class="chapter" data-level="6.1" data-path="midterm-exam.html"><a href="midterm-exam.html#important-instructions"><i class="fa fa-check"></i><b>6.1</b> Important Instructions</a></li>
<li class="chapter" data-level="6.2" data-path="midterm-exam.html"><a href="midterm-exam.html#part-i"><i class="fa fa-check"></i><b>6.2</b> Part I</a></li>
<li class="chapter" data-level="6.3" data-path="midterm-exam.html"><a href="midterm-exam.html#part-ii"><i class="fa fa-check"></i><b>6.3</b> Part II</a></li>
<li class="chapter" data-level="6.4" data-path="midterm-exam.html"><a href="midterm-exam.html#part-iii"><i class="fa fa-check"></i><b>6.4</b> Part III</a></li>
<li class="chapter" data-level="6.5" data-path="midterm-exam.html"><a href="midterm-exam.html#part-iv"><i class="fa fa-check"></i><b>6.5</b> Part IV</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="keyword-analysis.html"><a href="keyword-analysis.html"><i class="fa fa-check"></i><b>7</b> Keyword Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="keyword-analysis.html"><a href="keyword-analysis.html#about-keywords"><i class="fa fa-check"></i><b>7.1</b> About Keywords</a></li>
<li class="chapter" data-level="7.2" data-path="keyword-analysis.html"><a href="keyword-analysis.html#statistics-for-keyness"><i class="fa fa-check"></i><b>7.2</b> Statistics for Keyness</a></li>
<li class="chapter" data-level="7.3" data-path="keyword-analysis.html"><a href="keyword-analysis.html#implementation"><i class="fa fa-check"></i><b>7.3</b> Implementation</a></li>
<li class="chapter" data-level="7.4" data-path="keyword-analysis.html"><a href="keyword-analysis.html#tidy-data"><i class="fa fa-check"></i><b>7.4</b> Tidy Data</a><ul>
<li class="chapter" data-level="7.4.1" data-path="keyword-analysis.html"><a href="keyword-analysis.html#an-long-to-wide-example"><i class="fa fa-check"></i><b>7.4.1</b> An Long-to-Wide Example</a></li>
<li class="chapter" data-level="7.4.2" data-path="keyword-analysis.html"><a href="keyword-analysis.html#a-wide-to-long-example"><i class="fa fa-check"></i><b>7.4.2</b> A Wide-to-Long Example</a></li>
<li class="chapter" data-level="7.4.3" data-path="keyword-analysis.html"><a href="keyword-analysis.html#word-frequency-transformation"><i class="fa fa-check"></i><b>7.4.3</b> Word Frequency Transformation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="keyword-analysis.html"><a href="keyword-analysis.html#computing-keynesss"><i class="fa fa-check"></i><b>7.5</b> Computing Keynesss</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html"><i class="fa fa-check"></i><b>8</b> Chinese Text Processing</a><ul>
<li class="chapter" data-level="8.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#chinese-word-segmenter-jiebar"><i class="fa fa-check"></i><b>8.1</b> Chinese Word Segmenter <code>jiebaR</code></a><ul>
<li class="chapter" data-level="8.1.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#start"><i class="fa fa-check"></i><b>8.1.1</b> Start</a></li>
<li class="chapter" data-level="8.1.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#settings"><i class="fa fa-check"></i><b>8.1.2</b> Settings</a></li>
<li class="chapter" data-level="8.1.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#user-defined-dictionary"><i class="fa fa-check"></i><b>8.1.3</b> User-defined dictionary</a></li>
<li class="chapter" data-level="8.1.4" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#stopwords"><i class="fa fa-check"></i><b>8.1.4</b> Stopwords</a></li>
<li class="chapter" data-level="8.1.5" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#pos-tagging"><i class="fa fa-check"></i><b>8.1.5</b> POS Tagging</a></li>
<li class="chapter" data-level="8.1.6" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#default"><i class="fa fa-check"></i><b>8.1.6</b> Default</a></li>
<li class="chapter" data-level="8.1.7" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#reminder"><i class="fa fa-check"></i><b>8.1.7</b> Reminder</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#chinese-text-analytics-pipeline"><i class="fa fa-check"></i><b>8.2</b> Chinese Text Analytics Pipeline</a></li>
<li class="chapter" data-level="8.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#comparing-tokenization-methods"><i class="fa fa-check"></i><b>8.3</b> Comparing Tokenization Methods</a></li>
<li class="chapter" data-level="8.4" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#data"><i class="fa fa-check"></i><b>8.4</b> Data</a></li>
<li class="chapter" data-level="8.5" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#loading-text-data"><i class="fa fa-check"></i><b>8.5</b> Loading Text Data</a></li>
<li class="chapter" data-level="8.6" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#quantedatokens-vs.-jiebarsegment"><i class="fa fa-check"></i><b>8.6</b> <code>quanteda::tokens()</code> vs. <code>jiebaR::segment()</code></a></li>
<li class="chapter" data-level="8.7" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-1-word-frequency-and-wordcloud"><i class="fa fa-check"></i><b>8.7</b> Case Study 1: Word Frequency and Wordcloud</a></li>
<li class="chapter" data-level="8.8" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-2-patterns"><i class="fa fa-check"></i><b>8.8</b> Case Study 2: Patterns</a><ul>
<li class="chapter" data-level="8.8.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#define-own-tokenization-functions"><i class="fa fa-check"></i><b>8.8.1</b> Define Own Tokenization Functions</a></li>
<li class="chapter" data-level="8.8.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#transform-text-based-to-token-based-data-frame"><i class="fa fa-check"></i><b>8.8.2</b> Transform Text-Based to Token-Based Data Frame</a></li>
<li class="chapter" data-level="8.8.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#bei-construction"><i class="fa fa-check"></i><b>8.8.3</b> BEI Construction</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-3-lexical-bundles"><i class="fa fa-check"></i><b>8.9</b> Case Study 3: Lexical Bundles</a><ul>
<li class="chapter" data-level="8.9.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#n-grams-extraction"><i class="fa fa-check"></i><b>8.9.1</b> N-grams Extraction</a></li>
<li class="chapter" data-level="8.9.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#frequency-and-dispersion-1"><i class="fa fa-check"></i><b>8.9.2</b> Frequency and Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#afterwords"><i class="fa fa-check"></i><b>8.10</b> Afterwords</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html"><i class="fa fa-check"></i><b>9</b> Constructions and Idioms</a><ul>
<li class="chapter" data-level="9.1" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#collostruction"><i class="fa fa-check"></i><b>9.1</b> Collostruction</a></li>
<li class="chapter" data-level="9.2" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#corpus"><i class="fa fa-check"></i><b>9.2</b> Corpus</a></li>
<li class="chapter" data-level="9.3" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#word-segmentation"><i class="fa fa-check"></i><b>9.3</b> Word Segmentation</a></li>
<li class="chapter" data-level="9.4" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#extract-constructions"><i class="fa fa-check"></i><b>9.4</b> Extract Constructions</a></li>
<li class="chapter" data-level="9.5" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#distributional-information-needed-for-ca"><i class="fa fa-check"></i><b>9.5</b> Distributional Information Needed for CA</a><ul>
<li class="chapter" data-level="9.5.1" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#word-frequency-list"><i class="fa fa-check"></i><b>9.5.1</b> Word Frequency List</a></li>
<li class="chapter" data-level="9.5.2" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#construction-frequencies"><i class="fa fa-check"></i><b>9.5.2</b> Construction Frequencies</a></li>
<li class="chapter" data-level="9.5.3" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#other-information"><i class="fa fa-check"></i><b>9.5.3</b> Other Information</a></li>
<li class="chapter" data-level="9.5.4" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#creat-output-file"><i class="fa fa-check"></i><b>9.5.4</b> Creat Output File</a></li>
<li class="chapter" data-level="9.5.5" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#run-coll.analysis.r"><i class="fa fa-check"></i><b>9.5.5</b> Run <code>coll.analysis.r</code></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#chinese-four-character-idioms"><i class="fa fa-check"></i><b>9.6</b> Chinese Four-character Idioms</a></li>
<li class="chapter" data-level="9.7" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#dictionary-entries"><i class="fa fa-check"></i><b>9.7</b> Dictionary Entries</a></li>
<li class="chapter" data-level="9.8" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#case-study-x來y去"><i class="fa fa-check"></i><b>9.8</b> Case Study: <code>X來Y去</code></a></li>
<li class="chapter" data-level="9.9" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#exercises"><i class="fa fa-check"></i><b>9.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ckiptagger.html"><a href="ckiptagger.html"><i class="fa fa-check"></i><b>10</b> CKIP Tagger</a><ul>
<li class="chapter" data-level="10.1" data-path="ckiptagger.html"><a href="ckiptagger.html#installation"><i class="fa fa-check"></i><b>10.1</b> Installation</a></li>
<li class="chapter" data-level="10.2" data-path="ckiptagger.html"><a href="ckiptagger.html#download-the-model-files"><i class="fa fa-check"></i><b>10.2</b> Download the Model Files</a></li>
<li class="chapter" data-level="10.3" data-path="ckiptagger.html"><a href="ckiptagger.html#r-python-communication"><i class="fa fa-check"></i><b>10.3</b> R-Python Communication</a></li>
<li class="chapter" data-level="10.4" data-path="ckiptagger.html"><a href="ckiptagger.html#word-segmentation-in-r"><i class="fa fa-check"></i><b>10.4</b> Word Segmentation in R</a></li>
<li class="chapter" data-level="10.5" data-path="ckiptagger.html"><a href="ckiptagger.html#r-environment-setting"><i class="fa fa-check"></i><b>10.5</b> R Environment Setting</a></li>
<li class="chapter" data-level="10.6" data-path="ckiptagger.html"><a href="ckiptagger.html#loading-python-modules"><i class="fa fa-check"></i><b>10.6</b> Loading Python Modules</a></li>
<li class="chapter" data-level="10.7" data-path="ckiptagger.html"><a href="ckiptagger.html#segmenting-texts"><i class="fa fa-check"></i><b>10.7</b> Segmenting Texts</a></li>
<li class="chapter" data-level="10.8" data-path="ckiptagger.html"><a href="ckiptagger.html#define-own-dictionary"><i class="fa fa-check"></i><b>10.8</b> Define Own Dictionary</a></li>
<li class="chapter" data-level="10.9" data-path="ckiptagger.html"><a href="ckiptagger.html#beyond-word-boundaries"><i class="fa fa-check"></i><b>10.9</b> Beyond Word Boundaries</a></li>
<li class="chapter" data-level="10.10" data-path="ckiptagger.html"><a href="ckiptagger.html#tidy-up-the-results"><i class="fa fa-check"></i><b>10.10</b> Tidy Up the Results</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="structured-corpus.html"><a href="structured-corpus.html"><i class="fa fa-check"></i><b>11</b> Structured Corpus</a><ul>
<li class="chapter" data-level="11.1" data-path="structured-corpus.html"><a href="structured-corpus.html#nccu-spoken-mandarin"><i class="fa fa-check"></i><b>11.1</b> NCCU Spoken Mandarin</a></li>
<li class="chapter" data-level="11.2" data-path="structured-corpus.html"><a href="structured-corpus.html#childes-format"><i class="fa fa-check"></i><b>11.2</b> CHILDES Format</a></li>
<li class="chapter" data-level="11.3" data-path="structured-corpus.html"><a href="structured-corpus.html#loading-the-corpus"><i class="fa fa-check"></i><b>11.3</b> Loading the Corpus</a></li>
<li class="chapter" data-level="11.4" data-path="structured-corpus.html"><a href="structured-corpus.html#from-text-based-to-turn-based-df"><i class="fa fa-check"></i><b>11.4</b> From Text-based to Turn-based DF</a></li>
<li class="chapter" data-level="11.5" data-path="structured-corpus.html"><a href="structured-corpus.html#metadata-vs.utterances"><i class="fa fa-check"></i><b>11.5</b> Metadata vs. Utterances</a></li>
<li class="chapter" data-level="11.6" data-path="structured-corpus.html"><a href="structured-corpus.html#word-based-df-and-frequency-list"><i class="fa fa-check"></i><b>11.6</b> Word-based DF and Frequency List</a></li>
<li class="chapter" data-level="11.7" data-path="structured-corpus.html"><a href="structured-corpus.html#concordances"><i class="fa fa-check"></i><b>11.7</b> Concordances</a></li>
<li class="chapter" data-level="11.8" data-path="structured-corpus.html"><a href="structured-corpus.html#collocations-bigrams"><i class="fa fa-check"></i><b>11.8</b> Collocations (Bigrams)</a></li>
<li class="chapter" data-level="11.9" data-path="structured-corpus.html"><a href="structured-corpus.html#n-grams-lexical-bundles"><i class="fa fa-check"></i><b>11.9</b> N-grams (Lexical Bundles)</a></li>
<li class="chapter" data-level="11.10" data-path="structured-corpus.html"><a href="structured-corpus.html#connecting-spid-to-metadata"><i class="fa fa-check"></i><b>11.10</b> Connecting SPID to Metadata</a></li>
<li class="chapter" data-level="11.11" data-path="structured-corpus.html"><a href="structured-corpus.html#corpus-headers"><i class="fa fa-check"></i><b>11.11</b> Corpus Headers</a></li>
<li class="chapter" data-level="11.12" data-path="structured-corpus.html"><a href="structured-corpus.html#sociolinguistic-analyses"><i class="fa fa-check"></i><b>11.12</b> Sociolinguistic Analyses</a><ul>
<li class="chapter" data-level="11.12.1" data-path="structured-corpus.html"><a href="structured-corpus.html#check-bigrams-distribution-by-age-groups"><i class="fa fa-check"></i><b>11.12.1</b> Check Bigrams Distribution By Age Groups</a></li>
<li class="chapter" data-level="11.12.2" data-path="structured-corpus.html"><a href="structured-corpus.html#numbers-of-bigrams-above-cut-off-by-age"><i class="fa fa-check"></i><b>11.12.2</b> Numbers of Bigrams above Cut-off by Age</a></li>
<li class="chapter" data-level="11.12.3" data-path="structured-corpus.html"><a href="structured-corpus.html#bigram-word-clouds-by-age"><i class="fa fa-check"></i><b>11.12.3</b> Bigram Word clouds by Age</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="xml.html"><a href="xml.html"><i class="fa fa-check"></i><b>12</b> XML</a><ul>
<li class="chapter" data-level="12.1" data-path="xml.html"><a href="xml.html#bnc-spoken-2014"><i class="fa fa-check"></i><b>12.1</b> BNC Spoken 2014</a></li>
<li class="chapter" data-level="12.2" data-path="xml.html"><a href="xml.html#process-the-whole-directory-of-bnc2014-sample"><i class="fa fa-check"></i><b>12.2</b> Process the Whole Directory of BNC2014 Sample</a><ul>
<li class="chapter" data-level="12.2.1" data-path="xml.html"><a href="xml.html#define-function"><i class="fa fa-check"></i><b>12.2.1</b> Define Function</a></li>
<li class="chapter" data-level="12.2.2" data-path="xml.html"><a href="xml.html#process-the-all-files-in-the-directory"><i class="fa fa-check"></i><b>12.2.2</b> Process the all files in the Directory</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="xml.html"><a href="xml.html#metadata"><i class="fa fa-check"></i><b>12.3</b> Metadata</a><ul>
<li class="chapter" data-level="12.3.1" data-path="xml.html"><a href="xml.html#text-metadata"><i class="fa fa-check"></i><b>12.3.1</b> Text Metadata</a></li>
<li class="chapter" data-level="12.3.2" data-path="xml.html"><a href="xml.html#speaker-metadata"><i class="fa fa-check"></i><b>12.3.2</b> Speaker Metadata</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="xml.html"><a href="xml.html#bnc2014-for-socialinguistic-variation"><i class="fa fa-check"></i><b>12.4</b> BNC2014 for Socialinguistic Variation</a></li>
<li class="chapter" data-level="12.5" data-path="xml.html"><a href="xml.html#word-frequency-vs.gender"><i class="fa fa-check"></i><b>12.5</b> Word Frequency vs. Gender</a><ul>
<li class="chapter" data-level="12.5.1" data-path="xml.html"><a href="xml.html#preprocessing"><i class="fa fa-check"></i><b>12.5.1</b> Preprocessing</a></li>
<li class="chapter" data-level="12.5.2" data-path="xml.html"><a href="xml.html#target-structures"><i class="fa fa-check"></i><b>12.5.2</b> Target Structures</a></li>
<li class="chapter" data-level="12.5.3" data-path="xml.html"><a href="xml.html#analysis"><i class="fa fa-check"></i><b>12.5.3</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="xml.html"><a href="xml.html#degree-adv-adj"><i class="fa fa-check"></i><b>12.6</b> Degree ADV + ADJ</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="vector-space-representation.html"><a href="vector-space-representation.html"><i class="fa fa-check"></i><b>13</b> Vector Space Representation</a><ul>
<li class="chapter" data-level="13.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#data-processing-flowchart"><i class="fa fa-check"></i><b>13.1</b> Data Processing Flowchart</a></li>
<li class="chapter" data-level="13.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-feature-matrix-dfm"><i class="fa fa-check"></i><b>13.2</b> Document-Feature Matrix (<code>dfm</code>)</a></li>
<li class="chapter" data-level="13.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#corpus-1"><i class="fa fa-check"></i><b>13.3</b> Corpus</a></li>
<li class="chapter" data-level="13.4" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-feature-matrix-dfm-1"><i class="fa fa-check"></i><b>13.4</b> Document-Feature Matrix (<code>dfm</code>)</a></li>
<li class="chapter" data-level="13.5" data-path="vector-space-representation.html"><a href="vector-space-representation.html#distance-similarity-metrics"><i class="fa fa-check"></i><b>13.5</b> Distributional Hypothesis and Distance/Similarity Metrics</a><ul>
<li class="chapter" data-level="13.5.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#distance-based-metrics"><i class="fa fa-check"></i><b>13.5.1</b> Distance-based Metrics</a></li>
<li class="chapter" data-level="13.5.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#similarity-based-metrics"><i class="fa fa-check"></i><b>13.5.2</b> Similarity-based Metrics</a></li>
<li class="chapter" data-level="13.5.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#interim-summary"><i class="fa fa-check"></i><b>13.5.3</b> Interim Summary</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="vector-space-representation.html"><a href="vector-space-representation.html#multidimensiona-space"><i class="fa fa-check"></i><b>13.6</b> Multidimensiona Space</a></li>
<li class="chapter" data-level="13.7" data-path="vector-space-representation.html"><a href="vector-space-representation.html#vector-semantics-considerations"><i class="fa fa-check"></i><b>13.7</b> Vector Semantics Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="vector-space-representation.html"><a href="vector-space-representation.html#feature-selection"><i class="fa fa-check"></i><b>13.8</b> Feature Selection</a><ul>
<li class="chapter" data-level="13.8.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#granularity"><i class="fa fa-check"></i><b>13.8.1</b> Granularity</a></li>
<li class="chapter" data-level="13.8.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#informativeness"><i class="fa fa-check"></i><b>13.8.2</b> Informativeness</a></li>
<li class="chapter" data-level="13.8.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#distributional-properties"><i class="fa fa-check"></i><b>13.8.3</b> Distributional Properties</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="vector-space-representation.html"><a href="vector-space-representation.html#exploratory-analysis-of-dfm"><i class="fa fa-check"></i><b>13.9</b> Exploratory Analysis of <code>dfm</code></a></li>
<li class="chapter" data-level="13.10" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-similarity"><i class="fa fa-check"></i><b>13.10</b> Document Similarity</a></li>
<li class="chapter" data-level="13.11" data-path="vector-space-representation.html"><a href="vector-space-representation.html#feature-coocurrence-matrix-fcm"><i class="fa fa-check"></i><b>13.11</b> Feature-Coocurrence Matrix (<code>fcm</code>)</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html"><i class="fa fa-check"></i><b>14</b> Vector Space Representation II</a><ul>
<li class="chapter" data-level="14.1" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#chinese-text-analytics-flowchart"><i class="fa fa-check"></i><b>14.1</b> Chinese Text Analytics Flowchart</a></li>
<li class="chapter" data-level="14.2" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#a-quick-view"><i class="fa fa-check"></i><b>14.2</b> A Quick View</a></li>
<li class="chapter" data-level="14.3" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#loading-the-corpus-1"><i class="fa fa-check"></i><b>14.3</b> Loading the Corpus</a></li>
<li class="chapter" data-level="14.4" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#semgentation"><i class="fa fa-check"></i><b>14.4</b> Semgentation</a></li>
<li class="chapter" data-level="14.5" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#corpus-metadata"><i class="fa fa-check"></i><b>14.5</b> Corpus Metadata</a></li>
<li class="chapter" data-level="14.6" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#document-feature-matrix"><i class="fa fa-check"></i><b>14.6</b> Document-Feature Matrix</a></li>
<li class="chapter" data-level="14.7" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#top-features-and-wordcloud"><i class="fa fa-check"></i><b>14.7</b> Top Features and Wordcloud</a></li>
<li class="chapter" data-level="14.8" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#document-similarity-1"><i class="fa fa-check"></i><b>14.8</b> Document Similarity</a></li>
<li class="chapter" data-level="14.9" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#feature-similarity"><i class="fa fa-check"></i><b>14.9</b> Feature Similarity</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html"><i class="fa fa-check"></i><b>15</b> Vector Space Representation III</a><ul>
<li class="chapter" data-level="15.1" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#library"><i class="fa fa-check"></i><b>15.1</b> Library</a></li>
<li class="chapter" data-level="15.2" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#text-collection"><i class="fa fa-check"></i><b>15.2</b> Text Collection</a></li>
<li class="chapter" data-level="15.3" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#tokenization-and-vocabulary"><i class="fa fa-check"></i><b>15.3</b> Tokenization and Vocabulary</a></li>
<li class="chapter" data-level="15.4" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#pruning"><i class="fa fa-check"></i><b>15.4</b> Pruning</a></li>
<li class="chapter" data-level="15.5" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#term-cooccurrence-matrix"><i class="fa fa-check"></i><b>15.5</b> Term-Cooccurrence Matrix</a></li>
<li class="chapter" data-level="15.6" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#fitting-model"><i class="fa fa-check"></i><b>15.6</b> Fitting Model</a></li>
<li class="chapter" data-level="15.7" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#averaging-word-vectors"><i class="fa fa-check"></i><b>15.7</b> Averaging Word Vectors</a></li>
<li class="chapter" data-level="15.8" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#semantic-space"><i class="fa fa-check"></i><b>15.8</b> Semantic Space</a></li>
<li class="chapter" data-level="15.9" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#visualizing-multi-dimensional-space"><i class="fa fa-check"></i><b>15.9</b> Visualizing Multi-dimensional Space</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>16</b> References</a></li>
<li class="divider"></li>
<li><a href="https://web.ntnu.edu.tw/~alvinchen" target ="blank">Alvin Chen </a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Corpus Linguistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="vector-space-representation" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Vector Space Representation</h1>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb333-2" data-line-number="2"><span class="kw">library</span>(quanteda)</a></code></pre></div>
<p>In this chapter, I would like to talk about the idea of <strong>distributional semantics</strong>, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized and quantified using the distributional data of the linguistic units in the corpus.</p>
<p>Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts.</p>
<div id="data-processing-flowchart" class="section level2">
<h2><span class="header-section-number">13.1</span> Data Processing Flowchart</h2>
<p>In Chapter <a href="parts-of-speech-tagging.html#parts-of-speech-tagging">5</a>, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of semantic.</p>
<p>Most importantly, a new object class is introduced in Figure <a href="vector-space-representation.html#fig:engnlp2">13.1</a>, i.e., the <code>dfm</code> object in <code>quanteda</code>. It stands for <strong>Document-Feature-Matrix</strong>. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterizing the documents. The cells in the matrix often refer to the <em>co-occurrence statistics</em> between each document and the feature.</p>
<p>Different ways of operationalizing the <strong>features</strong> and the <strong>cell values</strong> may lead to different types of <code>dfm</code>. In this chapter, I would like to show you how we create a <code>dfm</code> of a corpus and what are the common ways to define <strong>features</strong> and <strong>cell valus</strong> for the analysis of semantics via vector space representation.</p>
<hr />
<div class="figure"><span id="fig:engnlp2"></span>
<div id="htmlwidget-bd4775bea274da4f340f" style="width:100%;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-bd4775bea274da4f340f">{"x":{"diagram":"digraph {\n  \ngraph[layout = dot]\n\nnode[style=filled, color = grey90, penwidth = 0.3, fontcolor = grey30]\nedge[color= grey40, alpha = 0.2, penwidth= 0.4, arrowsize=0.8, arrowhead = normal]\n\na [label = \"Raw Texts\", shape = folder, fillcolor= \"#B0E2FC\"]\naa1[label = \"spacyr::spacy_parse()\", shape=document, fillcolor= \"#96F597\"]\nbb [label = \"word-based DATA.FRAME\", shape = folder, fillcolor= \"#B0E2FC\"]\nbb1[label = \"dplyr\", shape=document, fillcolor= \"#96F597\"]\n\n\ncc [label = \"construction-based DATA.FRAME\", shape = folder, fillcolor= \"#B0E2FC\"]\na1 [label = \"quanteda::corpus()\", shape=document, fillcolor= \"#96F597\"]\nb [label = \"CORPUS object\", shape = folder, fillcolor= \"#B0E2FC\"]\nb1 [label = \"tidytext::tidy()\", shape=document, fillcolor= \"#96F597\"]\n\n\nb2 [label = \"quanteda::kwic()\", shape=document, fillcolor= \"#96F597\"]\nb3 [label = \"quanteda::summary()\", shape=document, fillcolor= \"#96F597\"]\nb21[label = \"Data Frame (KWIC)\", shape = folder, fillcolor= \"#B0E2FC\"]\nb31[label = \"Data Frame (Summary)\", shape = folder, fillcolor= \"#B0E2FC\"]\n\nb4 [label = \"quanteda::dfm()\", shape=document, fillcolor= \"#96F597\"]\n\ne [label = \"DFM object\", shape = folder, fillcolor= \"#B0E2FC\"]\ne1 [label = \"quanteda::dfm_select()/dfm_trim()\", shape=document, fillcolor= \"#96F597\"]\n\nf [label = \"Vector-based Analysis\", shape = folder, fillcolor= \"#B0E2FC\"]\n\n\n\n\n\nc [label = \"TIBBLE object (text-based)\", shape = folder, fillcolor= \"#B0E2FC\"]\nc1 [label = \"tidytext::unnest_tokens()\", shape=document, fillcolor= \"#96F597\"]\nd [label = \"TIBBLE object (token-based)\", shape = folder, fillcolor= \"#B0E2FC\"]\n\n\na -> a1 -> b -> b1 -> c -> c1 -> d\na -> aa1 -> bb -> bb1\n\nbb1-> d\nbb1 -> c\nbb1 -> cc\nb -> b2 -> b21\nb -> b3 -> b31\n\nb -> b4 -> e -> e1 -> f\n\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 13.1: English Text Analytics Flowchart (v2)
</p>
</div>
<hr />
</div>
<div id="document-feature-matrix-dfm" class="section level2">
<h2><span class="header-section-number">13.2</span> Document-Feature Matrix (<code>dfm</code>)</h2>
<p>To create a <code>dfm</code>, i.e., <strong>D</strong>cument-<strong>F</strong>eature-<strong>M</strong>atrix, <code>quanteda</code> provides two alterantives:</p>
<ul>
<li>create <code>dfm</code> based on an <code>corpus</code> object</li>
<li>create <code>dfm</code> based on an <code>tokens</code> object</li>
</ul>
<p>For English data, <code>quanteda</code> can take care of the word tokenization fairly well so you can create <code>dfm</code> directly from <code>corpus</code> (See Figure <a href="vector-space-representation.html#fig:engnlp2">13.1</a>)</p>
<div class="danger">
<p>
In Chapter <span class="citation">(<span class="citeproc-not-found" data-reference-id="ref"><strong>???</strong></span>)</span>(chinese-text-processing), we stress that the default tokenization method in <code>quanteda</code> with Chinese data may be limited in several ways. In order to create a <code>dfm</code> that takes into account the appropriateness of the Chinese word segmentation, I would highly recommend you to first create a<code>tokens</code> object using the self-defined word segmentation methods, and then feed it to <code>dfm()</code> to create the <code>dfm</code> for your corpus. In this way, the <code>dfm</code> will use the segmented results defined by your word segmenter.
</p>
<p>
In other words, with Chinese data, probably it is not really necessary to have a <code>corpus</code> object; rather, a <code>token</code> object of the corpus might be more useful/practical. (In <code>quanteda</code>, most of the functions for <code>corpus</code> can be applied to <code>tokens</code> as well, e.g., <code>kwic()</code>, <code>dfm()</code>)
</p>
</div>
</div>
<div id="corpus-1" class="section level2">
<h2><span class="header-section-number">13.3</span> Corpus</h2>
<p>In this chapter, I will use the same English dataset we discussed in Chapter <a href="parts-of-speech-tagging.html#parts-of-speech-tagging">5</a>, the <code>data_corpus_inaugural</code> preloaded in the package <code>quanteda</code>.</p>
<p>For English data, the process is simple: we first load the corpus and create a <code>dfm</code> object of the corpus using <code>dfm()</code>.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1">corp_us &lt;-<span class="st"> </span>data_corpus_inaugural</a>
<a class="sourceLine" id="cb334-2" data-line-number="2">corp_us_dfm &lt;-<span class="st"> </span>corp_us <span class="op">%&gt;%</span><span class="st"> </span>dfm</a></code></pre></div>
<div class="danger">
<p>
Please note that the default <code>data_corpus_inaugural</code> preloaded with <code>quanteda</code> is a <code>corpus</code> object already.
</p>
</div>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" data-line-number="1"><span class="kw">class</span>(data_corpus_inaugural)</a></code></pre></div>
<pre><code>## [1] &quot;corpus&quot;    &quot;character&quot;</code></pre>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1"><span class="kw">class</span>(corp_us)</a></code></pre></div>
<pre><code>## [1] &quot;corpus&quot;    &quot;character&quot;</code></pre>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="kw">class</span>(corp_us_dfm)</a></code></pre></div>
<pre><code>## [1] &quot;dfm&quot;
## attr(,&quot;package&quot;)
## [1] &quot;quanteda&quot;</code></pre>
</div>
<div id="document-feature-matrix-dfm-1" class="section level2">
<h2><span class="header-section-number">13.4</span> Document-Feature Matrix (<code>dfm</code>)</h2>
<p>What is <code>dfm</code> anyway? A document-feature-matrix is no different from a spead-sheet like table. In a <code>dfm</code>, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the <em>vocabulary</em> of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus.</p>
<p>What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the following example, we can see that in the first document, i.e., <em>1789-Washington</em>, there are <em>2</em> occurrences of <em>representatives</em>, <em>48</em> occurrences of <em>and</em></p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1">corp_us_dfm[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars.
##                  features
## docs              fellow-citizens  of the senate and house representatives :
##   1789-Washington               1  71 116      1  48     2               2 1
##   1793-Washington               0  11  13      0   2     0               0 1
##   1797-Adams                    3 140 163      1 130     0               2 0
##   1801-Jefferson                2 104 130      0  81     0               0 1
##   1805-Jefferson                0 101 143      0  93     0               0 0
##   1809-Madison                  1  69 104      0  43     0               0 0
##                  features
## docs              among vicissitudes
##   1789-Washington     1            1
##   1793-Washington     0            0
##   1797-Adams          4            0
##   1801-Jefferson      1            0
##   1805-Jefferson      7            0
##   1809-Madison        0            0
## [ reached max_ndoc ... 4 more documents ]</code></pre>
<p>A <code>dfm</code> with words as the features is the simplest way to characterize the texts in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each text as <strong>bags of words</strong>. In other words, how the words are arranged relative to each other is ignored (i.e., the morphosyntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word <code>dfm</code> should be a naive characterization of the texts.</p>
<p>In many computational tasks, however, it turns out that this simple <strong>bag-of-words</strong> model is very effective in modeling the semantics of the documents.</p>
</div>
<div id="distance-similarity-metrics" class="section level2">
<h2><span class="header-section-number">13.5</span> Distributional Hypothesis and Distance/Similarity Metrics</h2>
<p>The effectiveness of the <code>dfm</code> in semantic analysis lies in one important hypoethesis shared in the corpus linguistic community: distributional hypothesis.</p>
<p>Distributional properties like co-occurrences are very important information in corpus linguistics. Most of the studies in corpus linguistics adopt an implicit distributional hypothesis, which can be illustrated by a few famous quotes:</p>
<blockquote>
<p>You shall know a word by the comany it keeps. <span> (Firth, 1957, p.11) </span></p>
</blockquote>
<blockquote>
<p>[D]ifference of meaning correlates with difference of distribution. <span> (Harris, 1970, p.785) </span></p>
</blockquote>
<blockquote>
<p>The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves <span>(De Deyne et al. 2016)</span></p>
</blockquote>
<p>So in our current context, the idea is that if two documents have similar sets of linguistic units popping up in them, they are more likely to be similar in their semantics as well.</p>
<p>The advantage of creating a document-feature-matrix is that now each document is not only a series of <strong>character</strong> strings, but also a list of <strong>numeric</strong> values (i.e., a row of co-occurring frequencies), which can be compared mathematically with other documents (i.e., other rows).</p>
<p>Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically:</p>
<div class="figure"><span id="fig:unnamed-chunk-390"></span>
<img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-390-1.png" alt="Vector Representation" width="672" />
<p class="caption">
Figure 13.2: Vector Representation
</p>
</div>
<p>In Math, there are in general two types of metrics to measure the relationship between vectors: <strong>distance</strong>-based vs. <strong>similarity</strong>-based metrics.</p>
<div id="distance-based-metrics" class="section level3">
<h3><span class="header-section-number">13.5.1</span> Distance-based Metrics</h3>
<p>Many distance measures of vectors are based on the following formula and differ in individual parameter settings.</p>
<p><span class="math display">\[\big( \sum_{i = 1}^{n}{|x_i - y_i|^y}\big)^{\frac{1}{y}}\]</span></p>
<p>The <em>n</em> in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.)</p>
<p>When <em>y</em> is set to 2, it computes the famous <strong>Euclidean distance</strong> of two vectors, i.e., the direct spatial distance between two points on the <em>n</em>-dimensional space.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">9</span>)</a>
<a class="sourceLine" id="cb343-2" data-line-number="2">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb343-3" data-line-number="3">z &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb343-4" data-line-number="4"><span class="kw">sum</span>(<span class="kw">abs</span>(x<span class="op">-</span>y)<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="co"># XY distance</span></a></code></pre></div>
<pre><code>## [1] 6</code></pre>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1"><span class="kw">sum</span>(<span class="kw">abs</span>(y<span class="op">-</span>z)<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="co"># YZ distnace</span></a></code></pre></div>
<pre><code>## [1] 4.472136</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1"><span class="kw">sum</span>(<span class="kw">abs</span>(x<span class="op">-</span>z)<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="co"># YZ distnace</span></a></code></pre></div>
<pre><code>## [1] 8.944272</code></pre>
<p>The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure <a href="vector-space-representation.html#fig:distance">13.3</a>)</p>
<div class="figure"><span id="fig:distance"></span>
<img src="CorpusLinguistics_bookdown_files/figure-html/distance-1.png" alt="Distance-based Metric: Euclidean Distance" width="100%" />
<p class="caption">
Figure 13.3: Distance-based Metric: Euclidean Distance
</p>
</div>
</div>
<div id="similarity-based-metrics" class="section level3">
<h3><span class="header-section-number">13.5.2</span> Similarity-based Metrics</h3>
<p>In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is <strong>Cosine Similarity</strong>, which can be computed as follows:</p>
<p><span class="math display">\[cos(\vec{x},\vec{y}) = \frac{\sum_{i=1}^{n}{x_i\times y_i}}{\sqrt{\sum_{i=1}^{n}x_i^2}\times \sqrt{\sum_{i=1}^{n}y_i^2}}\]</span></p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1"><span class="kw">sum</span>(x<span class="op">*</span>y)<span class="op">/</span>(<span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(y<span class="op">^</span><span class="dv">2</span>))) <span class="co"># xy</span></a></code></pre></div>
<pre><code>## [1] 0.9778024</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" data-line-number="1"><span class="kw">sum</span>(y<span class="op">*</span>z)<span class="op">/</span>(<span class="kw">sqrt</span>(<span class="kw">sum</span>(y<span class="op">^</span><span class="dv">2</span>))<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(z<span class="op">^</span><span class="dv">2</span>))) <span class="co"># yz</span></a></code></pre></div>
<pre><code>## [1] 0.4961389</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1"><span class="kw">sum</span>(x<span class="op">*</span>z)<span class="op">/</span>(<span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(z<span class="op">^</span><span class="dv">2</span>))) <span class="co"># yz</span></a></code></pre></div>
<pre><code>## [1] 0.3032037</code></pre>
<p>The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the great their cosine similarity, the smaller the arcs, the closer they are.</p>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-393-1.png" width="100%" /></p>
</div>
<div id="interim-summary" class="section level3">
<h3><span class="header-section-number">13.5.3</span> Interim Summary</h3>
<p>The <strong>Euclidean Distance</strong> metric is a distance-based metric: the larger the value, the more distant the two vectors.</p>
<p>The <strong>Cosine Similarity</strong> metric is a similatiry-based metric: the larger the value, the closer the two vectors.</p>
<p>Based on our computations of the metrics for the three vectors, now in terms of the <strong>Euclidean Distance</strong>, <em>y</em> and <em>z</em> are closer; in terms of <strong>Cosine Similarity</strong>, <em>x</em> and <em>y</em> are closer.</p>
<p>Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the linguistic units:</p>
<ul>
<li>The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors)</li>
<li>The relative increase/decrease of the values on ecah dimension (i.e., the curvatures of vectors)</li>
</ul>
<div class="info">
<p>
There are many other distance-based or similarity-based metrics available. For more detail, please see <span class="citation"><span class="citation">Manning and Schütze (<a href="#ref-manning1999">1999</a>)</span></span> Ch15.2.2. and <span class="citation"><span class="citation">Jurafsky and Martin (<a href="#ref-jurafsky2020">2020</a>)</span></span> <a href="https://web.stanford.edu/~jurafsky/slp3/">Ch6: Vector Semantics and Embeddings</a>.
</p>
</div>
</div>
</div>
<div id="multidimensiona-space" class="section level2">
<h2><span class="header-section-number">13.6</span> Multidimensiona Space</h2>
<p>Back to our example of <code>dfm</code>, it is essentially the same vector representation, but in a multidimensional verson (cf. Figure <a href="vector-space-representation.html#fig:dfmdemo">13.4</a>). The document in each row is represented as a vector of <em>N</em> dimensional space. The size of <em>N</em> depends on the number of linguistic units that are included in the <code>analysis</code>dfm`.</p>
<div class="figure"><span id="fig:dfmdemo"></span>
<img src="images/dfm.png" alt="Example of Document-Feature Matrix" width="90%" />
<p class="caption">
Figure 13.4: Example of Document-Feature Matrix
</p>
</div>
</div>
<div id="vector-semantics-considerations" class="section level2">
<h2><span class="header-section-number">13.7</span> Vector Semantics Considerations</h2>
<p>When representing semantics of linguistic units in vector space, two factors would turn out to be very crucial:</p>
<ul>
<li>which features should be included in the analysis of multidimensional representation?</li>
<li>which quantitative metrics should be used to represent the relationship between the linguistic unit and the features?</li>
</ul>
<p>In our current <code>dfm</code> based on bags of words, our concerns would be:</p>
<ul>
<li>which words should be included in the analysis of multidimensional representation?</li>
<li>which quantitative metrics should be used to represent the relationship between the texts and the words?</li>
</ul>
</div>
<div id="feature-selection" class="section level2">
<h2><span class="header-section-number">13.8</span> Feature Selection</h2>
<p>A <code>dfm</code> may not be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered with respect to the features of the <code>dfm</code>:</p>
<ul>
<li>The <strong>granularity</strong> of the features</li>
<li>The <strong>informativeness</strong> of the features</li>
<li>The <strong>distributional properties</strong> of the features</li>
</ul>
<div id="granularity" class="section level3">
<h3><span class="header-section-number">13.8.1</span> Granularity</h3>
<p>In our previous example, we include only words, i.e., unigrams, as our features in the <code>dfm</code>. We can in fact include linguistic units at multiple granularities:</p>
<ul>
<li>words</li>
<li>(skipped) n-grams</li>
<li>lemmas/stems</li>
</ul>
<p>For example, if you want to include bigrams, not unigrams, as features in the <code>dfm</code>, you can do the following:</p>
<pre><code>- from `corpus` -&gt; `tokens`
- from `tokens` -&gt; `ngram-based tokens`
- from `ngram-based tokens` -&gt; `dfm`</code></pre>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb356-1" data-line-number="1">corp_us_dfm_ngram &lt;-<span class="st"> </span>corp_us <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb356-2" data-line-number="2"><span class="st">  </span><span class="kw">tokens</span>(<span class="dt">what =</span> <span class="st">&quot;word&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb356-3" data-line-number="3"><span class="st">  </span><span class="kw">tokens_ngrams</span>(<span class="dt">n=</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>dfm</a>
<a class="sourceLine" id="cb356-4" data-line-number="4">corp_us_dfm_ngram[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>## Document-feature matrix of: 10 documents, 10 features (71.0% sparse) and 4 docvars.
##                  features
## docs              fellow-citizens_of of_the the_senate senate_and and_of
##   1789-Washington                  1     20          1          1      2
##   1793-Washington                  0      4          0          0      1
##   1797-Adams                       0     29          0          0      2
##   1801-Jefferson                   0     28          0          0      3
##   1805-Jefferson                   0     17          0          0      1
##   1809-Madison                     0     20          0          0      2
##                  features
## docs              the_house house_of of_representatives representatives_:
##   1789-Washington         2        2                  2                 1
##   1793-Washington         0        0                  0                 0
##   1797-Adams              0        0                  0                 0
##   1801-Jefferson          0        0                  0                 0
##   1805-Jefferson          0        0                  0                 0
##   1809-Madison            0        0                  0                 0
##                  features
## docs              :_among
##   1789-Washington       1
##   1793-Washington       0
##   1797-Adams            0
##   1801-Jefferson        0
##   1805-Jefferson        0
##   1809-Madison          0
## [ reached max_ndoc ... 4 more documents ]</code></pre>
<p>Or for English data, if you want to ignore the stem variations between words (i.e., <em>house</em> and <em>houses</em> may not be differ so much), you can do it this way:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb358-1" data-line-number="1">corp_us_dfm_stem &lt;-<span class="st"> </span>corp_us <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">dfm</span>(<span class="dt">stem =</span> T)</a>
<a class="sourceLine" id="cb358-2" data-line-number="2">corp_us_dfm_stem[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>## Document-feature matrix of: 10 documents, 10 features (38.0% sparse) and 4 docvars.
##                  features
## docs              fellow-citizen  of the senat and hous repres : among
##   1789-Washington              1  71 116     1  48    2      2 1     1
##   1793-Washington              0  11  13     0   2    0      0 1     0
##   1797-Adams                   3 140 163     1 130    3      3 0     4
##   1801-Jefferson               2 104 130     0  81    0      1 1     1
##   1805-Jefferson               0 101 143     0  93    0      0 0     7
##   1809-Madison                 1  69 104     0  43    0      1 0     0
##                  features
## docs              vicissitud
##   1789-Washington          1
##   1793-Washington          0
##   1797-Adams               0
##   1801-Jefferson           0
##   1805-Jefferson           0
##   1809-Madison             1
## [ reached max_ndoc ... 4 more documents ]</code></pre>
<p>You need to decide which type of linguistic units is more relevant to your research question. In many text mining applications, people often make use of both unigrams and <em>n</em>-grams. However, these are only heuristics, not rules.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-398" class="exercise"><strong>Exercise 13.1  </strong></span>Based on the dataset <code>corp_us</code>, can you create a <code>dfm</code>, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below)
</div>

<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["document"],"name":[1],"type":["chr"],"align":["left"]},{"label":["fellow-citizen"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["of"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["the"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["senat"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["and"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["hous"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["repres"],"name":[8],"type":["dbl"],"align":["right"]},{"label":[":"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["among"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["vicissitud"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["incid"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["to"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["life"],"name":[14],"type":["dbl"],"align":["right"]},{"label":["no"],"name":[15],"type":["dbl"],"align":["right"]},{"label":["event"],"name":[16],"type":["dbl"],"align":["right"]},{"label":["could"],"name":[17],"type":["dbl"],"align":["right"]},{"label":["have"],"name":[18],"type":["dbl"],"align":["right"]},{"label":["fill"],"name":[19],"type":["dbl"],"align":["right"]},{"label":["me"],"name":[20],"type":["dbl"],"align":["right"]}],"data":[{"1":"1789-Washington","2":"1","3":"71","4":"116","5":"1","6":"48","7":"2","8":"2","9":"1","10":"1","11":"1","12":"1","13":"48","14":"1","15":"8","16":"2","17":"3","18":"13","19":"1","20":"8","_rn_":"1"},{"1":"1793-Washington","2":"0","3":"11","4":"13","5":"0","6":"2","7":"0","8":"0","9":"1","10":"0","11":"0","12":"0","13":"5","14":"0","15":"0","16":"0","17":"0","18":"1","19":"0","20":"1","_rn_":"2"},{"1":"1797-Adams","2":"3","3":"140","4":"163","5":"1","6":"130","7":"3","8":"3","9":"0","10":"4","11":"0","12":"0","13":"72","14":"2","15":"6","16":"0","17":"1","18":"7","19":"0","20":"5","_rn_":"3"},{"1":"1801-Jefferson","2":"2","3":"104","4":"130","5":"0","6":"81","7":"0","8":"1","9":"1","10":"1","11":"0","12":"0","13":"61","14":"1","15":"1","16":"0","17":"0","18":"11","19":"0","20":"4","_rn_":"4"},{"1":"1805-Jefferson","2":"0","3":"101","4":"143","5":"0","6":"93","7":"0","8":"0","9":"0","10":"7","11":"0","12":"0","13":"83","14":"2","15":"7","16":"1","17":"2","18":"24","19":"0","20":"8","_rn_":"5"},{"1":"1809-Madison","2":"1","3":"69","4":"104","5":"0","6":"43","7":"0","8":"1","9":"0","10":"0","11":"1","12":"0","13":"61","14":"1","15":"2","16":"0","17":"1","18":"9","19":"1","20":"8","_rn_":"6"},{"1":"1813-Madison","2":"1","3":"65","4":"100","5":"0","6":"44","7":"0","8":"0","9":"0","10":"1","11":"0","12":"0","13":"42","14":"1","15":"4","16":"0","17":"2","18":"15","19":"0","20":"2","_rn_":"7"},{"1":"1817-Monroe","2":"5","3":"164","4":"275","5":"0","6":"122","7":"0","8":"1","9":"0","10":"3","11":"0","12":"2","13":"126","14":"1","15":"5","16":"4","17":"1","18":"23","19":"0","20":"5","_rn_":"8"},{"1":"1821-Monroe","2":"1","3":"197","4":"360","5":"0","6":"141","7":"0","8":"2","9":"0","10":"1","11":"0","12":"0","13":"146","14":"2","15":"11","16":"4","17":"6","18":"51","19":"0","20":"6","_rn_":"9"},{"1":"1825-Adams","2":"0","3":"245","4":"304","5":"0","6":"116","7":"0","8":"2","9":"0","10":"3","11":"1","12":"0","13":"101","14":"1","15":"1","16":"1","17":"0","18":"36","19":"0","20":"6","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />
</div>
<div id="informativeness" class="section level3">
<h3><span class="header-section-number">13.8.2</span> Informativeness</h3>
<p>There are words that are not so informative in telling us the similarity and difference between the documents because they almost appear in every document of the corpus, but carray little (referential) semantic contents. These words are usually the function words, such as <em>and</em>, <em>the</em>, <em>of</em>. The common words in almost all documents are often referred to as <strong>stopwords</strong>. Therefore, it is not uncommon that analysts sometimes create a list of <strong>stopwords</strong> to be removed from the <code>dfm</code>. The library <code>quanteda</code> has defined a default English stopword list, i.e., <code>stopwords(&quot;en&quot;)</code>.</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" data-line-number="1"><span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>)</a></code></pre></div>
<pre><code>##   [1] &quot;i&quot;          &quot;me&quot;         &quot;my&quot;         &quot;myself&quot;     &quot;we&quot;        
##   [6] &quot;our&quot;        &quot;ours&quot;       &quot;ourselves&quot;  &quot;you&quot;        &quot;your&quot;      
##  [11] &quot;yours&quot;      &quot;yourself&quot;   &quot;yourselves&quot; &quot;he&quot;         &quot;him&quot;       
##  [16] &quot;his&quot;        &quot;himself&quot;    &quot;she&quot;        &quot;her&quot;        &quot;hers&quot;      
##  [21] &quot;herself&quot;    &quot;it&quot;         &quot;its&quot;        &quot;itself&quot;     &quot;they&quot;      
##  [26] &quot;them&quot;       &quot;their&quot;      &quot;theirs&quot;     &quot;themselves&quot; &quot;what&quot;      
##  [31] &quot;which&quot;      &quot;who&quot;        &quot;whom&quot;       &quot;this&quot;       &quot;that&quot;      
##  [36] &quot;these&quot;      &quot;those&quot;      &quot;am&quot;         &quot;is&quot;         &quot;are&quot;       
##  [41] &quot;was&quot;        &quot;were&quot;       &quot;be&quot;         &quot;been&quot;       &quot;being&quot;     
##  [46] &quot;have&quot;       &quot;has&quot;        &quot;had&quot;        &quot;having&quot;     &quot;do&quot;        
##  [51] &quot;does&quot;       &quot;did&quot;        &quot;doing&quot;      &quot;would&quot;      &quot;should&quot;    
##  [56] &quot;could&quot;      &quot;ought&quot;      &quot;i&#39;m&quot;        &quot;you&#39;re&quot;     &quot;he&#39;s&quot;      
##  [61] &quot;she&#39;s&quot;      &quot;it&#39;s&quot;       &quot;we&#39;re&quot;      &quot;they&#39;re&quot;    &quot;i&#39;ve&quot;      
##  [66] &quot;you&#39;ve&quot;     &quot;we&#39;ve&quot;      &quot;they&#39;ve&quot;    &quot;i&#39;d&quot;        &quot;you&#39;d&quot;     
##  [71] &quot;he&#39;d&quot;       &quot;she&#39;d&quot;      &quot;we&#39;d&quot;       &quot;they&#39;d&quot;     &quot;i&#39;ll&quot;      
##  [76] &quot;you&#39;ll&quot;     &quot;he&#39;ll&quot;      &quot;she&#39;ll&quot;     &quot;we&#39;ll&quot;      &quot;they&#39;ll&quot;   
##  [81] &quot;isn&#39;t&quot;      &quot;aren&#39;t&quot;     &quot;wasn&#39;t&quot;     &quot;weren&#39;t&quot;    &quot;hasn&#39;t&quot;    
##  [86] &quot;haven&#39;t&quot;    &quot;hadn&#39;t&quot;     &quot;doesn&#39;t&quot;    &quot;don&#39;t&quot;      &quot;didn&#39;t&quot;    
##  [91] &quot;won&#39;t&quot;      &quot;wouldn&#39;t&quot;   &quot;shan&#39;t&quot;     &quot;shouldn&#39;t&quot;  &quot;can&#39;t&quot;     
##  [96] &quot;cannot&quot;     &quot;couldn&#39;t&quot;   &quot;mustn&#39;t&quot;    &quot;let&#39;s&quot;      &quot;that&#39;s&quot;    
## [101] &quot;who&#39;s&quot;      &quot;what&#39;s&quot;     &quot;here&#39;s&quot;     &quot;there&#39;s&quot;    &quot;when&#39;s&quot;    
## [106] &quot;where&#39;s&quot;    &quot;why&#39;s&quot;      &quot;how&#39;s&quot;      &quot;a&quot;          &quot;an&quot;        
## [111] &quot;the&quot;        &quot;and&quot;        &quot;but&quot;        &quot;if&quot;         &quot;or&quot;        
## [116] &quot;because&quot;    &quot;as&quot;         &quot;until&quot;      &quot;while&quot;      &quot;of&quot;        
## [121] &quot;at&quot;         &quot;by&quot;         &quot;for&quot;        &quot;with&quot;       &quot;about&quot;     
## [126] &quot;against&quot;    &quot;between&quot;    &quot;into&quot;       &quot;through&quot;    &quot;during&quot;    
## [131] &quot;before&quot;     &quot;after&quot;      &quot;above&quot;      &quot;below&quot;      &quot;to&quot;        
## [136] &quot;from&quot;       &quot;up&quot;         &quot;down&quot;       &quot;in&quot;         &quot;out&quot;       
## [141] &quot;on&quot;         &quot;off&quot;        &quot;over&quot;       &quot;under&quot;      &quot;again&quot;     
## [146] &quot;further&quot;    &quot;then&quot;       &quot;once&quot;       &quot;here&quot;       &quot;there&quot;     
## [151] &quot;when&quot;       &quot;where&quot;      &quot;why&quot;        &quot;how&quot;        &quot;all&quot;       
## [156] &quot;any&quot;        &quot;both&quot;       &quot;each&quot;       &quot;few&quot;        &quot;more&quot;      
## [161] &quot;most&quot;       &quot;other&quot;      &quot;some&quot;       &quot;such&quot;       &quot;no&quot;        
## [166] &quot;nor&quot;        &quot;not&quot;        &quot;only&quot;       &quot;own&quot;        &quot;same&quot;      
## [171] &quot;so&quot;         &quot;than&quot;       &quot;too&quot;        &quot;very&quot;       &quot;will&quot;</code></pre>
<p>Also, there are tokens that usually carry very limited semantic contents, such as <em>numbers</em> and <em>punctuation</em>. Numbers, symbols and punctuations are often treated differently in computational text analytics.</p>
<p>When creating the <code>dfm</code> object, we can further specify a few parameters for the function <code>dfm()</code>:</p>
<ul>
<li><code>remove_punct = TRUE</code>: remove all punctuation tokens</li>
<li><code>remove = vector()</code>: remove all words specified in the character vector here</li>
</ul>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb362-1" data-line-number="1">corp_us_dfm_stp &lt;-<span class="st"> </span>corp_us <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb362-2" data-line-number="2"><span class="st">  </span><span class="kw">dfm</span>(<span class="dt">remove_punct =</span> T, </a>
<a class="sourceLine" id="cb362-3" data-line-number="3">      <span class="dt">remove =</span> <span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>))</a>
<a class="sourceLine" id="cb362-4" data-line-number="4">corp_us_dfm_stp[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>## Document-feature matrix of: 10 documents, 10 features (60.0% sparse) and 4 docvars.
##                  features
## docs              fellow-citizens senate house representatives among
##   1789-Washington               1      1     2               2     1
##   1793-Washington               0      0     0               0     0
##   1797-Adams                    3      1     0               2     4
##   1801-Jefferson                2      0     0               0     1
##   1805-Jefferson                0      0     0               0     7
##   1809-Madison                  1      0     0               0     0
##                  features
## docs              vicissitudes incident life event filled
##   1789-Washington            1        1    1     2      1
##   1793-Washington            0        0    0     0      0
##   1797-Adams                 0        0    2     0      0
##   1801-Jefferson             0        0    1     0      0
##   1805-Jefferson             0        0    2     0      0
##   1809-Madison               0        0    1     0      1
## [ reached max_ndoc ... 4 more documents ]</code></pre>
<p>We can see that the number of features drops significantly after we remove <code>stopwords</code>:</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb364-1" data-line-number="1"><span class="kw">nfeat</span>(corp_us_dfm_ngram) <span class="co"># bigram version</span></a></code></pre></div>
<pre><code>## [1] 63591</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" data-line-number="1"><span class="kw">nfeat</span>(corp_us_dfm) <span class="co"># default unigram version</span></a></code></pre></div>
<pre><code>## [1] 9360</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb368-1" data-line-number="1"><span class="kw">nfeat</span>(corp_us_dfm_stp) <span class="co"># unigram removing stopwords and punks</span></a></code></pre></div>
<pre><code>## [1] 9210</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb370-1" data-line-number="1"><span class="kw">nfeat</span>(corp_us_dfm_stem) <span class="co"># unigram stem version</span></a></code></pre></div>
<pre><code>## [1] 5544</code></pre>
</div>
<div id="distributional-properties" class="section level3">
<h3><span class="header-section-number">13.8.3</span> Distributional Properties</h3>
<p>Depending on the granularity of the features you are considering, you may get a considerably large number (e.g., thousands of ngrams) of features in your <code>dfm</code> matrix.</p>
<p>Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types:</p>
<ul>
<li><strong>Frequency</strong>
<ul>
<li>To make sure that the feature is important, we probably need to set a cut-off <strong>minimum</strong> frequency for a feature. For example, if the word occurs only once in the corpus (i.e., <a href="https://simple.wikipedia.org/wiki/Hapax_legomenon">hapax legomenon</a>), these words can be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the <strong>maximum</strong> frequency of the feature. If the word occurrs in all documents, they won’t help much as well.</li>
</ul></li>
<li><strong>Dispersion</strong>
<ul>
<li>If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate that this feature is too domain-specific. Therefore, sometimes we can control the document frequency of the features (i.e., in how many different texts does the feature occur?)</li>
</ul></li>
<li>Other self-defined weights
<ul>
<li>It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency <em>n</em> for a word <em>w</em> in a text <em>d</em>, the significance of this <em>n</em> may be connected to:
<ul>
<li>the document size of <em>d</em></li>
<li>the total number of <em>w</em></li>
</ul></li>
<li>Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the <code>dfm</code> we can include only words belonging to the lexical categories, such as nouns and verbs.</li>
</ul></li>
</ul>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-403" class="exercise"><strong>Exercise 13.2  </strong></span>Please get familar with the following functions, provided by <code>quanteda</code> for weighting of the document-feature matrix: <code>dfm_weight()</code>, <code>dfm_tfidf()</code>.
</div>

<hr />
<p>In the following demo, we adopt a few simple distrubtional criteria:</p>
<ul>
<li>we use a simple unigram model</li>
<li>we remove stopwords, punctuations, numbers, and symbols</li>
<li>we remove words whose freqency &lt;= 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS)</li>
</ul>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb372-1" data-line-number="1">corp_us_dfm_trimmed &lt;-<span class="st"> </span>corp_us <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb372-2" data-line-number="2"><span class="st">  </span><span class="kw">dfm</span>(<span class="dt">remove =</span> <span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>), </a>
<a class="sourceLine" id="cb372-3" data-line-number="3">      <span class="dt">remove_punct =</span> T,</a>
<a class="sourceLine" id="cb372-4" data-line-number="4">      <span class="dt">remove_numbers=</span> T,</a>
<a class="sourceLine" id="cb372-5" data-line-number="5">      <span class="dt">remove_symbols =</span> T) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb372-6" data-line-number="6"><span class="st">  </span><span class="kw">dfm_trim</span>(<span class="dt">min_termfreq =</span> <span class="dv">10</span>, <span class="dt">termfreq_type =</span> <span class="st">&quot;count&quot;</span>,</a>
<a class="sourceLine" id="cb372-7" data-line-number="7">           <span class="dt">min_docfreq =</span> <span class="dv">3</span>, <span class="dt">max_docfreq =</span> <span class="kw">ndoc</span>(corp_us)<span class="op">-</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb372-8" data-line-number="8">           <span class="dt">docfreq_type =</span> <span class="st">&quot;count&quot;</span>) </a>
<a class="sourceLine" id="cb372-9" data-line-number="9"><span class="kw">nfeat</span>(corp_us_dfm_trimmed)</a></code></pre></div>
<pre><code>## [1] 1387</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb374-1" data-line-number="1">corp_us_dfm_trimmed[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>## Document-feature matrix of: 10 documents, 10 features (52.0% sparse) and 4 docvars.
##                  features
## docs              fellow-citizens senate house representatives among life event
##   1789-Washington               1      1     2               2     1    1     2
##   1793-Washington               0      0     0               0     0    0     0
##   1797-Adams                    3      1     0               2     4    2     0
##   1801-Jefferson                2      0     0               0     1    1     0
##   1805-Jefferson                0      0     0               0     7    2     0
##   1809-Madison                  1      0     0               0     0    1     0
##                  features
## docs              greater order received
##   1789-Washington       1     2        1
##   1793-Washington       0     0        0
##   1797-Adams            0     4        0
##   1801-Jefferson        1     1        0
##   1805-Jefferson        0     3        0
##   1809-Madison          0     0        0
## [ reached max_ndoc ... 4 more documents ]</code></pre>
</div>
</div>
<div id="exploratory-analysis-of-dfm" class="section level2">
<h2><span class="header-section-number">13.9</span> Exploratory Analysis of <code>dfm</code></h2>
<ul>
<li>We can check the top features in the current corpus:</li>
</ul>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb376-1" data-line-number="1"><span class="kw">topfeatures</span>(corp_us_dfm_trimmed)</a></code></pre></div>
<pre><code>##     people government         us        can       upon       must      great 
##        575        564        478        471        371        366        340 
##        may     states      shall 
##        338        333        314</code></pre>
<ul>
<li>We can visualize the top features using a word cloud:</li>
</ul>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb378-1" data-line-number="1"><span class="kw">require</span>(RColorBrewer)</a>
<a class="sourceLine" id="cb378-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb378-3" data-line-number="3"><span class="kw">textplot_wordcloud</span>(corp_us_dfm_trimmed, <span class="dt">max_words =</span> <span class="dv">200</span>, <span class="dt">random_order =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb378-4" data-line-number="4">                   <span class="dt">rotation =</span> <span class="fl">.25</span>,</a>
<a class="sourceLine" id="cb378-5" data-line-number="5">                   <span class="dt">color =</span>  <span class="kw">c</span>(<span class="st">&#39;red&#39;</span>, <span class="st">&#39;pink&#39;</span>, <span class="st">&#39;green&#39;</span>, <span class="st">&#39;purple&#39;</span>, <span class="st">&#39;orange&#39;</span>, <span class="st">&#39;blue&#39;</span>))</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-406-1.png" width="100%" /></p>
</div>
<div id="document-similarity" class="section level2">
<h2><span class="header-section-number">13.10</span> Document Similarity</h2>
<p>As shown in <a href="vector-space-representation.html#fig:dfmdemo">13.4</a>, with the <em>N</em>-dimensional vector representation of each document, we can easily compute the mathematical distances/similarities between two documents.</p>
<p>In Section <a href="vector-space-representation.html#distance-similarity-metrics">13.5</a>, we introduced two important metrics:</p>
<ul>
<li>distance-based metric: Euclidean Distance</li>
<li>similarity-based metric: Cosine Similarity</li>
</ul>
<p><code>quanteda</code> provides useful functions to compute these metrics (as well as other alternatives): <code>textstat_simil()</code> and <code>textstat_dist()</code>:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" data-line-number="1">corp_us_euclidean &lt;-<span class="st"> </span>corp_us_dfm_trimmed <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb379-2" data-line-number="2"><span class="st">  </span><span class="kw">dfm_weight</span>(<span class="dt">scheme=</span><span class="st">&quot;prop&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb379-3" data-line-number="3"><span class="st">  </span><span class="kw">textstat_dist</span>(<span class="dt">method=</span><span class="st">&quot;euclidean&quot;</span>)</a>
<a class="sourceLine" id="cb379-4" data-line-number="4"></a>
<a class="sourceLine" id="cb379-5" data-line-number="5">corp_us_cosine &lt;-<span class="st"> </span>corp_us_dfm_trimmed <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb379-6" data-line-number="6"><span class="st">  </span><span class="kw">dfm_weight</span>(<span class="dt">scheme=</span><span class="st">&quot;prop&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb379-7" data-line-number="7"><span class="st">  </span><span class="kw">textstat_simil</span>(<span class="dt">method=</span><span class="st">&quot;cosine&quot;</span>)</a>
<a class="sourceLine" id="cb379-8" data-line-number="8"></a>
<a class="sourceLine" id="cb379-9" data-line-number="9">corp_us_euclidean[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>## 5 x 5 Matrix of class &quot;dspMatrix&quot;
##                 1789-Washington 1793-Washington 1797-Adams 1801-Jefferson
## 1789-Washington    1.070008e-08       0.1524448 0.07003297   7.371303e-02
## 1793-Washington    1.524448e-01       0.0000000 0.15290542   1.527390e-01
## 1797-Adams         7.003297e-02       0.1529054 0.00000000   7.148902e-02
## 1801-Jefferson     7.371303e-02       0.1527390 0.07148902   4.562530e-09
## 1805-Jefferson     7.126417e-02       0.1530932 0.07194156   6.427144e-02
##                 1805-Jefferson
## 1789-Washington     0.07126417
## 1793-Washington     0.15309316
## 1797-Adams          0.07194156
## 1801-Jefferson      0.06427144
## 1805-Jefferson      0.00000000</code></pre>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" data-line-number="1">corp_us_cosine[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>## 5 x 5 Matrix of class &quot;dspMatrix&quot;
##                 1789-Washington 1793-Washington 1797-Adams 1801-Jefferson
## 1789-Washington       1.0000000       0.2958278  0.5558610      0.4748221
## 1793-Washington       0.2958278       1.0000000  0.2865581      0.2763780
## 1797-Adams            0.5558610       0.2865581  1.0000000      0.4959554
## 1801-Jefferson        0.4748221       0.2763780  0.4959554      1.0000000
## 1805-Jefferson        0.4950774       0.2655092  0.4738698      0.5440536
##                 1805-Jefferson
## 1789-Washington      0.4950774
## 1793-Washington      0.2655092
## 1797-Adams           0.4738698
## 1801-Jefferson       0.5440536
## 1805-Jefferson       1.0000000</code></pre>
<p>Based on the distances/similarities, we can further examine how different documents may cluster together in terms of their features (lexical) distributions. Here we apply the hierarchical cluster analysis to examine the sub-groupings of the documents.</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" data-line-number="1"><span class="co"># distance-based</span></a>
<a class="sourceLine" id="cb383-2" data-line-number="2">corp_us_hist_euclidean &lt;-<span class="st"> </span>corp_us_euclidean <span class="op">%&gt;%</span><span class="st"> </span>as.dist <span class="op">%&gt;%</span><span class="st"> </span>hclust</a>
<a class="sourceLine" id="cb383-3" data-line-number="3"><span class="kw">plot</span>(corp_us_hist_euclidean,<span class="dt">hang =</span> <span class="dv">-1</span>, <span class="dt">cex =</span> <span class="fl">0.7</span>)</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-408-1.png" width="100%" /></p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1"><span class="co"># similarity</span></a>
<a class="sourceLine" id="cb384-2" data-line-number="2">corp_us_hist_cosine &lt;-<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>corp_us_cosine) <span class="op">%&gt;%</span><span class="st"> </span>as.dist <span class="op">%&gt;%</span><span class="st"> </span>hclust</a>
<a class="sourceLine" id="cb384-3" data-line-number="3"><span class="kw">plot</span>(corp_us_hist_cosine,<span class="dt">hang =</span> <span class="dv">-1</span>, <span class="dt">cex =</span> <span class="fl">0.7</span>)</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-408-2.png" width="100%" /></p>
<div class="danger">
<p>
Please note that <code>textstat_simil()</code> gives us the similarity matrix. In other words, the numbers in the matrix indicate how <em>similar</em> the documents are. However, for hierarchical cluster analysis, the function <code>hclust()</code> expects a distance-based matrix, namely one indicating how dissimilar the documents are. That is the main reason why we use <code>(1 - corp_us_cosine)</code> in the cosine example.
</p>
</div>
<div class="info">
<p>
Cluster anlaysis is a very useful exploratory technique to examine the emerging structure of a large dataset. For more detail introduction to this statistical method, I would recommend <span class="citation"><span class="citation">Gries (<a href="#ref-gries2013">2013</a>)</span></span> Ch 5.6.
</p>
</div>
</div>
<div id="feature-coocurrence-matrix-fcm" class="section level2">
<h2><span class="header-section-number">13.11</span> Feature-Coocurrence Matrix (<code>fcm</code>)</h2>
<p>We can convert a document-feature matrix into a feature-cooccurrence matrix <code>fmc</code>, using the <code>quanteda::fcm()</code>. The <code>fmc</code> includes the co-occurrence frequencies of any two features within the same documents.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" data-line-number="1"><span class="co"># convert `dfm` to `fcm`</span></a>
<a class="sourceLine" id="cb385-2" data-line-number="2">corp_us_fcm &lt;-<span class="st"> </span>corp_us_dfm_trimmed <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fcm</span>()</a>
<a class="sourceLine" id="cb385-3" data-line-number="3">corp_us_fcm[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>## Feature co-occurrence matrix of: 10 by 10 features.
##                  features
## features          fellow-citizens senate house representatives among life event
##   fellow-citizens              72     64    15              62   106   50    25
##   senate                        0     13    16              29    36   30     2
##   house                         0      0     3              11    11   32     6
##   representatives               0      0     0               8    45   33    11
##   among                         0      0     0               0   155  240    26
##   life                          0      0     0               0     0  208    21
##   event                         0      0     0               0     0    0     9
##   greater                       0      0     0               0     0    0     0
##   order                         0      0     0               0     0    0     0
##   received                      0      0     0               0     0    0     0
##                  features
## features          greater order received
##   fellow-citizens      82    52       26
##   senate               35    19        6
##   house                14    14        3
##   representatives      28    29        9
##   among               144   209       21
##   life                136   182       16
##   event                16    15       12
##   greater              44    76       19
##   order                 0    98       12
##   received              0     0        2</code></pre>
<hr />
<p>In <code>quanteda</code>, we can generate the <code>fcm</code> of a corpus, either directly from the <code>corpus</code> object or from the <code>dfm</code> object. The feature-cooccurrence matrix measures the co-occurrences of features within a user-defined <em>context</em>.</p>
<ul>
<li>If the input of <code>fcm</code> is a <code>dfm</code> object, the context is set to be <em>documents</em>. In other words, the counts in <code>fcm</code> refers to the number of co-occurrences the two features <strong>within the same document</strong>.</li>
<li>If the input of <code>fmc</code> is a <code>corpus</code> object, we can specify the context to be a window size. The counts in <code>fcm</code> refers to the number of co-occurrences the two features <strong>within the window size</strong>.</li>
</ul>
<p>We can the structure of the <code>fcm</code> with a simple example:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A B C A E F G&quot;</span>, <span class="st">&quot;B C D E F G&quot;</span>, <span class="st">&quot;B D A E F G&quot;</span>)</a>
<a class="sourceLine" id="cb387-2" data-line-number="2"></a>
<a class="sourceLine" id="cb387-3" data-line-number="3"><span class="kw">corpus</span>(x) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb387-4" data-line-number="4"><span class="st">  </span>dfm <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb387-5" data-line-number="5"><span class="st">  </span>fcm <span class="co"># fcm based on document context</span></a></code></pre></div>
<pre><code>## Feature co-occurrence matrix of: 7 by 7 features.
##         features
## features a b c e f g d
##        a 1 3 2 3 3 3 1
##        b 0 0 2 3 3 3 2
##        c 0 0 0 2 2 2 1
##        e 0 0 0 0 3 3 2
##        f 0 0 0 0 0 3 2
##        g 0 0 0 0 0 0 2
##        d 0 0 0 0 0 0 0</code></pre>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" data-line-number="1"><span class="kw">corpus</span>(x) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb389-2" data-line-number="2"><span class="st">  </span><span class="kw">fcm</span>(<span class="dt">context =</span> <span class="st">&quot;window&quot;</span>, <span class="dt">window =</span> <span class="dv">2</span>) <span class="co"># fcm based on window context</span></a></code></pre></div>
<pre><code>## Feature co-occurrence matrix of: 7 by 7 features.
##         features
## features A B C E F G D
##        A 0 3 2 2 2 0 1
##        B 0 0 2 0 0 0 2
##        C 0 0 0 2 0 0 1
##        E 0 0 0 0 3 3 2
##        F 0 0 0 0 0 3 1
##        G 0 0 0 0 0 0 0
##        D 0 0 0 0 0 0 0</code></pre>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb391-1" data-line-number="1"><span class="kw">corpus</span>(x) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb391-2" data-line-number="2"><span class="st">  </span><span class="kw">fcm</span>(<span class="dt">context =</span> <span class="st">&quot;document&quot;</span>) <span class="co"># same as the first one</span></a></code></pre></div>
<pre><code>## Feature co-occurrence matrix of: 7 by 7 features.
##         features
## features A B C E F G D
##        A 1 3 2 3 3 3 1
##        B 0 0 2 3 3 3 2
##        C 0 0 0 2 2 2 1
##        E 0 0 0 0 3 3 2
##        F 0 0 0 0 0 3 2
##        G 0 0 0 0 0 0 2
##        D 0 0 0 0 0 0 0</code></pre>
<hr />
<p><code>fmc</code> is an interesting structure because, similar to <code>dfm</code>, we can now examine the pairwise relationships between features. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent features are similar in their co-occurring contexts.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb393-1" data-line-number="1"><span class="co"># select top 30 features</span></a>
<a class="sourceLine" id="cb393-2" data-line-number="2">corp_us_topfeatures &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">topfeatures</span>(corp_us_fcm, <span class="dv">50</span>))</a>
<a class="sourceLine" id="cb393-3" data-line-number="3"><span class="co"># plot network</span></a>
<a class="sourceLine" id="cb393-4" data-line-number="4"><span class="kw">fcm_select</span>(corp_us_fcm, <span class="dt">pattern =</span> corp_us_topfeatures) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb393-5" data-line-number="5"><span class="st">    </span><span class="kw">textplot_network</span>(<span class="dt">min_freq =</span> <span class="dv">100</span>)</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-413-1.png" width="672" /></p>
<p>The <code>dfm</code> or <code>fcm</code> come with many potentials. Please refer to the <a href="https://quanteda.io/index.html"><code>quanteda</code> documentation</a> for more applications.</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-414" class="exercise"><strong>Exercise 13.3  </strong></span>Please create a network of the top 30 bigrams based on the corpus <code>corp_us</code>. The criteria for bigrams selection are as follows:</p>
<ul>
<li>Include bigrams that consist of alphanumeric characters only (no punctuations)</li>
<li>Include bigrams whose frequency &gt;= 10 and docfreq &gt;= 5 but &lt;= half number of the corpus</li>
</ul>
</div>

<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-415-1.png" width="672" /></p>
<hr />

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-gries2013">
<p>Gries, Stefan Th. 2013. <em>Statistics for Linguistics with R: A Practical Introduction. 2nd Edition.</em> Walter de Gruyter.</p>
</div>
<div id="ref-jurafsky2020">
<p>Jurafsky, Dan, and James H. Martin. 2020. “Speech &amp; Language Processing 3rd.” Available at <a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a> (Accessed on 2020/04/01).</p>
</div>
<div id="ref-manning1999">
<p>Manning, Christopher D, and Hinrich Schütze. 1999. <em>Foundations of Statistical Natural Language Processing</em>. MIT press.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https:\/\/alvinntnu.github.io" + location.pathname;  // Replace PAGE_URL with your page's canonical URL variable
/*
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
*/
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://enc2036.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="xml.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vector-space-representation-ii.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
