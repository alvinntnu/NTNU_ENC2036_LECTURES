[
["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Data Processing Flowchart 12.2 Document-Feature Matrix (dfm) 12.3 Corpus 12.4 Document-Feature Matrix (dfm) 12.5 Distributional Hypothesis and Distance/Similarity Metrics 12.6 Multidimensiona Space 12.7 Feature Selection 12.8 Applying DFM", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) In this chapter, I would like to talk about the idea of distributional semantics, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized and quantified using the distributional data of the linguistic units in the corpus. Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts. 12.1 Data Processing Flowchart In Chapter 5, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of semantic. Most importantly, a new object class is introduced in Figure 12.1, i.e., the dfm object in quanteda. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterizing the documents. The cells in the matrix often refer to the co-occurrence statistics between each document and the feature. Different ways of operationalizing the features and the cell values may lead to different types of dfm. In this chapter, I would like to show you how we create a dfm of a corpus and what are the common ways to define features and cell valus for the analysis of semantics via vector space representation. Figure 12.1: English Text Analytics Flowchart (v2) 12.2 Document-Feature Matrix (dfm) To create a dfm, i.e., Dcument-Feature-Matrix, quanteda provides two alterantives: create dfm based on an corpus object create dfm based on an token object For English data, quanteda can take care of the word tokenization fairly well so you can create dfm directly from corpus (See Figure 12.1) In Chapter (???)(chinese-text-processing), we stress that the default tokenization method in quanteda with Chinese data may be limited in several ways. In order to create a dfm that takes into account the appropriateness of the Chinese word segmentation, I would highly recommend you to first create atokens object using the self-defined word segmentation methods, and then feed it to dfm() to create the dfm for your corpus. In this way, the dfm will use the segmented results defined by your word segmenter. In other words, with Chinese data, probably it is not really necessary to have a corpus object; rather, a token object of the corpus might be more useful/practical. (In quanteda, most of the functions for corpus can be applied to tokens as well, e.g., kwic(), dfm()) 12.3 Corpus In this chapter, I will use the same English dataset we discussed in Chapter 5, the data_corpus_inaugural preloaded in the package quanteda. For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% dfm Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) ## [1] &quot;corpus&quot; &quot;character&quot; class(corp_us) ## [1] &quot;corpus&quot; &quot;character&quot; class(corp_us_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; 12.4 Document-Feature Matrix (dfm) What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the following example, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and corp_us_dfm[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] A dfm with words as the features is the simplest way to characterize the texts in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each text as bags of words. In other words, how the words are arranged relative to each other is ignored (i.e., the morphosyntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word dfm should be a naive characterization of the texts. In many computational tasks, however, it turns out that this simple bag-of-words model is very effective in modeling the semantics of the documents. 12.5 Distributional Hypothesis and Distance/Similarity Metrics The effectiveness of the dfm in semantic analysis lies in one important hypoethesis shared in the corpus linguistic community: distributional hypothesis. Distributional properties like co-occurrences are very important information in corpus linguistics. Most of the studies in corpus linguistics adopt an implicit distributional hypothesis, which can be illustrated by a few famous quotes: You shall know a word by the comany it keeps. (Firth, 1957, p.11) [D]ifference of meaning correlates with difference of distribution. (Harris, 1970, p.785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves (De Deyne et al. 2016) So in our current context, the idea is that if two documents have similar sets of linguistic units popping up in them, they are more likely to be similar in their semantics as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with other documents (i.e., other rows). Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically: Figure 12.2: Vector Representation In Math, there are in general two types of metrics to measure the relationship between vectors: distance-based vs. similarity-based metrics. 12.5.1 Distance-based Metrics Many distance measures of vectors are based on the following formula and differ in individual parameter settings. \\[\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^y}\\big)^{\\frac{1}{y}}\\] The n in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.) When y is set to 2, it computes the famous Euclidean distance of two vectors, i.e., the direct spatial distance between two points on the n-dimensional space. x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) sum(abs(x-y)^2)^(1/2) # XY distance ## [1] 6 sum(abs(y-z)^2)^(1/2) # YZ distnace ## [1] 4.472136 The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure 12.3) Figure 12.3: Distance-based Metric: Euclidean Distance 12.5.2 Similarity-based Metrics In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is Cosine Similarity, which can be computed as follows: \\[cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\] sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2))) # xy ## [1] 0.9778024 sum(y*z)/(sqrt(sum(y^2))*sqrt(sum(z^2))) # yz ## [1] 0.4961389 The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the great their cosine similarity, the smaller the arcs, the closer they are. 12.5.3 Interim Summary The Euclidean Distance metric is a distance-based metric: the larger the value, the more distant the two vectors. The Cosine Similarity metric is a similatiry-based metric: the larger the value, the closer the two vectors. Based on our computations of the metrics for the three vectors, now in terms of the Euclidean Distance, y and z are closer; in terms of Cosine Similarity, x and y are closer. Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the linguistic units: The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors) The relative increase/decrease of the values on ecah dimension (i.e., the curvatures of vectors) There are many other distance-based or similarity-based metrics available. For more detail, please see Manning and Schütze (1999) Ch15.2.2. and Jurafsky and Martin (2020) Ch6: Vector Semantics and Embeddings. 12.6 Multidimensiona Space Back to our example of dfm, it is essentially the same vector representation, but in a multidimensional verson (cf. Figure 12.4). The document in each row is represented as a vector of N dimensional space. The size of N depends on the number of linguistic units that are included in the analysisdfm`. Figure 12.4: Example of Document-Feature Matrix 12.7 Feature Selection A dfm may be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered when creating a dfm: The granularity of the linguistic unit Stopwords The distributional cut-offs of the linguistic unit 12.7.1 Determining Linguistic Granularity In our previous example, we include only words, i.e., unigrams, as our features in the dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: corp_us_dfm_ngram &lt;- corp_us %&gt;% dfm(ngrams = 2) corp_us_dfm_ngram[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: corp_us_dfm_stem &lt;- corp_us %&gt;% dfm(stem = T) corp_us_dfm_stem[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (38.0% sparse) and 4 docvars. ## features ## docs fellow-citizen of the senat and hous repres : among ## 1789-Washington 1 71 116 1 48 2 2 1 1 ## 1793-Washington 0 11 13 0 2 0 0 1 0 ## 1797-Adams 3 140 163 1 130 3 3 0 4 ## 1801-Jefferson 2 104 130 0 81 0 1 1 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 7 ## 1809-Madison 1 69 104 0 43 0 1 0 0 ## features ## docs vicissitud ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 1 ## [ reached max_ndoc ... 4 more documents ] You need to decide which type of linguistic units is more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, there is no rule for how to do this. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 12.7.2 Stopwords There are words that are not so informative in telling us the similarity and difference between the documents because they almost occur in every document of the corpus, but carray little (refential) semantic contents. These words are usually the function words, such as and, the, of. Also, there are tokens that usually carry limited semantic contents, such as numbers and punctuation. Therefore, it is not uncommon that analysts sometimes create a list of words to be removed from the dfm. These words are referred to as stopwords. The library quanteda has determined a default English stopword list, i.e., stopwords(&quot;en&quot;). When creating the dfm object, we can further specify a few parameters: remove_punct: remove all punctuation tokens remove: remove all words specified in the character vector here corp_us_dfm_stp &lt;- corp_us %&gt;% dfm(remove_punct = T, remove = stopwords(&quot;en&quot;)) corp_us_dfm_stp[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (60.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among ## 1789-Washington 1 1 2 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 ## 1801-Jefferson 2 0 0 0 1 ## 1805-Jefferson 0 0 0 0 7 ## 1809-Madison 1 0 0 0 0 ## features ## docs vicissitudes incident life event filled ## 1789-Washington 1 1 1 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 0 0 2 0 0 ## 1801-Jefferson 0 0 1 0 0 ## 1805-Jefferson 0 0 2 0 0 ## 1809-Madison 0 0 1 0 1 ## [ reached max_ndoc ... 4 more documents ] We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm) ## [1] 9360 nfeat(corp_us_dfm_ngram) ## [1] 9360 nfeat(corp_us_dfm_stem) ## [1] 5544 nfeat(corp_us_dfm_stp) ## [1] 9210 12.7.3 Distributional Cut-offs for Features Depending on the granularity of the linguistic units you consider, you may get a considerable number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the word occurs only once in the corpus (i.e., hapax legomenon), these words can be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurrs in all documents, they won’t help much as well. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate something else. Therefore, sometimes we can control the document frequency of the features (i.e., in how many different texts does the feature occur?) Other self-defined weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a text d, the significance of this n may be connected to: the document size of d the total number of w Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. In the following demo, we adopt a few simple distrubtional criteria: we remove stopwords and punctuations we remove words whose freqency &lt; 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% dfm(remove = stopwords(&quot;en&quot;), remove_punct = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) corp_us_dfm_trimmed[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (52.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among life event ## 1789-Washington 1 1 2 2 1 1 2 ## 1793-Washington 0 0 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 2 0 ## 1801-Jefferson 2 0 0 0 1 1 0 ## 1805-Jefferson 0 0 0 0 7 2 0 ## 1809-Madison 1 0 0 0 0 1 0 ## features ## docs greater order received ## 1789-Washington 1 2 1 ## 1793-Washington 0 0 0 ## 1797-Adams 0 4 0 ## 1801-Jefferson 1 1 0 ## 1805-Jefferson 0 3 0 ## 1809-Madison 0 0 0 ## [ reached max_ndoc ... 4 more documents ] 12.8 Applying DFM 12.8.1 Wordcloud With a dfm of a corpus, we can quickly explore the nature of this corpus by examining the top features of this corpus: topfeatures(corp_us_dfm_trimmed) ## people government us can upon must great ## 575 564 478 471 371 366 340 ## may states shall ## 338 333 314 Or we can visualize the distrubtion of these top features using the wordcloud: set.seed(123) corp_us_dfm_trimmed %&gt;% textplot_wordcloud(min_count = 60, rotation = .35) 12.8.2 Document Similarity As shown in 12.4, with the N-dimensional vector representation of each document, we can easily compute the mathematical similarities between two documents. Based on the similarities, we can further examine how different documents may cluster together in terms of their lexical similarities. corp_us_dist &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist() corp_us_hist &lt;- corp_us_dist %&gt;% as.dist %&gt;% hclust plot(corp_us_hist,hang = -1, cex = 0.7) 12.8.3 Feature Similarity What if we transpose a document-feature matrix? A transposed dfm would be a feature-document matrix. This is an interesting structure because we then can do the same tricks with all the features in the corpus. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent words are similar. # convert `dfm` to `fcm` corp_us_fcm &lt;- corp_us_dfm_trimmed %&gt;% fcm # select top 30 features corp_us_topfeatures &lt;- names(topfeatures(corp_us_fcm, 30)) # plot network fcm_select(corp_us_fcm, pattern = corp_us_topfeatures) %&gt;% textplot_network(min_freq = 0.5) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. Exercise 12.2 Please create a network of the top 30 bigrams based on the corpus corp_us. The criteria for bigrams selection are as follows: Include bigrams that consist of alphanumeric characters only (no punctuations) Include bigrams whose frequency &gt;= 10 and docfreq &gt;= 5 but &lt;= half number of the corpus References "]
]
