[["index.html", "Corpus Linguistics Preface Course Objective Reading Materials Course Requirement Exams Course Website Contributing to the Lecture Notes Course Demo Data Academic Integrity House-keeping Guidelines Necessary Packages Environment Settings", " Corpus Linguistics Alvin Cheng-Hsien Chen 2022-03-02 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to one of the most active sub-fields of applied linguistics, i.e., Corpus Linguistics. Have you decided to embark on a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offering necessary inter-disciplinary skills and knowledge for quantitative and computational analysis of language. This course requires as prerequisite basic-level knowledge of coding skills. Students are expected to have taken ENC2055 or other equivalents of introductory programming courses before taking this course. Please see the FAQ of the course website for more information about the prerequisite. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will guide the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Reading Materials The course lectures will follow the materials provided on the course website. Please preview the lecture notes before the class. Also, for each topic, please review the assigned readings on your own. These readings will be part of the midterm and final exams as well. In particular, we will be referring to two useful reference books as our main reading materials– Stefanowitsch (2019), Gries (2016). In addition, there are a few more reference books listed at the end of the section (See References), which we will refer to for specific topics, including Gries (2021), Baayen (2008), Brezina (2018), McEnery &amp; Hardie (2011), Wynne (2006), Winter (2020), Hunston (2022), Bird et al. (2009). In particular, assigned readings for each chapter are listed as shown below. Chapter Assigned Readings 1 Gries (2021), ch1; McEnery &amp; Hardie (2011), ch1; Stefanowitsch (2019), ch1-2 2 Check ENC2055; Wickham &amp; Grolemund (2017) 3 Wynne (2006), ch1-2; Munzert et al. (2014), ch2&amp;9 4 Hunston (2022), Ch2-3; Stefanowitsch (2019), Ch4; Gries (2016), Ch3; Evert (2007) 5 Bird et al. (2009), ch3&amp;5; Gries (2016), Ch3; Stefanowitsch (2019), Ch7-8 6 Stefanowitsch (2019), Ch10 7 Lecture Notes Only 8 Stefanowitsch &amp; Gries (2003); Stefanowitsch &amp; Gries (2005); Gries &amp; Stefanowitsch (2004) 9 Lecture Notes Only 10 Wynne (2006), ch3,4,6; 11 Gries (2016), ch3; Munzert et al. (2014), ch3-4 12 Jurafsky &amp; Martin (2022), ch6 Course Requirement Exams The midterm and final exams will be a timed in-class open-book examination. Questions will include both coding exercises as well as essay questions (on the course materials). All exams will be considered individual work with no inter-personal communication. Please see Academic Integrity below. Course Website We have a course website. You may need a password to access the course materials (i.e., the data sets). If you are an officially enrolled student, please ask the instructor for the pass code. Please read the FAQ of the course website before course registration. Contributing to the Lecture Notes Although I have tried every possible way to make sure that the contents are correct, I may still accidentally make mistakes in the materials. If you spot any errors and would like make suggestions for better solutions, I would really appreciate it. To contribute your ideas, you can annotate the lecture notes using Hypothes.is, which is an amazing tool for website annotations. Go to Hypothes.is, and click the “get-started” on the top-right corner of the homepage. Install the the add-on for chrome, or other browser. (Optional!) To add an annotation, select some text and then click the on the pop-up menu. To see the annotations of others, click the in the upper right-hand corner of the page. Please turn on the Hypothes.is add-on when you are reading the course lecture notes, and you will see all public/shared annotations made by other course participants. See Quick Start Guide for Students and Annotation Tips for Students. At the beginning of the semester, I will share with the class a link to invite all the enrolled students to join a private group for annotation. But one can always provide feedbacks via the public annotations of the website. Course Demo Data Dropbox Demo Data Directory (Password protected) Academic Integrity All coding assignments are to be individual work, but discussion or collaboration with others is allowed. However, direct copying of others’ codes is absolutely forbidden. Any incidents of academic misconduct such as cheating, plagiarism, copying others’ work, or other inappropriate assistance on projects or examinations will be treated with zero tolerance and will result in a grade of “F” for the course. In particular, the midterm and final exams are taken seriously as individual work with absolutely no outside help or assistance. Breaches of academic integrity may also result in other action being taken by the University. House-keeping Guidelines Please direct all your course-related questions to the Discussion Forum on Moodle. Do not send your coding questions to the TA via email. By posting all the questions on Moodle, we can also make sure that those with similar questions would get proper assistance as well. If you need to consult TA (Howard Su) for technical help, please make an appointment with TA first. A recommended session is the hour after each week’s meeting. Please submit your coding assignments on time. Late submissions are subject to points deduction as a late penalty. All assignments must be submitted via Moodle. Please make sure that your access to Moodle is active. It is the student’s responsiblity to keep themselves posted of the most recent announcements. Necessary Packages In this course, we will need the following R packages for tutorials and exercises. library(dplyr) library(ggplot2) library(ggpubr) library(ggrepel) library(gutenbergr) library(htmlwidgets) library(jiebaR) library(kableExtra) library(parallel) library(purrr) library(quanteda) library(RColorBrewer) library(readtext) library(reticulate) library(Rtsne) library(rvest) library(showtext) library(spacyr) library(stringr) library(text2vec) library(textdata) library(tidyr) library(tidytext) library(tidyverse) library(wordcloud) library(wordcloud2) Environment Settings The R Version to produce the lecture notes: R version 4.1.2 (2021-11-01) If your R version is older than the above one, please consider updating your R. Details about updating R can be found in: 3 Methods to Update R &amp; Rstudio (For Windows &amp; Mac) Updating R, Rstudio, and Your Packages References Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction to statistics using R. Cambridge University Press. Bird, S., Klein, E., &amp; Loper, E. (2009). Natural language processing with python: Analyzing text with the natural language toolkit. \" O’Reilly Media, Inc.\". https://www.nltk.org/book/ Brezina, V. (2018). Statistics in corpus linguistics: A practical guide. Cambridge University Press. Evert, S. (2007). Corpora and collocations. https://stephanie-evert.de/PUB/Evert2007HSK_extended_manuscript.pdf Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Gries, S. T. (2021). Statistics for linguistics with R: A practical introduction. 3rd edition. (3rd ed.). Walter de Gruyter. Gries, S. T., &amp; Stefanowitsch, A. (2004). Extending collostructional analysis: A corpus-based perspective onalternations’. International Journal of Corpus Linguistics, 9(1), 97–129. Hunston, S. (2022). Corpora in applied linguistics (2nd ed.). Cambridge University Press. Jurafsky, D., &amp; Martin, J. H. (2022). Speech and language processing. \" O’Reilly Media, Inc.\". https://web.stanford.edu/~jurafsky/slp3/ McEnery, T., &amp; Hardie, A. (2011). Corpus linguistics: Method, theory and practice. Cambridge University Press. Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Stefanowitsch, A., &amp; Gries, S. T. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. Stefanowitsch, A., &amp; Gries, S. T. (2005). Covarying collexemes. Corpus Linguistics and Linguistic Theory, 1, 1–43. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. Winter, B. (2020). Statistics for linguists: An introduction using R. Routledge. Wynne, M. (Ed.). (2006). Developing linguistic corpora—a guide to good practice. EADH: The European Association for Digital Humanities. https://users.ox.ac.uk/~martinw/dlc/index.htm "],["what-is-corpus-linguistics.html", "Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? 1.2 What is corpus? 1.3 What is a corpus linguistic study? 1.4 Additional Information on CL", " Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? There is an unnecessary dichotomy in linguistics “intuiting” linguistic data Inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality Corpus data Highlight the importance of language use in real context Highlight the linguistic tendency in the population (from a sample) Strengths of Corpus Data Data reliability How sure can we be that other people will arrive at the same observations/patterns/conclusions using the same method? Can others replicate the same logical reasoning in intuiting data? Can others make the same “grammatical judgement?” Data validity How well do we understand the true mental representation that the linguistic data correspond to? Can we know more about the mental representation of grammar based on one man’s constructed sentences and/or his grammatical judgement? Can we better generalize our insights from one man’s intuition or from the group minds (population vs. sample vs. one-man)? Please provide one or two examples that show the differences between one’s grammatical intuition and the corpus-based observations. The examples could be in English or in your native language (e.g., Chinese). 1.2 What is corpus? This can be tricky: different disciplines, different definitions Literature History Sociology Field Linguistics Linguistic Corpus in corpus linguistics (Stefanowitsch, 2019) Authentic Representative Large A few well-received definitions “a collection of texts which have been selected and brought together so that language can be studied on the computer” (Wynne, 2006) “A corpus is a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research.” (John Sinclair in (Wynne, 2006)) “A corpus refers to a machine-readable collection of (spoken or written) texts that were produced in a natural communitive setting, and in which the collection of texts is compiled with the intention (1) to be representative and balanced with respect to a particular linguistic language, variety, register, or genre and (2) to be analyzed linguistically.” (Gries, 2016) 1.3 What is a corpus linguistic study? CL characteristics No general agreement as to what it is Not a very homogeneous methodological framework (Compared to other sub-disciplines in linguistics) It’s quite new Intertwined with many linguistic fields Interactional linguistics, cognitive linguistics, functional syntax, usage-based grammar etc. Stylometry, computational linguistics, NLP, digital humanities, text mining, sentiment analysis Corpus-based vs. Corpus-driven (cf. Tognini-Bonelli, 2001) Corpus-based studies: Typically use corpus data in order to explore a theory or hypothesis, aiming to validate it, refute it or refine it. Take corpus linguistics as a method Corpus-driven studies: Typically reject the characterization of corpus linguistics as a method Claim instead that the corpus itself should be the sole source of our hypotheses about language It is thus claimed that the corpus itself embodies a theory of language (Tognini-Bonelli, 2001, pp. 84–85) The distinction between corpus-based and corpus-driven studies can be subtle. Both of these approaches, however, are often connected to the general usage-based grammar framework. I would highly recommend the chapter on usage-based model (Chapter 11) in Croft &amp; Cruse (2004). An example of corpus-driven grammatical framework is pattern grammar. Please see Hunston &amp; Francis (2000) for a comprehensive introduction. Stefanowitsch (2019) defines Corpus Linguistics as follows: Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus More on Conditional Distribution An exhaustive investigation Text-processing technology Retrieval and coding Regular expressions Common methods KWIC concordances Collocates Frequency lists A systematic investigation The distribution of a linguistic phenomenon under particular conditions (e.g. lexical, syntactic, social, pragmatic etc. contexts) Statistical properties of language Examples When do English speakers use the complementizer that? What are the differences between small and little? When do English speakers choose “He picked up the book” vs. “He picked the book up?” When do English speakers place the adverbial clauses before the matrix clause? Do speakers use different linguistic forms in different genres? Is the word “gay” used differently across different time periods? Do L2 learners use similar collocation patterns as do L1 speakers? Do speakers of different socio-economic classes talk differently? Now please try to think of one or two research questions that may fit into the category of a corpus linguistic study and elaborate on the idea of conditional distribution: You are interested in the linguistic structure of … and would like to see how it varies/changes/develops depending on … 1.4 Additional Information on CL Important Journals in Corpus Linguistics Corpus Linguistics and Linguistic Theory International Journal of Corpus Linguistics Corpora Applied Linguistics Computational Linguistics Digital scholarship in the Humanities Language Teaching Language Learning Journal of Second Language Writing CALL Language Teaching Research ReCALL System Important Conferences on Corpus Linguistics Corpus Linguistics Conference (usually held in UK) International Association of Applied Linguistics American Association for Corpus Linguistics American Association for Applied Linguistics Teaching and Language Corpora Conference International Cognitive Linguistics Conference International Conference on Construction Grammar References Croft, W., &amp; Cruse, D. A. (2004). Cognitive linguistics. Cambridge University Press. Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Hunston, S., &amp; Francis, G. (2000). Pattern grammar: A corpus-driven approach to the lexical grammar of English. John Benjamins Publishing. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Tognini-Bonelli, E. (2001). Corpus linguistics at work. John Benjamins. Wynne, M. (Ed.). (2006). Developing linguistic corpora—a guide to good practice. EADH: The European Association for Digital Humanities. https://users.ox.ac.uk/~martinw/dlc/index.htm "],["r-fundamentals.html", "Chapter 2 R Fundamentals A Quick Note", " Chapter 2 R Fundamentals A Quick Note In this course, we assume that students have a certain level of background knowledge of R. Please review fundamental concepts relating to the R language on your own. Important topics are covered in more detail in my other course, ENC2055. In particular, we assume that students have working knowledge on the following topics covered in the Lecture Notes of ENC2055: Chapter 2: R Fundamentals Chapter 3: Code Format Convention Chapter 4: Subsetting Chapter 6: Data Manipulation Chapter 7: Data Import Chapter 8: String Manipulation Chapter 9: Conditions and Loops Chapter 10: Iterations Please refer Winter (2020), Ch1-2 and Gries (2016), Ch3 for a comprehensive overview of R fundamentals (especially the parts on regular expressions). References Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Winter, B. (2020). Statistics for linguists: An introduction using R. Routledge. "],["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resources 3.6 Final Remarks", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract linguistic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections (cf. Structured Corpus and XML), chances are that sometimes you still need to collect your own data for a particular research question. But please note that when you are creating your own corpus for specific research questions, always pay attention to the three important criteria: representativeness, authenticity, and size. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. If you are new to tidyverse R, please check its official webpage for learning resources. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure The HyperText Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser. 3.1.1 HTML Syntax To illustrate the structure of the HTML, please download the sample html file from: demo_data/data-sample-html.html and first open it with your browser. &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? This is how to get back to the course page: &lt;a href=&quot;https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/&quot;, target=&quot;_blank&quot;&gt;ENC2036&lt;/a&gt;. &lt;/p&gt; &lt;h1&gt; Contents of the Page &lt;/h1&gt; &lt;p&gt; Anything you can say about the page.....&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; An HTML document includes several important elements (cf. Figure 3.1): DTD: document type definition which informs the browser about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= \"index.html\"&gt; Homepage &lt;/a&gt;). They are expressed as name = \"value\" pairs. Figure 3.1: Syntax of An HTML Tag Element An HTML document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the webpage textual contents would go into the &lt;body&gt; part. Most of the web-related codes and metadata (e.g., javascripts, CSS) are often included in the &lt;head&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in Figure 3.2. Figure 3.2: Tree Structure of An HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags and elements. However, in order to scrape the textual data from the Internet, you need to know at least from which parts of HTML elements you need your textual data from on the web pages. Usually, before you scrape the data from the webpage, bear the following questions in mind: From which HTML elements/tags would you like to extract the data for corpus construction? Do you need the textual content of the HTML element? Do you need a specific attribute of the HTML element? 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The idea is that CSS specifies the formats/styles of the HTML elements. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } You probably would wonder how to link a set of CSS style definitions to an HTML document. There are in general three ways: inline, internal and external. You can learn more about this in W3School.com. Here I will show you an example of the internal method. Below is a CSS style definition for &lt;h1&gt;. h1 { color: red; margin-bottom: 2em; } We can embed this within a &lt;style&gt;...&lt;/style&gt; element. Then you put the entire &lt;style&gt; element under &lt;head&gt; of the HTML file you would like to style. &lt;style&gt; h1 { color: red; margin-bottom: 1.5em; } &lt;/style&gt; After you include the &lt;style&gt; in the HTML file, refresh the web page to see if the CSS style works. 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In the following demonstration, the text data scraped from the PTT forum is presented as it is without adjustment. However, please note that the language on PTT may strike some readers as profane, vulgar or even offensive. library(tidyverse) library(rvest) In this tutorial, let’s assume that we like to scrape texts from PTT Forum. In particular, we will demonstrate how to scrape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create an session() (like we open a browser linking to the page) gossiping.session &lt;- session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created session() and create another session. gossiping &lt;- session_submit( x = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html Status: 200 Type: text/html; charset=utf-8 Size: 25745 Now our html session, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest [1] 39178 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% session_jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178530.A.39F.html&quot; [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178609.A.FDD.html&quot; [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178614.A.F04.html&quot; [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178677.A.876.html&quot; [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178703.A.2BE.html&quot; [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178776.A.74D.html&quot; [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178796.A.D9F.html&quot; [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178824.A.226.html&quot; [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178849.A.A34.html&quot; [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178980.A.0C6.html&quot; [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179060.A.92A.html&quot; [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179072.A.6F3.html&quot; [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179272.A.A2B.html&quot; [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179276.A.1B8.html&quot; [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179366.A.82F.html&quot; [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179388.A.2C0.html&quot; [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179469.A.1C2.html&quot; [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179472.A.7BE.html&quot; [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179515.A.0E3.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% session_jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. Because we are interested in the metadata and the contents of each article, now the question is: where are they in the HTML? We need to go back to the source page of the article HTML again: After a closer inspection of the article HTML, we know that: The metadata of the article are included in &lt;span&gt; tag elements, belonging to the class class=\"article-meta-value\" The contents of the article are included in the &lt;div&gt; element, whose ID is ID=\"main-content\" Now we are ready to extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() article.header [1] &quot;a520 (520)&quot; &quot;Gossiping&quot; [3] &quot;Re: [問卦] 為什麼不限制買房次數&quot; &quot;Wed Mar 2 07:48:42 2022&quot; The metadata of each PTT article in fact includes four pieces of information: author, board name, title, post time. The above code retrieves directly the values of these metadata. We can retrieve the tags of these metadata values as well: temp.html %&gt;% html_nodes(&quot;span.article-meta-tag&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() [1] &quot;作者&quot; &quot;看板&quot; &quot;標題&quot; &quot;時間&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author [1] &quot;a520&quot; article.title [1] &quot;Re: [問卦] 為什麼不限制買房次數&quot; article.datetime [1] &quot;Wed Mar 2 07:48:42 2022&quot; Now we extract the main contents of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content [1] &quot;: 大家都在罵炒房害了年輕人全世界實坪制\\n\\n看好了世界台灣依然虛坪制: 那如果房價是正常幅度漲價看好了世界台灣持續示範房價只漲不跌: 不動產不能作為投資標的看好了世界你們永遠趕不上台灣炒房客: 每人一輩子只能買房3次你是被某些人約制了吧.......\\n\\n被人洗腦了事實上做幾件事台灣房價就崩盤\\n\\n但不能做因為都有收錢不能背叛懂吧: 3次應該很夠了吧這樣人頭戶會暴增\\n\\n而且某些人要炒房很不方便\\n\\n你這樣講很不健康: 價錢應該就會掉回正常價格薪資 物價 房價 不成正比意圖令人複習\\n\\n某詐騙集團首腦講的\\n\\n這幾年你過得好嗎？你買得起房子嗎？油價、奶粉、泡麵、學費一直漲，你受得了嗎？你找到你要找的工作了嗎？你的薪水有增加嗎？你是不是覺得越來越窮、越來越累？再幾年你受得了嗎？給自己一個改變的機會，給蔡英文一個改變台灣的機會，某月某號實現公平正義\\n\\n呵呵 講幹話大家都會\\n\\n科科\\n\\n\\n\\n\\n-----\\nSent from JPTT on my Samsung SM-G9910.\\n\\n--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt;. These children &lt;div&gt; or &lt;span class=“f2”&gt; of the &lt;div id = “main-content”&gt; include the push comments (推文) of the article, which are not the main content of the article. Now we can combine all information related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push {xml_nodeset (2)} [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f3 ... [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f3 ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag [1] &quot;→&quot; &quot;→&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author [1] &quot;greensaru&quot; &quot;a520&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content [1] &quot;: 火車頭，打不得&quot; &quot;: 哈哈哈 不能說的秘密笑死&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime [1] &quot;223.137.213.87 03/02 07:49&quot; &quot;49.216.138.241 03/02 07:58&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to collect text data in large amounts: For each index page, we need to extract all the article hyperlinks of the page. For each article hyperlink, we need to extract the article content, metadata, and the push comments. So, it would be great if we can wrap these two routines into two functions. 3.3.1 extract_art_links() extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. It returns a vector of article links. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% session_jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } For example, we can extract all the article links from the most recent index page: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Get all article links from the most recent index page cur_art_links &lt;-extract_art_links(cur_index_page, gossiping) cur_art_links [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178530.A.39F.html&quot; [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178609.A.FDD.html&quot; [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178614.A.F04.html&quot; [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178677.A.876.html&quot; [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178703.A.2BE.html&quot; [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178776.A.74D.html&quot; [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178796.A.D9F.html&quot; [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178824.A.226.html&quot; [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178849.A.A34.html&quot; [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646178980.A.0C6.html&quot; [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179060.A.92A.html&quot; [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179072.A.6F3.html&quot; [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179272.A.A2B.html&quot; [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179276.A.1B8.html&quot; [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179366.A.82F.html&quot; [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179388.A.2C0.html&quot; [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179469.A.1C2.html&quot; [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179472.A.7BE.html&quot; [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1646179515.A.0E3.html&quot; 3.3.2 extract_article_push_tables() extract_article_push_tables(): This function takes an article link link as the argument and extracts the metadata, textual contents, and pushes of the article. It returns a list of two elements—article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% session_jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge push table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc For example, we can get the article and push tables from the first article link: extract_article_push_tables(cur_art_links[1]) $article.table # A tibble: 1 × 5 datetime title author content url &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Wed Mar 2 07:48:42 2022 Re: [問卦] 為什麼不限制買房次數 a520 &quot;: 大家… http… $push.table # A tibble: 2 × 5 tag author content datetime url &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 → greensaru : 火車頭，打不得 223.137.213.87 03/02 07:49 https://… 2 → a520 : 哈哈哈 不能說的秘密笑死 49.216.138.241 03/02 07:58 https://… 3.3.3 Streamline the Codes Now we can simplify our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scrape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # number of articles on this index page length(ptt_data) [1] 19 # Check the first contents of 1st hyperlink ptt_data[[1]]$article.table ptt_data[[1]]$push.table Finally, the last thing we can do is to combine all article tables from each index page into one; and all push tables into one for later analysis. # Merge all article.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows # Merge all push.tables into one push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph summarizes our work flowchart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scraped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resources Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the sustainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata representation of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Technical parts for corpus creation (cf. Sinclair’s Appendix) 3.6 Final Remarks Please pay attention to the ethical aspects involved in the process of web crawling (esp. with personal private matters). If the website has their own API built specifically for one to gather data, use it instead of scraping. Always read the terms and conditions provided by the website regarding data gathering. Always be gentle with the data scraping (e.g., off-peak hours, spacing out the requests) Value the data you gather and treat the data with respect. Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Your script may look something like: # I define my own `scrapePTT()` function: # ptt_url: specify the board to scrape texts from # num_index_page: specify the number of index pages to be scraped # return: list(article, push) PTT_data &lt;-scrapePTT(ptt_url = &quot;https://www.ptt.cc/bbs/Gossiping&quot;, num_index_page = 3) PTT_data$article %&gt;% head PTT_data$push %&gt;% head # corpus size PTT_data$article$content %&gt;% nchar %&gt;% sum [1] 16433 Exercise 3.3 Please choose a website (other than PTT) you are interested in and demonstrate how you can use R to retrieve textual data from the site. The final scraped text collection could be from only one static web page. This purpose of this exercise is to show that you know how to parse the HTML structure of the web page and retrieve the data you need from the website. References Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. "],["corpus-analysis-a-start.html", "Chapter 4 Corpus Analysis: A Start 4.1 Installing quanteda 4.2 Building a corpus from character vector 4.3 Keyword-in-Context (KWIC) 4.4 KWIC with Regular Expressions 4.5 Tidy Text Format of the Corpus 4.6 Processing Flowchart 4.7 Frequency Lists 4.8 Word Cloud 4.9 Collocations 4.10 Constructions", " Chapter 4 Corpus Analysis: A Start In this chapter, I will demonstrate how to perform a basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 4.1 Installing quanteda There are many packages that are made for computational text analytics in R. You may consult the CRAN Task View: Natural Language Processing for a lot more alternatives. To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. library(tidyverse) library(quanteda) library(readtext) library(tidytext) packageVersion(&quot;quanteda&quot;) [1] &#39;3.2.0&#39; 4.2 Building a corpus from character vector To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. data_corpus_inaugural Corpus consisting of 59 documents and 4 docvars. 1789-Washington : &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot; 1793-Washington : &quot;Fellow citizens, I am again called upon by the voice of my c...&quot; 1797-Adams : &quot;When it was first perceived, in early times, that no middle ...&quot; 1801-Jefferson : &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot; 1805-Jefferson : &quot;Proceeding, fellow citizens, to that qualification which the...&quot; 1809-Madison : &quot;Unwilling to depart from examples of the most revered author...&quot; [ reached max_ndoc ... 53 more documents ] class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; We create a corpus() object with the pre-loaded corpus in quanteda– data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) # save the `corpus` to a short obj name After the corpus is loaded, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. summary(corp_us) require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() Exercise 4.1 Could you reproduce the above line plot and add information of President to the plot as labels of the dots? Hints: Please check ggplot2::geom_text() or more advanced one, ggrepel::geom_text_repel() So the idea is that as long as you can load the text data into a character vector, you can easily create an corpus object with quanteda::corpus(). The library readtext provides a very effective function readtext() for you to load text data from external files. Please check its documentation for more effective usages. For example, if you have downloaded the file corp-alice.txt and stored it in demo_data, you can load in the file as follows: alice &lt;- readtext(file = &quot;demo_data/corp-alice.txt&quot;) alice.corpus &lt;- corpus(alice) summary(alice.corpus) 4.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examining how it is being used in a wider context. We first tokenize the corpus using tokens() and then we can use kwic() to perform a search for a word and retrieve its concordances from the corpus: ## word tokenization corp_us_tokens &lt;- tokens(corp_us) ## concordances kwic(corp_us_tokens, &quot;terror&quot;) kwic() returns a data frame, which can be easily exported to a CSV file for later use. Please note that kwic(), when taking a corpus object as the argument, will automatically tokenize the corpus data and do the keyword-in-context search on a word basis. Yet, the recommended way is to tokenize the corpus object first with tokens() before you perform the concordance analysis with kwic(). The pattern you look for cannot be a linguistic pattern across several words. We will talk about how to extract phrasal patterns/constructions later. Also, for languages without explicit word boundaries (e.g., Chinese), this may be a problem with quanteda. We will talk more about this in the later chapter on Chinese Texts Analytics. 4.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can create a regular expression for the concordances. kwic(corp_us_tokens, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us_tokens, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. Exercise 4.2 Please create a bar plot, showing the number of uses of the word country in each president’s address. Please include different variants of the word, e.g., countries, Countries, Country, in your kwic() search. 4.5 Tidy Text Format of the Corpus So far our corpus is a corpus object defined in quanteda. In most of the R standard packages, people normally follow the using tidy data principles to make handling data easier and more effective. As described by Hadley Wickham (Wickham &amp; Grolemund, 2017), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table Essentially, it is an idea of making an abstract object (i.e., corpus) a more intuitive data structure, i.e., a data.frame, which is easier for human readers to work with. With text data like a corpus, we can also define the tidy text format as being a data.frame with one-token-per-row. A token can be any meaningful unit of the text, such as a word that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. In computational text analytics, the token (i.e., each row in the data frame) is most often a single word, but can also be an n-gram, a sentence, or a paragraph. The tidytext package in R is made for the handling of the tidy text format of the corpus data. With a tidy data format of the corpus, we can manipulate the text data with a standard set of tidy tools and packages, including dplyr, tidyr, and ggplot2. The tidytext package includes a function, tidy(), to convert the corpus object from quanteda into a document/text-based data.frame. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) # convert `corpus` to `data.frame` class(corp_us_tidy) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.6 Processing Flowchart Figure 4.1: Computational Text Processing Flowchart 4.7 Frequency Lists 4.7.1 Word (Unigram) To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages (e.g., the semantics of the documents). The tidytext provides a powerful function, unnest_tokens() to tokenize a data frame with larger linguistic units (e.g., texts) into one with smaller units (e.g., words). That is, the unnest_tokens() convert a text-based data frame (each row is a text document) into a token-based data frame(each row is a token splitted from the text). corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(output = word, # new base unit column name input = text, # original base unit column name token = &quot;words&quot;) # tokenization method corp_us_words The unnest_tokens() is optimized for English tokenization of smaller linguistic units, such as words, ngrams, sentences, lines, and paragraphs (check ?unnest_tokens()). To handle Chinese data, however, we need to be more careful. We probably need to define own ways of tokenization method in unnest_tokens(…, token = …). We will discuss the principles for Chinese text processing in a later chapter. Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words,” strip_punct = F, strip_numeric = F). Now we can count the word frequencies by making use of the dplyr library: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort = TRUE) corp_us_words_freq 4.7.2 Bigrams Frequency lists can be generated for bigrams or any other multiword combinations as well. The key is we need to convert the text-based data frame into a bigram-based data frame. corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens( output = bigram, # new base unit column name input = text, # original base unit column name token = &quot;ngrams&quot;, # tokenization method n = 2 ) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) # size of unigrams [1] 137939 sum(corp_us_bigrams_freq$n) # size of bigrams [1] 137880 Exercise 4.3 Based on the bigram-based data frame, how can we create a frequency list showing each president’s uses of bigrams with the first-person plural pronoun we as the first word. Arrange the frequency list according to the year, the president’s last name, and the token frequency of the bigram. Exercise 4.4 The function unnest_tokens() does a lot of work behind the scene. Please take a closer look at the outputs of unnest_tokens() and examine how it takes care of the case normalization and punctuations within the sentence. Will these treatments affect the frequency lists we get in any important way? Please elaborate. 4.7.3 Ngrams (Lexical Bundles) corp_us_trigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigrams We then can examine which n-grams were most often used by each President: corp_us_trigrams %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) %&gt;% arrange(President, desc(n)) Exercise 4.5 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. 4.7.4 Frequency and Dispersion When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram can be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by many different Presidents? The degrees of n-gram dispersion has a lot to do with the significance of its frequency. So now let’s compute the dispersion of the n-grams in our corp_us_trigrams. Here we define the dispersion of an n-gram as the number of Presidents who have used the n-gram at least once in his address(es). # method 1 corp_us_trigrams %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = sum(n), DISPERSION = n()) %&gt;% filter(DISPERSION &gt;= 5) %&gt;% arrange(desc(DISPERSION)) # method2 corp_us_trigrams %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = n(), DISPERSION = n_distinct(President)) %&gt;% filter(DISPERSION &gt;= 5) %&gt;% arrange(desc(DISPERSION)) # Arrange according to frequency # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) In particular, cut-off values are often used to determine a list of meaningful n-grams. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. A subset of n-grams that are defined and selected based on these distributional criteria (i.e., frequency and dispersion) are often referred to as Lexical bundles (See Biber et al. (2004)). Exercise 4.6 Please create a list of four-word lexical bundles that have been used in at least FIVE different presidential addressess. Arrange the resulting data frame according to the frequency of the four-grams. 4.8 Word Cloud With frequency data, we can visualize important words in the corpus with a Word Cloud. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts. library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 400, min.freq = 30, scale = c(2,0.5), color = brewer.pal(8, &quot;Dark2&quot;), vfont=c(&quot;serif&quot;,&quot;plain&quot;))) Exercise 4.7 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. (Criteria: Frequency &gt;= 20; Max Number of Words Plotted = 400) Hint: Check dplyr::anti_join() require(tidytext) stop_words Exercise 4.8 Get yourself familiar with another R package for creating word clouds, wordcloud2, and re-create a word cloud as requested in Exercise 4.7 but in a fancier format, i.e., a star-shaped one. (Criteria: Frequency &gt;= 20; Max Number of Words Plotted = 400) 4.9 Collocations With unigram and bigram frequencies of the corpus, we can further examine the collocations within the corpus. Collocation refers to a frequent phenomenon where two words tend to co-occur very often in use. This co-occurrence is defined statistically by their lexical associations. 4.9.1 Cooccurrence Table and Observed Frequencies Cooccurrence frequency data for a word pair, w1 and w2, are often organized in a contingency table extracted from a corpus, as shown in Figure 4.2. The cell counts of this contingency table are referred to as the observed frequencies O11, O12, O21, and O22. Figure 4.2: Cooccurrence Freqeucny Table The sum of all four observed frequencies (called the sample size N) is equal to the total number of bigrams extracted from the corpus. And before we discuss the computation of lexical associations, there are a few terms that we often use when talking about the contingency table. R1 and R2 are the row totals of the observed contingency table, while C1 and C2 are the corresponding column totals. These row and column totals are referred to as marginal frequencies (because they are often written on the margins of the table) The frequency in O11 is referred to as the joint frequency of the two words. 4.9.2 Expected Frequencies For every contingency table as seen above, if one knows the marginal frequencies (i.e., the row and column cums), one can compute the expected frequencies of the four cells accordingly. These expected frequencies would be the expected distribution under the null hypothesis that W1 and W2 are statistically independent. And the idea of lexical association between W1 and W2 is to statistically access to what extent the observed frequencies in the contingency table are different from the expected frequencies (given the current the marginal frequencies). Therefore, equations for different association measures (i.e., mutual information, log-likelihood ratios, chi-square) are often given in terms of the observed frequencies, marginal frequencies, and the expected frequencies E11, …, E22. Please see Stefan Evert’s Computational Approaches to Collocation for a very detailed and comprehensive comparison of various statistical methods for lexical association. The expected frequencies can be computed from the marginal frequencies as shown in Figure 4.3. Figure 4.3: Computing Expected Frequencies Maybe it would be easier for us to illustrate this with a simple example: Figure 4.4: Computing Expected Frequencies How do we compute the expected frequencies of the four cells? Figure 4.5: Computing Expected Frequencies example &lt;- matrix(c(90, 10, 110, 290), byrow=T, nrow=2) Exercise 4.9 Please compute the expected frequencies for the above matrix example in R. 4.9.3 Association Measures The idea of lexical assoication is to measure how much the observed frequencies deviate from the expected. Some of the metrics (e.g., t-statistic, MI) consider only the joint frequency deviation (i.e., O11), while others (e.g., G2, a.k.a Log Likelihood Ratio) consider the deviations of ALL cells. Here I would like to show you how we can compute the most common two asssociation metrics for all the bigrams found in the corpus: t-test statistic and Mutual Information (MI). \\(t = \\frac{O_{11}-E_{11}}{\\sqrt{O_{11}}}\\) \\(MI = log_2\\frac{O_{11}}{E_{11}}\\) \\(G^2 = 2 \\sum_{ij}{O_{ij}log\\frac{O_{ij}}{E_{ij}}}\\) corp_us_bigrams_freq %&gt;% head(10) corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off rename(O11 = n) %&gt;% tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;\\\\s&quot;) %&gt;% # split bigrams into two columns mutate(R1 = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], C1 = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), t = (O11 - E11)/sqrt(O11)) %&gt;% # compute associations arrange(desc(MI)) # sorting corp_us_collocations Please note that in the above example, we compute the lexical associations for bigrams whose frequency &gt; 5. This is necessary in collocation studies because bigrams of very low frequency would not be informative even though its association can be very strong. However, the cut-off value can be arbitrary, depending on the corpus size or researchers’ considerations. How to compute lexical associations is a non-trivial issue. There are many more ways to compute the association strengths between two words. Please refer to Stefan Evert’s site for a very comprehensive review of lexical association measures. Probably the recommended method is G2 (Stefanowitsch, 2019). Exercise 4.10 Sort the collocation data frame corp_us_collocations according to the t-score and compare the results sorted by MI scores. Please describe the differences between the bigram collocations found with both metrics (i.e., MI and t-score). Exercise 4.11 Based on the formula provided above, please create a new column for corp_us_collocations, which gives the Log-Likelihood Ratios of all the bigrams. When you do the above exercise, you may run into a couple of problems: Some of the bigrams have NaN values in their LLR. This may be due to the issue of NAs produced by integer overflow. Please solve this. After solving the above overflow issue, you may still have a few bigrams with NaN in their LLR, which may be due to the computation of the log value. In Math, how do we define log(1/0) and log(0/1)? Do you know when you would get an undefined value NaN in the computation of log()? To solve the problems, please assign the value 0 if the log returns NaN values. Exercise 4.12 Find the top FIVE bigrams ranked according to MI values for each president. The result would be a data frame as shown below. Create a plot as shown below to visualize your results. 4.10 Constructions We are often interested in the use of linguistic patterns, which are beyond the lexical boundaries. My experience is that usually it is better to work with the corpus on a sentential level. We can use the same tokenization function, unnest_tokens() to convert our text-based corpus data frame, corpus_us_tidy, into a sentence-based tidy structure: corp_us_sents &lt;- corp_us_tidy %&gt;% unnest_tokens(output = sentence, input = text, token = &quot;sentences&quot;) # tokenize the `text` column into `sentence` corp_us_sents With each sentence, we can investigate particular constructions in more detail. Let’s assume that we are interested in the use of Perfect aspect in English by different presidents. We can try to extract Perfect constructions (including Present/Past Perfect) from each sentence using the regular expression. Here we make a simple naive assumption: Perfect constructions include all have/has/had + ...-en/ed tokens from the sentences. require(stringr) # Perfect corp_us_sents %&gt;% unnest_tokens( perfect, sentence, token = function(x) str_extract_all(x, &quot;ha(d|ve|s) \\\\w+(en|ed)&quot;) ) -&gt; result_perfect result_perfect In the above example, we specify the token = argument in unnest_tokens(…, token = …) with a self-defined function. The idea of tokenization in unnest_tokens() is that the token argument can be a function which takes a text-based vector as input (i.e, each element of the input vector may be a document text) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (see Figure below). In our demonstration, we define a tokenization function, which takes sentence as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions in individual sentences in sentence. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Figure 4.6: Intuition for token= in unnest_tokens() And of course we can do an exploratory analysis of the frequencies of Perfect constructions by different presidents: require(tidyr) # table result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) # graph result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) %&gt;% pivot_longer(c(&quot;TOKEN_FREQ&quot;, &quot;TYPE_FREQ&quot;), names_to = &quot;STATISTIC&quot;, values_to = &quot;NUMBER&quot;) %&gt;% ggplot(aes(President, NUMBER, fill = STATISTIC)) + geom_bar(stat = &quot;identity&quot;,position = position_dodge()) + theme(axis.text.x = element_text(angle=90)) There are quite a few things we need to take care of more thoroughly: The auxiliary HAVE and the past participle do not necessarily have to stand next to each other for Perfect constructions. We now lose track of one important information: from which sentence of the Presidential addresses was each construction token extracted? Any ideas how to solve all these issues? The following exercises will be devoted to these two important issues. Exercise 4.13 Please create a better regular expression to retrieve more tokens of English Perfect constructions, where the auxilliary and participle may not stand together. Exercise 4.14 Re-generate a result_perfect data frame, where you can keep track of: From the N-th sentence of the address did the Perfect come? (e.g., SENT_ID column below) From which president’s address did the Perfect come? (e.g., INDEX column below) You may have a data frame as shown below. Exercise 4.15 Re-create the above bar plot in a way that the type and token frequencies are computed based on each address and the x axis should be arranged accordingly (i.e., by the years and presidents). Your resulting graph should look similar to the one below. References Biber, D., Conrad, S., &amp; Cortes, V. (2004). If you look at…: Lexical bundles in university teaching and textbooks. Applied Linguistics, 25(3), 371–405. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. "],["parts-of-speech-tagging.html", "Chapter 5 Parts-of-Speech Tagging 5.1 Installing the Package 5.2 Quick Overview 5.3 Working Pipeline 5.4 Parsing Your Texts 5.5 Metalingusitic Analysis 5.6 Construction Analysis 5.7 Issues on Pattern Retrieval 5.8 Saving POS-tagged Texts 5.9 Finalize spaCy 5.10 Notes on Chinese Processing", " Chapter 5 Parts-of-Speech Tagging library(tidyverse) library(tidytext) library(quanteda) In many textual analyses, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. In particular, I will introduce a powerful package spacyr, which is an R wrapper to the spaCy— “industrial strength natural language processing” Python library from https://spacy.io. In addition to POS tagging, the package provides other linguistically relevant annotations for more in-depth analysis of the English texts. Again, the spaCy is optimized for many languages but Chinese. We will talk about Chinese text processing in a later chapter. 5.1 Installing the Package Please consult the spacyr github for more instructions on installing the package. There are at least four steps: Install and set up a python environment (e.g., miniconda, or Anaconda) that you would like to use in R. Because spacyr is an R wrapper to a Python package spaCy, we need to install the python module (and the language model files) in your python environment first. Install the spacyr R package. install.packages(&quot;spacyr&quot;) It is recommended that you have Python 3.6+ and spacy 3+ at least. My spacy Version: 3.0.5 If you have installed Anaconda and created python conda environments previously, you don’t have to install the python spaCy module within R. It’s better for you to install the python module in a pythonian way. You can install the python spacy and the language models in the python conda environment you would like to use in R. From within RStudio, you should be able to use spacy from that conda environment directly. If you haven’t used Python before, the easiest way to install Python spaCy is to install it in Rstudio through the R function spacyr::spacy_install(). This function by default creates a new conda environment called spacy_condaenv, as long as some version of conda has been installed on the user’s the system. library(spacyr) spacy_install(version=&#39;3.0.5&#39;) ## Please note that there are big differences between spacy v2 and v3. The default settings of spacy_install() is to: - create a stand-alone conda environment including a python executable separate from your system Python (or anaconda python); - install the latest version of spaCy (and its required packages); - download the English language model. Please check the documentation of spacy_instsall() if you would like to have more specific settings (e.g., spacy version, environment name, python version, language models etc.). To use the python module, spacy, in R can be tricky. The key is you need to have a running python kernel in your OS system. Therefore, Step 1 is very important. If you don’t have any conda version installed in your system, you can install miniconda from https://conda.io/miniconda.html on your own (Choose the 64-bit version). Alternatively, the spacy_install() can also automatically install the miniconda (if there’s no conda installed in your system) for MAC users. Windows users may need to consult the spacyr github for more important instructions on installation (I am not sure). For Windows, you need to run RStudio as an administrator to make installation work properly. To do so, right click the RStudio icon (or R desktop icon) and select “Run as administrator” when launching RStudio. Restart R and Initialize spaCy in R In the following code, I initialize the spacyr python environment with my pre-configured conda environment (e.g., python-notes). You may need to either specify your self-defined conda environment (or python virtual environment) or leave all the parameters with default values (i.e., spacy_initialize()). library(spacyr) spacy_initialize(model = &quot;en_core_web_sm&quot;, condaenv = &quot;python-notes&quot;) successfully initialized (spaCy Version: 3.0.5, language model: en_core_web_sm) (python options: type = &quot;condaenv&quot;, value = &quot;python-notes&quot;) #spacy_initialize() 5.2 Quick Overview The spacyr provides a useful function, spacy_parse(), which allows us to parse an English text in a very convenient way. txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt, pos = T, tag = T, lemma = T, entity = T, dependency = T) parsedtxt The output parsedtext is a data frame, which includes annotations of the original texts at multiple granularities. All texts have been tokenized into words with each word, sentence, and text given an unique ID (i.e., doc_id, sentence_id, token_id) Lemmatization is also done (i.e., lemma) POS Tags can also be found (i.e., pos and tag) pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set (cf. Penn Treebank Tagset) Depending on the argument setting for spacy_parse(), you can get more annotations, such as named entities (entity) and dependency relations (del_rel). In SpaCy, the English part-of-speech tagger uses the OntoNotes 5 version of the Penn Treebank tag set. It also maps the tags to the simpler Universal Dependencies v2 POS tag set. The following table shows the descriptions of the tag set. 5.3 Working Pipeline In Chapter 4, we provide a primitive working pipeline for text analytics. Here we like to revise the workflow to satisfy different goals in computational text analytics (See Figure 5.1). After we secure a collection of raw texts as our corpus, if we do not need additional parts-of-speech information, we follow the workflow on the right. If we need additional annotations from spacyr, we follow the workflow on the left. Figure 5.1: English Text Analytics Flowchart 5.4 Parsing Your Texts Now let’s use this spacy_parse() to analyze the presidential addresses we’ve seen in Chapter 4: the data_corpus_inaugural from quanteda. To illustrate the annotation more clearly, let’s parse the first text in data_corpus_inaugural: library(quanteda) library(tidytext) doc1 &lt;- spacy_parse(data_corpus_inaugural[1]) doc1 We can parse the whole corpus collection as well. The spacy_parse() can take a character vector as the input, where each element is a text/document of the corpus. system.time(corp_us_words &lt;- spacy_parse(data_corpus_inaugural)) user system elapsed 12.834 2.598 15.435 The function system.time() is a useful function which gives you the CPU times that the expression in the parenthesis used. In other words, you can put any R expression in the parenthesis of system.time() as its argument and measure the time required for the expression. This is sometimes necessary because some of the data processing can be very time consuming. And we would like to know HOW time-consuming it is in case that we may need to run the prodecure again. corp_us_words Exercise 5.1 In corpus linguistics analysis, we often need to examine constructions on a sentential level. It would be great if we can transform the word-based data frame into a sentence-based one for more efficient later analysis. Also, on the sentential level, it would be great if we can preserve the information of the lexical POS tags. How can you transform the corp_us_words into one as provided below? (You may name the sentence-based data frame as corp_us_sents.) 5.5 Metalingusitic Analysis Now spacy_parse() has enriched our corpus data with more linguistic annotations. We can now utilize the additional POS tags for more analysis. In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s\\;C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_words and first generate the frequencies of verbs, and number of words for each presidential speech text. syn_com &lt;- corp_us_words %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) %&gt;% ungroup syn_com With the syntactic complexity of each president, we can plot the tendency: syn_com %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = &quot;none&quot;) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 5.2 Please add a regression/smooth line to the above plot to indicate the downward trend? 5.6 Construction Analysis Now with parts-of-speech tags, we are able to look at more linguistic patterns or constructions in detail. These POS tags allow us to extract more precisely the target patterns we are interested in. In this section, we will use the output from Exercise 5.1. We assume that now we have a sentence-based corpus data frame, corp_us_sents. Here I like to provide a case study on English Preposition Phrases. ## ###################################### ## If you haven&#39;t finished the exercise, ## the dataset is also available in ## `demo_data/corp_us_sents.RDS ## ###################################### ## Uncomment this line if you dont have `corp_us_sents` # corp_us_sents &lt;- readRDS(&quot;demo_data/corp_us_sents.RDS&quot;) corp_us_sents We can utilize the regular expressions to extract PREP + NOUN combinations from the corpus data. # define regex patterns pattern_pat1 &lt;- &quot;[^/ ]+/ADP [^/]+/NOUN&quot; # extract patterns from corp corp_us_sents %&gt;% unnest_tokens(output = pat_pp, input = sentence_tag, token = function(x) str_extract_all(x, pattern=pattern_pat1)) -&gt; result_pat1 result_pat1 In the above example, we specify the token= argument in unnest_tokens(..., token = ...) with a self-defined function. The idea of tokenization in unnest_tokens() is that the token argument should be a function which takes a text-based vector as input (i.e, each element of the input vector may be a document text) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (cf. Figure 5.2). Figure 5.2: Intuition for token= in unnest_tokens() In our demonstration, we define a tokenization function, which takes sentence_tag as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions in individual sentences in sentence_tag. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Exercise 5.3 Create a new column, pat_clean, with all annotations removed in the data frame result_pat1. With these constructional tokens of English PP’s, we can then do further analysis. We first identify the PREP and NOUN for each constructional token. We then clean up the data by removing POS annotations. # extract the prep and head result_pat1 %&gt;% tidyr::separate(col=&quot;pat_pp&quot;, into=c(&quot;PREP&quot;,&quot;NOUN&quot;), sep=&quot;\\\\s+&quot; ) %&gt;% mutate(PREP = str_replace_all(PREP, &quot;/[^ ]+&quot;,&quot;&quot;), NOUN = str_replace_all(NOUN, &quot;/[^ ]+&quot;,&quot;&quot;)) -&gt; result_pat1a result_pat1a Now we are ready to explore the text data. We can look at how each preposition is being used by different presidents: # President Top 2 prep result_pat1a %&gt;% count(doc_id, PREP) %&gt;% arrange(doc_id, desc(n)) We can examine the most frequent NOUN that co-occurs with each PREP: # Most freq NOUN for each PREP result_pat1a %&gt;% count(PREP, NOUN) %&gt;% group_by(PREP) %&gt;% top_n(1,n) %&gt;% arrange(desc(n)) We can also look at a more complex usage pattern: how each president uses the PREP of in terms of their co-occurring NOUNs? # NOUNS for `of` uses across different presidents result_pat1a %&gt;% filter(PREP == &quot;of&quot;) %&gt;% count(doc_id, PREP, NOUN) %&gt;% tidyr::pivot_wider( id_cols = c(&quot;doc_id&quot;), names_from = &quot;NOUN&quot;, values_from = &quot;n&quot;, values_fill = list(n=0)) Exercise 5.4 In our earlier demonstration, we made a naive assumption: Preposition Phrases include only those cases where PREP and NOUN are adjacent to each other. But there are many more tokens where words do come between the PREP and the NOUN (e.g., with greater anxieties, by your order). Please revise the regular expression to improve the retrieval of the English Preposition Phrases from the corpus data corp_us_sents. Specifically, we can define an English PP as a sequence of words, which start with a preposition, and end at the first word after the preposition that is tagged as NOUN, PROPN, or PRON. Exercise 5.5 Based on the output from Exercise 5.4, please identify the PREP and NOUN for each constructional token and save information in two new columns. 5.7 Issues on Pattern Retrieval Any automatic pattern retrieval comes with a price: there are always errors returned by the system. I would like to discuss this issue based on the second text, 1793-Washington. First let’s take a look at the Preposition Phrases extracted by my regular expression used in Exercise 5.4 and 5.5: ## ###################################### ## If you haven&#39;t finished the exercise, ## the dataset is also available in ## `demo_data/result_pat2a.RDS ## ###################################### ## uncomment this line if you dont have `result_pat2a` # result_pat2a &lt;- readRDS(&quot;demo_data/result_pat2a.RDS&quot;) result_pat2a %&gt;% filter(doc_id == &quot;1793-Washington&quot;) My regular expression has identified 20 PP’s from the text. However, if we go through the text carefully and do the PP annotation manually, we may have different results. Figure 5.3: Manual Annotation of English PP’s in 1793-Washington There are two types of errors: False Positives: Patterns identified by the system but in fact they are not true patterns. False Negatives: True patterns in the data but are not successfully identified by the system. As shown in Figure 5.3, manual annotations have identified 21 PP’s from the text while the regular expression identified 20 tokens. A comparison of the two results shows that: In the regex result, the following returned tokens (rows highlighted in red) are False Positives—the regular expression identified them as PP but in fact they were NOT PP according to manual annotations. doc_id sentence_id PREP NOUN pat_pp row_id 1793-Washington 1 by voice by/adp the/det voice/noun 1 1793-Washington 1 of my of/adp my/pron 2 1793-Washington 1 of its of/adp its/pron 3 1793-Washington 2 for it for/adp it/pron 4 1793-Washington 2 of honor of/adp this/det distinguished/adj honor/noun 5 1793-Washington 2 of confidence of/adp the/det confidence/noun 6 1793-Washington 2 in me in/adp me/pron 7 1793-Washington 2 by people by/adp the/det people/noun 8 1793-Washington 2 of america of/adp united/adj america/propn 9 1793-Washington 3 to execution to/adp the/det execution/noun 10 1793-Washington 3 of act of/adp any/det official/adj act/noun 11 1793-Washington 3 of president of/adp the/det president/propn 12 1793-Washington 3 of office of/adp office/noun 13 1793-Washington 4 in your in/adp your/pron 14 1793-Washington 4 during my during/adp my/pron 15 1793-Washington 4 of government of/adp the/det government/propn 16 1793-Washington 4 in instance in/adp any/det instance/noun 17 1793-Washington 4 to upbraidings to/adp the/det upbraidings/noun 18 1793-Washington 4 of who of/adp all/det who/pron 19 1793-Washington 4 of ceremony of/adp the/det present/adj solemn/adj ceremony/noun 20 In the above manual annotation (Figure 5.3), phrases highlighted in red are NOT successfully identified by the current regex query, i.e., False Negatives. We can summarize the pattern retrieval results as: Most importantly, we can describe the quality of the pattern retrieval with two important measures. \\(Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Positives}\\) \\(Recall = \\frac{True\\;Positives}{True\\;Positives + False\\;Negatives}\\) In our case: \\(Precision = \\frac{18}{18+2} = 90%\\) \\(Recall = \\frac{18}{18 + 3} = 85.71%\\) It is always very difficult to reach 100% precision or 100% recall for automatic retrieval of the target patterns. Researchers often need to make a compromise. The following are some heuristics based on my experiences: For small datasets, probably manual annotations give the best result. For moderate-sized dataset, semi-automatic annotations may help. Do the automatic annotations first and follow up with manual checkups. For large datasets, automatic annotations are preferred in order to examine the general tendency. However, it is always good to have a random sample of the data to check the query performance. The more semantics-related the annotations, the more likely one would adopt a manual approach to annotation (e.g., conceptual metaphors, sense distinctions, dialogue acts). Common annotations of corpus data may prefer an automatic approach, such as Chinese word segmentation, POS tagging, named entity recognition, chunking, noun-phrase extractions, or dependency relations(?). In medicine, there are two similar metrics used for the assessment of the diagnostic medical tests—sensitivity (靈敏度) and specificity (特異性). Sensitivity refers to the proportion of true positives that are correctly identified by the medical test. This is indeed the recall rates we introduced earlier. Specificity refers to the proportion of true negatives that are correctly identified by the medical test. It is computed as follows: \\(Specificity = \\frac{True\\;Negatives}{False\\;Positives + True\\;Negatives}\\) In plain English, the sensitivity of a medical test indicates the percentage of sick people who are correctly identified as having the disease; the specificity of a medical test indciates the percentage of healthy people who are correctly identified as healthy (i.e., not having the disease). It should be obvious which metric is more crucial to the control of a pandemic. 5.8 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time we analyze the data, it would be more convenient if we save the tokenized texts with the POS tags in external files. Next time we can directly load these files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. A few suggestions: If you are dealing with a small corpus, I would probably suggest you to save the resulting data frame from spacy_parse() as a csv for later use. If you are dealing with a big corpus, I would probably suggest you to save the parsed output of each text file in an independent csv for later use. write_csv(corp_us_words, &quot;corp-inaugural-word.csv&quot;) 5.9 Finalize spaCy While running spaCy on Python through R, a Python process is always running in the background and R session will take up a lot of memory (typically over 1.5GB). spacy_finalize() terminates the Python process and frees up the memory it was using. spacy_finalize() 5.10 Notes on Chinese Processing The library spacyr supports Chinese processing as well. The key is you need to download the Chinese language model from the original spaCy and make sure that the language model is accessible in the python environment you are using in R. ############################ ### Chinese Processing ## ############################ library(spacyr) spacy_initialize(model = &quot;zh_core_web_sm&quot;, condaenv = &quot;python-notes&quot;) txt_ch &lt;- c(d1 = &quot;2022年1月7日 週五 上午4:42·1 分鐘 (閱讀時間) 桃園機場群聚感染案件確診個案增，也讓原本近日展開的尾牙餐會受到波及，桃園市某一家飯店不斷接到退訂，甚至包括住房、圍爐等，也讓飯店人員感嘆好不容易看見有點復甦景象，一下子又被疫情打亂。（李明朝報導）&quot;, d2 = &quot;桃園機場群聚感染，隨著確診個案增加，疫情好像短時間無法終結，原本期待在春節期間能夠買氣恢復的旅宿業者，首當其衝大受波及，桃園市一家飯店人員表示，「1月6日就開始不斷接到消費者打來電話，包括春酒尾牙、圍爐桌席和住房，其中單單一個上午就有約20桌宴席、近百間房取消或延期。&quot;) parsedtxt_ch &lt;- spacy_parse(txt_ch, pos = T, tag = T, # lemma = T, entity = T, dependency = T) parsedtxt_ch Exercise 5.6 PTT Corpus Processing Exercise 5.7 In this exercise, please use the corpus data provided in quanteda.textmodels::data_corpus_moviereviews. This dataset is provided as a corpus object in the package quanteda.textmodels (please install the package on your own). The data_corpus_moviereviews includes 2,000 movie reviews. Please use the spacyr to parse the texts and provide the top 20 adjectives for positive and negative reviews respectively. Adjectives are naively defined as any words whose pos tags start with “J” (please use the fine-grained version of the POS tags. i.e., tag, from spacyr). When computing the word frequencies, please use the lemmas instead of the word forms. Please provide the top 20 words that are content words for positive and negative reviews ranked by a weighted score, which is computed using the formula provided below. Content words are naively defined as any words whose pos tags start with N, V, or J. (In my results below, there is one additional criterion for word selection: words whose first letter starts with a non-alphanumeric character are removed from the frequency list.) \\[Word\\;Frequency \\times log(\\frac{Numbe\\; of \\; Documents}{Word\\;Diserpsion}) \\] For example, if the lemma action occurs 691 times in the negative reviews collection. These occurrences are scattered in 337 different documents. There are 1000 negative texts in the current corpus. Then the wegithed score for action is: \\[691 \\times log(\\frac{1000}{337}) = 751.58 \\] summary(quanteda.textmodels::data_corpus_moviereviews) ans1 ans2 In our earlier chapters, we have discussed the issues of word frequencies and their significance in relation to the dispersion of the words in the entire corpus. In terms of identifying important words from a text collection, our assumption is that: if a word is scattered in almost every document in the corpus collection, it is probably less informative. For example, words like a, the would probably be observed in every document in the corpus. Therefore, the high frequencies of these widely-dispersed words may not be as important compared to the high frequencies of those which occur in only a subset of the corpus collection. The word frequency is sometimes referred to as term frequency (tf) in information retrieval; the dispersion of the word is referred to as document frequency (df). In information retrieval, people often use a weighting scheme for word frequencies in order to extract informative words from the text collection. The scheme is as follows: \\[tf \\times log(\\frac{N}{df}) \\] N refers to the total number of documents in the corpus. The \\(log\\frac{N}{df}\\) is referred to as inversed document frequency (idf). This tf.idf weighting scheme is popular in many practical applications. The smaller the df of a word, the higher the idf, the larger the weight for its tf. "],["keyword-analysis.html", "Chapter 6 Keyword Analysis 6.1 About Keywords 6.2 Statistics for Keyness 6.3 Implementation 6.4 Tidy Data 6.5 Word Frequency Transformation 6.6 Computing Keynesss 6.7 Conclusion", " Chapter 6 Keyword Analysis In this chapter, I would like to talk about the idea of kyewords. Keywords in corpus linguistics are defined statistically using different measures of keyness. Keyness can be computed for words occurring in a target corpus by comparing their frequencies (in the target corpus) to the frequencies in a reference corpus. In other words, the idea of “keyness” is to evaluate whether the word occurs more frequently in the target corpus as compared to its occurrence in the reference corpus. If yes, the word may be a key term of the target corpus. We can quantify the relative attraction of each word to the target corpus by means of a statistical association metric. This idea of course can be extended to key phrases as well. Therefore, for keyword analysis, we assume that there is a reference corpus on which the keyness of the words in the target corpus is computed and evaluated. 6.1 About Keywords Qualitative Approach Keywords are important for research on language and ideology. Most researchers draw inspiration from Raymond Williams’s idea of keywords, which he defines as terms presumably carrying socio-cultural meanings characteristic of (Western capitalist) ideologies (Williams, 1976). Keywords in Williams’ study were determined based on the subjective judgement of the socio-cultural meanings of the predefined list of words. Quantitative Approach In contrast to William’s intuition-based approach, recent studies have promoted a bottom-up corpus-based method to discover key terms reflecting the ideological undercurrents of particular text collections. This data-driven approach to keywords is sympathetic to the notion of statistical keywords popularized by Michael Stubbs (Stubbs, 1996, 2003) (with his famous toolkit, Wordsmith) For a comprehensive discussion on the statistical nature of keyword analysis, please see Gabrielatos (2018). 6.2 Statistics for Keyness To compute the keyness of a word w, we need two frequency numbers: the frequency of w in the target corpus vs. the frequency of w in the reference corpus. These frequencies are often included in a contingency table as shown in Figure 6.1: Figure 6.1: Frequency distributions of a word and all other words in two corpora What are the important factors that may be connected to the significance of the frequencies of the word in two corpora? the frequency of the word w in general the sizes of the target/reference corpus In other words, the marginal frequencies of the contingency table are crucial to determining the significance of the word frequencies in two corpora. Different keyness statistics may have different ways to evaluate the relative importance of the co-occurrences of the word w with the target and the reference corpus (i.e., a and b in Figure 6.1) and statistically determine which connection is stronger. In this chapter, I would like to discuss three common statistics used in keyness analysis. This tutorial is based on (gries2018?), Ch. 5.2.6. Log-likelihood Ratio (G2) (Dunning, 1993); \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Difference Coefficient (Leech &amp; Fallon, 1992); \\[ difference\\;coefficient = \\frac{a - b}{a + b} \\] Relative Frequency Ratio (Damerau, 1993) \\[ rfr = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] 6.3 Implementation In this tutorial we will use two documents as our mini reference and target corpus. These two documents are old Wikipedia entries (provided in (gries2018?)) on Perl and Python respectively. First we initialize necessary packages in R library(tidyverse) library(tidytext) library(readtext) library(quanteda) Then we load the corpus data, which are available as two text files in our demo_data, and transform the corpus into a tidy structure flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) %&gt;% select(textid, text) We convert the text-based corpus into a word-based data frame, which allows us to easily extract word frequency information corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% head(100) Exercise 6.1 We mentioned this before. It is always good to keep track of the relative positions of the word in the original text. Please create a column in corpus_word, indicating the the relative position of each word in the text with unique word_id. (NB: Only the first 100 rows are shown here.) Now we need to get the frequencies of each word in the two corpora respectively. corpus_word %&gt;% count(word, textid, sort=T) -&gt; word_freq word_freq As now we are analyzing each word in relation to the two corpora, it would be better for us to have each word type as one independent row, and columns recording their co-occurrence frequencies with the two corpora (i.e., target and reference). How to achieve this? 6.4 Tidy Data Now I would like to talk about the idea of tidy dataset before we move on. Wickham &amp; Grolemund (2017) suggests that a tidy dataset needs to satisfy the following three interrelated principles: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In our current word_freq, our observation in each row is a word. This is OK because a tidy dataset needs to have every independent word (type) as an independent row in the table. However, there are two additional issues with our current data frame word_freq: Each row in word_freq in fact represents the combination of two factors, i.e., word, textid. In addition, the column n contains the token frequencies for each level combination of these two (nominal) variables. The same word type appears twice in the dataset in the rows (e.g., the, a) This is the real life: we often encounter datasets that are NOT tidy at all. Instead of expecting others to always provide you a perfect tidy dataset for analysis (which is very unlikely), we might as well learn how to deal with messy dataset. Wickham &amp; Grolemund (2017) suggest two common strategies that data scientists often apply: Long-to-Wide: One variable might be spread across multiple columns Create more variables/columns based on one old variable Wide-to-Long: One observation might be scattered across multiple rows Reduce several variables/columns by collapsing them into levels of a new variable 6.4.1 An Long-to-Wide Example Here I would like to illustrate the idea of Long-to-Wide transformation with a simple dataset from Wickham &amp; Grolemund (2017), Chapter 12. people &lt;- tribble( ~name, ~profile, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) people The above dataset people is not tidy because: the column profile contains more than one variable; an observation (e.g., Phillip Woods) is scattered across several rows. To tidy up people, we can apply the Long-to-Wide strategy: One variable might be spread across multiple columns The function tidyr::pivot_wider() is made for this. There are two important parameters in pivot_wider(): names_from = ...: The column from which we create new variable names. Here it’s profile. values_from = ...: The column from which we take values for new columns. Here it’s values. Figure 6.2: From Long to Wide: pivot_wider() We used pivot_wider() to transform people into a wide-format data frame. require(tidyr) people %&gt;% pivot_wider(names_from = profile, values_from = values) 6.4.2 A Wide-to-Long Example Now let’s take a look at an example of the second strategy, Long-to-Wide transformation. When do we need this? This type of data transformation is often needed when you see that some of your columns/variables are in fact connected to the same factor. That is, these columns in fact can be considered levels of another underlying factor. Take the following simple case for instance. preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) preg The dataset preg has three columns: pregnant, male, and female. Of particular interest here are the last two columns. It is clear that male and female can be considered levels of another underlying factor, i.e., gender. More specifically, the above dataset preg can be tidied up as follows: we can have a column gender we can have a column pregnant we can have a column count (representing the number of observations for the combinations of gender and pregnant) In other words, we need the Wide-to-Long transformation: One observation might be scattered across multiple rows. The function tidyr::pivot_longer() is made for this. There are three important parameters: cols = ...: The set of columns whose names are levels of an underlying factor. Here they are male and female. names_to = ...: The name of the new underlying factor. Here it is gender. values_to = ...: The name of the new column for values of level combinations. Here it is count. Figure 6.3: From Wide to Long: pivot_longer() We applied the Wide-to-Long transformation to preg: preg %&gt;% pivot_longer(cols=c(&quot;male&quot;,&quot;female&quot;), names_to = &quot;gender&quot;, values_to = &quot;count&quot;) 6.5 Word Frequency Transformation Now back to our word_freq: word_freq %&gt;% head(10) It is probably clearer to you now what we should do to tidy up word_freq. It is obvious that some words (observations) are scattered across several rows. The column textid can be pivoted into two variables: “perl” vs. “python.” In order words, we need strategy of Long-to-Wide. One variable might be spread across multiple columns We transformed our data accordingly. word_freq_wide &lt;- word_freq %&gt;% pivot_wider(names_from = &quot;textid&quot;, values_from = &quot;n&quot;) head(word_freq_wide) Exercise 6.2 In the above Long-to-Wide transformation, there is still one problem. There are quite a few words that occur in one corpus but not the other. For these words, their frequencies would be a NA because R cannot allocate proper values for these unseen cases. Could you fix this problem by assigning these unseen cases a 0 when transforming the data frame? Please name the updated wide version of the word frequency data frame as contingency_table. Hint: Please check the argument pivot_wider(..., values_fill = ...) Problematic unseen cases in one of the corpora: word_freq_wide %&gt;% filter(is.na(corp_perl.txt) | is.na(corp_python.txt)) Updated contingency_table: contingency_table Before we compute the keyness, we preprocess the data by: including words consisting of alphabets only; renaming the columns to match the cell labels in Figure 6.1 above; creating necessary frequencies (columns) for keyness computation contingency_table %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table 6.6 Computing Keynesss With all necessary frequencies, we can now compute the three keyness statistics for each word. contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) %&gt;% mutate_if(is.numeric, round, 2) -&gt; keyness_table keyness_table Although now we have the keyness values for words, we still don’t know to which corpus (target or reference corpus) the word is more attracted. What to do next? keyness_table %&gt;% mutate(preference = ifelse(a &gt; a.exp, &quot;perl&quot;,&quot;python&quot;)) %&gt;% select(word, preference, everything())-&gt; keyness_table keyness_table 6.7 Conclusion Keyness discussed in this chapter is based on the comparative study of word frequencies in two separate corpora: one as the target and the other as the reference corpus. Therefore, the statistical keyness found in the word distribution is connected to the difference between the target and reference corpus (i.e., their distinctive features). These statistical keywords should be therefore interpreted along with the distinctive features of the target and reference corpus. If the text collections in the target and reference corpus differ significantly in different time periods, then the keywords may reflect important terms prominent in specific time periods. If the text collections differ significantly in genres/registers, then the keywords may reflect important terms tied to particular genres/registers. Please note that the keyness discussed in this chapter is different from the tf.idf (we used in the previous chapter), which is designed to reflect how important a word is to a document in a corpus. tf.idf is more often used in information retrieval. Important words of a document are selected based on the tf.idf weighting scheme; the relevance of the document to the user’s query is determined based on how close the search word(s) is to the important words of the document. Exercise 6.3 The CSV in demo_data/data-movie-reviews.csv is the IMDB dataset with 50,000 movie reviews and their sentiment tags (source: Kaggle). The CSV has two columns—the first column review includes the raw texts of each movie review; the second column sentiment provides the sentiment tag for the review. Each review is either positive or negative. We can treat the dataset as two separate corpora: negative and positive corpora. Please find the top 10 keywords for each corpus ranked by the G2 statistics. In the data preprocessing, please use the default tokenization in unnest_tokens(..., token = \"words\"). When computing the keyness, please exclude: words with at least one non-alphanumeric symbols in them (e.g. regex class \\W) words whose frequency is &lt; 10 in each corpus The expected results are provided below for your reference. The first six rows of demo_data/data-movie-reviews.csv: Sample result: References Damerau, F. J. (1993). Generating and evaluating domain-oriented multi-word terms from texts. Information Processing &amp; Management, 29(4), 433–447. Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 61–74. https://www.aclweb.org/anthology/J93-1003 Gabrielatos, C. (2018). Keyness analysis: Nature, metrics and techniques. In Corpus approaches to discourse (pp. 225–258). Routledge. Leech, G., &amp; Fallon, R. (1992). Computer corpora–what do they tell us about culture. ICAME Journal, 16. Stubbs, M. (1996). Text and corpus analysis. Blackwell. Stubbs, M. (2003). Words and phrases. Blackwell. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. Williams, R. (1976). Keywords. Oxford University Press. "],["chinese-text-processing.html", "Chapter 7 Chinese Text Processing 7.1 Chinese Word Segmenter jiebaR 7.2 Chinese Text Analytics Pipeline 7.3 Comparing Tokenization Methods 7.4 Data 7.5 Loading Text Data 7.6 quanteda::tokens() vs. jiebaR::segment() 7.7 Case Study 1: Word Frequency and Wordcloud 7.8 Case Study 2: Patterns 7.9 Case Study 3: Lexical Bundles 7.10 Afterwords", " Chapter 7 Chinese Text Processing In this chapter, we will turn to the topic of Chinese text processing. In particular, we will discuss one of the most important issues in Chinese language processing, i.e., word segmentation. When we discuss English parts-of-speech tagging in Chapter 5, it is easy to do word tokenization in English because the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. In later Chapter 9, we will introduce another segmenter developed by the CKIP Group at the Academia Sinica. The CKIP Tagger seems to be the state-of-art tagger for Taiwan Mandarin, i.e., with more additional functionalities. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 7.1 Chinese Word Segmenter jiebaR 7.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: Initialize a jiebar object using worker() Tokenize the texts into words using the function segment() with the designated jiebar object created earlier seg1 &lt;- worker() segment(text, jiebar = seg1) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(seg1) [1] &quot;jiebar&quot; &quot;segment&quot; &quot;jieba&quot; To word-tokenize the document, text, you first initialize a jiebar object, i.e., seg1, using worker() and feed this jiebar to segment(jiebar = seg1)and tokenize text into words. 7.1.2 Parameters Setting There are many different parameters you can specify when you initialize the jiebar object. You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) Exercise 7.1 In our earlier example, when we created the jiebar object named seg1, we did not specify any arguments for worker(). Can you tell what the default settings are for the parameters of worker()? Please try to create worker() with different settings (e.g., symbols = T, bylines = T) and see how the tokenization results differ from each other. 7.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when tokenizing your texts because different corpora may have their own unique vocabulary (i.e., domain-specific lexicon). This can be done with the argument user = ... when you initialize the jiebar object, i.e, worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) segment(text, seg2) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a Chinese txt file created by Notepad may not be UTF-8. (Usually, it is encoded in big-5.) Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 7.1.4 Stopwords When you initialize the jiebar, you can also specify a stopword list, i.e., words that you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative, thus often excluded in the process of preprocessing. seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; [19] &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; [25] &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; [31] &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; [37] &quot;這麼&quot; &quot;壞&quot; Exercise 7.2 How do we quickly check which words in segment(text, seg2) were removed as compared to the results of segment(text, seg3)? (Note: seg2 and seg3 only differ in the stop_word=... argument.) [1] &quot;日&quot; &quot;是&quot; &quot;都&quot; 7.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = \"tag\" when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;, symbol = T) segment(text, seg4) n ns n x n x n &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;，&quot; &quot;指&quot; x x p v n x x &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;、&quot; x x x x x x d &quot;黃瀞瑩&quot; &quot;，&quot; &quot;在昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; &quot;才&quot; v x n x x x n &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; x ns n x x v x &quot;、&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; zg x p n v x df &quot;說&quot; &quot;，&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; &quot;不要&quot; p n x r a x &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;。&quot; The returned object is a named character vector, i.e., the POS tags of the words are included in the names of the vectors. Every POS tagger has its own predefined tagset. The following table lists the annotations of the POS tagset used in jiebaR: Exercise 7.3 How do we convert the named word vector with POS tags returned by segment(text, seg4) into a long string as shown below? [1] &quot;綠黨/n 桃園市/ns 議員/n 王浩宇/x 爆料/n ，/x 指/n 民眾黨/x 不分區/x 被/p 提名/v 人/n 蔡壁如/x 、/x 黃瀞瑩/x ，/x 在昨/x （/x 6/x ）/x 才/d 請辭/v 為領/x 年終獎金/n 。/x 台灣/x 民眾黨/x 主席/n 、/x 台北/ns 市長/n 柯文哲/x 7/x 受訪/v 時則/x 說/zg ，/x 按/p 流程/n 走/v ，/x 不要/df 把/p 人家/n 想得/x 這麼/r 壞/a 。/x&quot; 7.1.6 Default Word Lists in JiebaR You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: # show files under `dictpath` dir(show_dictpath()) # Check the default stop_words list # Please change the path to your default dict path # scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, # what=character(),nlines=50,sep=&#39;\\n&#39;, # encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) readLines(&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, n = 50) 7.1.7 Reminders When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and returns a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) [[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) [1] &quot;list&quot; class(text_tag_0) [1] &quot;character&quot; 7.2 Chinese Text Analytics Pipeline In Chapter 5, we have talked about the pipeline for English texts processing, as shown below: Figure 7.1: English Text Analytics Flowchart For Chinese texts, the pipeline is similar. In the following Chinese Text Analytics Flowchart (Figure 7.2), I have highlighted the steps that are crucial to Chinese processing. It is not recommended to use quanteda::summary() and quanteda::kwic() directly on the Chinese corpus object because the word tokenization in quanteda is not ideal (cf. dashed arrows in Figure 7.2). It is recommended to use self-defined word segmenter for analysis. For processing under tidy structure framework, use own segmenter in unnest_tokens(); for processing under quanteda framework, create the tokens object, which is defined in quanteda as well. Figure 7.2: Chinese Text Analytics Flowchart It is important to note that when we specify a self-defined unnest_tokens(…,token=…) function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument worker(…, byline = TRUE). 7.2.1 Creating a Corpus Object So based on our simple corpus example above, we first transform the character vector text into a corpus object—text_corpus. With this, like with the English data, we can apply quanteda::summary() and quanteda::kwic() with the corpus object. text_corpus &lt;- text %&gt;% corpus summary(text_corpus) kwic(text_corpus, pattern = &quot;柯文哲&quot;) kwic(text_corpus, pattern = &quot;柯&quot;) Exercise 7.4 Do you know why there are no tokens of concordance lines from kwic(text_corpus, pattern = \"柯文哲\")? 7.2.2 Tidy Structure Framework We can now transform the corpus object into a text-based TIBBLE using tidy(). Also, we generate an unique index for each row using row_number(). # a text-based tidy corpus text_corpus_tidy &lt;-text_corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) text_corpus_tidy For word segmentation, we initialize the jiebar object using worker(). # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) Finally, we use unnest_tokens() to tokenize the text-based TIBBLE text_corpus_tidy into a word-based TIBBLE text_corpus_tidy_word. That is, texts included in the text column are tokenized into words, which are unnested into rows of the word column in the new TIBBLE. # tokenization text_corpus_tidy_word &lt;- text_corpus_tidy %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) text_corpus_tidy_word In the above example, we specify our own tokenization function, function(x) segment(x, jiebar = my_seg). This is called anonymous functions. It is a real function object, which just happens to have not been assigned to any symbol before being used. You may check R language documentation for more detail on Writing Functions. Generally functions are assigned to symbols but they don’t need to be. The value returned by the call to function is a function. If this is not given a name it is referred to as an anonymous function. Anonymous functions are most frequently used as arguments to other functions such as the apply family or outer. 7.2.3 Quanteda Framework Under the quanteda framework, we can also create the tokens object of the corpus and do kwic() search. Most of the functions that work with corpus object can also work with tokens object in quanteda. text_tokens &lt;- text_corpus_tidy$text %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens kwic(text_tokens, pattern = &quot;柯文哲&quot;) 7.3 Comparing Tokenization Methods quanteda also provides its own default word tokenization for Chinese texts. However, its default tokenization method does not allow us to add our own dictionary to the segmentation process, which renders the results less reliable. We can compare the two results. we can use quanteda::tokens() to see how quanteda tokenizes Chinese texts. The function returns a tokens object. # create TOKENS object using quanteda default text_corpus %&gt;% tokens -&gt; text_tokens_qtd we can also use our own tokenization function segment() and convert the list to a tokens object using as.tokens(). (This of course will give us the same tokenization result as we get in the earlier unnest_tokens() because we are using the same segmenter my_seg.) # create TOKENS object manually text_corpus %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens -&gt; text_tokens_jb Now let’s compare the two resulting tokens objects: These are the tokens based on self-defined segmenter: # compare our tokenization with quanteda tokenization text_tokens_jb[[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;，&quot; [7] &quot;指&quot; &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;、&quot; &quot;黃瀞瑩&quot; &quot;，&quot; &quot;在昨&quot; &quot;（&quot; [19] &quot;6&quot; &quot;）&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; [25] &quot;為領&quot; &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; [31] &quot;、&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; [37] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;，&quot; &quot;都&quot; &quot;是&quot; [43] &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; &quot;不要&quot; &quot;把&quot; [49] &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;。&quot; These are the tokens based on default quanteda tokenizer: text_tokens_qtd[[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; [67] &quot;。&quot; Therefore, for linguistic analysis, I would suggest to define own Chinese word segmenter using jiebaR, which is tailored to specific tasks/corpora. 7.4 Data In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. (This dataset was collected by Meng-Chen Wu when he was working on his MA thesis project with me years ago. The demo data here was a random sample of the original Apple News Corpus.) 7.5 Loading Text Data When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext. The main function in this package, readtext(), which takes a file or a directory name from the disk or a URL, and returns a type of data.frame that can be used directly with the corpus() constructor function in quanteda, to create a quanteda corpus object. In other words, the output from readtext can be directly passed on to the processing in the tidy structure framework (i.e., tidytext::unnest_tokens()). The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. The corpus constructor command corpus() works directly on: a vector of character objects, for instance that you have already loaded into the workspace using other tools; a data.frame containing a text column and any other document-level metadata the output of readtext::readtext() # loading the corpus # NB: this may take some time apple_corpus &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% corpus summary(apple_corpus, 10) 7.6 quanteda::tokens() vs. jiebaR::segment() In Chapter 4, we’ve seen that after we create a corpus object, we can apply kwic() to get the concordance lines of a particular word. At that time, we emphasized that this worked because quanteda underlyingly tokenized the texts behind the scene. We can do the same the with Chinese texts as well: kwic(apple_corpus, &quot;勝率&quot;) In Section 7.3, I have made it clear that quanteda does have its own tokenization method (i.e., tokens()) for Chinese texts. It uses the tokenizer, stringi::stri_split_boundaries, which utilizes a library called ICU (International Components for Unicode) and the library uses dictionaries for segmentation of texts in Chinese. The biggest problem is that we cannot add our own dictionary when using the default tokenization tokens() (at least I don’t know how). In other words, when we apply kwic() to apple_corpus, quanteda tokenizes the Chinese texts using its default tokenizer (i.e., tokens())and perform the keyword-in-context search. Like we did in Section 7.3, we can compare the word segmentation results between quanteda defaults and jiebaR (with own dictionary) with our current news corpus. To create tokens using jiebaR: First we tokenize all texts in apple_corpus using jiebaR::segment() and the jiebar initialized with our user-defined dictionary. Second, we convert the returned list from segment() into a tokens object using as.tokens(). To create tokens using quanteda: We use quanteda default tokens() to convert the corpus object into tokens object. # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) # Tokenization using jiebaR apple_corpus %&gt;% segment(jiebar = segmenter) %&gt;% as.tokens -&gt; apple_tokens # Tokenization using qunateda::tokens() apple_corpus %&gt;% tokens -&gt; apple_tokens_qtd Now we can compare the two versions of word segmentation. Let’s take a look at the first document: apple_tokens[[1]] %&gt;% length [1] 168 apple_tokens_qtd[[1]] %&gt;% length [1] 148 apple_tokens[[1]] %&gt;% as.character [1] &quot;《&quot; &quot;蘋果&quot; &quot;體育&quot; &quot;》&quot; &quot;即日起&quot; &quot;進行&quot; &quot;虛擬&quot; &quot;賭盤&quot; [9] &quot;擂台&quot; &quot;，&quot; &quot;每名&quot; &quot;受邀&quot; &quot;參賽者&quot; &quot;進行&quot; &quot;勝負&quot; &quot;預測&quot; [17] &quot;，&quot; &quot;每周&quot; &quot;結算&quot; &quot;在&quot; &quot;周二&quot; &quot;公布&quot; &quot;，&quot; &quot;累積&quot; [25] &quot;勝率&quot; &quot;前&quot; &quot;3&quot; &quot;高&quot; &quot;參賽者&quot; &quot;可&quot; &quot;繼續&quot; &quot;參賽&quot; [33] &quot;，&quot; &quot;單周&quot; &quot;勝率&quot; &quot;最高者&quot; &quot;，&quot; &quot;將&quot; &quot;加封&quot; &quot;「&quot; [41] &quot;蘋果&quot; &quot;波神&quot; &quot;」&quot; &quot;頭銜&quot; &quot;。&quot; &quot;註&quot; &quot;:&quot; &quot;賭盤&quot; [49] &quot;賠率&quot; &quot;如有&quot; &quot;變動&quot; &quot;，&quot; &quot;以&quot; &quot;台灣&quot; &quot;運彩&quot; &quot;為主&quot; [57] &quot;。&quot; &quot;\\n&quot; &quot;資料&quot; &quot;來源&quot; &quot;：&quot; &quot;NBA&quot; &quot;官網&quot; &quot;http&quot; [65] &quot;:&quot; &quot;/&quot; &quot;/&quot; &quot;www&quot; &quot;.&quot; &quot;nba&quot; &quot;.&quot; &quot;com&quot; [73] &quot;\\n&quot; &quot;\\n&quot; &quot;金塊&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;103&quot; [81] &quot;：&quot; &quot;92&quot; &quot; &quot; &quot;76&quot; &quot;人&quot; &quot;騎士&quot; &quot;(&quot; &quot;主&quot; [89] &quot;)&quot; &quot; &quot; &quot;88&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;快艇&quot; &quot;活塞&quot; [97] &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;92&quot; &quot;：&quot; &quot;75&quot; &quot; &quot; [105] &quot;公牛&quot; &quot;勇士&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;108&quot; &quot;：&quot; [113] &quot;82&quot; &quot; &quot; &quot;灰熊&quot; &quot;熱火&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; [121] &quot;103&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;灰狼&quot; &quot;籃網&quot; &quot;(&quot; &quot;客&quot; [129] &quot;)&quot; &quot; &quot; &quot;90&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;公鹿&quot; &quot;溜&quot; [137] &quot;馬&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;111&quot; &quot;：&quot; &quot;100&quot; [145] &quot; &quot; &quot;馬刺&quot; &quot;國王&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;112&quot; [153] &quot;：&quot; &quot;102&quot; &quot; &quot; &quot;爵士&quot; &quot;小牛&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; [161] &quot; &quot; &quot;108&quot; &quot;：&quot; &quot;106&quot; &quot; &quot; &quot;拓荒者&quot; &quot;\\n&quot; &quot;\\n&quot; apple_tokens_qtd[[1]] %&gt;% as.character [1] &quot;《&quot; &quot;蘋果&quot; &quot;體育&quot; [4] &quot;》&quot; &quot;即日起&quot; &quot;進行&quot; [7] &quot;虛擬&quot; &quot;賭&quot; &quot;盤&quot; [10] &quot;擂台&quot; &quot;，&quot; &quot;每名&quot; [13] &quot;受邀&quot; &quot;參賽者&quot; &quot;進行&quot; [16] &quot;勝負&quot; &quot;預測&quot; &quot;，&quot; [19] &quot;每周&quot; &quot;結算&quot; &quot;在&quot; [22] &quot;周二&quot; &quot;公布&quot; &quot;，&quot; [25] &quot;累積&quot; &quot;勝率&quot; &quot;前&quot; [28] &quot;3&quot; &quot;高&quot; &quot;參賽者&quot; [31] &quot;可&quot; &quot;繼續&quot; &quot;參賽&quot; [34] &quot;，&quot; &quot;單&quot; &quot;周&quot; [37] &quot;勝率&quot; &quot;最高&quot; &quot;者&quot; [40] &quot;，&quot; &quot;將&quot; &quot;加封&quot; [43] &quot;「&quot; &quot;蘋果&quot; &quot;波&quot; [46] &quot;神&quot; &quot;」&quot; &quot;頭銜&quot; [49] &quot;。&quot; &quot;註&quot; &quot;:&quot; [52] &quot;賭&quot; &quot;盤&quot; &quot;賠&quot; [55] &quot;率&quot; &quot;如有&quot; &quot;變動&quot; [58] &quot;，&quot; &quot;以&quot; &quot;台灣&quot; [61] &quot;運&quot; &quot;彩&quot; &quot;為主&quot; [64] &quot;。&quot; &quot;資料&quot; &quot;來源&quot; [67] &quot;：&quot; &quot;NBA&quot; &quot;官&quot; [70] &quot;網&quot; &quot;http://www.nba.com&quot; &quot;金塊&quot; [73] &quot;(&quot; &quot;客&quot; &quot;)&quot; [76] &quot;103&quot; &quot;：&quot; &quot;92&quot; [79] &quot;76&quot; &quot;人&quot; &quot;騎士&quot; [82] &quot;(&quot; &quot;主&quot; &quot;)&quot; [85] &quot;88&quot; &quot;：&quot; &quot;82&quot; [88] &quot;快艇&quot; &quot;活塞&quot; &quot;(&quot; [91] &quot;客&quot; &quot;)&quot; &quot;92&quot; [94] &quot;：&quot; &quot;75&quot; &quot;公牛&quot; [97] &quot;勇士&quot; &quot;(&quot; &quot;客&quot; [100] &quot;)&quot; &quot;108&quot; &quot;：&quot; [103] &quot;82&quot; &quot;灰&quot; &quot;熊&quot; [106] &quot;熱火&quot; &quot;(&quot; &quot;客&quot; [109] &quot;)&quot; &quot;103&quot; &quot;：&quot; [112] &quot;82&quot; &quot;灰&quot; &quot;狼&quot; [115] &quot;籃網&quot; &quot;(&quot; &quot;客&quot; [118] &quot;)&quot; &quot;90&quot; &quot;：&quot; [121] &quot;82&quot; &quot;公鹿&quot; &quot;溜&quot; [124] &quot;馬&quot; &quot;(&quot; &quot;客&quot; [127] &quot;)&quot; &quot;111&quot; &quot;：&quot; [130] &quot;100&quot; &quot;馬&quot; &quot;刺&quot; [133] &quot;國王&quot; &quot;(&quot; &quot;客&quot; [136] &quot;)&quot; &quot;112&quot; &quot;：&quot; [139] &quot;102&quot; &quot;爵士&quot; &quot;小牛&quot; [142] &quot;(&quot; &quot;客&quot; &quot;)&quot; [145] &quot;108&quot; &quot;：&quot; &quot;106&quot; [148] &quot;拓荒者&quot; kwic(apple_tokens, &quot;勝率&quot;) kwic(apple_tokens_qtd, &quot;勝率&quot;) Any significant differences in the word tokenization? To work with the Chinese texts, if you need to utilize more advanced text-analytic functions provided by quanteda, please perform the word tokenization on the texts using your own word segmenter first and convert the object into a tokens, which can then be properly passed on to other functions in quanteda (e.g., dfm). (In other words, for Chinese text analytics, probably corpus object is less practical; rather, creating a tokens object of your corpus might be more useful.) In the later demonstrations, we will use our own defined segmenter for word segmentation/tokenization. 7.7 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart 7.2 and deal with the Chinese texts using the tidy structure framework: Load the corpus data using readtext() and convert it into an corpus object Create a text-based tidy structure DF apple_corpus_tidy (i.e., a tibble) Intialize a word segmenter using worker() Tokenize the text-based data frame into a word-based tidy data frame using unnest_tokens() # loading corpus apple_df &lt;- apple_corpus %&gt;% tidy %&gt;% filter(text !=&quot;&quot;) %&gt;% #remove empty documents mutate(doc_id = row_number()) # create doccument index # Initialize the `jiebar` segmenter_word &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) # Tokenization: Word-based DF apple_word &lt;- apple_df %&gt;% unnest_tokens(output = word, input= text, token = function(x) segment(x, jiebar = segmenter_word)) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup apple_word %&gt;% head(100) These tokenization results should be the same as our earlier apple_tokens: apple_word %&gt;% filter(doc_id == 1) %&gt;% mutate(word_quanteda_tokens = apple_tokens[[1]]) Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to trace back to the original context where the word, phrase or sentence comes from. With all these unique indices, we can easily keep track of the sources of all tokenized linguistic units. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) With a word-based tidy DF, we can easily generate a word frequency list as well as a wordcloud to have a quick overview of the word distribution in the corpus. stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch.txt&quot;) apple_word_freq &lt;- apple_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% # remove words consisting of digits count(word) %&gt;% arrange(desc(n)) library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 400) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) 7.8 Case Study 2: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. Our steps are as follows: Initilize jiebar object Define own function to word-seg and pos-tag each text and combine all tokens, word/tag, into a long string for each text With the text-based apple_df, create a new column, which includes the tokenized version of each text, using mutate() # initilize `jiebar` segmenter_word_pos &lt;- worker(type = &quot;tag&quot;, # get pos user = &quot;demo_data/dict-ch-user.txt&quot;, # use own dict symbol = T, # keep symbols bylines = FALSE) # for `mutate()` use # define own function tag_text &lt;- function(x, jiebar){ segment(x, jiebar) %&gt;% paste(names(.), sep=&quot;/&quot;, collapse=&quot; &quot;) } # Testing tag_text(apple_df$text[2], segmenter_word_pos) [1] &quot;【/x 動/v 新聞/n ╱/x 綜合/vn 報導/n 】/x 新北市/x 一名/m 18/m 歲/zg 李姓/x 男子/n ，/x 疑因/x 女友/n 要/v 分手/v ，/x 避不見面/i 也/d 不/d 接電話/l ，/x 他/r 為/zg 見/v 女友/n 一面/m ，/x 挽回/v 感情/n ，/x 竟學/x 蜘蛛人/n 攀爬/v 鐵窗/n ，/x 欲/d 潛入/v 女友/n 位於/v 5/x 樓/n 住處/n ，/x 行經/n 2/x 樓時/x 被/p 住戶/n 發現/v ，/x 他/r 誆稱/x 「/x 撿/v 鑰匙/n 」/x 矇混過關/l ，/x 但/c 仍/zg 被/p 4/x 樓/n 住戶/n 懷疑/v 是/v 小偷/d 報案/n ，/x 最後/x 雖/zg 成功/x 進入/v 5/x 樓/n 見到/v 女友/n ，/x 仍/zg 無法挽回/i 感情/n ，/x 因/p 侵入/v 住宅/n 罪嫌/n 被/p 帶回/v 警局/x ，/x 女方/n 家屬/n 不/d 提告/x 作罷/v 。/x 警方/n 指出/v ，/x 李男為/x 挽回/v 感情/n ，/x 鋌而走險/x 攀爬/v 鐵窗/n 至/p 5/x 樓/n ，/x 其後/x 方/n 就是/d 一處/m 工地/n ，/x 萬一/x 失足/v 墜落/v ，/x 後果/n 不堪設想/i ，/x 經/zg 勸說/v 後/f ，/x 讓/v 李/nr 男/n 離去/v 。/x \\n/x /x&quot; # apply tagger function to each text system.time(apple_df %&gt;% mutate(text_tag = map_chr(text, tag_text, segmenter_word_pos)) -&gt; apple_df_2) user system elapsed 5.306 0.111 5.417 apple_df_2 %&gt;% head 7.8.1 BEI Construction This section will show you how we can make use of the POS tags for construction analysis. I would like to illustrate their usefulness with a case study: 被 + ... Construction. The data retrieval process is now very straighforward: we only need to create a regular expression that matches our construction and go through the word-segmented and pos-tagged texts (i.e., text_tag column in apple_df_2) to identify these matches. In the following example, we: define a regular expression \\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v for BEI-Construction, i.e., 被 + VERB use unnest_tokens() and str_extract_all() to extract target patterns # define regex patterns pattern_bei &lt;- &quot;\\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v&quot; # extract patterns from corp apple_df_2 %&gt;% select(-text) %&gt;% # `text` is the column with original raw texts unnest_tokens(output = pat_bei, input = text_tag, token = function(x) str_extract_all(x, pattern=pattern_bei)) -&gt; result_bei result_bei Please check Chapter 5 Parts of Speech Tagging on evaluating the quality of the data retrieved by a regular expression (i.e., precision and recall). To have a more in-depth analysis of BEI construction, we like to automatically extract the verb used in the BEI construction. # Extract BEI + WORD result_bei &lt;- result_bei %&gt;% mutate(VERB = str_replace(pat_bei,&quot;.+\\\\s([^/]+)/v$&quot;,&quot;\\\\1&quot;)) result_bei # Calculate WORD frequency require(wordcloud2) result_bei %&gt;% count(VERB) %&gt;% mutate(n = log(n)) %&gt;% top_n(100, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.3) Exercise 7.5 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native-speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 7.6 To more properly evaluate the quality of the pattern queries, it would be great if we still have the original texts available in the resulting data frame result_bei. How do we keep this information? That is, please have one column in result_bei, which shows the original texts from which the construction token is extracted. Exercise 7.7 Please use the sample corpus, apple_df_2 as your data source and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. Please (a) extract all construction tokens with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 7.8 Following Exercise 7.7, please generate a frequency list of the LMs for each space particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Also, you may visualize the top 10 landmarks that co-occur with each space particle in a bar plot as shown below. Exercise 7.9 Following Exercise 7.8, for each space particle, please create a word cloud of its co-occuring LMs based on the top 100 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 7.10 Based on the word clouds provided in Exercise 7.9, do you find any bizarre cases? Can you tell us why? What would be the problems? Or what did we do wrong in the text preprocessing that may lead to these cases? Please discuss these issues in relation to the steps in our data processing, i.e., word segmentation, POS tagging, and pattern retrievals. 7.9 Case Study 3: Lexical Bundles 7.9.1 N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at the recurrent four-grams in Chinese. As the default n-gram tokenization in unnest_tokens() only works with the English data, we start this task by defining our own tokenization functions. In this section, we define three functions: ngram_chi(): This function takes a word vector and returns a ngram-based vectors tokenizer_ngrams(): This function takes texts vector and returns a list of ngram-based vectors tokenizer_chunks(): This function takes texts vector and returns a list of chunk-based vectors # Generate ngram sequences from a word vector # By default, `word_vec` is assumed to be the word tokens of the text ngram_chi &lt;- function(word_vec, n = 2, delimiter = &quot;_&quot;){ if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc # test word_vec &lt;- c(&quot;這&quot;, &quot;是&quot;, &quot;一個&quot;, &quot;例子&quot;) ngram_chi(word_vec, n=2, delimiter=&quot;_&quot;) [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_例子&quot; ngram_chi(word_vec, n=3, delimiter=&quot;_&quot;) [1] &quot;這_是_一個&quot; &quot;是_一個_例子&quot; This ngram_chi() takes a word vector as an input, and returns a vector of n-grams. In other words, it can take the output of segment() as the input. Before we define the tokenization functions, we first initialize the jiebar object for jiebar. # define `jiebar` for jiebar segmenter_word &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) Now we can define own ngram tokenizer tokenizer_ngrams(): # define own tokenizer for ngrams tokenizer_ngrams &lt;- function(input, jiebar, n, delimiter){ input %&gt;% segment(jiebar) %&gt;% # segment texts into word vectors map(ngram_chi, n, delimiter) # convert word vectors into ngram vectors } # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams(input=texts, jiebar=segmenter_word, n = 2, delimiter = &quot;_&quot;) [[1]] [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; [[2]] [1] &quot;這_句子&quot; [[3]] [1] &quot;超短_句&quot; [[4]] [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams(input=texts, jiebar=segmenter_word, n = 5, delimiter = &quot;_&quot;) [[1]] [1] &quot;這是_一個_測試_的_句子&quot; [[2]] [1] &quot;&quot; [[3]] [1] &quot;&quot; [[4]] [1] &quot;最後_一個_超長_的_句子&quot; &quot;一個_超長_的_句子_測試&quot; And then we define own chunk tokenizer tokenizer_chunks(): # define chunk tokenization function tokenizer_chunks &lt;- function(input){ str_split(input, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;) } texts &lt;- apple_df$text[1:2] tokenizer_chunks(input = texts) [[1]] [1] &quot;&quot; &quot;蘋果體育&quot; [3] &quot;即日起進行虛擬賭盤擂台&quot; &quot;每名受邀參賽者進行勝負預測&quot; [5] &quot;每周結算在周二公布&quot; &quot;累積勝率前&quot; [7] &quot;高參賽者可繼續參賽&quot; &quot;單周勝率最高者&quot; [9] &quot;將加封&quot; &quot;蘋果波神&quot; [11] &quot;頭銜&quot; &quot;註&quot; [13] &quot;賭盤賠率如有變動&quot; &quot;以台灣運彩為主&quot; [15] &quot;資料來源&quot; &quot;官網&quot; [17] &quot;金塊&quot; &quot;客&quot; [19] &quot;人騎士&quot; &quot;主&quot; [21] &quot;快艇活塞&quot; &quot;客&quot; [23] &quot;公牛勇士&quot; &quot;客&quot; [25] &quot;灰熊熱火&quot; &quot;客&quot; [27] &quot;灰狼籃網&quot; &quot;客&quot; [29] &quot;公鹿溜馬&quot; &quot;客&quot; [31] &quot;馬刺國王&quot; &quot;客&quot; [33] &quot;爵士小牛&quot; &quot;客&quot; [35] &quot;拓荒者&quot; &quot;&quot; [[2]] [1] &quot;&quot; &quot;動新聞&quot; [3] &quot;綜合報導&quot; &quot;新北市一名&quot; [5] &quot;歲李姓男子&quot; &quot;疑因女友要分手&quot; [7] &quot;避不見面也不接電話&quot; &quot;他為見女友一面&quot; [9] &quot;挽回感情&quot; &quot;竟學蜘蛛人攀爬鐵窗&quot; [11] &quot;欲潛入女友位於&quot; &quot;樓住處&quot; [13] &quot;行經&quot; &quot;樓時被住戶發現&quot; [15] &quot;他誆稱&quot; &quot;撿鑰匙&quot; [17] &quot;矇混過關&quot; &quot;但仍被&quot; [19] &quot;樓住戶懷疑是小偷報案&quot; &quot;最後雖成功進入&quot; [21] &quot;樓見到女友&quot; &quot;仍無法挽回感情&quot; [23] &quot;因侵入住宅罪嫌被帶回警局&quot; &quot;女方家屬不提告作罷&quot; [25] &quot;警方指出&quot; &quot;李男為挽回感情&quot; [27] &quot;鋌而走險攀爬鐵窗至&quot; &quot;樓&quot; [29] &quot;其後方就是一處工地&quot; &quot;萬一失足墜落&quot; [31] &quot;後果不堪設想&quot; &quot;經勸說後&quot; [33] &quot;讓李男離去&quot; &quot;&quot; In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a working unit (i.e., chunks). This can be controversial to many grammarians and syntacticians. However, in practice, this may not be a bad idea for n-grams extraction. For more information related to the unicode range for the punctuations in CJK languages, please see this SO discussion thread. With all these functions ready, the extraction of n-grams may follow the steps as shown below: We transform the text-based data frame into a chunk-based data frame using unnest_tokens(...) with the self-defined tokenization function tokenizer_chunks() We transform the chunk-based data frame into an n-gram-based data frame using unnest_tokens(...) with the self-defined tokenization function tokenizer_ngrams() We remove empty n-grams entries (Chunks with less than four words will have NO four-grams extracted.) # text-based to chunk-based apple_df %&gt;% unnest_tokens(chunk, text, token = tokenizer_chunks) %&gt;% filter(nzchar(chunk)) -&gt; apple_chunk apple_chunk %&gt;% head # chunked-based to ngram-based system.time( apple_chunk %&gt;% unnest_tokens(ngram, chunk, token = function(x) tokenizer_ngrams(input=x, jiebar=segmenter_word, n = 4, delimiter=&quot;_&quot;)) %&gt;% filter(nzchar(ngram)) -&gt; apple_ngram) # end system.time user system elapsed 24.346 0.195 24.547 nrow(apple_ngram) [1] 974022 apple_ngram %&gt;% head(20) 7.9.2 Frequency and Dispersion As we discussed in Chapter 4, a multiword unit can be defined based on at least two important distributional properties: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) Now that we have the four-grams-based DF, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams at: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) system.time( apple_ngram_dist &lt;- apple_ngram %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 5) ) #end system.time user system elapsed 45.608 0.120 45.739 Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;被&quot;)) %&gt;% arrange(desc(dispersion)) apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;以&quot;)) %&gt;% arrange(desc(dispersion)) Exercise 7.11 In the above example, if we are only interested in the four-grams with the word 以, how can we revise the regular expression so that we can get rid of tokens like ngrams with 以及, 以上 etc. 7.10 Afterwords Figure 7.3: Chinese Word Segmentation and POS Tagging Tokenizations are complex in Chinese text processing. Many factors may need to be taken into account when determining the right tokenization method. While word segmentation is almost a necessary step in Chinese computational text analytics, several important questions may also be relevant to the data processing methods: Do you need the parts-of-speech tags of words in your research? What is the base linguistic unit you would like to work with? Texts? Chunks? Sentences? N-grams? Words? Do you need non-word tokens such as symbols, punctuations, or numbers in your analysis? Your answers to the above questions should help you determine the most effective structure of the tokenization methods for your data. Exercise 7.12 Please scrape the articles on the most recent 10 index pages of the PTT Gossipping board. Analyze all the articles whose titles start with [問卦], [新聞], or [爆卦] (Please ignore all articles that start with Re:). Specifically, please create the word frequency list of these target articles by: including only words that are tagged as nouns or verbs by JiebaR (i.e., all words whose POS tags start with n or v) removing words on the stopword list (cf. demo_data/stopwords-ch.txt) providing both the word frequency and dispersions (i.e., number of articles where it occurs) In addition, please visualize your results with a wordcloud as shown below, showing the recent hot words based on these recently posted target articles on PTT Gossipping. In the wordcloud, please include words whose (a) nchar() &gt;=2, and (b) dispersion &lt;= 5. Note: For Chinese word segementation, you may use the dictionary provided in demo_data/dict-ch-user.txt user system elapsed 7.746 0.051 53.097 The target articles from PTT Gossipping: Word Frequency List Wordclound "],["constructions-and-idioms.html", "Chapter 8 Constructions and Idioms 8.1 Collostruction 8.2 Corpus 8.3 Word Segmentation 8.4 Extract Constructions 8.5 Distributional Information Needed for CA 8.6 Exercises", " Chapter 8 Constructions and Idioms library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 8.1 Collostruction In this chapter, I would like to talk about the relationship between a construction and words. Words may co-occur to form collocation patterns. When words co-occur with a particular morphosyntactic pattern, they would form collostruction patterns. Here I would like to introduce a widely-applied method for research on the meanings of constructional schemas—Collostructional Aanalysis (Stefanowitsch &amp; Gries, 2003). This is the major framework in corpus linguistics for the study of the relationship between words and constructions. The idea behind collostructional analysis is simple: the meaning of a morphosyntactic construction can be determined very often by its co-occurring words. In particular, words that are strongly associated (i.e., co-occurring) with the construction are referred to as collexemes of the construction. Collostruction Analysis is an umbrella term, which covers several sub-analyses for constructional semantics: collexeme analysis co-varying collexeme analysis distinctive collexeme analysis This chapter will focus on the first one, collexeme analysis, whose principles can be extended to the other analyses. Also, I will demonstrate how we can conduct a collexeme analysis by using the R script written by Stefan Gries (Collostructional Analysis). 8.2 Corpus In this chapter, I will use the Apple News Corpus from Chapter 7 as our corpus. (It is available in: demo_data/applenews10000.tar.gz.) And in this demonstration, I would like to look at a particular morphosyntactic frame in Chinese, X + 起來. Our goal is simple: in order to find out the semantics of this constructional schema, it would be very informative if we can find out which words tend to strongly occupy this X slot of the constructional schema. So our first step is to load the text collections of Apple News into R and create a corpus object. apple_corpus &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% corpus 8.3 Word Segmentation Because Apple News Corpus is a raw-text corpus, we first need to word-tokenize the corpus. First we convert the corpus object into a tidy structure text-based tibble. Second, because later we need to extract constructions from texts, we add a new column to our text-based corpus, which includes the segmented version of the texts utilizing the jiebaR segmenter. # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = F, symbol = T) # Define own tokenization function word_seg_text &lt;- function(text, jiebar){ segment(text, jiebar) %&gt;% # vector output str_c(collapse=&quot; &quot;) } # From `corpus` to `tibble` apple_df &lt;- apple_corpus %&gt;% tidy %&gt;% filter(text !=&quot;&quot;) %&gt;% #remove empty documents mutate(doc_id = row_number()) # Tokenization apple_df &lt;- apple_df %&gt;% # create doccument index mutate(text_tag = map_chr(text, word_seg_text, segmenter)) 8.4 Extract Constructions With the word boundary information, we can now extract our target patterns from the corpus using regular expressions with unnest_tokens(). # Define regex pattern_qilai &lt;- &quot;[^\\\\s]+\\\\s起來\\\\b&quot; # Extract patterns apple_df %&gt;% select(-text) %&gt;% unnest_tokens(output = construction, input = text_tag, token = function(x) str_extract_all(x, pattern=pattern_qilai)) -&gt; apple_qilai # Print apple_qilai 8.5 Distributional Information Needed for CA To perform the collostructional analysis, which is essentially a statistical analysis of the association between the words and the constructions, we need to collect necessary distributional information of the words and constructions. In particular, to use Stefan Gries’ R script of Collostructional Analysis, we need the following information: Joint Frequencies of Words and Constructions Frequencies of Words in Corpus Corpus Size (total number of words in corpus) Construction Size (total number of constructions in corpus) 8.5.1 Word Frequency List It is easy to get the word frequencies. With the tokenized texts, we first convert the text-based tibble into a word-based one; then we create the word frequency list via simple data manipulation tricks. # word freq apple_df %&gt;% select(-text) %&gt;% unnest_tokens(word, text_tag, token = function(x) str_split(x, &quot;\\\\s+|\\u3000&quot;)) %&gt;% filter(nzchar(word)) %&gt;% count(word, sort = T) -&gt; apple_word apple_word %&gt;% head(100) 8.5.2 Joint Frequencies With all the extracted construction tokens, apple_qilai, it is also easy to get the joint frequencies of words and constructions as well as the construction frequencies. # Joint frequency table apple_qilai %&gt;% count(construction, sort=T) %&gt;% tidyr::separate(col=&quot;construction&quot;, into = c(&quot;w1&quot;,&quot;construction&quot;), sep=&quot;\\\\s&quot;) %&gt;% mutate(w1_freq = apple_word$n[match(w1,apple_word$word)]) -&gt; apple_qilai_table apple_qilai_table 8.5.3 Input for coll.analysis.r Specifically, Stefan Gries’ coll.analysis.r expects a particular input format. The input file should be a tsv file, which includes a three-column table: Words Word frequency in the corpus Word joint frequency with the construction # prepare for coll analysis apple_qilai_table %&gt;% select(w1, w1_freq, n) %&gt;% write_tsv(&quot;qilai.tsv&quot;) In the later Stefan Gries’ R script, we need to have our input as a tab-delimited file (tsv), not a comma-delimited file (csv). 8.5.4 Other Information In addition to the input file, Stefan Gries’ coll.analysis.r also requires a few statistics for the computing of association measures. We prepare necessary distributional information for the later collostructional analysis: Corpus size Construction size # corpus information cat(&quot;Corpus Size: &quot;, sum(apple_word$n), &quot;\\n&quot;) Corpus Size: 3209790 cat(&quot;Construction Size: &quot;, sum(apple_qilai_table$n), &quot;\\n&quot;) Construction Size: 546 Sometimes you may want to keep important information printed in the R console in an external file for later use. There’s a very useful function, sink(), which allows you to easily keep track of the outputs printed in the R console and save these outputs in an external text file. # save info in a text sink(&quot;qilai_info.txt&quot;) cat(&quot;Corpus Size: &quot;, sum(apple_word$n), &quot;\\n&quot;) cat(&quot;Construction Size: &quot;, sum(apple_qilai_table$n), &quot;\\n&quot;) sink() 8.5.5 Create Output File Stefan Gries’ coll.analysis.r can automatically output the results into an external file. Before running the CA script, we can first create an empty output txt file to keep the results from the CA script. # Create new file file.create(&quot;qilai_results.txt&quot;) 8.5.6 Run coll.analysis.r Finally we are now ready to perform the collostructional analysis using Stefan Gries’ coll.analysis.r. We can use source() to run an entire R script. The coll.analysis.r is availablel on Stefan Gries’ website. You can save the script onto your laptop and run it offline or source the online version ( coll.analysis.r) directly. Stefan Gries’ coll.analysis.r will initialize the analysis by first removing all the objects in your current R session. Please make sure that you have saved all necerssary information/objects in your current R session before you source the script. #################################### # WARNING!!!!!!!!!!!!!!! # # The script re-starts a R session # #################################### source(&quot;http://www.stgries.info/teaching/groningen/coll.analysis.r&quot;) coll.analysis.r is an R script with interactive instructions. When you run the analysis, you will be prompted with guided questions, to which you would need to fill in necessary information/answers in the R console. For our current example, the answers to be entered for each prompt include: analysis to perform: 1 name of construction: QILAI corpus size: 3209790 freq of constructions: 546 index of association strength: 1 (=fisher-exact) sorting: 4 (=collostruction strength) decimals: 2 text file with the raw data: &lt;qilai.tsv&gt; Where to save output: 1 (= text file) output file: &lt;qilai_results.txt&gt; If everything works properly, you should get the output of coll.analysis.r as a text file qilai_results.txt in your working directory. The text output may look as follows. 8.5.7 Interpretations The output from coll.analysis.r is a text file with both the result data frame (i.e., the data frame with all the statistics) as well as detailed annotations/explanations provided by Stefan Gries. We can also extract the result data frame from the text file. The output file from the collexeme analysis of QILAI has been made available in demo_data/qilai_results.txt. To extract the result data frame from the output text file: We first load the result txt file like a normal text file using readlines() We extract the lines which include the statistics and parse them as a delimited table (i.e., TSV) into a data frame using read_tsv() # load the output txt results &lt;-readLines(&quot;demo_data/qilai_results.txt&quot;, encoding = &quot;utf-8&quot;) # subset lines results&lt;-results[-c(1:17, (length(results)-17):length(results))] # convert into CSV collo_table&lt;-read_tsv(I(results)) # auto-print collo_table %&gt;% filter(relation ==&quot;attraction&quot;) %&gt;% arrange(desc(coll.strength)) %&gt;% head(100) %&gt;% select(words, coll.strength, everything()) With the collexeme analysis statistics, we can therefore explore the top N collexemes according to specific association metrics. Here we look at the top 10 collexemes according to four different distributional metrics: obs.freq: the raw joint frequency of the word and construction. delta.p.constr.to.word: the delta P of the construction to the word delta.p.word.to.constr: the delta P of the word to the construction coll.strength: the log-transformed p-value based on Fisher exact test # from wide to long collo_table %&gt;% filter(relation == &quot;attraction&quot;) %&gt;% filter(obs.freq &gt;=5) %&gt;% select(words, obs.freq, delta.p.constr.to.word, delta.p.word.to.constr, coll.strength) %&gt;% pivot_longer(cols=c(&quot;obs.freq&quot;, &quot;delta.p.constr.to.word&quot;, &quot;delta.p.word.to.constr&quot;, &quot;coll.strength&quot;), names_to = &quot;metric&quot;, values_to = &quot;strength&quot;) %&gt;% mutate(metric = factor(metric, levels = c(&quot;obs.freq&quot;, &quot;delta.p.constr.to.word&quot;, &quot;delta.p.word.to.constr&quot;, &quot;coll.strength&quot;))) %&gt;% group_by(metric) %&gt;% top_n(10, strength) %&gt;% #arrange(strength) %&gt;% #mutate(strength_rank = row_number()) %&gt;% ungroup %&gt;% arrange(metric, desc(strength)) -&gt; coll_table_long # plot graphs &lt;- list() for(i in levels(coll_table_long$metric)){ coll_table_long %&gt;% filter(metric %in% i) %&gt;% ggplot(aes(reorder(words, strength), strength, fill=strength)) + geom_col(show.legend = F) + coord_flip() + labs(x = &quot;Collexemes&quot;, y = &quot;Strength&quot;, title = i)+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;))-&gt; graphs[[i]] } require(ggpubr) ggpubr::ggarrange(plotlist = graphs) The bar plots above show the top 10 collexemes based on four different metrics: obs.freq, delta.p.contr.to.word, delta.p.word.to.contr, and coll.strength. Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. Therefore, four-character idioms are often one of the hot topics in the study of constructions in Chinese. In our demo_data directory, there is a file dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. You can load the dataset in R for more exploration of idioms. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;) head(all_idioms) tail(all_idioms) length(all_idioms) 8.6 Exercises The following exercises should use the dataset Yet Another Chinese News Dataset from Kaggle. The dataset is availabe on our dropbox demo_data/corpus-news-collection.csv. The dataset is a collection of news articles in Traditional and Simplified Chinese, including some Internet news outlets that are NOT Chinese state media. Exercise 8.1 Please conduct a collexeme analysis for the aspectual construction “X + 了” in Chinese. Extract all tokens of this consturction from the news corpus and identify all words preceding the aspectual marker. Based on the distributional information, conduct the collexemes analysis using the coll.analysis.r and present the collexemes that significantly co-occur with the construction “X + 了” in the X slot. Rank the collexemes according to the collostrength provided by Stefan Gries’ script. When you tokenize the texts using jiebaR, you may run into an error message as shown below. If you do, please figure out what leads to the issue and solve the problem on your own. It is suggested that you parse/tokenize the corpus data and create two columns to the text-based tibble— text_id, and text_tag. The following is an example of the first ten articles. A word frequency list of the top 100 words is attached below (word tokens that are pure whitespaces or empty strings were not considered) After my data preprocessing and tokenization, here is relevant distributional information for coll.analysis.r: Corpus Size: 7898476 Consturction Size: 25569 The output of the Collexeme Analysis (coll.analysis.r) When plotting the results, if you have Inf values in the coll.strength column, please replace all the Infvalues with the maximum numeric value of the coll.strength column. Exercise 8.2 Using the same Chinese news corpus—demo_data/corpus-news-collection.csv, please create a frequency list of all four-character words/idioms that are included in the four-character idiom dictionary demo_data/dict-ch-idiom.txt. Please include both the frequency as well as the dispersion of each four-character idiom in the corpus. Dispersion is defined as the number of articles where it is observed. Please arrange the four-character idioms according to their dispersion. user system elapsed 12.238 0.152 12.393 Exercise 8.3 Let’s assume that we are particularly interested in the idioms of the schema of X_X_, such as “一心一意,” “民脂民膏,” “滿坑滿谷” (i.e., idioms where the first character is the same as the third character). Please find the top 20 frequent idioms of this schema and visualize their frequencies in a bar plot as shown below. Exercise 8.4 Continuing the previous exercise, the idioms of the schema X_X_ may have different types of X. Here we refer to the character X as the pivot of the idiom. Please identify all the pivots for idioms of this schema which have at least two types of constructional variants in the corpus (i.e., its type frequency &gt;= 2) and visualize their type frequencies as shown below. For example, the type frequency of the most productive pivot schema, “不_不_,” is 21 in the news corpus. That is, there are 21 types of constructional variants of this schema, as shown below: Exercise 8.5 Continuing the previous exercise, to further study the semantic uniqueness of each pivot schema, please identify the top 5 idioms of each pivot schema according to the frequencies of the idioms in the corpus. Please present the results for schemas whose type frequencies &gt;= 5 (i.e., the pivot schema has at least FIVE different idioms as its constructional instances). Please visualize your results as shown below. Exercise 8.6 Let’s assume that we are interested in how different media may use the four-character words differently. Please show the average number of idioms per article by different media and visualize the results in bar plots as shown below. The average number of idioms per article can be computed based on token frequency (i.e., on average how many idioms were observed per article?) or type frequency (i.e., on average how many different idiom types were observed per article?). For example, there are 2529 tokens (1443 types) of idioms observed in the 1756 articles published by “Zaobao.” The average token frequency of idiom uses would be: 2529/1756 = 1.440205; the average type frequency of idiom uses would be: 1443/1756 = 0.821754. References Stefanowitsch, A., &amp; Gries, S. T. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. "],["ckiptagger.html", "Chapter 9 CKIP Tagger 9.1 Installation 9.2 Download the Model Files 9.3 R-Python Communication 9.4 Word Segmentation in R 9.5 R Environment Setting 9.6 Creating Conda Environment for ckiptagger 9.7 Initialization 9.8 Segmenting Texts 9.9 Define Own Dictionary 9.10 Beyond Word Boundaries 9.11 Tidy Up the Results", " Chapter 9 CKIP Tagger library(tidyverse) library(reticulate) The current state-of-art Chinese segmenter for Taiwan Mandarin available is probably the CKIP tagger, created by the Chinese Knowledge and Information Processing (CKIP) group at the Academia Sinica. The ckiptagger is released as a python module. In this chapter, I will demonstrate how to use the module for Chinese word segmentation but in an R environment, i.e., how to integrate Python modules in R coherently to perform complex tasks. Alternatively, we can run python codes directly in RStudio. In that case, we don’t need to worry about the Python-to-R interface issues. If you are familiar with Python, you are encouraged to take this option and run the ckiptagger word segmentation directly with Python. 9.1 Installation Because ckiptagger is built in python, we need to have python installed in our working environment. Please install the following applications on your own before you start: Anaconda + Python 3.6+ ckiptagger module in Python (Please install the module using the Anaconda Navigator or pip install in the terminal) (Please consult the github of the ckiptagger for more details on installation.) For some reasons, the module ckiptagger may not be found in the base channel. In Anaconda Navigator, if you cannot find this module, please add specifically the following channel to the environment so that your Anaconda can find ckiptagger module: https://conda.anaconda.org/roccqqck To install ckiptagger in the terminal: pip install -U ckiptagger 9.2 Download the Model Files All NLP applications have their models behind their fancy performances. To use the tagger provided in ckiptagger, we need to download their pre-trained model files. Please go to the github of CKIP tagger to download the model files, which is provided as a zipped file. (The file is very big. It takes a while.) After you download the zipped file, unzip it under your working directory to the data/ directory. 9.3 R-Python Communication In order to call Python functions in R/Rstudio, we need to install an R library in your R. The R-Python communication is made possible through the R library reticulate. Please make sure that you have this library installed in your R. install.packages(&quot;reticulate&quot;) 9.4 Word Segmentation in R Before we proceed, please check if you have everything ready (The following includes the versions of the modules used for this session): Anaconda + Python 3.6+ (Python 3.6.10) Python modules: ckiptagger (ckiptagger 0.2.1 + tensorflow 2.4.1) R library: reticulate(reticulate 1.15) CKIP model files under your working directory ./data If yes, then we are ready to go. 9.5 R Environment Setting We first load the library reticulate and specify in R which Python we will be using in the current R(It is highly likely that there is more than one Python version installed in your system). Please change the path_to_python to your own path, which includes the Anaconda Python you just installed. library(reticulate) 9.6 Creating Conda Environment for ckiptagger I would suggest to install all necessary Python modules in a conda environment and use it in R. In the following demonstration, I assume that you have created a conda environment ckiptagger, where all the necessary modules (i.e., ckiptagger, tensorflow) have been pip-installed. # isntsall in terminal source activate CONDA_ENVIRONMENT_NAME pip install -U ckiptagger conda deactivate 9.7 Initialization There are three important steps in initialization before you can perform word segmentation in R: Activate a specific conda environment in R Import the ckiptagger module in R Initialize the tagger models # Activate a specific conda env in R #use_condaenv(&quot;spacy_condaenv&quot;) #use_condaenv(&quot;r-reticulate&quot;) use_condaenv(&quot;python-notes&quot;) ## Import ckiptagger module ckip &lt;- reticulate::import(module = &quot;ckiptagger&quot;) ## Intialize models ws &lt;- ckip$WS(&quot;./data&quot;) 9.8 Segmenting Texts The initialized word segmenter object, ws(), can tokenize any input character vectors into a list of word vectors of the same size. ## Raw text corpus texts &lt;- c(&quot;傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。&quot;, &quot;美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。&quot;, &quot;土地公有政策?？還是土地婆有政策。.&quot;, &quot;… 你確定嗎… 不要再騙了……他來亂的啦&quot;, &quot;最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.&quot;, &quot;科長說:1,坪數對人數為1:3。2,可以再增加。&quot;) words &lt;- ws(texts) words [[1]] [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; &quot;，&quot; &quot;卻&quot; &quot;突然&quot; [9] &quot;爆出&quot; &quot;自己&quot; &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來&quot; &quot;體育台&quot; [17] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; [25] &quot;電視台&quot; &quot;。&quot; [[2]] [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; [37] &quot;。&quot; [[3]] [1] &quot;土地公&quot; &quot;有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地&quot; &quot;婆&quot; [9] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; [[4]] [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; &quot;他&quot; &quot;來&quot; &quot;亂&quot; &quot;的&quot; &quot;啦&quot; [[5]] [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; [[6]] [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; The word segmenter ws() returns a list object, each element of which is a word-based vector of the original sentence. 9.9 Define Own Dictionary The performance of Chinese word segmenter depends highly on the dictionary. Texts in different disciplines may have very domain-specific vocabulary. To prioritize a set of words in a dictionary, we can further ensure the accuracy of the word segmentation. To create a dictionary for ckiptagger, we need a named list, i.e., to create a list with element names = “the new words” and element values = “the weights.” Then we use the python function ckip$construct_dictionary() to create the dictionary Python object, which is the input argument for word segmenter ws(..., recommend_dictionary = ...). # Define new words in own dictionary new_words &lt;- c(&quot;土地公有&quot;, &quot;土地公&quot;, &quot;土地婆&quot;, &quot;來亂的&quot;, &quot;啦&quot;, &quot;緯來體育台&quot;) # Transform the `vector` into `list` for Python new_words_py &lt;- c(2, 1, 1, 1, 1, 1) %&gt;% as.list # cf. `list(rep, 1 , length(new_words))` names(new_words_py) &lt;- new_words # To create a dictionary for `construct_dictionary()` # We need a list, with names as the words and list elements as the weights in the dictionary # Create Python `dictionary` object, required by `ckiptagger.wc()` dictionary&lt;-ckip$construct_dictionary(new_words_py) # Segment texts using dictionary words_1 &lt;- ws(texts, recommend_dictionary = dictionary) words_1 [[1]] [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; [[2]] [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; [37] &quot;。&quot; [[3]] [1] &quot;土地公有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; [7] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; [[4]] [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; [9] &quot;再&quot; &quot;騙&quot; &quot;了&quot; &quot;…&quot; &quot;…&quot; &quot;他&quot; &quot;來亂的&quot; &quot;啦&quot; [[5]] [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; [[6]] [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; Exercise 9.1 We usually have a list of new words saved in a text file. Can you write a R function, which loads the words in the demo_data/dict-sample.txt into a named list, i.e., new_words, which can easily serve as the input for ckip$construct_dictionary() to create the python dictionary object? (Note: All weights are default to 1) new_words&lt;-loadDictionary(input = &quot;demo_data/dict-sample.txt&quot;) dictionary&lt;-ckip$construct_dictionary(new_words) # Segment texts using dictionary words_2 &lt;- ws(texts, recommend_dictionary = dictionary) words_2 [[1]] [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; [[2]] [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; [37] &quot;。&quot; [[3]] [1] &quot;土地公有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; [7] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; [[4]] [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; [9] &quot;再&quot; &quot;騙&quot; &quot;了&quot; &quot;…&quot; &quot;…&quot; &quot;他&quot; &quot;來亂的&quot; &quot;啦&quot; [[5]] [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; [[6]] [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; 9.10 Beyond Word Boundaries In addition to primitive word segmentation, the ckiptagger provides also the parts-of-speech tags for words and named entity recognitions for the texts. The ckiptagger follows the pipeline below for text processing. Load the models To perform these additional tasks, we need to load the necessary models (pre-trained and provided by the CKIP group) first as well. They should all have been included in the model directory you unzipped earlier (cf. ./data). # loading other necessary models system.time((pos &lt;- ckip$POS(&quot;./data&quot;))) # 詞性 6s user system elapsed 3.563 2.047 5.996 system.time((ner &lt;- ckip$NER(&quot;./data&quot;))) # 實體辨識 8s user system elapsed 3.383 2.042 5.634 POS tagging and NER # Parts-of-speech Tagging pos_words &lt;- pos(words_1) pos_words [[1]] [1] &quot;Nb&quot; &quot;Nd&quot; &quot;D&quot; &quot;VC&quot; [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; [9] &quot;VJ&quot; &quot;Nh&quot; &quot;Neu&quot; &quot;Nf&quot; [13] &quot;Ng&quot; &quot;P&quot; &quot;Nc&quot; &quot;VC&quot; [17] &quot;COMMACATEGORY&quot; &quot;Nh&quot; &quot;D&quot; &quot;VK&quot; [21] &quot;Nh&quot; &quot;Ncd&quot; &quot;VJ&quot; &quot;Nc&quot; [25] &quot;PERIODCATEGORY&quot; [[2]] [1] &quot;Nc&quot; &quot;Nc&quot; &quot;P&quot; &quot;Nd&quot; [5] &quot;Na&quot; &quot;Nb&quot; &quot;D&quot; &quot;VC&quot; [9] &quot;DE&quot; &quot;Na&quot; &quot;Nb&quot; &quot;VC&quot; [13] &quot;VC&quot; &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;VE&quot; [17] &quot;Nh&quot; &quot;D&quot; &quot;D&quot; &quot;Dfa&quot; [21] &quot;VH&quot; &quot;VC&quot; &quot;Nc&quot; &quot;VC&quot; [25] &quot;COMMACATEGORY&quot; &quot;VG&quot; &quot;Nes&quot; &quot;Nc&quot; [29] &quot;D&quot; &quot;Neu&quot; &quot;Nf&quot; &quot;DE&quot; [33] &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; [37] &quot;PERIODCATEGORY&quot; [[3]] [1] &quot;VH&quot; &quot;Na&quot; &quot;QUESTIONCATEGORY&quot; &quot;QUESTIONCATEGORY&quot; [5] &quot;Caa&quot; &quot;Nb&quot; &quot;V_2&quot; &quot;Na&quot; [9] &quot;PERIODCATEGORY&quot; &quot;PERIODCATEGORY&quot; [[4]] [1] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;Nh&quot; &quot;VK&quot; &quot;T&quot; [6] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;D&quot; &quot;D&quot; &quot;VC&quot; [11] &quot;Di&quot; &quot;ETCCATEGORY&quot; &quot;ETCCATEGORY&quot; &quot;Nh&quot; &quot;VA&quot; [16] &quot;T&quot; [[5]] [1] &quot;VH&quot; &quot;VJ&quot; &quot;Neu&quot; &quot;Nf&quot; [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;Caa&quot; &quot;Neu&quot; [9] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; [13] &quot;D&quot; &quot;VH&quot; &quot;T&quot; &quot;PERIODCATEGORY&quot; [17] &quot;Nep&quot; &quot;SHI&quot; &quot;Na&quot; &quot;DE&quot; [21] &quot;Na&quot; &quot;PERIODCATEGORY&quot; [[6]] [1] &quot;Na&quot; &quot;VE&quot; &quot;Neu&quot; &quot;Na&quot; [5] &quot;P&quot; &quot;Na&quot; &quot;VG&quot; &quot;Neu&quot; [9] &quot;PERIODCATEGORY&quot; &quot;Neu&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; [13] &quot;D&quot; &quot;VHC&quot; &quot;PERIODCATEGORY&quot; # Named Entity Recognition ner &lt;- ner(words_1, pos_words) ner [[1]] {(23, 28, &#39;ORG&#39;, &#39;緯來體育台&#39;), (0, 3, &#39;PERSON&#39;, &#39;傅達仁&#39;), (18, 22, &#39;DATE&#39;, &#39;20年前&#39;)} [[2]] {(42, 45, &#39;ORG&#39;, &#39;參議院&#39;), (11, 13, &#39;PERSON&#39;, &#39;布什&#39;), (60, 62, &#39;NORP&#39;, &#39;華裔&#39;), (17, 21, &#39;ORG&#39;, &#39;勞工部長&#39;), (7, 9, &#39;DATE&#39;, &#39;今天&#39;), (21, 24, &#39;PERSON&#39;, &#39;趙小蘭&#39;), (2, 5, &#39;ORG&#39;, &#39;參議院&#39;), (56, 58, &#39;ORDINAL&#39;, &#39;第一&#39;), (0, 2, &#39;GPE&#39;, &#39;美國&#39;)} [[3]] {(10, 13, &#39;PERSON&#39;, &#39;土地婆&#39;)} [[4]] set() [[5]] {(4, 10, &#39;CARDINAL&#39;, &#39;59,000&#39;), (14, 18, &#39;CARDINAL&#39;, &#39;5.9萬&#39;)} [[6]] {(14, 15, &#39;CARDINAL&#39;, &#39;3&#39;), (12, 13, &#39;CARDINAL&#39;, &#39;1&#39;), (16, 17, &#39;CARDINAL&#39;, &#39;2&#39;), (4, 6, &#39;CARDINAL&#39;, &#39;1,&#39;)} 9.11 Tidy Up the Results We can tidy up results provided by ckiptagger and create a word-based tidy structure of our data: word_df &lt;- data.frame(text_id = mapply(rep, c(1:length(texts)), sapply(words_1, length)) %&gt;% unlist, words = do.call(c, words_1), pos = do.call(c, pos_words)) word_df Exercise 9.2 With a word-based tidy structure of the corpus, it is easy to convert it into a text-based one with both the information of word boundaries and parts-of-speech tag. Please convert the above word_df into a text-based data frame, as shown below. Exercise 9.3 How to tidy up the results of ner so that we can include the recognized named entities in the same word-based data frame word_df? You may need to convert the output of ner from ckiptagger into a data frame like this: And figure out a way to add the annotations of named entities in the word-based data frame, word_df, by including another column, as shown below: The above result data frame makes use of the IOB format (short for inside, outside, beginning) for the annotations of the named entities. It is a common tagging format for tagging (multiword) tokens in a chunking task in computational linguistics (e.g., NP-chunking, named entitity, semantic roles). The B- prefix before a tag indicates that the tag is the beginning of a chunk. The I- prefix before a tag indicates that the tag is inside a chunk. The O tag indicates that a token belongs to no chunk (i.e., outside of all relevant chunks). "],["structured-corpus.html", "Chapter 10 Structured Corpus 10.1 NCCU Spoken Mandarin 10.2 CHILDES Format 10.3 Loading the Corpus 10.4 From Text-based to Turn-based DF 10.5 Metadata vs. Utterances 10.6 Word-based DF and Frequency List 10.7 Concordances 10.8 Collocations (Bigrams) 10.9 N-grams (Lexical Bundles) 10.10 Connecting SPID to Metadata 10.11 Corpus Headers 10.12 Sociolinguistic Analyses", " Chapter 10 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for linguistic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. To facilitate the sharing of corpus data, the corpus linguistic community has now settled on a few common schemes for textual data storage and exchange. In particular, I would like to talk about two common types of corpus data representation: CHILDES (this chapter) and XML (Chapter 11). 10.1 NCCU Spoken Mandarin In this demonstration, I will use the dataset of Taiwan Mandarin Corpus for illustration. This dataset, collected by Prof. Kawai Chui at National Cheng-Chi University, includes spontaneous face-to-face conversations of Taiwan Mandarin. The data transcription conventions can be found on the NCCU Corpus Official Website. Generally, the NCCU corpus transcripts follow the conventions of CHILDES format. In computational text analytics, the first step is always to analyze the structure of the textual data. 10.2 CHILDES Format The following is an excerpt from the file demo_data/data-nccu-M001.cha from the NCCU Corpus of Taiwan Mandarin. The conventions of CHILDES transcription include: The lines with header information begin with @ The lines with utterances begin with * The indented lines refer to the utterances of the continuing speaker turn Words are separated by spaces (i.e., a word-segmented corpus) The meanings of transcription symbols used in the corpus can be found in the documention of the corpus. 10.3 Loading the Corpus The corpus data is available in our demo_data/corp-NCCU-SPOKEN.tar.gz, which is a zipped archived file, i.e., one zipped tar file including all the corpus documents. We can use the readtext::readtext() to load the data. In this step, we treat all the *.cha files as if they are normal text files (i.e. .txt) and load the entire corpus into a data frame with two columns: doc_id and text (The warning messages only warn you that by default readtext() takes only .txt files). NCCU &lt;- readtext(&quot;demo_data/corp-NCCU-SPOKEN.tar.gz&quot;) %&gt;% as_tibble 10.4 From Text-based to Turn-based DF Now the data frame NCCU is a text-based one, where each row refers to one transcript file in the corpus. Before we do further tokenization, we first need to concatenate all same-turn utterances (i.e., utterances with no speaker ID at the initial of the line) with their initial utterance of the speaker turn, and then we use unnest_tokens() to transform the text-based DF into a turn-based DF. NCCU_turns &lt;- NCCU %&gt;% mutate(text = str_replace_all(text,&quot;\\n\\t&quot;,&quot; &quot;)) %&gt;% # deal with same-speaker-turn utterances unnest_tokens(turn, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) # inspect first file NCCU_turns %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.5 Metadata vs. Utterances Lines starting with @ are the headers of the transcript while lines starting with * are the utterances of the conversation. We split our NCCU_turns into: NCCU_turns_meta: a DF with all header lines NCCU_turns_utterance: a DF with all utterance lines # Metadata NCCU_turns_meta &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^@&quot;)) # extract all lines starting with `@` # Utterance NCCU_turns_utterance &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^\\\\*&quot;)) %&gt;% # extract all lines starting with `*` group_by(doc_id) %&gt;% mutate(turn_id = row_number()) %&gt;% ungroup %&gt;% tidyr::separate(col=&quot;turn&quot;, # extract SPID into = c(&quot;SPID&quot;, &quot;turn&quot;), sep = &quot;:\\t&quot;) %&gt;% mutate(turn2 = turn %&gt;% # clean up utterances str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) When extracting all the utterances of the speaker turns, we perform data preprocessing as well. Specifically, we: Replace all pause tags with &lt;PAUSE&gt; Replace all extralinguistic tags with &lt;EXTRACLING&gt; Remove all overlapping talk tags Remove all code-switching tags Remove duplicate/trailing/leading spaces The turn-based DF, NCCU_turns_utterance, includes the utterance of each speark turn as well as the doc_id, turn_id and the SPID. All this metadata information can help us connect each utterance back to the original conversation. # Turns of `M001.cha` NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.6 Word-based DF and Frequency List Because the NCCU corpus has been word segmented, we can easily transform the turn-based DF into a word-based DF using unnest_tokens(). The key is that we specify our own tokenization function token =.... The tokenization method is simple: tokenize the utterance into words based on the delimiter of whitespaces. NCCU_words &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(word, turn2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words %&gt;% head(100) With the word-based DF, we can create a word frequency list of the NCCU corpus. NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) NCCU_words_freq %&gt;% head(100) With word frequencies, we can generate a word cloud to have a quick overview of the word distributions in NCCU corpus. # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% # remove annotations/tags select(word, freq) %&gt;% top_n(150, freq) %&gt;% mutate(freq = log(freq)) %&gt;% # deal with Zipfian distribution wordcloud2::wordcloud2(size=0.2, shape=&quot;diamonds&quot;) 10.7 Concordances If we need to identify turns with a particular linguistic unit, we can make use of the data wrangling tricks to easily extract speaker turns with the target pattern. You can of course make use of regular expressions to extract more complex constructions and patterns from the utterances. # extracting particular patterns NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;覺得&quot;)) NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;這樣子&quot;)) Exercise 10.1 If we are interested in the use of the verb 覺得. After we extract all the speaker turns with the verb 覺得, we may need to know the subjects that often go with the verb. Please identify the word before the verb for each concordance token as one independent column of the resulting data frame (see below). Please note that one speaker turn may have more than one use of 覺得. Please create a barplot as shown below to summarize the distribution of the top 10 frequent words that directly precedes 覺得. Among the top 10 words, you would see “的 覺得” combinations, which are counter-intuitive. Please examine these tokens and explain why. Alternatively, we can also create a tokens object and apply the kwic() from quanteda for concordance lines: # `tokens` object NCCU_tokens&lt;- NCCU_turns_utterance$turn2 %&gt;% str_split(&quot;\\\\s+&quot;) %&gt;% map(function(x) x[nzchar(x)]) %&gt;% set_names(str_c(NCCU_turns_utterance$doc_id, NCCU_turns_utterance$turn_id, sep=&quot;-&quot;)) %&gt;% as.tokens # check token numbers sum(sapply(NCCU_tokens, length)) # based on `tokens` [1] 194670 nrow(NCCU_words) # based on `unnest_tokens` [1] 194670 # We can add docvars to `tokens` docvars(NCCU_tokens)&lt;- NCCU_turns_utterance[,c(&quot;doc_id&quot;,&quot;SPID&quot;, &quot;turn_id&quot;)] %&gt;% mutate(Text = doc_id) # KWIC kwic(NCCU_tokens, &quot;覺得&quot;, 8) kwic(NCCU_tokens, &quot;機車&quot;, 8) 10.8 Collocations (Bigrams) Now we extend our analysis beyond single words. Please recall the ngram_chi() function we have defined and used several times in previous chapters. # functions from ch Chinese Text Processing ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s|\\u3000&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc We use the self-defined tokenization function together with unnest_tokens() to transform the turn-based DF into a bigram-based DF. system.time( NCCU_bigrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(bigrams, turn2, token = function(x) map(x, ngram_chi, n = 2)) %&gt;% filter(bigrams!=&quot;&quot;) ) user system elapsed 1.555 0.007 1.564 NCCU_bigrams %&gt;% filter(doc_id == &quot;M001.cha&quot;) Please note that when we perform the n-gram tokenization, we take each speaker turn as our input. This step is important because this would make sure that we don’t get bigrams that span different speaker turns. To determine significant collocations in conversation, we can compute the relevant distributional statistics for each bigram type, including: frequencies dispersion collocation strength (lexical associations) We first compute the frequencies and dispersions of bigrams. NCCU_bigrams_freq &lt;- NCCU_bigrams %&gt;% count(bigrams, doc_id) %&gt;% group_by(bigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% top_n(100, freq) Exercise 10.2 In the above example, we compute the dispersion based on the number of documents where the bigram occurs. Please note that the dispersion can be defined on the basis of the speakers as well, i.e., the number of speakers who use the bigram at least once in the corpus. How do we get dispersion statistics like this? Please show the top frequent 100 bigrams and their SPID-based dispersion statistics. To compute the lexical associations, we need to: remove bigrams with para-linguistic tags exclude bigrams of low dispersion get necessary observed frequencies (e.g., w1 and w2 frequencies) get expected frequencies (for more advanced lexical association metrics) NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% # exclude bigrams with para tags filter(dispersion &gt;= 5) %&gt;% # set bigram dispersion cut-off rename(O11 = freq) %&gt;% tidyr::separate(col=&quot;bigrams&quot;, c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;_&quot;) %&gt;% # split bigrams into two columns mutate(R1 = NCCU_words_freq$freq[match(w1, NCCU_words_freq$word)], C1 = NCCU_words_freq$freq[match(w2, NCCU_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), # compute associations t = (O11 - E11)/sqrt(E11)) %&gt;% mutate_if(is.double, round,2) -&gt; NCCU_collocations NCCU_collocations %&gt;% arrange(desc(dispersion), desc(MI)) # sorting by MI NCCU_collocations %&gt;% arrange(desc(dispersion), desc(t)) # sorting by t Exercise 10.3 Please compute the lexical associations of the bigrams using the log-likelihood ratios. 10.9 N-grams (Lexical Bundles) We can also extend our analysis to n-grams of larger sizes, i.e., the lexical bundles. system.time( NCCU_ngrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(ngram, turn2, token = function(x) map(x, ngram_chi, n = 4, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) ) user system elapsed 1.576 0.016 1.591 NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion), desc(freq)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq %&gt;% filter(dispersion &gt;= 5) 10.10 Connecting SPID to Metadata So far the previous analyses have not used any information of the headers. In other words, the connection between the utterances and their corresponding speakers’ profiles are not transparent in our current corpus analysis. However, for socio-linguists, the headers of the transcripts can be very informative. For example, in the NCCU_turns_meta, we have more demographic information of the speakers, which allows us to further examine the linguistic variations on various social factors (e.g., areas, ages, gender etc.) NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) NCCU_turns_meta %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.11 Corpus Headers In this section, I would like to demonstrate how to extract speaker-related information from the headers (i.e., NCCU_turns_meta) and link these speaker profiles to our corpus data (i.e., NCCU_turns_utterance). Based on the metadata of each file header, we can extract demographic information related to each speaker, including their ID, age, gender, etc. In the headers of each transcript, the demographic profiles of each speaker are provided in the lines starting with @id:\\t; and each piece of information is separated by a pipe sign | in the line. All speakers’ profiles in the corpus follow the same structure. NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) To parse the data of the speaker profiles in the turn column of NCCU_turns_meta: we extract all lines starting with @id separate the column into several columns using | select relevant columns (speaker profiles) rename the columns create unique IDs for each speaker of each transcript NCCU_meta &lt;- NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) %&gt;% separate(col=&quot;turn&quot;, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 10.12 Sociolinguistic Analyses Now with NCCU_meta and NCCU_turns_utterance, we can now connect each utterance to a particular speaker (via SPID in NCCU_turns_utterance and DOC_SPID in NCCU_meta) and therefore study the linguistic variation across speakers of varying sub-groups/communities. The steps are as follows: We first extract the patterns we are interested in from NCCU_turns_utterance; We then connect the concordance tokens to their corresponding SPID profiles in NCCU_meta; We analyze how the patterns vary according to speakers of different profiles. NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.12.1 Check Bigrams Distribution By Age Groups For example, we can look at bigrams used by speakers of varying age groups. The analysis requires the following steps: we retrieve target bigrams from NCCU_bigrams we generate DOC_SPID for all bigrams tokens extracted we map the DOC_SPID to NCCU_meta to get the speaker profiles of each bigram token we recode the speaker’s age into a three-level factor for more comprehensive analysis (i.e., AGE_GROUP) for each age group, we compute the bigram frequencies and dispersion NCCU_bigrams_with_meta &lt;- NCCU_bigrams %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% mutate(DOC_SPID = str_c(doc_id, str_replace_all(SPID,&quot;\\\\*&quot;,&quot;&quot;), sep=&quot;_&quot;)) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_bigrams_by_age &lt;- NCCU_bigrams_with_meta %&gt;% count(bigrams,AGE_GROUP, DOC_SPID) %&gt;% group_by(bigrams, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt;= 5) %&gt;% ungroup NCCU_bigrams_by_age 10.12.2 Numbers of Bigrams above Cut-off by Age We can examine the type frequencies of the bigrams that are used by at least five different speakers for each age group. NCCU_bigrams_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Type Frequency \\nof Bigrams of Dispersion &gt;= 5 (Speaker)&quot;) We can also analyze the token frequencies of the bigrams above the cut-off dispersion value. NCCU_bigrams_by_age %&gt;% group_by(AGE_GROUP) %&gt;% summarize(freq = sum(freq)) %&gt;% ggplot(aes(x=AGE_GROUP, y = freq, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Token Frequencies \\nof Bigrams of Dispersion &gt;= 5 (Speaker)&quot;) 10.12.3 Bigram Word clouds by Age require(wordcloud2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(bigrams, freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2(size = 0.6, shape=&quot;diamond&quot;, rotateRatio = 0.2, minRotation = -pi/2, maxRotation = -pi/2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(bigrams, freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2(size = 0.6, shape=&quot;diamond&quot;, rotateRatio = 0.2, minRotation = -pi/2, maxRotation = -pi/2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(bigrams,freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2(size = 0.6, shape=&quot;diamond&quot;, rotateRatio = 0.2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 10.4 Please create a barplot, showing the top 20 bigrams ranked according to the bigram frequencies for each age group. Also, in the bar graph please include the information of dispersion for each bigram, using the transparency of the bars. The more transparent, the less dispersed (See below). Exercise 10.5 Please create a barplot, showing the top 20 bigrams ranked according to the bigram token frequencies for each male and female speakers. Also, in the graph please include the information of dispersion for each bigram, using the transparency of the bars. The more transparent, the less dispersed (See below). Exercise 10.6 Please analyze the bigram variations in terms of speaker relations and present your findings/observations in your own way. In NCCU Corpus, speaker relations include 14 different relations. You may consider collapsing these levels into larger categories in order to identify the general tendencies. Observations may differ depending on how you analyze your data:) "],["xml.html", "Chapter 11 XML 11.1 BNC Spoken 2014 11.2 Process the Whole Directory of BNC2014 Sample 11.3 Metadata 11.4 BNC2014 for Socio-linguistic Variation 11.5 Lexical Analysis 11.6 Constructions Analysis", " Chapter 11 XML library(tidyverse) library(readtext) library(rvest) library(tidytext) library(quanteda) This chapter shows you how to process the recently released BNC 2014, which is by far the largest representative collection of spoken English collected in UK. For the purpose of our in-class tutorials, I have included a small sample of the BNC2014 in our demo_data. However, the whole dataset is now available via the official website: British National Corpus 2014. Please sign up for the complete access to the corpus if you need this corpus for your own research. 11.1 BNC Spoken 2014 XML is similar to HTML. Before you process the data, you need to understand the structure of the XML tags in the files. Usually we would start from the documentation of the corpus. Please read The BNC 2014: User Manual amd Reference Guide for more detail. Other than that, the steps are pretty much similar to what we have learned before. First, we read the XML using read_html(): # read one file at a time corp_bnc&lt;-read_html(&quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Now it is intuitive that our next step is to extract all utterances (with the tag of &lt;u&gt;...&lt;/u&gt;) in the XML file. So you may want to do the following: corp_bnc %&gt;% html_nodes(xpath = &quot;//u&quot;) %&gt;% html_text %&gt;% head [1] &quot;\\r\\nanhourlaterhopeshestaysdownratherlate&quot; [2] &quot;\\r\\nwellshehadthosetwohoursearlier&quot; [3] &quot;\\r\\nyeahIknowbutthat&#39;swhywe&#39;reanhourlateisn&#39;tit?mmI&#39;mtirednow&quot; [4] &quot;\\r\\n&quot; [5] &quot;\\r\\ndidyoutext--ANONnameM&quot; [6] &quot;\\r\\nyeahyeahhewrotebacknobotherlad&quot; See the problem? Using the above method, you lose the word boundary information from the corpus. What if you do the following? corp_bnc %&gt;% html_nodes(xpath = &quot;//w&quot;) %&gt;% html_text %&gt;% head(20) [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; [8] &quot;rather&quot; &quot;late&quot; &quot;well&quot; &quot;she&quot; &quot;had&quot; &quot;those&quot; &quot;two&quot; [15] &quot;hours&quot; &quot;earlier&quot; &quot;yeah&quot; &quot;I&quot; &quot;know&quot; &quot;but&quot; At the first sight, probably it seems that we have solved the problem but we have not. There are even more problems created: Our second method does not extract non-word tokens within each utterance (e.g., &lt;pause .../&gt;, &lt;vocal .../&gt;) Our second method loses the utterance information (i.e., we don’t know which utterance each word belongs to) So we cannot extract &lt;u&gt; elements all at once; nor can we extract all &lt;w&gt; elements all at once. Probably we need to process each &lt;u&gt; node one at a time. First, let’s get all the &lt;u&gt; nodes. node_u &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;//u&quot;) node_u[[1]] {html_node} &lt;u n=&quot;1&quot; who=&quot;S0024&quot; trans=&quot;nonoverlap&quot; whoconfidence=&quot;high&quot;&gt; [1] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;an&lt;/w&gt; [2] &lt;w pos=&quot;NNT1&quot; lemma=&quot;hour&quot; class=&quot;SUBST&quot; usas=&quot;T1:3&quot;&gt;hour&lt;/w&gt; [3] &lt;w pos=&quot;RRR&quot; lemma=&quot;later&quot; class=&quot;ADV&quot; usas=&quot;T4&quot;&gt;later&lt;/w&gt; [4] &lt;pause dur=&quot;short&quot;&gt;&lt;/pause&gt; [5] &lt;w pos=&quot;VV0&quot; lemma=&quot;hope&quot; class=&quot;VERB&quot; usas=&quot;X2:6&quot;&gt;hope&lt;/w&gt; [6] &lt;w pos=&quot;PPHS1&quot; lemma=&quot;she&quot; class=&quot;PRON&quot; usas=&quot;Z8&quot;&gt;she&lt;/w&gt; [7] &lt;w pos=&quot;VVZ&quot; lemma=&quot;stay&quot; class=&quot;VERB&quot; usas=&quot;M8&quot;&gt;stays&lt;/w&gt; [8] &lt;w pos=&quot;RP&quot; lemma=&quot;down&quot; class=&quot;ADV&quot; usas=&quot;Z5&quot;&gt;down&lt;/w&gt; [9] &lt;pause dur=&quot;short&quot;&gt;&lt;/pause&gt; [10] &lt;w pos=&quot;RG&quot; lemma=&quot;rather&quot; class=&quot;ADV&quot; usas=&quot;A13:5&quot;&gt;rather&lt;/w&gt; [11] &lt;w pos=&quot;JJ&quot; lemma=&quot;late&quot; class=&quot;ADJ&quot; usas=&quot;T4&quot;&gt;late&lt;/w&gt; Take the first node in the XML document for example, each utterance node includes words as well as non-word tokens (i.e., paralinguistic annotations &lt;pause ...&gt;&lt;/pause&gt;). We can retrieve: words in an utterance lemma forms of all words in the utterance pos tags of all words in the utterance (BNC2014 uses UCREL CLAWS6 Tagset) paralinguistic tags in the utterance node_u[[1]] %&gt;% html_children %&gt;% html_text [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; [9] &quot;&quot; &quot;rather&quot; &quot;late&quot; node_u[[1]] %&gt;% html_children %&gt;% html_attr(&quot;pos&quot;) [1] &quot;AT1&quot; &quot;NNT1&quot; &quot;RRR&quot; NA &quot;VV0&quot; &quot;PPHS1&quot; &quot;VVZ&quot; &quot;RP&quot; NA [10] &quot;RG&quot; &quot;JJ&quot; node_u[[1]] %&gt;% html_children %&gt;% html_attr(&quot;lemma&quot;) [1] &quot;a&quot; &quot;hour&quot; &quot;later&quot; NA &quot;hope&quot; &quot;she&quot; &quot;stay&quot; &quot;down&quot; [9] NA &quot;rather&quot; &quot;late&quot; In the above extraction of words, parts-of-speech tags, and lemma forms of each word token in the utterance, there are NA’s in the return. Do you know why? For each utterance, the XML file provides also the metadata information for each utterance, including: its unique index (n) speaker id (who) transition type (trans, i.e., whether or not the transition between turns was overlapping) attribution confidence (whoconfidence, whether or not the transcriber was confident that they had correctly identified the speaker of the turn) node_u[[1]] %&gt;% html_attrs() n who trans whoconfidence &quot;1&quot; &quot;S0024&quot; &quot;nonoverlap&quot; &quot;high&quot; Exercise 11.1 Now we know how to extract token-level information and utterance-level annotation from each utterance. Please come up with a way to extract all relevant linguistic data from all utterances in the file S2A5-tgd.xml, including their word and non-word tokens as well as their metadata. Ideally, the resulting data frame should consist of rows being the tokens of the utterances, and columns including the attributes and strings of each token. Most importantly, the data frame should include not only the strings of the tokens, but at the same time for the word tokens, it should preserve the BNC token-level annotations of part-of-speech tags, lemmas, and semantic tags (i.e., usas). Also, each token is connected to the utterance-level metadata, such as the utterance ID, speaker ID etc. A sample utterance-based data frame is provided below. 11.2 Process the Whole Directory of BNC2014 Sample 11.2.1 Define Function In Section 11.1, if you have figured out how to extract the token-based data frame from all utterances in an XML file, you can easily wrap the whole procedure as one function. With this function, we can perform the same procedure to all the xml files of the BNC2014. For example, let’s assume that we have defined a function: read_xml_bnc2014 &lt;- function(xml){ ... ... ... } # endfunc This function takes one xml file as an argument and returns a token-based data frame, consisting of token texts and other relevant utteracne-level and token-level information from the xml. word_df &lt;- read_xml_bnc2014(xmlfile = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) word_df %&gt;% filter(n %in% c(&quot;1&quot;,&quot;2&quot;)) Exercise 11.2 Now your job is to create this function, read_xml_bnc2014(xmlfile = \"\"). The function should take the path to one XML file as the input and return a token-based data frame of the XML file as shown above. 11.2.2 Process the all files in the Directory Now we can utilize the self-defined function, read_xml_bnc2014(), and process all xml files in the demo_data/corp-bnc-spoken2014-sample/. Also, we combine the individual data.frame returned from each XML into a bigger one, i.e., corp_bnc_df: # file list bnc_flist &lt;- dir(&quot;demo_data/corp-bnc-spoken2014-sample/&quot;,full.names = T) # extract token-based df system.time(corp_bnc_list &lt;- lapply(bnc_flist, read_xml_bnc2014)) # combine all df&#39;s corp_bnc_token_df &lt;- corp_bnc_list %&gt;% # do.call(rbind, .) %&gt;% bindrows() %&gt;% ## thanks to 申少帥 for this suggested method to speed up mutate(xml_id = rep(basename(bnc_flist), map(corp_bnc_list,nrow))) # save file write_csv(corp_bnc_token_df, path= &quot;demo_data/data-corp-token-bnc2014.csv&quot;) It takes a while to process/parse all the files included in the sample directory because we parse the entire XML file and extract almost everything from the file. You may store this corp_bnc_token_df data frame output for later use so that you don’t have to process the XML files every time you work with BNC2014. The parsed token-based data frame of the BNC2014 is available in our demo_data/data-corp-token-bnc2014.csv: corp_bnc_token_df &lt;- read_csv(&quot;demo_data/data-corp-token-bnc2014.csv&quot;) corp_bnc_token_df %&gt;% filter(xml_id == &quot;S2A5-tgd.xml&quot; &amp; n == &quot;1&quot;) 11.3 Metadata The best thing about BNC2014 is its rich demographic information relating to the settings and speakers of the conversations collected. The whole corpus comes with two metadata sets: bnc2014spoken-textdata.tsv: metadata for each text transcript bnc2014spoken-speakerdata.tsv: metadata for each speaker ID These two metadata sets allow us to get more information about each transcript as well as the speakers in those transcripts. 11.3.1 Text Metadata There are two files that are relevant to the text metadata: bnc2014spoken-textdata.tsv: This file includes the header/metadata information of each text file metadata-fields-text.txt: This file includes the column names/meanings of the previous text metadata tsv, i.e., bnc2014spoken-textdata.tsv. bnc_text_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-textdata.tsv&quot;, col_names = FALSE) bnc_text_meta bnc_text_meta_names &lt;-read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-text.txt&quot;, skip =2, col_names = F) bnc_text_meta_names names(bnc_text_meta) &lt;- c(&quot;textid&quot;, bnc_text_meta_names$X2) bnc_text_meta 11.3.2 Speaker Metadata There are two files that are relevant to the speaker metadata: bnc2014spoken-speakerdata.tsv: This file includes the demographic information of each speaker metadata-fields-speaker.txt: This file includes the column names/meanings of the previous speaker metadata tsv, i.e., bnc2014spoken-speakerdata.tsv. bnc_sp_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-speakerdata.tsv&quot;, col_names = F) bnc_sp_meta bnc_sp_meta_names &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-speaker.txt&quot;, skip = 3, col_names = F) bnc_sp_meta_names names(bnc_sp_meta) &lt;- c(&quot;spid&quot;, bnc_sp_meta_names$X2) bnc_sp_meta 11.4 BNC2014 for Socio-linguistic Variation Now with both the text-level and speaker-level metadata, bnc_text_meta and bnc_sp_meta, we can easily connect the utterances to speaker and text profiles using their unique ID’s. BNC2014 was born for the study of socio-linguistic variation. Here I would like to show you some naive examples, but you should get the ideas and the potentials of BNC2014. 11.5 Lexical Analysis With the token-based data frame, we can perform lexical analysis on the lexical variations on specific social dimensions. 11.5.1 Word Frequency vs. Gender In this section, I would like to demonstrate how to explore the gender differences in language. Let’s assume that we like to know which adjectives are most frequently used by men and women. corp_bnc_adj_gender &lt;- corp_bnc_token_df %&gt;% filter(str_detect(pos, &quot;^(JJ[RT]?$)&quot;)) %&gt;% left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) %&gt;% mutate(gender = factor(gender, levels=c(&quot;F&quot;,&quot;M&quot;))) %&gt;% filter(!is.na(gender)) corp_bnc_adj_gender %&gt;% head(100) 11.5.2 Frequency and Keyword Analysis After we extract word tokens that are adjectives, we can create a frequency list: freq_adj_by_gender &lt;- corp_bnc_adj_gender %&gt;% count(gender, lemma, sort = T) freq_adj_by_gender %&gt;% group_by(gender) %&gt;% top_n(10, n) %&gt;% ungroup %&gt;% arrange(gender, desc(n)) Female wordcloud require(wordcloud2) freq_adj_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% top_n(100,n) %&gt;% select(lemma, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Male wordcloud freq_adj_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% top_n(100,n) %&gt;% select(lemma, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 11.3 Which adjectives are more often used by male and female speakers? This should be a statistical problem. We can in fact extend our keyword analysis (cf. Chapter 6) to this question. Please use the statistics of keyword analysis to find out the top 20 adjectives that are strongly attracted to female and male speakers according to G2 statistics. Please include in the analysis words whose frequencies &gt;= 20 in the entire corpus. Also, please note the problem of the NaN values out of the log(). 11.6 Constructions Analysis 11.6.1 From Token-based to Turn-based Data Frame We can also conduct analysis of specific constructions. Because constructions often span word boundaries, what we have right now is a token-based data frame of the BNC2014. For construction or multiword-unit analysis, we can convert the token-based DF into a turn-based DF, but keep necessary token-level annotations relevant to your research project. In this demonstration, I would like to convert the token-based DF into a turn-based DF, and keep the strings of word forms as well as parts-of-speech tags of words for each token. But for non-word tokens, we use the name of the tag, enclosed by &lt; and &gt;, to represent the nature of the extralinguistic annotations in the utteracnes. corp_bnc_token_df &lt;- read_csv(&quot;demo_data/data-corp-token-bnc2014.csv&quot;) head(corp_bnc_token_df) corp_bnc_utterance_df &lt;- corp_bnc_token_df %&gt;% group_by(xml_id, n, who, trans, whoconfidence) %&gt;% nest %&gt;% ungroup extract_wordtag_string &lt;- function(u_df){ utterance_df &lt;- u_df cur_text &lt;-utterance_df$text cur_pos &lt;- utterance_df$pos tag_index &lt;-which(utterance_df$name != &quot;w&quot;) if(length(tag_index)&gt;0){ cur_text[tag_index] &lt;- paste(&quot;&lt;&quot;,utterance_df$name[tag_index],&quot;&gt;&quot;, sep=&quot;&quot;) cur_pos[tag_index] &lt;- paste(&quot;&lt;&quot;,utterance_df$name[tag_index],&quot;&gt;&quot;, sep=&quot;&quot;) } paste(cur_text, cur_pos, sep = &quot;_&quot;, collapse=&quot; &quot;) } corp_bnc_utterance_df %&gt;% mutate(utterance = map_chr(data, extract_wordtag_string)) %&gt;% select(-data) -&gt; corp_bnc_utterance_df corp_bnc_utterance_df %&gt;% head(50) With the above utterance-based DF of the corpus, we can extract constructions or morphosyntactic patterns from the utterance column, utilizing the parts-of-speech tags provided by BNC2014. 11.6.2 Degree ADV + ADJ In this section I would like to show you an example where we can extend our lexical analysis to a particular syntactic pattern. Specifically, I like to look at the adjectives that are emphasized in conversations (e.g., too bad, very good, quite cheap) and examine how these emphatic adjectives may differ in speakers of different genders. Here we define our patterns, utilizing the POS tags and the regular expressions: [^_]+_RG [^_]+_JJ We first extract the target patterns by converting the utterance-based DF into a pattern-based DF. We at the same time link each match with the speaker metadata. corp_bnc_pat_gender &lt;- corp_bnc_utterance_df %&gt;% unnest_tokens(pattern, utterance, token = function(x) str_extract_all(x, &quot;[^_ ]+_RG [^_ ]+_JJ&quot;), to_lower = F) %&gt;% left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) corp_bnc_pat_gender %&gt;% head(100) Then we can create pattern freqeucies by genders. freq_pat_by_gender &lt;- corp_bnc_pat_gender %&gt;% mutate(pattern = str_replace_all(pattern, &quot;_[^_ ]+&quot;,&quot;&quot;)) %&gt;% # remove pos tags select(gender, pattern) %&gt;% count(gender, pattern, sort=T) # print top 100 freq_pat_by_gender %&gt;% group_by(gender) %&gt;% top_n(10,n) %&gt;% ungroup %&gt;% arrange(gender, desc(n)) We can also create wordclouds for the patterns by gender # wordcloud freq_pat_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% top_n(100, n) %&gt;% select(pattern, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) freq_pat_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% top_n(100, n) %&gt;% select(pattern, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 11.4 In the previous task, we have got the frequency list of the patterns (i.e., “Adverb + Adjective”) by gender, i.e., freq_pat_by_gender. Please create a wide version of the frequency list, where each row is a pattern type and the columns include the frequencies of the patterns in male and female speakers, as well as the dispersion of the pattern in male and female speakers. A sample has been provided below. Dispersion is defined as the number of speakers who use the pattern at least once. Exercise 11.5 So far we have been looking at the constructional schema of ADV + ADJ. Now let’s examine further how adjectives that are emphasized differ among speakers of different genders. That is, do male speakers tend to emphasize adjectives that are different from those that are emphasized by females? Now it should be clear to you that which adjectives are more likely to be emphasized by ADV in male and female utterances should be a statistical question. Please use the statistics G2 from keyword analysis to find out the top 10 Adjectives that are strongly attracted to female and male speakers according to G2 statistics. Please include in the analysis adjectives whose dispersion &gt;= 2 in the respective corpus, i.e., adjectives that have been used by at least TWO different male or female speakers. Also, please note the problem of the NaN values out of the log(). You first need to get the frequency list of the ajectives that occur in this constructional schema, ADV + ADJ: Then you convert the frequency list from a long format into a wide format for keyness computation. With the above distributional information, you can compute the keyness of the adjectives. Exercise 11.6 Please analyze the verbs that co-occur with the first-person pronoun I in BNC2014 in terms of speakers of different genders. Please create a frequency list of the verbs that follow the first person pronoun I in demo_data/corp-bnc-spoken2014-sample. Verbs are defined as any words whose POS tag starts with VV. Also, please create the word clouds of the top 100 verbs for male and female speakers. All verb types on the top 100 lists of male and female speakers: Female Wordcloud Male Wordcloud Exercise 11.7 Please analyze the recurrent four-grams used by male and female speakers by showing the top 20 four-grams used by males and females respectively ranked according to their dispersions. Dispersion of four-grams is defined as the number of texts (i.e., XML files) where the four-gram is observed. "],["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Distributional Semantics 12.2 Vector Space Semantics: Parameters 12.3 Vector Space Model for Documents 12.4 Vector Space Model for Words 12.5 Exercises", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) In this chapter, I would like to talk about the idea of distributional semantics, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized computationally and quantified using the distributional data of the linguistic units in the corpus. Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts. 12.1 Distributional Semantics Distributional approach to semantics was first formulated by John Firth in his famous quotation: “You shall know a word by the company it keeps” (Firth, 1957, p. 11). In other words, words that occur in the same contexts tend to have similar meanings (Z. Harris, 1954). “[D]ifference of meaning correlates with difference of distribution.” (Z. S. Harris, 1970, p. 785) “The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves.” (De Deyne et al., 2016) In computational linguistics, this idea has been implemented in the modeling of lexical semantics and documents topics. The lexical meanings of words or topics of documents can be computationally represented by the distributional information of their co-occurring words. VSR For Words (Lexcial Semantics) On the one hand, one can extract the distributional information of target words automatically from large corpora, which are referred to as the contextual features of the target words. These co-occurrence frequencies (raw or weigthed) between target words and contextual features can be combined in long vectors, which can be utilized to computationally measure the lexical semantic distance or similarity. VSR For Documents (Document Semantics/Topics) On the other hand, this distributional model can be applied to the semantic representation of documents in corpus as well. One can extract the distributional information of target documents automatically from large corpora, i.e., their contextual features. The co-occurrence frequencies between target documents and contextual features can also be combined in long vectors, which can also be utilized to computationally measure the document similarity/difference. Therefore, this distributional approach to meanings is sometimes referred to as Vector Space Semantics. 12.2 Vector Space Semantics: Parameters Different studies may however develop different operational definitions in their extraction of the contextual features for vector space model. Contextual Features Types: Bag-of-words: One can include all co-occurring words of the target word/document as contextual features (without considering the linear syntagmatic ordering of words). Structurally-dependent words: One can include co-occurring words of the target word/document within only particular morpho-syntactic frames (or of particular morpho-syntactic categories). Contextual Features Window: When adopting the bag-of-words approach to word meanings, one can determine the size of the context window for the contextual features inclusion (i.e., specify a certain number of tokens on the right and left from the target words). Co-occurrence Metrics The co-occurrence frequencies between target words/documents and contextual features can be statistically weighted to better represent their relationships. Common weights include tf.idf or Pointwise Mutual Information. Marginal Frequencies The co-occurrence frequencies may need to be evaluated according to the marginal frequencies of the contextual features. For example, given a co-occurrence frequency, 50, of a contextual feature with a document, it would be more indicative when the marginal frequency of the contextual feature is 100 (because half occurrences of the contextual feature go with the document). It would be much less indicative when its marginal frequency is 10,000 (because only a tidy proportion of the occurrences of the contextual feature go with the document). Dimensional Reduction When the target words/documents are represented as long vectors, they are often sparse vectors because most of their co-occurrence frequencies would be zero. There are some computational methods, which allow us to automatically extract the semantic fields from the word-based contextual features by reducing the dimensions of the long vectors. For example, it is possible that some of the contextual features (e.g., green, blue, and red) are connected semantically to form a semantic field (e.g., COLOR). A classic example is Latent Semantic Analysis, which is reminiscent of Principal Component Analysis. For more information on vector-space semantics, I would highly recommend the chapter of Vector Semantics and Embeddings, by Dan Jurafsky and James Martin. 12.3 Vector Space Model for Documents Now I would like to demonstrate how we can adopt this vector space model to study the semantics of documents. 12.3.1 Data Processing Flowchart In Chapter 5, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of the corpus documents. Most importantly, a new object class is introduced in Figure 12.1, i.e., the dfm object in quanteda. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterize the documents. The cells in the matrix are the co-occurrence statistics between each document and the feature. Different ways of operationalizing the features and the cell values may lead to different types of dfm. In this section, I would like to show you how we create a dfm of a corpus and what are the common ways to define features and cell valus for the analysis of document semantics via vector space representation. Figure 12.1: English Text Analytics Flowchart (v2) 12.3.2 Document-Feature Matrix (dfm) To create a dfm, i.e., Dcument-Feature-Matrix, quanteda provides two alterantives: create dfm based on an corpus object create dfm based on an tokens object For English data, quanteda can take care of the word tokenization fairly well, so you can create dfm directly from corpus (See Figure 12.1 above) In the chapter, Chinese Text Processing, we stress that the default tokenization method in quanteda with Chinese data may be limited in several ways. In order to create a dfm that takes into account the appropriateness of the Chinese word segmentation, I would highly recommend you to first create a tokens object using the self-defined word segmentation methods, and then feed it to dfm() to create the dfm for your corpus. In this way, the dfm will use the segmented results defined by your word segmenter. In other words, with Chinese data, probably it is not really necessary to have a corpus object; rather, a tokens object of the corpus might be more useful/practical. (In quanteda, most of the functions for corpus can be applied to tokens as well, e.g., kwic(), dfm()) 12.3.3 Corpus In this tutorial, I will use the same English dataset as we discussed in Chapter 5, the data_corpus_inaugural, preloaded in the package quanteda. For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% tokens() %&gt;% dfm Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; class(corp_us) [1] &quot;corpus&quot; &quot;character&quot; class(corp_us_dfm) [1] &quot;dfm&quot; attr(,&quot;package&quot;) [1] &quot;quanteda&quot; 12.3.4 Document-Feature Matrix (dfm) What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s) (i.e., the contextual features in the vector space model). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the corpus data_corpus_inaugural, based on the corp_us_dfm created earlier, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and. A dfm with words as the contextual features is the simplest way to characterize the documents in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each text as bags of words. That is, how the words are arranged relative to each other is ignored (i.e., the morphosyntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word dfm should be a naive characterization of the texts. In many computational tasks, however, it turns out that this simple bag-of-words model is very effective in modeling the semantics of the documents. 12.3.5 Distributional Hypothesis and Distance/Similarity Metrics With the dfm, the idea is that if two documents have similar sets of linguistic units (i.e., contextual features) popping up in them, they are more likely to be similar in their semantics as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with the other documents (i.e., the other rows). Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically: Figure 12.2: Vector Representation In Math, there are in general two types of metrics to measure the relationship between vectors: distance-based vs. similarity-based metrics. 12.3.5.1 Distance-based Metrics Many distance measures of vectors are based on the following formula and differ in individual parameter settings. \\[\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^y}\\big)^{\\frac{1}{y}}\\] The n in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.) When y is set to 2, it computes the famous Euclidean distance of two vectors, i.e., the direct spatial distance between two points on the n-dimensional space. \\[\\sqrt{\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^2}\\big)}\\] x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) # computing pairwise euclidean distance sum(abs(x-y)^2)^(1/2) # XY distance [1] 6 sum(abs(y-z)^2)^(1/2) # YZ distnace [1] 4.472136 sum(abs(x-z)^2)^(1/2) # XZ distnace [1] 8.944272 The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure 12.3) Figure 12.3: Distance-based Metric: Euclidean Distance 12.3.5.2 Similarity-based Metrics In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is Cosine Similarity, which can be computed as follows: \\[cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\] # comuting pairwise cosine similarity sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2))) # xy [1] 0.9778024 sum(y*z)/(sqrt(sum(y^2))*sqrt(sum(z^2))) # yz [1] 0.4961389 sum(x*z)/(sqrt(sum(x^2))*sqrt(sum(z^2))) # xz [1] 0.3032037 The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the greater their cosine similarity, the smaller the arcs, the closer they are. 12.3.5.3 Computing pairwise distance/similarity using quanteda In quanteda.textstats library, there are many functions that can help us compute pairwise similarities/distances between vectors using two useful functions: textstat_simil() textstat_dist() The expected input argument of these two functions is a dfm: # check quantida similarity and distance metrics xyz_dfm &lt;-matrix(c(1,9,1,3,5,1), byrow=T, ncol=2) %&gt;% as.dfm xyz_dfm Document-feature matrix of: 3 documents, 2 features (0.00% sparse) and 0 docvars. features docs feat1 feat2 text1 1 9 text2 1 3 text3 5 1 # Computing pairwise distance/similarity # using `quanteda` functions textstat_simil(xyz_dfm, method=&quot;cosine&quot;) textstat_simil object; method = &quot;cosine&quot; text1 text2 text3 text1 1.000 0.978 0.303 text2 0.978 1.000 0.496 text3 0.303 0.496 1.000 textstat_dist(xyz_dfm, method = &quot;euclidean&quot;) textstat_dist object; method = &quot;euclidean&quot; text1 text2 text3 text1 0 6.00 8.94 text2 6.00 0 4.47 text3 8.94 4.47 0 (1- amap::Dist(xyz_dfm, method=&quot;pearson&quot;)) text1 text2 text2 0.9778024 text3 0.3032037 0.4961389 12.3.5.4 Interim Summary The Euclidean Distance metric is a distance-based metric: the larger the value, the more distant the two vectors. The Cosine Similarity metric is a similarity-based metric: the larger the value, the closer the two vectors. Based on our computations of the metrics for the three vectors, now in terms of the Euclidean Distance, y and z are closer; in terms of Cosine Similarity, x and y are closer. Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the documents/words: The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors) The relative increase/decrease of the values on ecah dimension (i.e., the curvatures of vectors) There are many other distance-based or similarity-based metrics available. For more detail, please see Manning &amp; Schütze (1999) Ch15.2.2. and Jurafsky &amp; Martin (2020) Ch6: Vector Semantics and Embeddings. 12.3.6 Multidimensiona Space Back to our example of corp_us_dfm, it is essentially the same vector representation, but in a multidimensional space (cf. Figure 12.4). The document in each row is represented as a vector of N dimensional space. The size of N depends on the number of contextual features that are included in the analysis of the dfm. Figure 12.4: Example of Document-Feature Matrix 12.3.7 Vector Semantics Considerations When representing the document semantics with contextual features in a multi-dimensional vector space, two factors would turn out to be very crucial: which contextual features are more representative to be included in the semantic analysis of documents? which quantitative metrics should be used to better represent the co-occurring relationship (i.e., association) between the documents and the contextual features? In our current dfm based on bags of words, our concerns would be: which words should be included in the analysis of multidimensional representation? which quantitative metrics should be used to represent the relationship between the documents and the words? 12.3.8 Feature Selection A dfm may not be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered with respect to the contextual features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features 12.3.8.1 Granularity In our previous example, we include only words, i.e., unigrams, as our features in the corp_us_dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: from corpus to tokens from tokens to ngram-based tokens from ngram-based tokens to dfm corp_us_dfm_bigram &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;) %&gt;% tokens_ngrams(n=2) %&gt;% dfm Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: # unigram dfm + stem corp_us_dfm_unigram_stem &lt;- corp_us %&gt;% tokens() %&gt;% dfm() %&gt;% dfm_wordstem() # bigram dfm + stem corp_us_dfm_bigram_stem &lt;- corp_us %&gt;% tokens() %&gt;% tokens_ngrams(n = 2) %&gt;% dfm() %&gt;% dfm_wordstem() You need to decide which types of contextual features are more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, these are only heuristics, not rules. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 12.3.8.2 Informativeness There are words that are not so informative in telling us the similarity and difference between the documents because they almost appear in every document of the corpus, but carray little (referential) semantic contents. These words are usually function words, such as and, the, of. These common words observed in almost all documents are often referred to as stopwords. Therefore, it is not uncommon that analysts sometimes create a list of stopwords to be removed from the dfm. The library quanteda has defined a default English stopword list, i.e., stopwords(\"en\"). stopwords(&quot;en&quot;) %&gt;% head(50) [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; length(stopwords(&quot;en&quot;)) [1] 175 Also, there are tokens that usually carry very limited semantic contents, such as numbers and punctuation. Numbers, symbols and punctuations are often treated differently in computational text analytics. When creating the dfm object, we can further specify a few parameters for the function dfm(): remove_punct = TRUE: remove all punctuation tokens remove = vector(): remove all words specified in the character vector here corp_us_dfm_unigram_stop_punct &lt;- corp_us %&gt;% tokens(remove_punct = T) %&gt;% dfm() %&gt;% dfm_remove(stopwords(&quot;en&quot;)) We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm) # default unigram version [1] 9439 nfeat(corp_us_dfm_unigram_stem) # unigram + stem [1] 5596 nfeat(corp_us_dfm_bigram) # bigram [1] 64442 nfeat(corp_us_dfm_bigram_stem) # bigram + stem [1] 58045 nfeat(corp_us_dfm_unigram_stop_punct) # unigram removing stopwords and punks [1] 9285 12.3.8.3 Distributional Properties Depending on the granularity of the contextual features you are considering, you may get a considerably large number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the contextual word occurs only once in the corpus (i.e., hapax legomenon), these words may be highly idiosyncratic, which can be of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurs very frequently, they may be a function word, carrying little semantic content. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate that this feature is too domain-specific. Therefore, sometimes we can control the document frequency of the contextual features (i.e., in how many different texts does the feature occur?) Other Self-defined Weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a document d, the significance of this n may be connected to: the document size of d the total number of w We can therefore utilize association-based metrics as weighted versions of the co-occurrence frequencies (e.g., PMI, LLR etc.) Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. Exercise 12.2 Please get familar with the following functions, provided by quanteda for weighting of the document-feature matrix: dfm_weight(), dfm_tfidf(). In the following demo, we adopt a few simple distrubtional criteria: we create a simple unigram dfm based on the word-forms we remove stopwords, punctuations, numbers, and symbols we remove contextual words whose freqency &lt; 10, docfreq &lt; 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% tokens(remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% dfm() %&gt;% dfm_remove(stopwords(&quot;en&quot;)) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) nfeat(corp_us_dfm_trimmed) [1] 1401 12.3.9 Exploratory Analysis of dfm We can check the top features in the current corpus: topfeatures(corp_us_dfm_trimmed) people government us can must upon great 584 564 505 487 376 371 344 may states world 343 334 319 We can visualize the top features using a word cloud: require(RColorBrewer) set.seed(100) textplot_wordcloud(corp_us_dfm_trimmed, max_words = 200, random_order = FALSE, rotation = .25, color = c(&#39;red&#39;, &#39;pink&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;blue&#39;)) 12.3.10 Document Similarity As shown in 12.4, with the N-dimensional vector representation of each document, we can compute the mathematical distances/similarities between two documents. In Section 12.3.5, we introduced two important metrics: Distance-based metric: Euclidean Distance Similarity-based metric: Cosine Similarity quanteda provides useful functions to compute these metrics (as well as other alternatives): textstat_simil() and textstat_dist() When computing the document similarity/distance, we usually normalize the joint frequencies of contextual features (i.e., words) and documents, to reduce the impact of the marginal frequencies on the significance of the co-occurrence frequencies. In our current example, we adopted a simple method of normalization, i.e., converting the raw joint-frequencies into percentages. (i.e., dfm_weight(…, scheme = “prop”)) # Understanding `dfm_weight` prop normalization # Check the first FIVE contextual features # of the first document # raw joint frequencies corp_us_dfm_trimmed[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 1 1 2 2 1 # normalized prop using `dfm_weight()` dfm_weight(corp_us_dfm_trimmed, scheme=&quot;prop&quot;)[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives 1789-Washington 0.002427184 0.002427184 0.004854369 0.004854369 features docs among 1789-Washington 0.002427184 # intuition corp_us_dfm_trimmed[1,1:5]/sum(corp_us_dfm_trimmed[1,]) Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives 1789-Washington 0.002427184 0.002427184 0.004854369 0.004854369 features docs among 1789-Washington 0.002427184 corp_us_euclidean &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme=&quot;prop&quot;) %&gt;% textstat_dist(method=&quot;euclidean&quot;) corp_us_cosine &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme=&quot;prop&quot;) %&gt;% textstat_simil(method=&quot;cosine&quot;) Distance-based Results Cosine-based Results Based on the distances/similarities, we can further examine how different documents may cluster together in terms of their contextual features (lexical) distributions. Here we apply the hierarchical cluster analysis to examine the sub-groupings of the documents. # distance-based corp_us_hist_euclidean &lt;- corp_us_euclidean %&gt;% as.dist %&gt;% hclust plot(corp_us_hist_euclidean,hang = -1, cex = 0.6) # similarity corp_us_hist_cosine &lt;- (1 - corp_us_cosine) %&gt;% as.dist %&gt;% hclust plot(corp_us_hist_cosine,hang = -1, cex = 0.6) Please note that textstat_simil() gives us the similarity matrix. In other words, the numbers in the matrix indicate how similar the documents are. However, for hierarchical cluster analysis, the function hclust() expects a distance-based matrix, namely one indicating how dissimilar the documents are. Therefore, we need to use (1 - corp_us_cosine) in the cosine example before performing the cluster analysis. Cluster anlaysis is a very useful exploratory technique to examine the emerging structure of a large dataset. For more detail introduction to this statistical method, I would recommend (gries2013?) Ch 5.6 and the very nice introductory book, Kaufman &amp; Rousseeuw (1990). 12.4 Vector Space Model for Words So far, we have been talking about applying the vector space model to study the document semantics. Now let’s take a look at how this distributional semantic approach can facilitate a lexical semantic analysis. With a corpus, we can also study the distribution, or contextual features, of words based on their co-occurring words. Now I would like to introduce another object defined in quanteda, i.e., the Feature-Cooccurrence Matrix fcm. 12.4.1 Feature-Coocurrence Matrix (fcm) A Feature-Cooccurrence Matrix is essentially a word cooccurrence matrix. There are two ways to create a fcm: from corpus/tokens to fcm from dfm to fcm 12.4.2 From corpus/tokens to fcm We can create a word-cooccurrence matrix fcm directly from the corpus object. We can further operationalize our contextual features for words: Window-based: Only words co-occurring within a defined window size will be included as contextual features Document-based: All words co-occurring in the same document will be included as contextual features. Example of Window-based fcm: # window-based corp_fcm_win &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;window&quot;, window = 1) # fcm based on window context topfeatures(corp_fcm_win,n = 50) our the in we is to 1978 1766 1359 1217 1062 884 be a all for We people 875 777 749 740 715 679 will their are and it that 661 652 608 555 554 546 been this us States by not 530 527 498 467 444 434 upon as do its world those 433 428 427 426 426 412 must should peace Government It them 397 392 390 386 377 367 great power Constitution any only freedom 361 355 339 325 321 314 may government with they shall nation 302 301 299 297 289 288 I so 283 274 A look at the corp_fcm_win: corp_fcm_win[&quot;our&quot;,] %&gt;% topfeatures(10) national institutions upon common Constitution Union 38 26 22 21 20 19 children Nation fathers within 19 19 18 15 corp_fcm_win[&quot;must&quot;,] %&gt;% topfeatures(10) We America do go continue There citizen keep 46 9 7 5 5 4 4 4 carry realize 4 4 Example of Document-based fcm: corp_fcm_doc &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;document&quot;) # same as the first one topfeatures(corp_fcm_doc,n = 50) our the to in and a 3176109 3022080 2619459 2586044 2310486 1982691 be is we for their that 1923444 1828034 1671039 1449786 1286232 1254497 are it will The not as 1212962 1185232 1140058 1136470 1081138 993573 We by all which has its 979336 923827 908249 861352 857148 835208 people upon or this us been 831527 822480 816930 809940 728218 716072 of I should It must any 681721 678465 672000 668992 641750 641596 have with them so States great 638159 626747 615322 600749 575961 574693 Government power may they only at 570366 556246 544947 525462 522589 518657 from Constitution 515615 491481 corp_fcm_doc[&quot;our&quot;,] %&gt;% topfeatures(10) we our We us must upon should world any power 54260 47064 24405 21543 17170 15701 13697 12791 11170 10692 corp_fcm_doc[&quot;must&quot;,] %&gt;% topfeatures(10) We us upon do any must only America peace when 5147 3914 3258 2126 2015 1996 1995 1956 1846 1506 In the above examples of fcm, you can in fact create the fcm directly with the corpus object (i.e., no need to transform the corpus into tokens). But we choose the tokenize our corpus into tokens first and then create the fcm. The advantage of our current method is that we can manipulate the number as well as the types of tokens we would like to include in the fcm. 12.4.3 From dfm to fcm A fcm can also be created from dfm. The limitation is that we can only create a document-based fcm from dfm. But we can make use of the feature selection discussed in the previous sections to remove irrelevant contextual features before we create the fcm. # convert `dfm` to `fcm` corp_fcm_dfm &lt;- corp_us_dfm_trimmed %&gt;% fcm() topfeatures(corp_fcm_dfm,n = 50) upon us must peace freedom power 179044 151948 127748 116086 106320 104929 government constitution part spirit law people 100582 98820 95195 89563 88701 79380 laws business congress war state shall 78924 77645 76174 74119 73947 73720 today best make within union world 72941 72190 71803 71153 70608 70446 work let progress political come great 68762 68484 67070 65616 65338 65214 always america purpose states god revenue 63302 61248 61052 60565 60324 59702 americans force justice rights institutions countrymen 57926 57685 56809 56506 55619 54818 republic true federal yet foreign others 54714 54148 54024 53330 52400 52114 long action 51463 51055 corp_fcm_dfm[&quot;people&quot;,] %&gt;% topfeatures(10) government upon us states great must 8094 5850 5292 5279 4658 4483 people power shall constitution 4217 4060 3891 3881 corp_fcm_win[&quot;people&quot;,] %&gt;% topfeatures(10) our American The are free themselves great 79 40 27 19 12 8 8 It we Our 8 8 8 corp_fcm_doc[&quot;people&quot;,] %&gt;% topfeatures(10) our we their are The We upon States us It 24668 12532 10826 9777 9064 5785 5759 5217 5202 5182 12.4.4 Which method to choose? In quanteda, we can generate the fcm of a corpus, either directly from the corpus/tokens object or from the dfm object. The feature-cooccurrence matrix measures the co-occurrences of features within a user-defined context. If the input of fcm is a dfm object, the context is set to be documents. In other words, the counts in fcm refers to the number of co-occurrences the two features within the same document. If the input of fmc is a corpus/tokens object, we can specify the context to be a window size. The counts in fcm refers to the number of co-occurrences the two features within the window size. We can conceptualize the structure of the fcm with a simple example: x &lt;- c(&quot;A B C A E F G&quot;, &quot;B C D E F G&quot;, &quot;B D A E F G&quot;) corpus(x) %&gt;% tokens() %&gt;% dfm %&gt;% fcm # fcm based on document context Feature co-occurrence matrix of: 7 by 7 features. features features a b c e f g d a 1 3 2 3 3 3 1 b 0 0 2 3 3 3 2 c 0 0 0 2 2 2 1 e 0 0 0 0 3 3 2 f 0 0 0 0 0 3 2 g 0 0 0 0 0 0 2 d 0 0 0 0 0 0 0 corpus(x) %&gt;% tokens() %&gt;% fcm(context = &quot;window&quot;, window = 2) # fcm based on window context Feature co-occurrence matrix of: 7 by 7 features. features features A B C E F G D A 0 3 2 2 2 0 1 B 0 0 2 0 0 0 2 C 0 0 0 2 0 0 1 E 0 0 0 0 3 3 2 F 0 0 0 0 0 3 1 G 0 0 0 0 0 0 0 D 0 0 0 0 0 0 0 corpus(x) %&gt;% tokens() %&gt;% fcm(context = &quot;document&quot;) # same as the first one Feature co-occurrence matrix of: 7 by 7 features. features features A B C E F G D A 1 3 2 3 3 3 1 B 0 0 2 3 3 3 2 C 0 0 0 2 2 2 1 E 0 0 0 0 3 3 2 F 0 0 0 0 0 3 2 G 0 0 0 0 0 0 2 D 0 0 0 0 0 0 0 12.4.5 Lexical Similarity fcm is an interesting structure because, similar to dfm, we can now examine the pairwise relationships between features. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent features are similar in their co-occurring contexts. - We can perform feature selection using `fcm_select()` - We can identify top important features from the `fcm` - We can create the semantic network of the top features - We can create the dendrogram of the top features # Create window-based `fcm` corp_fcm_win_5 &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;window&quot;, window = 5) # fcm based on window context # Feauture Selection: remove stopwords corp_fcm_win_5_select &lt;- corp_fcm_win_5 %&gt;% fcm_select(pattern = stopwords(), selection = &quot;remove&quot;, case_insensitive = T) # Find top features corp_fcm_win_5_top50 &lt;- names(topfeatures(corp_fcm_win_5_select, 50)) corp_fcm_win_5_top100 &lt;- names(topfeatures(corp_fcm_win_5_select, 100)) # plot network fcm_select(corp_fcm_win_5_select, pattern = corp_fcm_win_5_top50) %&gt;% textplot_network(min_freq = 5) # plot the dendrogram ## compute cosine similarity corp_fcm_win_5_top100_cosine &lt;- corp_fcm_win_5_select[corp_fcm_win_5_top100,] %&gt;% textstat_simil(method=&quot;cosine&quot;) ## create hclust (1-corp_fcm_win_5_top100_cosine) %&gt;% as.dist %&gt;% hclust -&gt; corp_us_fcm_win_5_top100_hclust ## plot dendrogram plot(corp_us_fcm_win_5_top100_hclust, cex = 0.8, main = &quot;Top 50 Features&quot;, xlab=&quot;&quot;, sub=&quot;&quot;) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. 12.5 Exercises Exercise 12.3 In this exercise, please create a dendrogram of the documents included in corp_us according to their similarities in trigram uses. Specific steps are as follows: Please create a dfm, where the contextual features are the trigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only trigrams consisting of \\\\w characters Include only trigrams whose frequencies are larger than 2. Include only trigrams whose document frequencies are larger than 2 (i.e., used in at least two different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trigram-based dfm (after the trimming according to the above distributional cut-off, the total number of trigrams in the dfm is: 7748): Example of the dendrogram based on the trigram-based dfm: Exercise 12.4 Based on the corp_us, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corp_us according to their similarities in their co-occurring words. Specific steps are as follows: Please create a tokens object of corpus by removing punctuations, symbols, and numbers first. Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in quanteda::stopwords() from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (after removing the stopwords, there are 9833 features in the fcm): The dimension of the input matrix for textstats_simil() should be: 50 rows and 9833 columns. Example of the dendrogram of the top 50 features in fcm: Exercise 12.5 In this exercise, please create a dendrogram of the Taiwan Presidential Addresses included in demo_data/TW_President.tar.gz according to their similarities in bigram uses. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus During the word-tokenization, please remove symbols by setting worker(..., symbols=T) Create the dfm of the corpus, where the contextual features are the bigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only bigrams whose frequencies &gt;= 5. Include only bigrams whose document frequencies &gt;= 3 (i.e., used in at least three different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trimmed dfm (Number of features: 337): Example of the dendrogram: Exercise 12.6 Based on the Taiwan Presidential Addresses Corpus included in demo_data/TW_President.tar.gz, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corpus according to their similarities in their co-occurring words. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus During the word-tokenization, please remove symbols by setting worker(..., symbols=T) Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in demo_data/stopwords-ch.txt from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (After trimming, the number of features is: 4908): The dimension of the input matrix for textstats_simil() should be: 50 rows and 4908 columns. Example of the dendrogram: References De Deyne, S., Verheyen, S., &amp; Storms, G. (2016). Structure and organization of the mental lexicon: A network approach derived from syntactic dependency relations and word associations [Book Section]. In A. Mehler, A. Lücking, S. Banisch, P. Blanchard, &amp; B. Job (Eds.), Towards a theoretical framework for analyzing complex linguistic networks (pp. 47–79). Springer. https://doi.org/10.1007/978-3-662-47238-5_3 Firth, J. R. (1957). A synopsis of linguistic theory 1930-1955. In J. R. Firth (Ed.), Studies in linguistic analysis (pp. 1–32). Oxford: Blackwell. Harris, Z. (1954). Distributional structure. Word, 10(2/3), 146–162. Harris, Z. S. (1970). Papers in structural and transformational linguistics [Book]. Reidel. Jurafsky, D., &amp; Martin, J. H. (2020). Speech &amp; language processing 3rd. Available at https://web.stanford.edu/~jurafsky/slp3/ (Accessed on 2020/04/01). Kaufman, L., &amp; Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis. New York: Wiley-Interscience. Manning, C. D., &amp; Schütze, H. (1999). Foundations of statistical natural language processing. MIT press. "],["python-environment.html", "Chapter 13 Python Environment Install the python with Anaconda Create Conda Environment Install Python Modules", " Chapter 13 Python Environment In this course, we use the Anaconda python version. Necessary installation steps include: Install the python with Anaconda Please go to the official website of Anaconda for more information. Create Conda Environment The following codes should be run in the terminal. For Windows users, we assume that you run the following steps in Anaconda Powershell Prompt. For Mac users, we assume that you run the following steps in Mac terminal. Create a specific conda environment with python 3.6+ to be used in R/Rstudio conda create --name XXX You need to specify the conda environment name XXX on your own. Install Python Modules The following codes should be run in the terminal. For Windows users, we assume that you run the following steps in Anaconda Powershell Prompt. For Mac users, we assume that you run the following steps in Mac terminal. Install all required python modules in this self-defined conda environment. There are usually two procedures: activate the conda environment named XXX in your terminal; ## Mac source activate XXX ## Window conda activate XXX install the module in the activated conda environment XXX in terminal. conda install spacy "],["references.html", "Chapter 14 References", " Chapter 14 References Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction to statistics using R. Cambridge University Press. Biber, D., Conrad, S., &amp; Cortes, V. (2004). If you look at…: Lexical bundles in university teaching and textbooks. Applied Linguistics, 25(3), 371–405. Bird, S., Klein, E., &amp; Loper, E. (2009). Natural language processing with python: Analyzing text with the natural language toolkit. \" O’Reilly Media, Inc.\". https://www.nltk.org/book/ Brezina, V. (2018). Statistics in corpus linguistics: A practical guide. Cambridge University Press. Croft, W., &amp; Cruse, D. A. (2004). Cognitive linguistics. Cambridge University Press. Damerau, F. J. (1993). Generating and evaluating domain-oriented multi-word terms from texts. Information Processing &amp; Management, 29(4), 433–447. De Deyne, S., Verheyen, S., &amp; Storms, G. (2016). Structure and organization of the mental lexicon: A network approach derived from syntactic dependency relations and word associations [Book Section]. In A. Mehler, A. Lücking, S. Banisch, P. Blanchard, &amp; B. Job (Eds.), Towards a theoretical framework for analyzing complex linguistic networks (pp. 47–79). Springer. https://doi.org/10.1007/978-3-662-47238-5_3 Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 61–74. https://www.aclweb.org/anthology/J93-1003 Evert, S. (2007). Corpora and collocations. https://stephanie-evert.de/PUB/Evert2007HSK_extended_manuscript.pdf Firth, J. R. (1957). A synopsis of linguistic theory 1930-1955. In J. R. Firth (Ed.), Studies in linguistic analysis (pp. 1–32). Oxford: Blackwell. Gabrielatos, C. (2018). Keyness analysis: Nature, metrics and techniques. In Corpus approaches to discourse (pp. 225–258). Routledge. Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Gries, S. T. (2021). Statistics for linguistics with R: A practical introduction. 3rd edition. (3rd ed.). Walter de Gruyter. Gries, S. T., &amp; Stefanowitsch, A. (2004). Extending collostructional analysis: A corpus-based perspective onalternations’. International Journal of Corpus Linguistics, 9(1), 97–129. Harris, Z. (1954). Distributional structure. Word, 10(2/3), 146–162. Harris, Z. S. (1970). Papers in structural and transformational linguistics [Book]. Reidel. Hunston, S. (2022). Corpora in applied linguistics (2nd ed.). Cambridge University Press. Hunston, S., &amp; Francis, G. (2000). Pattern grammar: A corpus-driven approach to the lexical grammar of English. John Benjamins Publishing. Jurafsky, D., &amp; Martin, J. H. (2020). Speech &amp; language processing 3rd. Available at https://web.stanford.edu/~jurafsky/slp3/ (Accessed on 2020/04/01). Jurafsky, D., &amp; Martin, J. H. (2022). Speech and language processing. \" O’Reilly Media, Inc.\". https://web.stanford.edu/~jurafsky/slp3/ Kaufman, L., &amp; Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis. New York: Wiley-Interscience. Leech, G., &amp; Fallon, R. (1992). Computer corpora–what do they tell us about culture. ICAME Journal, 16. Manning, C. D., &amp; Schütze, H. (1999). Foundations of statistical natural language processing. MIT press. McEnery, T., &amp; Hardie, A. (2011). Corpus linguistics: Method, theory and practice. Cambridge University Press. Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Stefanowitsch, A., &amp; Gries, S. T. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. Stefanowitsch, A., &amp; Gries, S. T. (2005). Covarying collexemes. Corpus Linguistics and Linguistic Theory, 1, 1–43. Stubbs, M. (1996). Text and corpus analysis. Blackwell. Stubbs, M. (2003). Words and phrases. Blackwell. Tognini-Bonelli, E. (2001). Corpus linguistics at work. John Benjamins. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. Williams, R. (1976). Keywords. Oxford University Press. Winter, B. (2020). Statistics for linguists: An introduction using R. Routledge. Wynne, M. (Ed.). (2006). Developing linguistic corpora—a guide to good practice. EADH: The European Association for Digital Humanities. https://users.ox.ac.uk/~martinw/dlc/index.htm "]]
