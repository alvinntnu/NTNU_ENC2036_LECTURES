[
["keyword-analysis.html", "Chapter 7 Keyword Analysis 7.1 About Keywords 7.2 Statistics for Keyness 7.3 Implementation 7.4 Tidy Data 7.5 Word Frequency Transformation 7.6 Computing Keynesss", " Chapter 7 Keyword Analysis In this chapter, I would like to talk about the idea of kyewords. Keywords in corpus linguistics are defined statistically using different measures of keyness. Keyness can be computed for words occurring in a target corpus by comparing their frequencies (in the target corpus) to the frequencies in a reference corpus. In other words, the idea of “keyness” is to evaluate whether the word occurs more frequently in the target corpus as compared to its occurrence in the reference corpus. If yes, the word may be a key term of the target corpus. We can quantify the relative attraction of each word to the target corpus by means of a statistical association metric. This idea of course can be extended to key phrases as well. Therefore, for keyword analysis, we assume that there is a reference corpus on which the keyness of the words in the target corpus is computed and evaluated. 7.1 About Keywords Qualitative Approach Keywords are important for research on language and ideology. Most researchers draw inspiration from Raymond Williams’s idea of keywords, which he defines as terms presumably carrying socio-cultural meanings characteristic of (Western capitalist) ideologies (Williams 1976). Keywords in Williams’ study were determined based on the subjective judgement of the socio-cultural meanings of the predefined list of words. Quantitative Approach In contrast to William’s intuition-based approach, recent studies have promoted a bottom-up corpus-based method to discover key terms reflecting the ideological undercurrents of particular text collections. This data-driven approach to keywords is sympathetic to the notion of statistical keywords popularized by Michael Stubbs (Stubbs 1996, 2003) (with his famous toolkit, Wordsmith) For a comprehensive discussion on the statistical nature of keyword analysis, please see Gabrielatos (2018). 7.2 Statistics for Keyness To compute the keyness of a word w, we need two frequency numbers: the frequency of w in the target corpus vs. the frequency of w in the reference corpus. These frequencies are often included in a contingency table as shown in Figure 7.1: Figure 7.1: Frequency distributions of a word and all other words in two corpora What are the important factors that may be connected to the significance of the frequencies of the word in two corpora? the frequency of the word w in general the sizes of the target/reference corpus In other words, the marginal frequencies of the contingency table are crucial to determining the significance of the word frequencies in two corpora. Different keyness statistics may have different ways to evaluate the relative importance of the co-occurrences of the word w with the target and the reference corpus (i.e., a and b in Figure 7.1) and statistically determine which connection is stronger. In this chapter, I would like to discuss three common statistics used in keyness analysis. This tutorial is based on Gries (2018), Ch. 5.2.6. Log-likelihood Ratio (G2) (Dunning 1993); \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Difference Coefficient (Leech and Fallon 1992); \\[ difference\\;coefficient = \\frac{a - b}{a + b} \\] Relative Frequency Ratio (Damerau 1993) \\[ rfr = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] 7.3 Implementation In this tutorial we will use two documents as our mini reference and target corpus. These two documents are old Wikipedia entries (provided in Gries (2018)) on Perl and Python respectively. First we initialize necessary packages in R library(tidyverse) library(tidytext) library(readtext) library(quanteda) Then we load the corpus data, which are available as two text files in our demo_data, and transform the corpus into a tidy structure flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) %&gt;% select(textid, text) We convert the text-based corpus into a word-based data frame, which allows us to easily extract word frequency information corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% head(100) Exercise 7.1 We mentioned this before. It is always good to keep track of the relative positions of the word in the original text. Please create a column in corpus_word, indicating the the relative position of each word in the text with unique word_id. (NB: Only the first 100 rows are shown here.) Now we need to get the frequencies of each word in the two corpora respectively. corpus_word %&gt;% count(word, textid, sort=T) -&gt; word_freq word_freq As now we are analyzing each word in relation to the two corpora, it would be better for us to have each word type as one independent row, and columns recording their co-occurrence frequencies with the two corpora (i.e., target and reference). How to achieve this? 7.4 Tidy Data Now I would like to talk about the idea of tidy dataset before we move on. Wickham and Grolemund (2017) suggests that a tidy dataset needs to satisfy the following three interrelated principles: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In our current word_freq, our observation in each row is a word. This is OK because a tidy dataset needs to have every independent word (type) as an independent row in the table. However, there are two additional issues with our current data frame word_freq: Each row in word_freq in fact represents the combination of two factors, i.e., word, textid. In addition, the column n contains the token frequencies for each level combination of these two (nominal) variables. The same word type appears twice in the dataset in the rows (e.g., the, a) This is the real life: we often encounter datasets that are NOT tidy at all. Instead of expecting others to always provide you a perfect tidy dataset for analysis (which is very unlikely), we might as well learn how to deal with messy dataset. Wickham and Grolemund (2017) suggest two common strategies that data scientists often apply: Long-to-Wide: One variable might be spread across multiple columns Create more variables/columns based on one old variable Wide-to-Long: One observation might be scattered across multiple rows Reduce several variables/columns by collapsing them into levels of a new variable 7.4.1 An Long-to-Wide Example Here I would like to illustrate the idea of Long-to-Wide transformation with a simple dataset from Wickham and Grolemund (2017), Chapter 12. people &lt;- tribble( ~name, ~profile, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) people The above dataset people is not tidy because: the column profile contains more than one variable; an observation (e.g., Phillip Woods) is scattered across several rows. To tidy up people, we can apply the Long-to-Wide strategy: One variable might be spread across multiple columns The function tidyr::pivot_wider() is made for this. There are two important parameters in pivot_wider(): names_from = ...: The column from which we create new variable names. Here it’s profile. values_from = ...: The column from which we take values for new columns. Here it’s values. Figure 7.2: From Long to Wide: pivot_wider() We used pivot_wider() to transform people into a wide-format data frame. require(tidyr) people %&gt;% pivot_wider(names_from = profile, values_from = values) 7.4.2 A Wide-to-Long Example Now let’s take a look at an example of the second strategy, Long-to-Wide transformation. When do we need this? This type of data transformation is often needed when you see that some of your columns/variables are in fact connected to the same factor. That is, these columns in fact can be considered levels of another underlying factor. Take the following simple case for instance. preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) preg The dataset preg has three columns: pregnant, male, and female. Of particular interest here are the last two columns. It is clear that male and female can be considered levels of another underlying factor, i.e., gender. More specifically, the above dataset preg can be tidied up as follows: we can have a column gender we can have a column pregnant we can have a column count (representing the number of observations for the combinations of gender and pregnant) In other words, we need the Wide-to-Long transformation: One observation might be scattered across multiple rows. The function tidyr::pivot_longer() is made for this. There are three important parameters: cols = ...: The set of columns whose names are levels of an underlying factor. Here they are male and female. names_to = ...: The name of the new underlying factor. Here it is gender. values_to = ...: The name of the new column for values of level combinations. Here it is count. Figure 7.3: From Wide to Long: pivot_longer() We applied the Wide-to-Long transformation to preg: preg %&gt;% pivot_longer(cols=c(&quot;male&quot;,&quot;female&quot;), names_to = &quot;gender&quot;, values_to = &quot;count&quot;) 7.5 Word Frequency Transformation Now back to our word_freq: word_freq %&gt;% head(10) It is probably clearer to you now what we should do to tidy up word_freq. It is obvious that some words (observations) are scattered across several rows. The column textid can be pivoted into two variables: “perl” vs. “python”. In order words, we need strategy of Long-to-Wide. One variable might be spread across multiple columns We transformed our data accordingly. word_freq_wide &lt;- word_freq %&gt;% pivot_wider(names_from = &quot;textid&quot;, values_from = &quot;n&quot;) head(word_freq_wide) Exercise 7.2 In the above Long-to-Wide transformation, there is still one problem. There are quite a few words that occur in one corpus but not the other. For these words, their frequencies would be a NA because R cannot allocate proper values for these unseen cases. Could you fix this problem by assigning these unseen cases a 0 when transforming the data frame? Please name the updated wide version of the word frequency data frame as contingency_table. Hint: Please check the argument pivot_wider(..., values_fill = ...) Problematic unseen cases in one of the corpora: word_freq_wide %&gt;% filter(is.na(corp_perl.txt) | is.na(corp_python.txt)) Updated contingency_table: contingency_table Before we compute the keyness, we preprocess the data by: including words consisting of alphabets only; renaming the columns to match the cell labels in Figure 7.1 above; creating necessary frequencies (columns) for keyness computation contingency_table %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table 7.6 Computing Keynesss With all necessary frequencies, we can now compute the three keyness statistics for each word. contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) %&gt;% mutate_if(is.numeric, round, 2) -&gt; keyness_table keyness_table Although now we have the keyness values for words, we still don’t know to which corpus (target or reference corpus) the word is more attracted. What to do next? keyness_table %&gt;% mutate(preference = ifelse(a &gt; a.exp, &quot;perl&quot;,&quot;python&quot;)) %&gt;% select(word, preference, everything())-&gt; keyness_table keyness_table Exercise 7.3 The CSV in demo_data/data-movie-reviews.csv is the IMDB dataset with 50,000 movie reviews and their sentiment tags (source: Kaggle). The CSV has two columns—the first column review includes the raw texts of each movie review; the second column sentiment provides the sentiment tag for the review. Each review is either positive or negative. We can treat the dataset as two separate corpora: negative and positive corpora. Please find the top 10 keywords for each corpus ranked by the G2 statistics. In the data preprocessing, please use the default tokenization in unnest_tokens(..., token = &quot;words&quot;). When computing the keyness, please exclude: words with at least one non-alphanumeric symbols in them (e.g. regex class \\W) words whose frequency is &lt; 10 in each corpus The expected results are provided below for your reference. The first six rows of demo_data/data-movie-reviews.csv: Sample result: References "]
]
