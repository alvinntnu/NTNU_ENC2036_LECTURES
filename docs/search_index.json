[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2020-02-27 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark on a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offerring necessary inter-disciplinary skills and knowledge. This course requires as prerequisite basic knoweldge of computational coding. It is highly recommended for students to have taken ENC2055 or other equivalents before taking this course. Please see the FAQ of the course webiste for more information about the prerequisite. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will lead the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our reference material for the course. However, we will stress the hands-on implementation of the ideas and methods covered in the book. Also, there are a few more reference books listed at the end of the section, which I would highly recommend (e.g., Gries (2018), Baayen (2008), Brezina (2018), McEnery and Hardie (2011)). Course Website We have a course website. You may need a password to access the course materials. If you are an officially enrolled student, please ask the instructor for the passcode. Please read the FAQ of the course website before course registration. Course Demo Data Dropbox Demo Data Directory References "],
["what-is-corpus-linguistics.html", "Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? 1.2 What is corpus? 1.3 What is a corpus linguistic study? 1.4 Additional Information on CL", " Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? There is an unnecessary dichotomy in linguistics “intuiting” linguistic data Inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality Corpus data Highlight the importance of language use in real context Highlight the linguistic tendency in the population (cf. sample) Strengths of Corpus Data Data reliability How sure can we be that other people will arrive at the same observations/patterns/conclusions using the same method Can others replicate the same logical reasoning in intuiting data? Can others make the same “grammatical judgement”? Data validity How well do we understand what real world phenomenon the data correspond to Can we know more about language based on one man’s constructed sentences or his grammatical judgement? Can we better generalize our insights from one man’s intuition or the group minds (population vs. sample vs. one-man)? 1.2 What is corpus? This can be tricky: different disciplines, different definitions Literature History Sociology Field Linguistics Linguistic Corpus in corpus linguistics Authentic Representative Large 1.3 What is a corpus linguistic study? CL characteristics No general agreement as to what it is Not a very homogenous methodological framework (Compared to other sub-disciplines in linguistics) It’s quite new In connection with many linguistic fields Interactional linguistics, cognitive linguistics, functional syntax, usage-based grammar etc. Stylometry, computational linguistics, NLP, digital humanities, text mining, sentiment analysis Stefanowitch (2019) defines Corpus Linguistics as follows: Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus More on Conditional Distribution An exhaustive investigation Text-processing technology Retrieval and coding Regular expressions Common methods KWIC concordances Collocates Frequency lists A systematic investigation The distribution of a linguistic phenomenon under particular conditions (e.g. lexical, syntactic, social, pragmatic etc. contexts) Statistical properties of language Examples When do English speakers use the complementizer that? What are the differences between small and little? When do English speaker choose “He picked up the book” vs. “He picked the book up”? When do English speaker place the adverbial clauses before the matrix clause? Do speakers use different phrases in different genres? Is the word “gay” used differently across different time periods? Do L2 learners use similar collocation patterns as do L1 speakers? Do speakers of different socio-economic classes talk differently? 1.4 Additional Information on CL Important Journals in Corpus Linguistics Corpus Linguistics and Linguistic Theory International Journal of Corpus Linguistics Corpora Applied Linguistics Computational Linguistics Digital scholarship in the Humanities Language Teaching Language Learning Journal of Second Language Writing CALL Language Teaching Research ReCALL System References "],
["r-fundamentals.html", "Chapter 2 R Fundamentals A Quick Note", " Chapter 2 R Fundamentals A Quick Note This course assumes that students have a certain level of background knowledge of R. We will have a quick overview of several fundamental concepts relating to the R language. These topics will be covered in more detail in my other course, ENC2055. Therefore, in the following weeks, we will go over (or review) some of the important chapters in the lectures of ENC2055, including: Chapter 2: R Fundamentals Chapter 3: Code Format Convention Chapter 4: Subsetting Chapter 6: Data Manipulation Chapter 7: Data Import Chapter 8: String Manipulation Chapter 9: Conditions and Loops Chpater 10: Iterations Please refer Winter (2020) Chapter 1 and Chapter 2 for a comprehensive overview of R fundamentals. References "],
["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. # Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;, &quot;stringr&quot;, &quot;jiebaR&quot;, &quot;tmcn&quot;, &quot;RCurl&quot;)) library(tidyverse) library(rvest) # Packages needed for further text processing # library(jiebaR) # library(tmcn) #library(RCurl) 3.1 HTML Structure 3.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 3.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 3.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 3.2. Figure 3.2: Tree Structure of A HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT. In particular, we want to extract texts from the Gossiping board. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html session (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 22425 Now gossiping should be on the front page of the Gossiping board. Now we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% str_extract(&quot;[0-9]+&quot;) %&gt;% as.numeric() page.latest ## [1] 39358 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771363.A.1B2.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771389.A.BC1.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771394.A.42F.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771394.A.25E.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771474.A.CEB.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771492.A.20A.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771555.A.745.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771574.A.BD0.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771575.A.07A.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771603.A.B9A.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771626.A.F00.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771635.A.CA5.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771645.A.732.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771650.A.5E8.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771650.A.FAB.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771712.A.4DD.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771728.A.5A3.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771734.A.F5C.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771754.A.275.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1582771756.A.4EE.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;OpenGoodHate (什麼東西什麼東西)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;Re: [新聞] 高三女困湖北求救「不想放棄台大」 母淚&quot; ## [4] &quot;Thu Feb 27 10:42:39 2020&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;OpenGoodHate&quot; article.title ## [1] &quot;Re: [新聞] 高三女困湖北求救「不想放棄台大」 母淚&quot; article.datetime ## [1] &quot;Thu Feb 27 10:42:39 2020&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) article.content ## [1] &quot;中國因為有金盾管制\\n所以你上網上一上就會卡住\\n\\n在中國上台灣官方的網站會掛\\n台灣IP上中國官方的網站也會掛\\n\\n所以金盾卡住造成她無法連回來是有可能的\\n\\n但是中華電信有漫遊不會被卡住https://i.imgur.com/Ekk76aQ.jpg幾百塊換小孩線上填台大很合算吧\\n都能打電話回台灣靠腰了\\n打個電話跟中華申請一下漫遊不難吧\\n\\n連這個都不要，那只能恭喜妳後面的同學\\n少一個人填志願\\n\\n去怪你媽吧，把你帶去湖北: ※ 引述《SakuraYui (櫻花蝦)》之銘言：: : ※發文無1~6小標格式或未依順序任意刪除者會被刪文: : 1.媒體來源:: : ※ 例如蘋果日報、自由時報（請參考版規下方的核准媒體名單）: : ettoday: : 2.記者署名: : ※ 沒有在這打上記者署名的新聞會被水桶14天 編輯非記者: : ※ 外電至少要有來源或編輯 如:法新社: : 陳俊宏: : 3.完整新聞標題:: : ※ 標題沒有完整寫出來 ---&gt; 依照板規刪除文章: : 高三女困湖北求救「不想放棄台大」 母淚：孩子成績考這麼好！我害了她: : 4.完整新聞內文:: : ※ 社論特稿都不能貼！違者刪除（政治類水桶3個月)，貼廣告也會被刪除喔！可詳看: 版規: : 記者陳俊宏／綜合報導: : 一名台灣高三女學生Cindy表示，她和媽媽現在還困在大陸湖北山上的外公家，幾乎沒有: : 網路，想參加大學甄試報考台大外文系，但現在無法上網找資料。她的媽媽也非常內疚說: : ，女兒學測成績很高，幾乎可以台大的理想科系，現在很後悔，早知道就不要叫女兒回湖: : 北過年。對此，中央流行疫情指揮中心也回應了。: : 根據Cindy拍下湖北現況的影片，只見家門口對外道路被檢疫人員用車擋起來，完全無法: 拍個影片傳到台灣某媒體爆料，這影片，意思意思給他算個100MB就好。: : 外出買生活用品。她向《東森新聞》求救，眼見3月23日、3月24就是選填志願的截止日期: : ，時間越來越近，「我沒有辦法去下載東西，因為速度真的非常非常慢。我手邊沒有這些: 看來中國的網路跟台灣不太一樣上傳上百MB的東西很快很順利都不會斷線: 然後下載個簡章4.49MB非常慢還會斷線傳輸失敗是吧: https://imgur.com/smG3BXL 這是去年的簡章: : 科系的資料，我非常想了解這個科系，可是我完全無能為力。」: : Cindy是1月20日和媽媽回湖北外公家過年，原本1月30日就要回台，就能趕上3月的選填志: : 願；沒想到因新冠肺炎（COVID-19）疫情遇到封城，外公家又在山上收訊差，也無法走出: : 家門，到市區網咖上網找資料。: : Cindy和媽媽不管是和記者視訊還是網路電話都超lag，最後是自己拍影片，花好久時間才: 能視訊跟網路電話，但是下載4.49MB很困難...看來我做了十幾年的網管是做假的: 除非是台灣的這些學校招生簡章鎖IP，或防火牆檔下: : 把畫面傳出來。Cindy媽媽啜泣說，女兒學測考57級分，原本回台可好好準備接下來的面: : 試，現在連選填志願都有困難，「看到孩子這麼辛苦，這麼努力考這麼好的成績下來，我: : 沒有辦法幫她，我害了她。」: : 而Cindy最想報考的就是台大外文系，她對學測成績很有把握，覺得應該可以考上，但卡: : 在無法回到台灣，現在很擔心必須放棄這個志願，期盼能有包機趕快回到台灣。: : 對此，疫情指揮中心應變官莊人祥表示，會再了解給予協助，「如果可以藉由那些航班回: : 來的話，我們會做特別的安排。」而大學招聯會也將在3月5日找常委學校討論，類似: : Cindy這種無法回台，或面試時學生在居家隔離狀況的因應辦法，希望防疫為重，也能顧: : 及學生該有的權益。: 相信這麼優秀的學生，認真向學的態度，正是台灣最高首府最希望收到的學生: 所以我想Cindy應該不是找疫情中心，而是找教育部或台大校長信箱: 相信台大的招生應該也有能力為了這麼優秀的學生，來個線上遠端面試吧: 而且按照上面的說法，中國的上傳很順都能傳出影片了，只是下載不順4.49MB都會失敗: 要達成線上面試不成問題阿，台灣提問，你在當地拍片回傳多符合當地的上傳ok下載no: : 5.完整新聞連結 (或短網址):: : ※ 當新聞連結過長時，需提供短網址方便網友點擊: : https://www.ettoday.net/news/20200227/1655053.htm: : 6.備註:: : ※ 一個人一天只能張貼一則新聞，被刪或自刪也算額度內，超貼者水桶，請注意-----\\nSent from JPTT on my iPhone\\n\\n--中華把漫遊弄的這麼便宜了\\n連漫遊都不會/懶的弄\\n那念台大只會拖低台大人的水準&quot; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all 3.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? "],
["corpus-analysis-a-start.html", "Chapter 4 Corpus Analysis: A Start 4.1 Installing quanteda 4.2 Building a corpus from character vector 4.3 Keyword-in-Context (KWIC) 4.4 KWIC with Regular Expressions 4.5 Tidy Text Format of the Corpus 4.6 Frequency Lists 4.7 Collocations 4.8 Word Cloud", " Chapter 4 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 4.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 4.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. We create a corpus() object with the pre-loaded character vector data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) summary(corp_us) After the corpus is created, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() 4.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or Concordances, is the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. 4.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. 4.5 Tidy Text Format of the Corpus Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson 2017). By keeping the input and output in tidy tables, users can transition fluidly between these packages. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy() objects (see the broom package [Robinson et al cited above]) from popular text mining R packages such as tm (Feinerer, Hornik, and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow where importing, filtering, and processing is done using dplyr and other tidy tools, after which the data is converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) 4.6 Frequency Lists To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(word, text) corp_us_words Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) ## [1] 135562 sum(corp_us_bigrams_freq$n) ## [1] 135504 4.7 Collocations corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% mutate(w1freq = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], w2freq = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% mutate(w12freq_exp = (w1freq*w2freq)/sum(n)) %&gt;% mutate(MI = log2(n/w12freq_exp), t = (n - w12freq_exp)/sqrt(n)) %&gt;% arrange(desc(MI)) corp_us_collocations Exercise 4.1 Create a collocation data frame arranged by other association metrics, such as t-score. Exercise 4.2 Find the top FIVE bigrams ranked according to MI values for each president. 4.8 Word Cloud library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 100, min.freq = 10, scale = c(5,1), color = brewer.pal(8, &quot;Dark2&quot;))) Exercise 4.3 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. require(tidytext) stop_words "],
["tokenization.html", "Chapter 5 Tokenization 5.1 English Tokenization 5.2 Text Analytics Pipeline 5.3 Proper Units for Analysis 5.4 Lexical Bundles (n-grams)", " Chapter 5 Tokenization library(quanteda) library(tidyverse) library(readtext) library(tidytext) Tokenization refers to the process of segmenting a long piece of discourse into smaller linguistic units. These linguistic units, depending on your purposes, may vary in many different ways: paragraphs sentences words syllables/characters letters phonemes In this chapter, we are going to look at this issue in more detail. Specifically, we will discuss the idea of word co-occurrence, which is one of the most fundamental method in corpus linguistics, and relate it to the issue of tokenization. 5.1 English Tokenization To get a clearer idea how tokenization works in unnest_tokens, we first create a simple corpus x in a tidy structure, i.e., a tibble, with one text only. x &lt;- tibble(id = 1, text = &quot;&#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone\\nthough), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very\\n well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;...&quot;) x writeLines(x$text[1]) ## &#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone ## though), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very ## well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;... (Please note that there are two line breaks in the text.) If we use the default setting token = &quot;words&quot; in unnest_tokens, we will get: x %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) Exercise 5.1 Please check the word tokens in the output data frame carefully and list characters that disappear in the word tokens but exist in the original text. Exercise 5.2 For those missing characters, how do you preserve these characters in your output then? 5.2 Text Analytics Pipeline 5.3 Proper Units for Analysis 5.3.1 Sentence Tokenization In text analysitcs, what we often do is the sentence tokenization corp_us &lt;-corpus(data_corpus_inaugural) corp_us_df &lt;- tidy(corp_us) class(corp_us_df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In unnest_token of the tidytext library, you can specify the parameter token to customize tokenzing function. As this library is designed to deal with English texts, there are several built-in options for English text tokenizatios, including words(default), characters, character_shingles, ngrams, skip_ngrams, sentences, lines, paragraphs, regex and ptb (Penn Treebank). corp_us_sent &lt;- corp_us_df %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) corp_us_sent Sometimes it is good to give each sentence of the document an index, e.g., ID, which can help us easily keep track of the relative position of the sentence in the original document. corp_us_sent %&gt;% group_by(Year) %&gt;% mutate(sentID = row_number()) 5.3.2 Words Tokenization Corpus linguistics deal with words all the time. Word tokenization therefore is the most often used method to segment texts. This is not a big concern for languages like English, which usually puts a whitespace between words. corp_us_word &lt;- corp_us_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) corp_us_word Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). 5.4 Lexical Bundles (n-grams) Sometimes it is helpful to identify frequently occurring n-grams, i.e., recurrent multiple word sequences. You can easily create an n-gram frequency list using unnest_tokens(): corp_us_trigram &lt;- corp_us_df %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigram We then can examine which n-grams were most often used by each President: corp_us_trigram %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) Exercise 5.3 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram would be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by different Presidents? So now let’s compute the dispersion of the n-grams in our corp_us_df. Here we define the dispersion of an n-gram as the number of documents where it occurs. corp_us_trigram %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) # # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) Therefore, usually lexical bundles or n-grams are defined based on distrubtional patterns of these multiword units. In particular, cut-off values are often determined to select a list of meaningful lexical bundles. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. "],
["parts-of-speech-tagging.html", "Chapter 6 Parts-of-Speech Tagging 6.1 Parts-of-Speech Tagging 6.2 Metalingusitic Analysis 6.3 Saving POS-tagged Texts", " Chapter 6 Parts-of-Speech Tagging library(tidyverse) library(tidytext) In many textual analysis, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. 6.1 Parts-of-Speech Tagging # install spacyR # devtools::install_github(&quot;quanteda/spacyr&quot;, build_vignettes = FALSE) library(spacyr) #spacy_install() spacy_initialize() txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt,pos = T, tag = T, lemma = T) parsedtxt Two tagsets are included in the output of spacy_parse: pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set. library(quanteda) library(tidytext) corp_us_df &lt;- data_corpus_inaugural %&gt;% corpus %&gt;% tidy corp_us_df corp_us_df$text[1] %&gt;% spacy_parse() %&gt;% unnest_tokens(word, token) One trick here. If the input text character vecotr for spacy_parse() does not specify names() attributes for each text, then by default in the column doc_id of the output, it will use an autoamtic number to refer to each text. If we specify the names() of all our texts, then we can keep the meta information of each text. documents &lt;- corp_us_df$text names(documents)&lt;-str_c(corp_us_df$Year, corp_us_df$FirstName, corp_us_df$President, sep=&quot;_&quot;) corp_us_word_tag &lt;- documents %&gt;% spacy_parse(pos=T) %&gt;% unnest_tokens(word,token) 6.2 Metalingusitic Analysis In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_word_tag and first generate the frequencies of verbs, and number of words for each presidential speech text. corp_us_word_tag_2 &lt;-corp_us_word_tag %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) corp_us_word_tag_2 With the syntactic complexity of each president, we can plot the tendency: corp_us_word_tag_2 %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 6.1 Please add a regression/smooth line to the above plot to indicate the downward trend? 6.3 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time when we process the data, it would be more convenient if we save the tokenized texts with the POS tags in the hard drive. Next time we can import those files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. documents %&gt;% spacy_parse(tag=T) %&gt;% unnest_tokens(word, token, to_lower = F, strip_punct = F) -&gt; corp_us_word_tag_3 spacy_finalize() "],
["keyword-analysis.html", "Chapter 7 Keyword Analysis", " Chapter 7 Keyword Analysis G2 \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Relative Frequency Ratio \\[ RFR = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] Difference Coefficient \\[ DC = \\frac{a-b}{a+b} \\] library(tidyverse) library(tidytext) library(readtext) library(quanteda) #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) corpus corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% count(word, textid) %&gt;% tidyr::spread(textid, n, fill = 0) %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) "],
["constructions-and-idioms.html", "Chapter 8 Constructions and Idioms 8.1 Chinese Four-character Idioms 8.2 Dictionary Entries 8.3 Case Study: X來Y去 8.4 Exercises", " Chapter 8 Constructions and Idioms library(tidyverse) 8.1 Chinese Four-character Idioms Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. This chapter will provide a exploratory analysis of four-character idioms in Chinese. 8.2 Dictionary Entries In our demo_data directory, there is a file dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first import the idioms in the file. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;) head(all_idioms) ## [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; tail(all_idioms) ## [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; length(all_idioms) ## [1] 56536 In order to make use of the tidy structure in R, we convert the data into a tibble: idiom &lt;- tibble(string = all_idioms) 8.3 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來X去: idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we may need to extract the X and Y in the schema: idiom_laiqu &lt;-idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% mutate(pattern = str_replace(string, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(pattern, into = c(&quot;w1&quot;, &quot;w2&quot;), sep = &quot;_&quot;) idiom_laiqu One empirical question is how many of these idioms are of the pattern X=Y (e.g., 想來想去, 直來直去) and how many are of X!=Y (e.g., 說來道去, 朝來暮去): idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() 8.4 Exercises Exercise 8.1 Please use idiom and extract the idioms with the schema of 一X一Y. Exercise 8.2 Also with the idiom as our data source, now if we are interested in all idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Also, provide the distribution of the two types. Exercise 8.3 Following Exercise 8.2, for each type of the idioms, please provide their respective proportions of X=Y vs. X!=Y. Exercise 8.4 Folloing Exercise 8.3, please identify the character that is duplicated in the idioms. One follow-up analysis would be to look at the distribution of these pivotal characters. Can you reproduce a graph as shown below as closely as possible? "],
["chinese-text-processing.html", "Chapter 9 Chinese Text Processing 9.1 Chinese Word Segmenter jiebaR 9.2 Chinese Text Analytics Pipeline 9.3 Case Study 1: Word Frequency and Wordcloud 9.4 Case Study 2: Patterns 9.5 Case Study 3: Lexical Bundles", " Chapter 9 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss tokenization in Chapter 5, it is easy to do the word tokenization in English as the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. This chapter is devoted to Chinese text processing. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 9.1 Chinese Word Segmenter jiebaR 9.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) ## [1] &#39;0.10.99&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: initilzie a word segmenter object using worker() segment the texts using segment() seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment the document, text, you first initialize a segmenter seg1 using worker() and feed this segmenter to segment(jiebar = seg1)and segment text into words. 9.1.2 Settings There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not 9.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. This can be done when you initialize the segmenter using worker(). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 9.1.4 Stopwords When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative. seg3 &lt;- worker(stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣民眾&quot; &quot;黨&quot; ## [25] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; &quot;哲&quot; &quot;7&quot; ## [31] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; ## [37] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; 9.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = &quot;tag&quot; when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; The following table lists the annotations of the POS tagsets used in jiebaR: 9.1.6 Default You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: dir(show_dictpath()) ## [1] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/jiebaRD/dict&quot; ## [1] &quot;backup.rda&quot; &quot;hmm_model.utf8&quot; &quot;hmm_model.zip&quot; &quot;idf.utf8&quot; ## [5] &quot;idf.zip&quot; &quot;jieba.dict.utf8&quot; &quot;jieba.dict.zip&quot; &quot;model.rda&quot; ## [9] &quot;README.md&quot; &quot;stop_words.utf8&quot; &quot;user.dict.utf8&quot; scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, what=character(),nlines=50,sep=&#39;\\n&#39;, encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) ## [1] &quot;\\&quot;&quot; &quot;.&quot; &quot;。&quot; &quot;,&quot; &quot;、&quot; &quot;！&quot; &quot;？&quot; &quot;：&quot; &quot;；&quot; &quot;`&quot; &quot;﹑&quot; &quot;•&quot; ## [13] &quot;＂&quot; &quot;^&quot; &quot;…&quot; &quot;‘&quot; &quot;’&quot; &quot;“&quot; &quot;”&quot; &quot;〝&quot; &quot;〞&quot; &quot;~&quot; &quot;\\\\&quot; &quot;∕&quot; ## [25] &quot;|&quot; &quot;¦&quot; &quot;‖&quot; &quot;— &quot; &quot;(&quot; &quot;)&quot; &quot;〈&quot; &quot;〉&quot; &quot;﹞&quot; &quot;﹝&quot; &quot;「&quot; &quot;」&quot; ## [37] &quot;‹&quot; &quot;›&quot; &quot;〖&quot; &quot;〗&quot; &quot;】&quot; &quot;【&quot; &quot;»&quot; &quot;«&quot; &quot;』&quot; &quot;『&quot; &quot;〕&quot; &quot;〔&quot; ## [49] &quot;》&quot; &quot;《&quot; 9.1.7 Reminder When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and return a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; 9.2 Chinese Text Analytics Pipeline In Chapter 5, we have talked about the work pipeline for normal English texts processing, as shown below: For Chinese texts, the work flow is pretty much the same. The most important trick is in the step of tokenization, i.e., unnest_tokens(): we need to specify our own tokenzier for the argument token = ... in the unnest_tokens(). It is important to note that when we specify a self-defined token function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument byline = TRUE for worker(byline = TRUE). So based on our simple-corpus example above, we can create # a text-based tidy corpus a_small_tidy_corpus &lt;- text %&gt;% corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) a_small_tidy_corpus # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;) # tokenization a_small_tidy_corpus_by_word &lt;- a_small_tidy_corpus %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) a_small_tidy_corpus_by_word In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. 9.3 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart ??: create a text-based tidy corpus object apple_df (i.e., a tibble) intialize a word segmenter using worker() tokenize the corpus into a word-based tidy corpus object using unnest_tokens() # loading the corpus # NB: this may take some time apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% as_tibble() %&gt;% filter(text !=&quot;&quot;) %&gt;% mutate(doc_id = row_number()) apple_df # tokenization segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T) apple_word &lt;- apple_df %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = segmenter)) apple_word With a word-based corpus object, we can easily generate a word frequency list as well as a wordcloud to have a quick view of the word distribution in the corpus. library(showtext) # font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) # `wordcloud` version # require(wordcloud) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 100) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 9.4 Case Study 2: Patterns In this case study, we are looking at a more complex example. In linguistics analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. Important steps are as follows: Create a self-defined tokenization function, which takes a text and returns the same text but with the POS-tags of all the words included. # define a function # to concatenate the jieba pos results into: w1_tag1 w2_tag2 w3_tag3 ... sequence chi_pos_tagger &lt;- function(txt, tagger){ txt_tag &lt;- segment(txt, tagger) str_c(txt_tag, names(txt_tag), collapse=&quot; &quot;, sep=&quot;_&quot;) } We defined the function chi_pos_tagger(), which takes a text in and returns the text out with the POS tags of the words appended at the end of each word. mytagger &lt;- worker(type=&quot;tag&quot;, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = FALSE) chi_pos_tagger(txt = text, tagger = mytagger) ## [1] &quot;綠黨_n 桃園市_x 議員_n 王浩宇_x 爆料_n ，_x 指民眾_x 黨_n 不_d 分區_n 被_p 提名_v 人_n 蔡壁如_x 、_x 黃_zg 瀞_x 瑩_zg ，_x 在昨_x （_x 6_x ）_x 日_m 才_d 請辭_v 是_v 為領_x 年終獎金_n 。_x 台灣_x 民眾_x 黨_n 主席_n 、_x 台北_x 市長_x 柯文_nz 哲_n 7_x 日_m 受訪_v 時則_x 說_zg ，_x 都_d 是_v 按_p 流程_n 走_v ，_x 不要_df 把_p 人家_n 想得_x 這麼_x 壞_a 。_x&quot; chi_pos_tagger(txt = &quot;我是在測試一個句子&quot;, tagger = mytagger) ## [1] &quot;我_r 是_v 在_p 測試_vn 一個_x 句子_n&quot; Tokenize the text-based tidy corpus into a sentence-based one, and POS-tag each sentence and put the tagged version of the sentences in a new column # IPU tokenization apple_ipu &lt;-apple_df %&gt;% unnest_tokens(IPU, text, token = function(x) str_split(x, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;))%&gt;% #IPU tokenization filter(IPU!=&quot;&quot;) %&gt;% # remove empty IPU mutate(IPU_tag = map_chr(IPU, function(x) chi_pos_tagger(txt=x, tagger = mytagger))) # tag each IPU apple_ipu In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a possible sentence-like unit. This can be controversial to many grammarians and syntaticians. However, in practice, this may not be a bad choice as it will become obvious when we extract patterns. For more information related to the unicode ranage for the punctuations in CJK languages, please see this SO discussion thread. After we tokenize our text-based tidy corpus into a inter-punctuation-unit-based (IPU) tidy corpus, we can make use of the words as well as their parts-of-speech tags to extract the target pattern we are interested: 被 + ... constructions. The data retrieval process is now very straighforward: we only need to go through all the IPU units in the corpus object and see if our target pattern matches any of these IPU units. In the following example, we: subset IPUs with the target pattern \\\\b被_p\\\\b using str_detect() extract the strings that match the target pattern \\\\b被_p\\\\s([^_]+_[^\\\\s]+\\\\s)*?[^_]+_v using str_extract() and add these strings to a new column using mutate() extract the verb in the BEI construction using str_extract() and add these verbs to a new column using mutate # Extract BEI + WORD apple_bei &lt;-apple_ipu %&gt;% filter(str_detect(IPU_tag, pattern = &quot;\\\\b被_p\\\\b&quot;)) %&gt;% mutate(PATTERN = str_extract(IPU_tag, pattern = &quot;\\\\b被_p\\\\s([^_]+_[^\\\\s]+\\\\s)*?[^_]+_v&quot;)) %&gt;% filter(PATTERN !=&quot;&quot;) %&gt;% mutate(VERB = str_extract(PATTERN, pattern = &quot;[^_\\\\s]+_v&quot;) %&gt;% str_replace_all(&quot;_v&quot;,&quot;&quot;)) apple_bei # Calculate WORD frequency require(wordcloud2) apple_bei %&gt;% count(VERB) %&gt;% top_n(300, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.6) # Statistical Assoication Exercise 9.1 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 9.2 Please use the apple_ipu as your tidy corpus and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. So please (a) extract all concordance lines with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 9.3 Following Exercise 9.2, please generate a frequency list of the LMs for each spac particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Exercise 9.4 Following Exercise 9.3, for each space particle, please create a word cloud of its co-occuring LMs based on the top 200 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 9.5 From the above provided in Exercise 9.3, the graph of 內 shows a few LMs that are counter intuitive to our native knowledge: for example, 出車, 的, 做, 到. Can you tell us why? What would be problems? What did we do wrong in the previous processing? 9.5 Case Study 3: Lexical Bundles With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at recurrent four-grams. As we discussed in Chapter 5, a multiword unit can be defined based on at least two metrics: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) As the default tokenization in unnest_tokens() only works with the English data, we start this task by defining our own token function ngram_chi() to extract Chinese n-grams. # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes ONE text (scalar) as an input, and returns a vector of n-grams. Most importantly, this function assumes that in the text string, each word token is delimited by a whitespace. s &lt;- &quot;這 是 一個 測試 的 句子 。&quot; ngram_chi(text = s, n = 2, delimiter = &quot;_&quot;) ## [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; &quot;句子_。&quot; ngram_chi(text = s, n = 4, delimiter = &quot;_&quot;) ## [1] &quot;這_是_一個_測試&quot; &quot;是_一個_測試_的&quot; &quot;一個_測試_的_句子&quot; ## [4] &quot;測試_的_句子_。&quot; ngram_chi(text = s, n = 5, delimiter = &quot; &quot;) ## [1] &quot;這 是 一個 測試 的&quot; &quot;是 一個 測試 的 句子&quot; &quot;一個 測試 的 句子 。&quot; We vectorize the function ngram_chi(). This step is important because in unnest_tokens() the self-defined token function should take a text-based vector as input and return a list of token-based vectors of the same length as the output (cf. Section 9.2). # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) #system.time(parallel::pvec(x&lt;-apple_ipu$IPU_tag,function(x) vngram_chi(x, n=4))) # 47s Vectorized functions are a very useful feature of R, but programmers who are used to other languages often have trouble with this concept at first. A vectorized function works not just on a single value, but on a whole vector of values at the same time. In our first defined ngram_chi function, it takes one text vector as an input and process it one at a time. However, we would expect ngram_chi to process a vector of texts (i.e., multiple texts) at the same time and return a list of resulting ngrams vectors at the same time. Therefore, we use Vectorize() as a wrapper to vectorize our function and specifically tell R that the argument text is vectorized, i.e., process each value in the text vector in the same way. Now we can tokenize our corpus into n-grams using our own token function vngram_chi() and the unnest_tokens(). In this case study, we demonstrate the analysis of four-grams in our Apple News corpus. Because we need calculate not only the frequencies of the n-grams but also their dispersions, we begin by first creating a sentence ID for the IPUs of each article. Then we remove all the POS tags because n-grams extraction do not need the POS tag information. Finally, these cleaned versions of IPU, stored in the new column IPU_word, are used as input for unnest_tokens() and we specify our own token function vngram_chi() to tokenize IPU_word into four-grams. apple_ngram &lt;-apple_ipu %&gt;% group_by(doc_id) %&gt;% mutate(sentID = row_number()) %&gt;% ungroup %&gt;% mutate(IPU_word = str_replace_all(IPU_tag, &quot;_[^ ]+&quot;,&quot;&quot;)) %&gt;% unnest_tokens(ngram, IPU_word, token = function(x) vngram_chi(text = x, n= 4)) %&gt;% filter(ngram!=&quot;&quot;) apple_ngram Now that we have the four-grams-based tidy corpus object, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. apple_ngram_dist &lt;- apple_ngram %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) "],
["structured-corpus.html", "Chapter 10 Structured Corpus 10.1 NCCU Spoken Mandarin 10.2 Connecting SPID to Metadata 10.3 More Socialinguistic Analyses", " Chapter 10 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for lingustic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. 10.1 NCCU Spoken Mandarin CHILDES format 10.1.1 Loading the Corpus NCCU &lt;- readtext(&quot;demo_data/NCCU_SPOKEN.tar.gz&quot;) %&gt;% as_tibble 10.1.2 Line Segmentation NCCU_lines &lt;- NCCU %&gt;% unnest_tokens(line, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) 10.1.3 Metadata vs. Transcript NCCU_lines_meta &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^@&quot;)) NCCU_lines_data &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^[^@]&quot;)) %&gt;% group_by(doc_id) %&gt;% mutate(lineID = row_number()) %&gt;% ungroup %&gt;% separate(line, into = c(&quot;SPID&quot;,&quot;line&quot;), sep=&quot;\\t&quot;) %&gt;% mutate(line2 = line %&gt;% str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) NCCU_lines_data 10.1.4 Word Tokenization NCCU_words &lt;- NCCU_lines_data %&gt;% unnest_tokens(word, line2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words 10.1.5 Word frequencies and Wordcloud NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% select(word, freq) %&gt;% #mutate(freq = log(freq)) %&gt;% wordcloud2::wordcloud2(minSize = 0.5, size=1, shape=&quot;diamonds&quot;) 10.1.6 Concordances # extracting particular patterns NCCU_lines_data %&gt;% filter(str_detect(line2, &quot;覺得&quot;)) 10.1.7 N-grams (Lexical Bundles) ########################## # Chinse ngrams functin # ########################## # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) NCCU_ngrams &lt;- NCCU_lines_data %&gt;% select(-line, -SPID) %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 5, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq 10.2 Connecting SPID to Metadata NCCU_lines_meta NCCU_lines_data # Self-defined function fill_spid &lt;- function(vec){ vec_filled &lt;-vec for(i in 1:length(vec_filled)){ if(vec_filled[i]==&quot;&quot;){ vec_filled[i]&lt;-vec_filled[i-1] }else{ i &lt;- i+1 } #endif }#endfor return(vec_filled) }#endfunc # Please check M005.cha NCCU_lines_data %&gt;% group_by(doc_id) %&gt;% filter(lineID == 1 &amp; SPID==&quot;&quot;) # Remove the typo case NCCU_lines_data_filled &lt;- NCCU_lines_data %&gt;% filter(!(doc_id ==&quot;M005.cha&quot; &amp; lineID==1)) %&gt;% group_by(doc_id) %&gt;% mutate(SPID = str_replace_all(SPID, &quot;[*:]&quot;,&quot;&quot;)) %&gt;% mutate(SPID_FILLED = fill_spid(SPID)) %&gt;% mutate(DOC_SPID = str_c(doc_id, SPID_FILLED, sep=&quot;_&quot;)) %&gt;% ungroup %&gt;% select(doc_id, lineID, line2, DOC_SPID) NCCU_lines_data_filled Based on the metadata of each file hedaer, we can extract demographic information related to each speaker, including their ID, age, gender, etc. NCCU_meta &lt;- NCCU_lines_meta %&gt;% filter(str_detect(line, &quot;^@(id)&quot;)) %&gt;% separate(line, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 10.3 More Socialinguistic Analyses 10.3.1 Check Ngram Distribution By Age Groups NCCU_ngram_with_meta &lt;- NCCU_lines_data_filled %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 3, delimiter = &quot;_&quot;)) %&gt;% filter(ngram!=&quot;&quot;) %&gt;% filter(!(str_detect(ngram,&quot;[&lt;a-z:]&quot;))) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_ngram_by_age &lt;- NCCU_ngram_with_meta %&gt;% count(ngram,AGE_GROUP, DOC_SPID) %&gt;% group_by(ngram, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) Below20 Word Cloud Order ggplot barplots by factor frequencies require(wordcloud2) NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(ngram,freq) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_age_ordered &lt;- NCCU_ngram_by_age %&gt;% group_by(AGE_GROUP) %&gt;% top_n(20, freq) %&gt;% ungroup() %&gt;% arrange(AGE_GROUP, freq) %&gt;% mutate(order=row_number()) # transparency indicates dispersion NCCU_ngram_by_age_ordered %&gt;% ggplot(aes(order, freq, fill = AGE_GROUP, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ AGE_GROUP, scales = &quot;free&quot;) + labs(y = &quot;Ngram Frequency&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_age_ordered$order, labels=NCCU_ngram_by_age_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) 10.3.2 Check Word Distribution of different genders NCCU_ngram_by_gender &lt;- NCCU_ngram_with_meta %&gt;% count(ngram, GENDER, DOC_SPID) %&gt;% group_by(ngram, GENDER) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_gender NCCU_ngram_by_gender %&gt;% #filter(dispersion &gt; 10) %&gt;% count(GENDER) %&gt;% ggplot(aes(x=GENDER, y = n, fill=GENDER))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;male&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;female&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_gender_ordered &lt;- NCCU_ngram_by_gender %&gt;% group_by(GENDER) %&gt;% top_n(20, dispersion) %&gt;% ungroup() %&gt;% arrange(GENDER,dispersion) %&gt;% mutate(order=row_number()) NCCU_ngram_by_gender_ordered %&gt;% ggplot(aes(order, dispersion, fill = GENDER, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ GENDER, scales = &quot;free&quot;) + labs(y = &quot;Ngram Dispersion&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_gender_ordered$order, labels=NCCU_ngram_by_gender_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) "],
["xml.html", "Chapter 11 XML 11.1 BNC Spoken 2014 11.2 Process the Whole Directory of BNC2014 Sample 11.3 Metadata 11.4 BNC2014 for Socialinguistic Variation", " Chapter 11 XML library(tidyverse) library(readtext) library(rvest) library(tidytext) library(quanteda) This chapter shows you how to process the recently released BNC 2014, which is by far the largest representative collection of spoken English collected in UK. For the purpose of our in-class tutorials, I have included a small sample of the BNC2014 in our demo_data. However, the whole dataset is now available via the official website: British National Corpus 2014. Please sign up for the complete access to the corpus if you need this corpus for your own research. 11.1 BNC Spoken 2014 XML is similar to HTML. Before you process the data, you need to understand the structure of the XML tags in the files. Other than that, the steps are pretty much similar to what we have done before. First, we read the XML using read_html(): # read one file at a time corp_bnc&lt;-read_html(&quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Now it is intuitive that our next step is to extract all utterances (with the tag of &lt;u&gt;...&lt;/u&gt;) in the XML file. So you may want to do the following: corp_bnc %&gt;% html_nodes(xpath = &quot;//u&quot;) %&gt;% html_text %&gt;% head ## [1] &quot;\\r\\nanhourlaterhopeshestaysdownratherlate&quot; ## [2] &quot;\\r\\nwellshehadthosetwohoursearlier&quot; ## [3] &quot;\\r\\nyeahIknowbutthat&#39;swhywe&#39;reanhourlateisn&#39;tit?mmI&#39;mtirednow&quot; ## [4] &quot;\\r\\n&quot; ## [5] &quot;\\r\\ndidyoutext--ANONnameM&quot; ## [6] &quot;\\r\\nyeahyeahhewrotebacknobotherlad&quot; See the problem? Using the above method, you lose the word boundary information from the corpus. What if you do the following? corp_bnc %&gt;% html_nodes(xpath = &quot;//w&quot;) %&gt;% html_text %&gt;% head(20) ## [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; ## [8] &quot;rather&quot; &quot;late&quot; &quot;well&quot; &quot;she&quot; &quot;had&quot; &quot;those&quot; &quot;two&quot; ## [15] &quot;hours&quot; &quot;earlier&quot; &quot;yeah&quot; &quot;I&quot; &quot;know&quot; &quot;but&quot; At the first sight, probably it seems that we have solved the problem but we don’t. There are even more problems created: Our second method does not extract non-word tokens within each utterance (e.g., &lt;pause .../&gt;, &lt;vocal .../&gt;) Our second method loses the utterance information (i.e., we don’t know which utterance each word belongs to) Exercise 11.1 Please come up with a way to extract both words and non-word tokens from each utterance. Ideally, the resulting data frame would consist of rows being the utterances, and columns recording the attributes of each autterances. Most importantly, the data frame should record not only the tokens of the utterance but at the same time the token-level attributes of each word/non-word token as well, e.g., the parts-of-speech, duration of pause etc. # xml_to_df function read_xml_bnc2014 &lt;- function(xmlfile){ # read one file at a time corp_bnc&lt;-read_html(xmlfile) # word/pause token nodes node_w &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;(//w)|(//pause)&quot;) # u nodes node_u &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;//u&quot;) node_u_id &lt;- node_u %&gt;% html_attr(name = &quot;n&quot;) node_u_who &lt;- node_u %&gt;% html_attr(name =&quot;who&quot;) node_u_trans &lt;- node_u %&gt;% html_attr(name=&quot;trans&quot;) node_u_whoConfidence &lt;- node_u %&gt;% html_attr(name=&quot;whoConfidence&quot;) # Define function # to extract word-based data frame from each u nodeset extractWords &lt;- function(u_node){ cur_w_nodeset &lt;- u_node %&gt;% html_children() corp_df &lt;- tibble( word = cur_w_nodeset %&gt;% html_text, lemma = cur_w_nodeset %&gt;% html_attr(name=&quot;lemma&quot;, default=&quot;@&quot;), pos = cur_w_nodeset %&gt;% html_attr(name=&quot;pos&quot;, default=&quot;@&quot;), usas = cur_w_nodeset %&gt;% html_attr(name = &quot;usas&quot;, default=&quot;@&quot;), dur = cur_w_nodeset %&gt;% html_attr(name = &quot;dur&quot;, default=&quot;@&quot;)) return(corp_df %&gt;% mutate(word = ifelse(word==&quot;&quot;,&quot;&lt;PAUSE&gt;&quot;,word))) }# endfunc extractWords(node_u[[1]]) -&gt;x x x$word[4]==&quot;&quot; # create tidy word-based df for current xml file corp_df &lt;- tibble( node_u_id, node_u_who, node_u_trans, node_w = map(node_u, extractWords) ) %&gt;% unnest_tokens(word, node_w, token = function(x) map(x, function(y) str_c(y$word, y$lemma, y$pos, y$usas, y$dur, sep=&quot;_&quot;))) %&gt;% mutate(xmlid = basename(xmlfile)) %&gt;% separate(word, into = c(&quot;word&quot;,&quot;lemma&quot;,&quot;pos&quot;,&quot;usas&quot;,&quot;dur&quot;), sep=&quot;_&quot;) corp_df } # endfunc 11.2 Process the Whole Directory of BNC2014 Sample 11.2.1 Define Function In Section 11.1, if you have figured how to extract utterances as well as token-based information from the xml file, you can easily wrap the whole procedure as one function. With this function, we can perform the same procedure to all the xml files of the BNC2014. For example, let’s assume that we have defined a function: read_xml_bnc2014 &lt;- function(xml){ ... } This function takes one xml file as an argument and return a data frame, consisting of utterances and other relevant information from the xml. read_xml_bnc2014(xmlfile = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Exercise 11.2 Now your job is to write this function, read_xml_BNC2014(). 11.2.2 Process the all files in the Directory Now we utilize the self-defined function, read_xml_BNC2014(), and process all xml files in the demo_data/corp-bnc-spoken2014-sample/. Also, we combine the data.frame returned from each xml into a bigger one, i.e., corp_bnc_df: s.t &lt;- Sys.time() bnc_flist &lt;- dir(&quot;demo_data/corp-bnc-spoken2014-sample/&quot;,full.names = T) corp_bnc_df &lt;- map(bnc_flist, function(x) read_xml_bnc2014(x)) %&gt;% do.call(rbind, .) Sys.time()-s.t ## Time difference of 2.161701 mins It takes about one and half minute to process the sample directory. You may store this corp_bnc_df data frame output for later use so that you don’t have to process the XML files every time you work on BNC2014. write_csv(corp_bnc_df, &quot;demo_data/corp_bnc_df.csv&quot;,col_names = T) 11.3 Metadata The best thing about BNC2014 is its rich demographic information relating to the settings and speakers of the conversations collected. The whole corpus comes with two metadata sets: bnc2014spoken-textdata.tsv: metadata for each text transcript bnc2014spoken-speakerdata.tsv: metadata for each speaker ID These two metadata sets allow us to get more information about each transcript as well as the speakers in those transcripts. 11.3.1 Text Metadata bnc_text_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-textdata.tsv&quot;, col_names = FALSE) bnc_text_meta_names &lt;-read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-text.txt&quot;, skip =2, col_names = F) names(bnc_text_meta) &lt;- c(&quot;textid&quot;, bnc_text_meta_names$X2) bnc_text_meta 11.3.2 Speaker Metadata bnc_sp_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-speakerdata.tsv&quot;, col_names = F) bnc_sp_meta_names &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-speaker.txt&quot;, skip = 3, col_names = F) names(bnc_sp_meta) &lt;- c(&quot;spid&quot;, bnc_sp_meta_names$X2) bnc_sp_meta 11.4 BNC2014 for Socialinguistic Variation BNC2014 was born for the study of socialinguistic variation. Here we show you some naitve examples, but you should get the ideas. 11.4.1 Word Frequency vs. Gender Now we are ready to explore the gender differences in language. 11.4.1.1 Preprocessing To begin with, there are some utterances with no words at all. We probably like to remove these tokens. #corp_bnc_df &lt;- read_csv(&quot;demo_data/corp_bnc_df.csv&quot;) corp_bnc_df &lt;- corp_bnc_df %&gt;% filter(!is.na(utterance)) corp_bnc_df 11.4.1.2 Target Structures Let’s assume that we like to know which verbs are most frequently used by men and women. corp_bnc_verb_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame corp_bnc_verb_gender ## Problems ### Use own tokenization function ### Default tokenization increase the number of tokens quite a bit word_by_gender &lt;- corp_bnc_verb_gender %&gt;% unnest_tokens(word, utterance, to_lower = F,token = function(x) strsplit(x, split = &quot;\\\\s&quot;)) %&gt;% # tokenize utterance into words filter(str_detect(word, &quot;[^_]+_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # include VERB only mutate(word = str_replace(word, &quot;_(JJ)|(JJR)|(JJT)&quot;,&quot;&quot;)) %&gt;% # remove pos tags #filter(!str_detect(word, &quot;^&#39;&quot;)) %&gt;% # remove contraction #filter(!word %in% stopwords(&quot;en&quot;)) %&gt;% count(gender, word) %&gt;% group_by(gender) %&gt;% top_n(200,n) %&gt;% ungroup Female wordcloud require(wordcloud2) word_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) Male wordcloud word_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) 11.4.2 Degree ADV + ADJ corp_bnc_pattern_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;[^_]+_RG [^_]+_JJ&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame pattern_by_gender &lt;- corp_bnc_pattern_gender %&gt;% unnest_tokens(pattern, utterance, to_lower = F, token = function(x) str_extract_all(x, &quot;[^_ ]+_RG [^_ ]+_JJ &quot;)) %&gt;% mutate(pattern = pattern %&gt;% str_trim %&gt;% str_replace_all(&quot;_[^_ ]+&quot;,&quot;&quot;)) %&gt;% # remove pos tags separate(pattern, into = c(&quot;ADV&quot;,&quot;ADJ&quot;), sep = &quot;\\\\s&quot;) %&gt;% count(gender, ADJ) %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup corp_bnc_pattern_gender %&gt;% #select(n, utterance) %&gt;% filter(n == 235) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) 11.4.3 Trigrams trigram_by_gender &lt;- corp_bnc_df %&gt;% mutate(utterance = utterance %&gt;% str_replace_all(&quot;_[^ ]+&quot;,&quot;&quot;)) %&gt;% unnest_tokens(trigram, utterance, token = &quot;ngrams&quot;, n =3, ngram_delim = &quot;_&quot;) %&gt;% filter(!is.na(trigram)) %&gt;% left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) %&gt;% count(gender, trigram) %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup Exercise 11.3 Remove stopwords from the frequency list of words (unigrams). Exercise 11.4 Include dispersion metrics in the n-gram freuency list. trigram_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(trigram, n) %&gt;% wordcloud2(minSize = 0.5, size = 3, rotateRatio = 0.3) trigram_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(trigram, n) %&gt;% wordcloud2(minSize = 0.5, size = 1.5, rotateRatio = 0.8) "],
["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Data Processing Flowchart 12.2 Document-Feature Matrix (dfm) 12.3 Defining Feature in dfm 12.4 Feature Selection 12.5 Applying DFM", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) 12.1 Data Processing Flowchart 12.2 Document-Feature Matrix (dfm) Two ways to create Dcument-Feature-Matrix: create dfm based on an corpus object create dfm based on an token object For English data, quanteda can take care of the word tokenization fairly well so you can create dfm directly from corpus. However, for Chinese data, it is suggested to create your own corpus token object first, and then feed it to dfm() to create dfm for your corpus. In this section, we demonstrate the document-feature-matrix using the English data we discussed in Chapter 5, the data_corpus_inaugural preloaded in the library quanteda. corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% dfm For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) ## [1] &quot;corpus&quot; &quot;list&quot; 12.3 Defining Feature in dfm What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the following example, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and corp_us_dfm[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse). ## 10 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## 1813-Madison 1 65 100 0 44 0 0 0 ## 1817-Monroe 5 164 275 0 122 0 1 0 ## 1821-Monroe 1 197 360 0 141 0 0 0 ## 1825-Adams 0 245 304 0 116 0 1 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## 1813-Madison 1 0 ## 1817-Monroe 3 0 ## 1821-Monroe 1 0 ## 1825-Adams 3 1 Distributional properties like co-occurrences are very important information in corpus linguistics. Most of the studies in corpus linguistics adopt an implicit distributional hypothesis, which can be illustrated by a few famous quotes: You shall know a word by the comany it keeps. (Firth, 1957, p.11) [D]ifference of meaning correlates with difference of distribution. (Harris, 1970, p.785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves (De Deyne et al. 2016) So in our current context, the idea is that if two documents have similar sets of linguistic units popping up in them, they are more likely to be similar in meaning as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with other documents (i.e., other rows). This is essentially a vector computation (cf. Figure 12.1): the document in each row is represented as a vector of N dimensional space. The size of N depends on the number of linguistic units that are included in the analysis. Figure 12.1: Example of Document-Feature Matrix 12.4 Feature Selection A dfm may be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered when creating a dfm: The granularity of the linguistic unit Stopwords The distributional cut-offs of the linguistic unit 12.4.1 Determining Linguistic Granularity In our previous example, we include only words, i.e., unigrams, as our features in the dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: corp_us_dfm_ngram &lt;- corp_us %&gt;% dfm(ngrams = 2) corp_us_dfm_ngram[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (71.0% sparse). ## 10 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs fellow-citizens_of of_the the_senate senate_and and_of ## 1789-Washington 1 20 1 1 2 ## 1793-Washington 0 4 0 0 1 ## 1797-Adams 0 29 0 0 2 ## 1801-Jefferson 0 28 0 0 3 ## 1805-Jefferson 0 17 0 0 1 ## 1809-Madison 0 20 0 0 2 ## 1813-Madison 0 19 0 0 1 ## 1817-Monroe 1 49 0 0 3 ## 1821-Monroe 0 63 0 0 5 ## 1825-Adams 0 73 0 0 6 ## features ## docs the_house house_of of_representatives representatives_: ## 1789-Washington 2 2 2 1 ## 1793-Washington 0 0 0 0 ## 1797-Adams 0 0 0 0 ## 1801-Jefferson 0 0 0 0 ## 1805-Jefferson 0 0 0 0 ## 1809-Madison 0 0 0 0 ## 1813-Madison 0 0 0 0 ## 1817-Monroe 0 0 0 0 ## 1821-Monroe 0 0 0 0 ## 1825-Adams 0 0 0 0 ## features ## docs :_among ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 0 ## 1813-Madison 0 ## 1817-Monroe 0 ## 1821-Monroe 0 ## 1825-Adams 0 Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: corp_us_dfm_stem &lt;- corp_us %&gt;% dfm(stem = T) corp_us_dfm_stem[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (38.0% sparse). ## 10 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs fellow-citizen of the senat and hous repres : among ## 1789-Washington 1 71 116 1 48 2 2 1 1 ## 1793-Washington 0 11 13 0 2 0 0 1 0 ## 1797-Adams 3 140 163 1 130 3 3 0 4 ## 1801-Jefferson 2 104 130 0 81 0 1 1 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 7 ## 1809-Madison 1 69 104 0 43 0 1 0 0 ## 1813-Madison 1 65 100 0 44 0 0 0 1 ## 1817-Monroe 5 164 275 0 122 0 1 0 3 ## 1821-Monroe 1 197 360 0 141 0 2 0 1 ## 1825-Adams 0 245 304 0 116 0 2 0 3 ## features ## docs vicissitud ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 1 ## 1813-Madison 0 ## 1817-Monroe 0 ## 1821-Monroe 0 ## 1825-Adams 1 You need to decide which type of linguistic units is more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, there is no rule for how to do this. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 12.4.2 Stopwords There are words that are not so informative in telling us the similarity and difference between the documents because they almost occur in every document of the corpus, but carray little (refential) semantic contents. These words are usually the function words, such as and, the, of. Also, there are tokens that usually carry limited semantic contents, such as numbers and punctuation. Therefore, it is not uncommon that analysts sometimes create a list of words to be removed from the dfm. These words are referred to as stopwords. The library quanteda has determined a default English stopword list, i.e., stopwords(&quot;en&quot;). When creating the dfm object, we can further specify a few parameters: remove_punct: remove all punctuation tokens remove: remove all words specified in the character vector here corp_us_dfm_stp &lt;- corp_us %&gt;% dfm(remove_punct = T, remove = stopwords(&quot;en&quot;)) corp_us_dfm_stp[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (60.0% sparse). ## 10 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs fellow-citizens senate house representatives among ## 1789-Washington 1 1 2 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 ## 1801-Jefferson 2 0 0 0 1 ## 1805-Jefferson 0 0 0 0 7 ## 1809-Madison 1 0 0 0 0 ## 1813-Madison 1 0 0 0 1 ## 1817-Monroe 5 0 0 1 3 ## 1821-Monroe 1 0 0 0 1 ## 1825-Adams 0 0 0 1 3 ## features ## docs vicissitudes incident life event filled ## 1789-Washington 1 1 1 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 0 0 2 0 0 ## 1801-Jefferson 0 0 1 0 0 ## 1805-Jefferson 0 0 2 0 0 ## 1809-Madison 0 0 1 0 1 ## 1813-Madison 0 0 1 0 0 ## 1817-Monroe 0 2 1 3 0 ## 1821-Monroe 0 0 2 1 0 ## 1825-Adams 1 0 1 0 0 We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm) ## [1] 9357 nfeat(corp_us_dfm_ngram) ## [1] 63597 nfeat(corp_us_dfm_stem) ## [1] 5541 nfeat(corp_us_dfm_stp) ## [1] 9205 12.4.3 Distributional Cut-offs for Features Depending on the granularity of the linguistic units you consider, you may get a considerable number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the word occurs only once in the corpus (i.e., hapax legomenon), these words can be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurrs in all documents, they won’t help much as well. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate something else. Therefore, sometimes we can control the document frequency of the features (i.e., in how many different texts does the feature occur?) Other self-defined weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a text d, the significance of this n may be connected to: the document size of d the total number of w Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. In the following demo, we adopt a few simple distrubtional criteria: we remove stopwords and punctuations we remove words whose freqency &lt; 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% dfm(remove = stopwords(&quot;en&quot;), remove_punct = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) corp_us_dfm_trimmed[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (52.0% sparse). ## 10 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs fellow-citizens senate house representatives among life event ## 1789-Washington 1 1 2 2 1 1 2 ## 1793-Washington 0 0 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 2 0 ## 1801-Jefferson 2 0 0 0 1 1 0 ## 1805-Jefferson 0 0 0 0 7 2 0 ## 1809-Madison 1 0 0 0 0 1 0 ## 1813-Madison 1 0 0 0 1 1 0 ## 1817-Monroe 5 0 0 1 3 1 3 ## 1821-Monroe 1 0 0 0 1 2 1 ## 1825-Adams 0 0 0 1 3 1 0 ## features ## docs greater order received ## 1789-Washington 1 2 1 ## 1793-Washington 0 0 0 ## 1797-Adams 0 4 0 ## 1801-Jefferson 1 1 0 ## 1805-Jefferson 0 3 0 ## 1809-Madison 0 0 0 ## 1813-Madison 1 0 2 ## 1817-Monroe 3 1 1 ## 1821-Monroe 0 3 2 ## 1825-Adams 0 0 0 12.5 Applying DFM 12.5.1 Wordcloud With a dfm of a corpus, we can quickly explore the nature of this corpus by examining the top features of this corpus: topfeatures(corp_us_dfm_trimmed) ## people government us can upon must great ## 575 564 478 471 371 366 340 ## may states shall ## 338 333 314 Or we can visualize the distrubtion of these top features using the wordcloud: set.seed(123) corp_us_dfm_trimmed %&gt;% textplot_wordcloud(min_count = 60, rotation = .35) 12.5.2 Document Similarity As shown in 12.1, with the N-dimensional vector representation of each document, we can easily compute the mathematical similarities between two documents. Based on the similarities, we can further examine how different documents may cluster together in terms of their lexical similarities. corp_us_dist &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist() corp_us_hist &lt;- corp_us_dist %&gt;% as.dist %&gt;% hclust plot(corp_us_hist,hang = -1, cex = 0.7) 12.5.3 Feature Similarity What if we transpose a document-feature matrix? A transposed dfm would be a feature-document matrix. This is an interesting structure because we then can do the same tricks with all the features in the corpus. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent words are similar. # convert `dfm` to `fcm` corp_us_fcm &lt;- corp_us_dfm_trimmed %&gt;% fcm # select top 30 features corp_us_topfeatures &lt;- names(topfeatures(corp_us_fcm, 30)) # plot network fcm_select(corp_us_fcm, pattern = corp_us_topfeatures) %&gt;% textplot_network(min_freq = 0.5) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. Exercise 12.2 Please create a network of the top 30 bigrams based on the corpus corp_us. The criteria for bigrams selection are as follows: Include bigrams that consist of alphanumeric characters only (no punctuations) Include bigrams whose frequency &gt;= 10 and docfreq &gt;= 5 but &lt;= half number of the corpus "],
["vector-space-representation-ii.html", "Chapter 13 Vector Space Representation II 13.1 A Quick View 13.2 Loading the Corpus 13.3 Semgentation 13.4 Corpus Metadata 13.5 Document-Feature Matrix 13.6 Wordcloud 13.7 Document Similarity 13.8 Feature Similarity", " Chapter 13 Vector Space Representation II library(tidyverse) library(quanteda) library(readtext) library(jiebaR) Figure 13.1: Corpus Processing Flowchart In Chapter 12, we have demonstrated the potential of a vector representation of documents with the English data. Here, we would like to look at the Chinese data in more detail. In the corpus data processing flowchart, as repeated below (Figure 13.1), we need to deal with the word segmentation with the Chinese data. This prevents us from creating a dfm directly from a corpus object because the default internal word tokenization in quanteda is not optimized for non-English languages. In this chapter, we will be using the dataset TaiwanPresidentalSpeech.zip in our demo_data directory. Please make sure that you have downloaded the dataset from demo_data. 13.1 A Quick View For Chinese data, the major preprocessing steps have been highlighted in Figure 13.1: First read in the corpus using readtext() and create corpus data.frame object Subset the text column of the corpus data.frame for word segmentation Tokenize the texts using segment() in jiebaRand convert the output into a token object using as.token(). A token object is properly defined in quanteda, with many similar functions as a corpus object. This is the most important trick with the Chinese data. When you utilize many different libaries in R for your tasks, one thing you need to keep in your mind is that you need to fully understand what kind of objects you are dealing with. That is, you need to keep track of every variable you create in your script in terms of their object type/class. A vector is different from a list; a list is different from a token. Also, some of the object classes are predefined in R (e.g., vector, data.frame) while others are defined in specific libararies (e.g., corpus, token, dfm). As a habit, always check your object class (i.e., class()). 13.2 Loading the Corpus corp_tw &lt;- readtext(file=&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% as_tibble class(corp_tw) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; corp_tw %&gt;% mutate(text = str_sub(text, 1,20)) NB: readtext() creates a readtext or data.frame object. Following the tidy principle, we convert everything into tibble. 13.3 Semgentation Three important sub-steps in this part: initialize word segmenter, where a user dictionary is defined (Always use own dictionary to improve the performance of word segmentation) subset the text column of corp_tw tokenize the texts and convert the output into a quanteda-compatible object, token # initialize segmenter chi_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user.txt&quot;) corp_tw_tokens &lt;- corp_tw$text %&gt;% segment(jiebar = chi_seg) %&gt;% as.tokens class(corp_tw_tokens) ## [1] &quot;tokens&quot; corp_tw_tokens$text14[1:10] ## [1] &quot;為&quot; &quot;年輕人&quot; &quot;打造&quot; &quot;一個&quot; &quot;更好&quot; &quot;的&quot; &quot;國家&quot; &quot;各位&quot; ## [9] &quot;友邦&quot; &quot;的&quot; 13.4 Corpus Metadata When we subset the texts from corp_tw for word segmentation, all the metadata connected to the texts did not go with the texts. So the corp_tw_tokens did not have any metadata information. All you have is some arbitrary index to each text. docvars(corp_tw_tokens) So here we extract metadata information from the original filenames of each text stored in the corp_tw, and attach this metadata to corp_tw_tokens. corp_tw_meta &lt;-corp_tw %&gt;% dplyr::select(-text) %&gt;% separate(doc_id, into = c(&quot;YEAR&quot;,&quot;TERM&quot;,&quot;PRESIDENT&quot;),sep = &quot;_&quot;) %&gt;% mutate(PRESIDENT = str_replace(PRESIDENT, &quot;.txt&quot;,&quot;&quot;)) corp_tw_meta docvars(corp_tw_tokens) &lt;- corp_tw_meta 13.5 Document-Feature Matrix Now that we have a token version of our corpus, we can create dfm using the dfm() in quanteda. Also, we can take care of the feature selection (cf. 12.4) using functions like dfm_trim(), dfm_select(). corp_tw_dfm &lt;- corp_tw_tokens %&gt;% dfm(reove_punc = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 2, max_docfreq = 10, docfreq_type = &quot;count&quot;) class(corp_tw_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; corp_tw_dfm[1:5, 1:10] ## Document-feature matrix of: 5 documents, 10 features (32.0% sparse). ## 5 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs 中正 國民大會 國父 環境 以及 到 以來 知道 自己 此 ## text1 5 1 3 2 2 1 1 2 1 2 ## text2 4 4 1 1 2 2 0 3 0 0 ## text3 5 1 1 0 0 5 0 4 4 0 ## text4 7 3 7 0 0 0 1 1 2 1 ## text5 5 1 1 0 0 0 0 0 0 3 13.6 Wordcloud require(wordcloud2) require(wordcloud) top_features &lt;- corp_tw_dfm %&gt;% topfeatures(100) word_freq &lt;- data.frame(word = names(top_features), freq = top_features) word_freq %&gt;% wordcloud2() 13.7 Document Similarity corp_tw_dist &lt;- corp_tw_dfm %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist corp_tw_hist &lt;- corp_tw_dist %&gt;% as.dist %&gt;% hclust hist_labels &lt;- str_c(docvars(corp_tw_dfm,&quot;YEAR&quot;), docvars(corp_tw_dfm,&quot;PRESIDENT&quot;), sep=&quot;_&quot;) plot(corp_tw_hist, hang = -1, cex = 1.2, label = hist_labels) 13.8 Feature Similarity # convert `dfm` to `fcm` corp_tw_fcm &lt;- corp_tw_dfm %&gt;% fcm # select top 30 features corp_tw_topfeatures &lt;- names(topfeatures(corp_tw_fcm, 50)) # plot network library(showtext) font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) #par(family = &quot;Arial Unicode MS&quot;) fcm_select(corp_tw_fcm, pattern = corp_tw_topfeatures) %&gt;% textplot_network(min_freq = 0.5) -&gt;g ggsave(&quot;test.png&quot;, g) Exercise 13.1 Create the network of top 30 bigrams for the corpus corp_tw. The critera for bigrams selection are as follows: include bigrams whose frequency &gt;= 10 and docfreq &gt;=5 Exercise 13.2 There is an interesting application. When we analyze the document similarity, we create the graph of a dendrogram using hierarchical cluster analysis. In fact, document relations can also be represented by a network as well, as we do with the features in 12.5.3. How could you make use of the function textplot_network() in quanteda to create a network of the presidents? Please create a similar president network as shown below. "],
["references.html", "Chapter 14 References", " Chapter 14 References "]
]
