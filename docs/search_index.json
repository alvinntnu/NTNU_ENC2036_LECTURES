[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2019-12-07 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark upon a digital journey to your future career, there are a series of courses provided in the [Department of English, NTNU, Taiwan], offerring necessary skills and knowledge in important disciplines. This course requires as prerequisites basic knoweldge of computational coding. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of : operationalization data retrieval quantifying research questions significance testing the applications of corpus-linguistic methodology in the sub-disciplines of linguistic studies The objective of this course is two-fold. On the one hand, it will introduce the theoretical constructs behind corpus linguistics as well as the theoretical foundations that motivate such an empirical method. On the other hand, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics, Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our course material. There are a few reference books that I would highly recommend: Gries (2018) Baayen (2008) Brezina (2018) Course Website We have a course website. You may need a password to get access to the course materials. If you are an officially enrolled student, please ask the instructor for the access code. Course Demo Data Dropbox Demo Data Directory References "],
["creating-corpus.html", "Chapter 1 Creating Corpus 1.1 HTML Structure 1.2 Web Crawling 1.3 Functional Programming 1.4 Save Corpus", " Chapter 1 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. # Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;, &quot;stringr&quot;, &quot;jiebaR&quot;, &quot;tmcn&quot;, &quot;RCurl&quot;)) library(tidyverse) library(rvest) # Packages needed for further text processing # library(jiebaR) # library(tmcn) #library(RCurl) 1.1 HTML Structure 1.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 1.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 1.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 1.2. Figure 1.2: Tree Structure of A HTML Document 1.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 1.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 1.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT. In particular, we want to extract texts from the Gossiping board. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html session (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 19302 Now gossiping should be on the front page of the Gossiping board. Now we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% str_extract(&quot;[0-9]+&quot;) %&gt;% as.numeric() page.latest ## [1] 39286 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575691977.A.062.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575691993.A.2D8.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692072.A.803.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692099.A.020.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692191.A.228.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692287.A.FBD.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692381.A.547.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692483.A.1A5.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692499.A.047.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692566.A.E57.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692570.A.96B.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692594.A.6E2.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692617.A.D3E.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692623.A.9F9.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692626.A.9CA.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692704.A.747.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692704.A.01B.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692749.A.259.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692769.A.B4A.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575692815.A.C55.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;goldenhill (我的人權時代)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;[新聞] 歐盟確定5G網路風險 美國國務院歡迎&quot; ## [4] &quot;Sat Dec 7 12:12:54 2019&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;goldenhill&quot; article.title ## [1] &quot;[新聞] 歐盟確定5G網路風險 美國國務院歡迎&quot; article.datetime ## [1] &quot;Sat Dec 7 12:12:54 2019&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) article.content ## [1] &quot;1.媒體來源:新唐人亞太台\\n\\n2.記者署名:畢心慈 張琪\\n\\n3.完整新聞標題:[新聞] 歐盟確定5G網路風險 美國國務院歡迎\\n\\n4.完整新聞內文:\\n更新時間：2019-12-06 22:51:04\\n網址內含新聞影片\\n\\n 【新唐人亞太台 2019 年 12 月 06 日訊】美國國務卿蓬佩奧，週四在葡萄牙里\\n斯本講話，重申中共5G風險。此前2天，歐盟理事會確認了 歐洲若使用不可信任的5G供\\n應商，將面臨嚴重風險。\\n\\n 歐盟理事會週二就慎選5G網路供應商，發聲明說：「5G網路安全建設需要全面（\\n考慮），要考慮到風險。5G安全是一個連續過程，始於供應商。」\\n\\n 美國國務院週三發聲明，支持歐盟的結論，表示華為、中興等中國供應商將帶來\\n風險。\\n\\n 聲明說：「在一個國家5G網路的任何地方，允許這些（華為、中興）供應商存在\\n，都將對該國隱私、人權以及公民安全帶來重大風險。」\\n\\n 美國國務院還敦促所有國家，在建設5G網路中，拒絕這些供應商。\\n\\n 週四，美國國務卿蓬佩奧在里斯本重申，使用華為參與5G有風險。\\n\\n 美國國務卿 蓬佩奧：「我們廣泛討論了，從不信任網路（供應商）引入5G將帶來\\n的風險。我們必須仔細評估，中共在這些戰略和敏感領域進行投資帶來的風險。」\\n\\n 蓬佩奧直言，中共利用任何工具鎮壓本國，以及世界各地民眾。\\n\\n 美國國務卿 蓬佩奧：「我們有信心，（美國公民信息）不會最終落入聲名狼藉的\\n人手中，或中共手中。」\\n\\n 英國首相 強生週三表示，確保國家安全，以及和「五眼聯盟」情報機構的合作，\\n是決定是否允許華為參與5G的重要因素。\\n\\n 英國首相 強生：「我們不能損害至關重要的國家利益，也不能損害和五眼安全合\\n作伙伴的合作能力，這將成為我們關於華為決定的重要標準。」\\n\\n 德國執政黨的資深議員日前要求，禁止華為參與德國5G建設。\\n\\n 德國外長馬斯正在提出動議案，希望成立機構監管5G安全，將華為等不可信任公\\n司排除在外。\\n\\n 德國電信則決定推遲簽署5G採購合約。\\n\\n 新唐人記者 畢心慈 張琪 綜合報導\\n\\n5.完整新聞連結 (或短網址):http://www.ntdtv.com.tw/b5/20191206/video/259464.html?ptt6.備註:\\n\\n--&quot; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 1.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all 1.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) Exercise 1.1 Can you modify the R codes so that the script can automatically scrape more than one index page? "],
["corpus-analysis-a-start.html", "Chapter 2 Corpus Analysis: A Start 2.1 Installing quanteda 2.2 Building a corpus from character vector 2.3 Keyword-in-Context (KWIC) 2.4 KWIC with Regular Expressions 2.5 Tidy Text Format of the Corpus 2.6 Frequency Lists 2.7 Collocations 2.8 Word Cloud", " Chapter 2 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 2.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 2.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. We create a corpus() object with the pre-loaded character vector data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) summary(corp_us) After the corpus is created, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() 2.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or Concordances, is the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. 2.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. 2.5 Tidy Text Format of the Corpus Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson 2017). By keeping the input and output in tidy tables, users can transition fluidly between these packages. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy() objects (see the broom package [Robinson et al cited above]) from popular text mining R packages such as tm (Feinerer, Hornik, and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow where importing, filtering, and processing is done using dplyr and other tidy tools, after which the data is converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) 2.6 Frequency Lists To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(word, text) corp_us_words Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) ## [1] 135562 sum(corp_us_bigrams_freq$n) ## [1] 135504 2.7 Collocations corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% mutate(w1freq = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], w2freq = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% mutate(w12freq_exp = (w1freq*w2freq)/sum(n)) %&gt;% mutate(MI = log2(n/w12freq_exp), t = (n - w12freq_exp)/sqrt(n)) %&gt;% arrange(desc(MI)) corp_us_collocations Exercise 2.1 Create a collocation data frame arranged by other association metrics, such as t-score. Exercise 2.2 Find the top FIVE bigrams ranked according to MI values for each president. 2.8 Word Cloud library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 100, min.freq = 10, scale = c(5,1), color = brewer.pal(8, &quot;Dark2&quot;))) Exercise 2.3 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. require(tidytext) stop_words "],
["tokenization.html", "Chapter 3 Tokenization 3.1 English Tokenization 3.2 Text Analytics Pipeline 3.3 Proper Units for Analysis 3.4 Lexical Bundles (n-grams)", " Chapter 3 Tokenization library(quanteda) library(tidyverse) library(readtext) library(tidytext) Tokenization refers to the process of segmenting a long piece of discourse into smaller linguistic units. These linguistic units, depending on your purposes, may vary in many different ways: paragraphs sentences words syllables/characters letters phonemes In this chapter, we are going to look at this issue in more detail. Specifically, we will discuss the idea of word co-occurrence, which is one of the most fundamental method in corpus linguistics, and relate it to the issue of tokenization. 3.1 English Tokenization To get a clearer idea how tokenization works in unnest_tokens, we first create a simple corpus x in a tidy structure, i.e., a tibble, with one text only. x &lt;- tibble(id = 1, text = &quot;&#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone\\nthough), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very\\n well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;...&quot;) x writeLines(x$text[1]) ## &#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone ## though), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very ## well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;... (Please note that there are two line breaks in the text.) If we use the default setting token = &quot;words&quot; in unnest_tokens, we will get: x %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) Exercise 3.1 Please check the word tokens in the output data frame carefully and list characters that disappear in the word tokens but exist in the original text. Exercise 3.2 For those missing characters, how do you preserve these characters in your output then? 3.2 Text Analytics Pipeline 3.3 Proper Units for Analysis 3.3.1 Sentence Tokenization In text analysitcs, what we often do is the sentence tokenization corp_us &lt;-corpus(data_corpus_inaugural) corp_us_df &lt;- tidy(corp_us) class(corp_us_df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In unnest_token of the tidytext library, you can specify the parameter token to customize tokenzing function. As this library is designed to deal with English texts, there are several built-in options for English text tokenizatios, including words(default), characters, character_shingles, ngrams, skip_ngrams, sentences, lines, paragraphs, regex and ptb (Penn Treebank). corp_us_sent &lt;- corp_us_df %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) corp_us_sent Sometimes it is good to give each sentence of the document an index, e.g., ID, which can help us easily keep track of the relative position of the sentence in the original document. corp_us_sent %&gt;% group_by(Year) %&gt;% mutate(sentID = row_number()) 3.3.2 Words Tokenization Corpus linguistics deal with words all the time. Word tokenization therefore is the most often used method to segment texts. This is not a big concern for languages like English, which usually puts a whitespace between words. corp_us_word &lt;- corp_us_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) corp_us_word Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). 3.4 Lexical Bundles (n-grams) Sometimes it is helpful to identify frequently occurring n-grams, i.e., recurrent multiple word sequences. You can easily create an n-gram frequency list using unnest_tokens(): corp_us_trigram &lt;- corp_us_df %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigram We then can examine which n-grams were most often used by each President: corp_us_trigram %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) Exercise 3.3 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram would be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by different Presidents? So now let’s compute the dispersion of the n-grams in our corp_us_df. Here we define the dispersion of an n-gram as the number of documents where it occurs. corp_us_trigram %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) # # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) Therefore, usually lexical bundles or n-grams are defined based on distrubtional patterns of these multiword units. In particular, cut-off values are often determined to select a list of meaningful lexical bundles. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. "],
["parts-of-speech-tagging.html", "Chapter 4 Parts-of-Speech Tagging 4.1 Parts-of-Speech Tagging 4.2 Metalingusitic Analysis 4.3 Saving POS-tagged Texts", " Chapter 4 Parts-of-Speech Tagging library(tidyverse) library(tidytext) In many textual analysis, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. 4.1 Parts-of-Speech Tagging # install spacyR # devtools::install_github(&quot;quanteda/spacyr&quot;, build_vignettes = FALSE) library(spacyr) #spacy_install() spacy_initialize() txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt,pos = T, tag = T, lemma = T) parsedtxt Two tagsets are included in the output of spacy_parse: pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set. library(quanteda) library(tidytext) corp_us_df &lt;- data_corpus_inaugural %&gt;% corpus %&gt;% tidy corp_us_df corp_us_df$text[1] %&gt;% spacy_parse() %&gt;% unnest_tokens(word, token) One trick here. If the input text character vecotr for spacy_parse() does not specify names() attributes for each text, then by default in the column doc_id of the output, it will use an autoamtic number to refer to each text. If we specify the names() of all our texts, then we can keep the meta information of each text. documents &lt;- corp_us_df$text names(documents)&lt;-str_c(corp_us_df$Year, corp_us_df$FirstName, corp_us_df$President, sep=&quot;_&quot;) corp_us_word_tag &lt;- documents %&gt;% spacy_parse(pos=T) %&gt;% unnest_tokens(word,token) 4.2 Metalingusitic Analysis In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_word_tag and first generate the frequencies of verbs, and number of words for each presidential speech text. corp_us_word_tag_2 &lt;-corp_us_word_tag %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) corp_us_word_tag_2 With the syntactic complexity of each president, we can plot the tendency: corp_us_word_tag_2 %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 4.1 Please add a regression/smooth line to the above plot to indicate the downward trend? 4.3 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time when we process the data, it would be more convenient if we save the tokenized texts with the POS tags in the hard drive. Next time we can import those files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. documents %&gt;% spacy_parse(tag=T) %&gt;% unnest_tokens(word, token, to_lower = F, strip_punct = F) -&gt; corp_us_word_tag_3 spacy_finalize() "],
["keyword-analysis.html", "Chapter 5 Keyword Analysis", " Chapter 5 Keyword Analysis G2 \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Relative Frequency Ratio \\[ RFR = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] Difference Coefficient \\[ DC = \\frac{a-b}{a+b} \\] library(tidyverse) library(tidytext) library(readtext) library(quanteda) flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) corpus corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% count(word, textid) %&gt;% tidyr::spread(textid, n, fill = 0) %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) "],
["text-processing.html", "Chapter 6 Text Processing", " Chapter 6 Text Processing "],
["references.html", "Chapter 7 References", " Chapter 7 References "]
]
