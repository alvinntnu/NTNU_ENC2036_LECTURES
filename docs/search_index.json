[
["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resourcess", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure 3.1.1 HTML Syntax To illustrate the structure of the HTML, please download the sample html file from: demo_data/data-sample-html.html and first open the with your browser. &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; An HTML document includes several important elements (cf. 3.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 3.1: Syntax of An HTML Tag Element An HTML document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in Figure 3.2. Figure 3.2: Tree Structure of An HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The idea is that CSS specifies the formats/styles of the HTML elements. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT Forum. In particular, we will demonstrate how to scape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html_session() (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created html_session and create another session. gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 22250 Now our html sesseion, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest ## [1] 38966 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584325868.A.D13.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584325890.A.3CA.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584325913.A.9DE.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584325921.A.B00.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326000.A.663.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326023.A.6ED.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326028.A.01A.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326031.A.148.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326100.A.A23.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326110.A.BD5.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326118.A.39F.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326193.A.DA5.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326194.A.1C5.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326204.A.476.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326230.A.486.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326288.A.0EE.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326421.A.779.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326511.A.930.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326521.A.C35.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584326545.A.2C9.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. Because we are interested in the metadata and the contents of each articule, now the question is: where are they in the HTML? We need to go back to the source page of the article HTML again: After a closer inspection of the article HTML, we know that: The metadata of the article are included in &lt;span&gt; tag elements, belonging to the class class=&quot;article-meta-value&quot; The contents of the article are included in the &lt;div&gt; element, whose ID is ID=&quot;main-content&quot; Now we are ready to extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() article.header ## [1] &quot;zrct5566 (美麗又殘酷的世界)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;[問卦] 我買哀鳳11只分三期是否已達到財富自由&quot; ## [4] &quot;Mon Mar 16 10:31:05 2020&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;zrct5566&quot; article.title ## [1] &quot;[問卦] 我買哀鳳11只分三期是否已達到財富自由&quot; article.datetime ## [1] &quot;Mon Mar 16 10:31:05 2020&quot; Now we extract the main contents of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content ## [1] &quot;在公司兩年四個月\\n\\n盡心盡力\\n\\n莫忘初衷\\n\\n就在剛剛收到通知\\n\\n我\\n\\n終於一路從實習助理爬到正職助理\\n\\n下個月的薪資單\\n\\n從23K\\n\\n直接給他多三千\\n\\n人生達到前所未有的巔峰\\n\\n感謝主管的肯定\\n\\n感謝父母的栽培\\n\\n感謝自己的努力\\n\\n\\n“如果說我能看的更遠一些\\n ，那是因為我站在巨人的肩膀上。”\\n\\n ─ 牛頓\\n\\n遙想那段吃苦的日子\\n\\n我決定揮別過去的自己\\n\\n從明天起就是嶄新的人生\\n\\n迎接我的是前途一片光明\\n\\n薪資水平來到前所未有的高度\\n\\n雖然離繳稅人口還有一段距離\\n\\n但我會努力\\n\\n所以我要買一個禮物\\n\\n犒賞事業有成的自己\\n\\n哀鳳11\\n\\n64G\\n\\n兩萬四千九\\n\\n沒有猶豫\\n\\n直接只給他分三期\\n\\n像我這樣的任性\\n\\n是否已經達到財富自由\\n\\n\\n\\n\\n--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push ## {xml_nodeset (99)} ## [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [4] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [5] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [6] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [7] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [8] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [9] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [10] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [11] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [12] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [13] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [14] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [15] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [16] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [17] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [18] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [19] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [20] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag ## [1] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;噓&quot; &quot;噓&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; ## [16] &quot;推&quot; &quot;推&quot; &quot;噓&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;→&quot; ## [31] &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; ## [46] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;噓&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; ## [61] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;噓&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; ## [76] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; ## [91] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author ## [1] &quot;KLGlikeshit&quot; &quot;haha98&quot; &quot;dageegee&quot; &quot;jax0803chiu&quot; &quot;gay7788&quot; ## [6] &quot;Tattood&quot; &quot;hotlatte&quot; &quot;DASHOCK&quot; &quot;papercutt&quot; &quot;hkahka&quot; ## [11] &quot;scott8257&quot; &quot;xbit&quot; &quot;Workforme&quot; &quot;longkiss0618&quot; &quot;smallbrother&quot; ## [16] &quot;GaryOp&quot; &quot;TAIPEI2010&quot; &quot;hh800315&quot; &quot;s27052705&quot; &quot;leo0821&quot; ## [21] &quot;chino32818&quot; &quot;chino32818&quot; &quot;xxx60133&quot; &quot;waqw&quot; &quot;vowpool&quot; ## [26] &quot;lingsk&quot; &quot;esunbank&quot; &quot;longkiss0618&quot; &quot;Daedolon&quot; &quot;esunbank&quot; ## [31] &quot;Roger0503&quot; &quot;Roger0503&quot; &quot;wbreeze&quot; &quot;mundane5566&quot; &quot;david8339&quot; ## [36] &quot;BRANFORD&quot; &quot;elzohar&quot; &quot;rockyegg&quot; &quot;leecoco&quot; &quot;marginal5566&quot; ## [41] &quot;Arnol&quot; &quot;je321654&quot; &quot;tung3567752&quot; &quot;w328065&quot; &quot;TingTT&quot; ## [46] &quot;sole772pk37&quot; &quot;ijk77692&quot; &quot;alienjj&quot; &quot;panzerbug&quot; &quot;Aina1111&quot; ## [51] &quot;despised&quot; &quot;panzerbug&quot; &quot;s32214&quot; &quot;winstonuno&quot; &quot;s32214&quot; ## [56] &quot;IngramBrando&quot; &quot;cheetahspeed&quot; &quot;maxmeyer&quot; &quot;IngramBrando&quot; &quot;hakkacandy&quot; ## [61] &quot;sarsenwen&quot; &quot;O300&quot; &quot;zeldalin&quot; &quot;glacialfire&quot; &quot;HotShotBB&quot; ## [66] &quot;mdb750816&quot; &quot;HotShotBB&quot; &quot;ericeric1232&quot; &quot;DarkerDuck&quot; &quot;danny91074&quot; ## [71] &quot;wwvvkai&quot; &quot;Leo0408&quot; &quot;ajugo&quot; &quot;herced&quot; &quot;brian3639&quot; ## [76] &quot;miler22020&quot; &quot;cyanmedoc&quot; &quot;STE23&quot; &quot;a0000959&quot; &quot;axzs1111&quot; ## [81] &quot;jackq&quot; &quot;goldmouse&quot; &quot;lunawolke&quot; &quot;goldmouse&quot; &quot;marco0715&quot; ## [86] &quot;kilm665&quot; &quot;A80211ab&quot; &quot;yamacha&quot; &quot;louis520&quot; &quot;bearbig0203&quot; ## [91] &quot;yvonne10126&quot; &quot;abdgmnzc&quot; &quot;hunder31&quot; &quot;fishtree&quot; &quot;tassadar1&quot; ## [96] &quot;tassadar1&quot; &quot;este1a&quot; &quot;LunaRin&quot; &quot;RachelMcAdam&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content ## [1] &quot;: 恭喜嘻嘻&quot; ## [2] &quot;: 挖靠 太有錢了ㄅ&quot; ## [3] &quot;: 對&quot; ## [4] &quot;: 要考慮出書鼓勵其他人嗎&quot; ## [5] &quot;: 幹，有錢人。怒噓&quot; ## [6] &quot;: 羨慕推&quot; ## [7] &quot;: ？羨慕&quot; ## [8] &quot;: 羨慕&quot; ## [9] &quot;: 肯定的&quot; ## [10] &quot;: 嫩我可以分36期&quot; ## [11] &quot;: 噓炫富&quot; ## [12] &quot;: 那代表你信用卡額度少的可憐..幫qq&quot; ## [13] &quot;: 是 人生勝利組了 pr99.9999％&quot; ## [14] &quot;: 羨慕 我上次要分12期 她不給我分&quot; ## [15] &quot;: 施主，請問您助理界的霸主是什麼？&quot; ## [16] &quot;: 屌打板上9成的人了 恭喜&quot; ## [17] &quot;: 晚上犒賞自己多點一份滷肉飯 加小菜&quot; ## [18] &quot;: 好羨慕&quot; ## [19] &quot;: 幹我才分一期我輸了ORZ&quot; ## [20] &quot;: 羨慕&quot; ## [21] &quot;: 炒一盤拿手好菜 倒一杯散裝白酒來慶祝&quot; ## [22] &quot;: 啊&quot; ## [23] &quot;: 原來是 有錢人要寫自傳了&quot; ## [24] &quot;: 4&quot; ## [25] &quot;: 樓下直接付清 是有錢人&quot; ## [26] &quot;: 到第3期前 遇到新機發表可能會降價&quot; ## [27] &quot;: 兆豐 有一張 給我自動分三期繳 還算全額&quot; ## [28] &quot;: 不過你去隔壁板買只要兩萬兩千元&quot; ## [29] &quot;: 再配一副Airpods你人生就是完美的了&quot; ## [30] &quot;: 刷 這不就屌翻天了&quot; ## [31] &quot;: 希望大師可以出一本書分享成功的心得&gt;&quot; ## [32] &quot;: &lt;&quot; ## [33] &quot;: 有錢 羨慕&quot; ## [34] &quot;: 4&quot; ## [35] &quot;: 幹 有笑有推&quot; ## [36] &quot;: 嗚嗚我才分12期&quot; ## [37] &quot;: 已羨慕 開放認乾爹嗎?&quot; ## [38] &quot;: 空機去COSTCO買 24059還有刷卡6期0利率&quot; ## [39] &quot;: 4&quot; ## [40] &quot;: 26k太誇張 整整我的7倍&quot; ## [41] &quot;: 炒一盤拿手小菜 倒一杯散裝白酒&quot; ## [42] &quot;: 棒棒&quot; ## [43] &quot;: 發&quot; ## [44] &quot;: 好有錢...我連愛瘋都刷不過，只能付現&quot; ## [45] &quot;: 真的恭喜，在人生旅途上有所突破！&quot; ## [46] &quot;: 恭喜QQ我還是米蟲沒工作QQ&quot; ## [47] &quot;: 羨慕推&quot; ## [48] &quot;: 4&quot; ## [49] &quot;: 威猛欸，羨慕！&quot; ## [50] &quot;: 好好笑 給推&quot; ## [51] &quot;: 恭喜&quot; ## [52] &quot;: 我分12期，還到吐血，3期真神人&quot; ## [53] &quot;: 羨慕居然只分4期就買到了&quot; ## [54] &quot;: 讚讚&quot; ## [55] &quot;: 3&quot; ## [56] &quot;: 有錢人 apple watch也刷下去連潮度&quot; ## [57] &quot;: 羨慕嫉妒噓 財富自由&quot; ## [58] &quot;: 太勵志了吧&quot; ## [59] &quot;: 度都有惹&quot; ## [60] &quot;: 恭喜...好羨慕&quot; ## [61] &quot;: 太奢侈了吧!!!!&quot; ## [62] &quot;: 4 滿意了嗎&quot; ## [63] &quot;: 我好羨慕，真的。你是個成功者。&quot; ## [64] &quot;: 你應該去104上班的&quot; ## [65] &quot;: 您好我是出版社小編 不知道有沒有榮幸&quot; ## [66] &quot;: 是&quot; ## [67] &quot;: 邀請您出一本小資助理財富自由的書呢！&quot; ## [68] &quot;: 人生勝利組！&quot; ## [69] &quot;: 你分三十期有更多的財富可自由&quot; ## [70] &quot;: 炫富三小&quot; ## [71] &quot;: 靠 你好有錢哦&quot; ## [72] &quot;: 勝利&quot; ## [73] &quot;: 感動&quot; ## [74] &quot;: 羨慕&quot; ## [75] &quot;: 我用零元安卓，好羨慕有錢哥&quot; ## [76] &quot;: 好友前...&quot; ## [77] &quot;: 拍拍&quot; ## [78] &quot;: 我分24期...&quot; ## [79] &quot;: 贏過95%台灣人了 羨慕&quot; ## [80] &quot;: 人生勝利組 讚讚讚&quot; ## [81] &quot;: 太過於自由 太危險惹&quot; ## [82] &quot;: 真希望有天你跟你一樣成功&quot; ## [83] &quot;: 太猛了吧&quot; ## [84] &quot;: 能&quot; ## [85] &quot;: 4&quot; ## [86] &quot;: 恭喜老爺&quot; ## [87] &quot;: 你公司在哪 下班我跟你借錢&quot; ## [88] &quot;: 本版不接受炫富！滾&quot; ## [89] &quot;: 強者，佩服&quot; ## [90] &quot;: 羨慕&quot; ## [91] &quot;: 4&quot; ## [92] &quot;: 還要收人嗎？&quot; ## [93] &quot;: XDDDDDDDDDDDDDDDDDDDDDDDDDDD&quot; ## [94] &quot;: 感動&quot; ## [95] &quot;: 恭喜，接下來只要再繼承哪個有錢親戚的&quot; ## [96] &quot;: 遺產就可以退休了&quot; ## [97] &quot;: 笑死&quot; ## [98] &quot;: 太強了吧 3310都還要分48期&quot; ## [99] &quot;: 哥太勵志了吧，崇拜&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime ## [1] &quot;203.74.125.252 03/16 10:31&quot; &quot;140.109.23.25 03/16 10:31&quot; ## [3] &quot;180.204.147.1 03/16 10:31&quot; &quot;223.136.198.147 03/16 10:31&quot; ## [5] &quot;60.250.181.4 03/16 10:31&quot; &quot;110.30.120.209 03/16 10:31&quot; ## [7] &quot;111.71.32.30 03/16 10:32&quot; &quot;61.231.233.182 03/16 10:32&quot; ## [9] &quot;125.230.18.35 03/16 10:32&quot; &quot;110.50.139.110 03/16 10:32&quot; ## [11] &quot;114.34.176.82 03/16 10:32&quot; &quot;218.173.76.147 03/16 10:32&quot; ## [13] &quot;114.37.160.247 03/16 10:32&quot; &quot;163.27.132.43 03/16 10:33&quot; ## [15] &quot;101.12.48.121 03/16 10:33&quot; &quot;111.83.218.244 03/16 10:33&quot; ## [17] &quot;121.13.229.205 03/16 10:33&quot; &quot;42.77.182.35 03/16 10:33&quot; ## [19] &quot;223.141.14.40 03/16 10:33&quot; &quot;163.30.85.101 03/16 10:33&quot; ## [21] &quot;49.159.107.145 03/16 10:33&quot; &quot;49.159.107.145 03/16 10:33&quot; ## [23] &quot;111.83.206.157 03/16 10:33&quot; &quot;36.226.1.74 03/16 10:34&quot; ## [25] &quot;125.227.40.62 03/16 10:34&quot; &quot;36.233.109.69 03/16 10:34&quot; ## [27] &quot;219.70.254.29 03/16 10:34&quot; &quot;163.27.132.43 03/16 10:34&quot; ## [29] &quot;223.200.104.205 03/16 10:34&quot; &quot;219.70.254.29 03/16 10:34&quot; ## [31] &quot;124.219.107.196 03/16 10:34&quot; &quot;124.219.107.196 03/16 10:34&quot; ## [33] &quot;101.12.52.191 03/16 10:35&quot; &quot;42.74.175.46 03/16 10:35&quot; ## [35] &quot;117.19.213.61 03/16 10:35&quot; &quot;101.13.197.166 03/16 10:35&quot; ## [37] &quot;1.171.47.57 03/16 10:36&quot; &quot;60.248.45.103 03/16 10:36&quot; ## [39] &quot;111.83.168.98 03/16 10:36&quot; &quot;61.223.16.177 03/16 10:36&quot; ## [41] &quot;61.223.59.26 03/16 10:36&quot; &quot;223.136.131.6 03/16 10:36&quot; ## [43] &quot;42.73.236.8 03/16 10:36&quot; &quot;223.136.199.30 03/16 10:37&quot; ## [45] &quot;180.217.239.159 03/16 10:37&quot; &quot;223.140.204.89 03/16 10:37&quot; ## [47] &quot;113.185.40.246 03/16 10:37&quot; &quot;223.136.2.19 03/16 10:37&quot; ## [49] &quot;223.136.88.218 03/16 10:38&quot; &quot;223.138.23.16 03/16 10:38&quot; ## [51] &quot;110.26.96.38 03/16 10:38&quot; &quot;223.136.88.218 03/16 10:38&quot; ## [53] &quot;1.136.107.18 03/16 10:38&quot; &quot;114.136.38.35 03/16 10:38&quot; ## [55] &quot;1.136.107.18 03/16 10:39&quot; &quot;118.163.167.82 03/16 10:39&quot; ## [57] &quot;101.13.128.108 03/16 10:39&quot; &quot;101.12.44.60 03/16 10:39&quot; ## [59] &quot;118.163.167.82 03/16 10:39&quot; &quot;101.13.147.203 03/16 10:39&quot; ## [61] &quot;202.39.185.61 03/16 10:39&quot; &quot;223.137.240.72 03/16 10:40&quot; ## [63] &quot;111.246.44.194 03/16 10:40&quot; &quot;27.247.200.183 03/16 10:40&quot; ## [65] &quot;110.28.227.238 03/16 10:40&quot; &quot;223.136.192.87 03/16 10:40&quot; ## [67] &quot;110.28.227.238 03/16 10:40&quot; &quot;223.140.63.116 03/16 10:40&quot; ## [69] &quot;36.236.169.245 03/16 10:41&quot; &quot;180.217.253.77 03/16 10:41&quot; ## [71] &quot;101.15.169.24 03/16 10:41&quot; &quot;180.217.102.164 03/16 10:41&quot; ## [73] &quot;223.137.7.224 03/16 10:42&quot; &quot;39.11.196.103 03/16 10:42&quot; ## [75] &quot;223.138.101.105 03/16 10:42&quot; &quot;36.231.35.228 03/16 10:43&quot; ## [77] &quot;117.19.160.231 03/16 10:43&quot; &quot;114.27.47.146 03/16 10:43&quot; ## [79] &quot;1.34.236.2 03/16 10:44&quot; &quot;223.136.112.140 03/16 10:44&quot; ## [81] &quot;111.71.213.72 03/16 10:45&quot; &quot;1.200.42.152 03/16 10:45&quot; ## [83] &quot;114.37.201.190 03/16 10:45&quot; &quot;1.200.42.152 03/16 10:45&quot; ## [85] &quot;111.71.33.229 03/16 10:45&quot; &quot;111.83.15.221 03/16 10:45&quot; ## [87] &quot;36.235.14.197 03/16 10:46&quot; &quot;12.28.116.70 03/16 10:46&quot; ## [89] &quot;223.138.157.80 03/16 10:46&quot; &quot;223.136.77.51 03/16 10:46&quot; ## [91] &quot;42.76.18.171 03/16 10:47&quot; &quot;223.140.106.156 03/16 10:47&quot; ## [93] &quot;111.240.5.109 03/16 10:47&quot; &quot;36.234.196.134 03/16 10:48&quot; ## [95] &quot;220.133.118.148 03/16 10:48&quot; &quot;220.133.118.148 03/16 10:48&quot; ## [97] &quot;42.74.14.245 03/16 10:48&quot; &quot;111.71.79.156 03/16 10:48&quot; ## [99] &quot;114.136.47.250 03/16 10:49&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. It returns a vector of article links. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables(): This function takes an article link link as the argument and extract the metadata, contents, and pushes of the article. It returns a list of two elements–article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows # Merge all push.tables into one push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph provides a flow chart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resourcess Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the substainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata representation of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Teachnical parts for corpus creation (cf. Sinclair’s Appendix) Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Exercise 3.3 Now you should have basic ideas how we can crawl data from the Internet. Sometimes, we not only collect texts but statistics as well. Please try to collect the statistics of COVID-19 outbreak from the Wikipedia 2019–20 coronavirus pandemic. Specifically, write a short script to automatically get the table included in the Wiki page, where the numbers of confirmed cases, deaths, and recoveries for each country are recorded. Your script should output a data frame as follows. Please name the columns of your data frame as follows as well. (Note: The numbers may vary because of the constant updates of the wiki page.) References "]
]
