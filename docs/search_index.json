[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2019-12-11 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark upon a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offerring necessary skills and knowledge in important disciplines. This course requires as prerequisite basic knoweldge of computational coding. It is highly recommended for students to take ENC2055 or other equivalents before taking this course. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will lead the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our course material. There are a few reference books that I would highly recommend: Gries (2018) Baayen (2008) Brezina (2018) Course Website We have a course website. You may need a password to get access to the course materials. If you are an officially enrolled student, please ask the instructor for the access code. Course Demo Data Dropbox Demo Data Directory References "],
["creating-corpus.html", "Chapter 1 Creating Corpus 1.1 HTML Structure 1.2 Web Crawling 1.3 Functional Programming 1.4 Save Corpus", " Chapter 1 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. # Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;, &quot;stringr&quot;, &quot;jiebaR&quot;, &quot;tmcn&quot;, &quot;RCurl&quot;)) library(tidyverse) library(rvest) # Packages needed for further text processing # library(jiebaR) # library(tmcn) #library(RCurl) 1.1 HTML Structure 1.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 1.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 1.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 1.2. Figure 1.2: Tree Structure of A HTML Document 1.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 1.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 1.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT. In particular, we want to extract texts from the Gossiping board. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html session (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 9386 Now gossiping should be on the front page of the Gossiping board. Now we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% str_extract(&quot;[0-9]+&quot;) %&gt;% as.numeric() page.latest ## [1] 39006 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576003645.A.E17.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576004232.A.B13.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576004361.A.427.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576004606.A.720.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576004832.A.2DD.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576004896.A.518.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576004949.A.B59.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576005124.A.B36.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576005420.A.C66.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576005449.A.128.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576005716.A.9F1.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576005959.A.401.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576006623.A.858.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576006625.A.1F9.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576006915.A.AC9.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576007008.A.FBE.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576007470.A.2DF.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576007560.A.803.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576007566.A.920.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576008910.A.82A.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;dennis7653 (狗兒)&quot; &quot;Gossiping&quot; ## [3] &quot;Re: [問卦] 康師傅很難吃？&quot; &quot;Wed Dec 11 02:47:23 2019&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;dennis7653&quot; article.title ## [1] &quot;Re: [問卦] 康師傅很難吃？&quot; article.datetime ## [1] &quot;Wed Dec 11 02:47:23 2019&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) article.content ## [1] &quot;: 對在台灣時的康師傅沒啥印象: 那時候還太小 後來台灣就沒有康師傅可以買了: 之前在大陸吃到他們的紅燒牛肉麵: 覺得還行啊 而且麵體感覺還比台灣的泡麵好: 比較粗 過水也沒那麼油: 看到很多人說康師傅的難吃: 是真的覺得難吃嗎 還是以前台灣的康師傅跟現在大陸吃到的康師傅味道不一樣: 有沒有八卦以前康師傅跟大陸康師傅味道完全不同。\\n\\n還記得康師傅剛出去同學家，他媽煮給我們吃，第一次吃海鮮口味的，大概跟統一來一杯\\n海鮮有87%像，但多了一個臭油味跟麵體很軟爛，小時候很愛吃泡麵的我居然吃不完一包\\n泡的一碗，後來又吃到牛肉口味，也是臭油味加麵體爛，口味部份跟統一來一杯很像假設\\n來一杯10分來算，康師傅我只能給到3分。\\n\\n後來有傳聞說大陸那邊工人去外面打工都用熱水壺的水，沒有像台灣都用開水，所以康師\\n傅用60.70度的水泡下去就好，雖然聽說過，但這麼難吃的泡麵根本不想花錢買更何況台\\n灣同樣口味選擇泡麵一堆。\\n\\n至於大陸的部份就很像雙響砲等級的，說難吃還是吞的下去，但出差去買還是會吃酸辣粉\\n之類的怪泡麵，若要吃普通的，也幾乎都是買日清系列，但真的比起來台灣台酒系列還有\\n滿漢大餐之類的，幾乎是屌打所有中國泡麵。\\n\\n--&quot; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 1.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all 1.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) Exercise 1.1 Can you modify the R codes so that the script can automatically scrape more than one index page? "],
["corpus-analysis-a-start.html", "Chapter 2 Corpus Analysis: A Start 2.1 Installing quanteda 2.2 Building a corpus from character vector 2.3 Keyword-in-Context (KWIC) 2.4 KWIC with Regular Expressions 2.5 Tidy Text Format of the Corpus 2.6 Frequency Lists 2.7 Collocations 2.8 Word Cloud", " Chapter 2 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 2.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 2.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. We create a corpus() object with the pre-loaded character vector data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) summary(corp_us) After the corpus is created, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() 2.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or Concordances, is the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. 2.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. 2.5 Tidy Text Format of the Corpus Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson 2017). By keeping the input and output in tidy tables, users can transition fluidly between these packages. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy() objects (see the broom package [Robinson et al cited above]) from popular text mining R packages such as tm (Feinerer, Hornik, and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow where importing, filtering, and processing is done using dplyr and other tidy tools, after which the data is converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) 2.6 Frequency Lists To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(word, text) corp_us_words Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) ## [1] 135562 sum(corp_us_bigrams_freq$n) ## [1] 135504 2.7 Collocations corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% mutate(w1freq = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], w2freq = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% mutate(w12freq_exp = (w1freq*w2freq)/sum(n)) %&gt;% mutate(MI = log2(n/w12freq_exp), t = (n - w12freq_exp)/sqrt(n)) %&gt;% arrange(desc(MI)) corp_us_collocations Exercise 2.1 Create a collocation data frame arranged by other association metrics, such as t-score. Exercise 2.2 Find the top FIVE bigrams ranked according to MI values for each president. 2.8 Word Cloud library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 100, min.freq = 10, scale = c(5,1), color = brewer.pal(8, &quot;Dark2&quot;))) Exercise 2.3 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. require(tidytext) stop_words "],
["tokenization.html", "Chapter 3 Tokenization 3.1 English Tokenization 3.2 Text Analytics Pipeline 3.3 Proper Units for Analysis 3.4 Lexical Bundles (n-grams)", " Chapter 3 Tokenization library(quanteda) library(tidyverse) library(readtext) library(tidytext) Tokenization refers to the process of segmenting a long piece of discourse into smaller linguistic units. These linguistic units, depending on your purposes, may vary in many different ways: paragraphs sentences words syllables/characters letters phonemes In this chapter, we are going to look at this issue in more detail. Specifically, we will discuss the idea of word co-occurrence, which is one of the most fundamental method in corpus linguistics, and relate it to the issue of tokenization. 3.1 English Tokenization To get a clearer idea how tokenization works in unnest_tokens, we first create a simple corpus x in a tidy structure, i.e., a tibble, with one text only. x &lt;- tibble(id = 1, text = &quot;&#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone\\nthough), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very\\n well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;...&quot;) x writeLines(x$text[1]) ## &#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone ## though), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very ## well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;... (Please note that there are two line breaks in the text.) If we use the default setting token = &quot;words&quot; in unnest_tokens, we will get: x %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) Exercise 3.1 Please check the word tokens in the output data frame carefully and list characters that disappear in the word tokens but exist in the original text. Exercise 3.2 For those missing characters, how do you preserve these characters in your output then? 3.2 Text Analytics Pipeline 3.3 Proper Units for Analysis 3.3.1 Sentence Tokenization In text analysitcs, what we often do is the sentence tokenization corp_us &lt;-corpus(data_corpus_inaugural) corp_us_df &lt;- tidy(corp_us) class(corp_us_df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In unnest_token of the tidytext library, you can specify the parameter token to customize tokenzing function. As this library is designed to deal with English texts, there are several built-in options for English text tokenizatios, including words(default), characters, character_shingles, ngrams, skip_ngrams, sentences, lines, paragraphs, regex and ptb (Penn Treebank). corp_us_sent &lt;- corp_us_df %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) corp_us_sent Sometimes it is good to give each sentence of the document an index, e.g., ID, which can help us easily keep track of the relative position of the sentence in the original document. corp_us_sent %&gt;% group_by(Year) %&gt;% mutate(sentID = row_number()) 3.3.2 Words Tokenization Corpus linguistics deal with words all the time. Word tokenization therefore is the most often used method to segment texts. This is not a big concern for languages like English, which usually puts a whitespace between words. corp_us_word &lt;- corp_us_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) corp_us_word Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). 3.4 Lexical Bundles (n-grams) Sometimes it is helpful to identify frequently occurring n-grams, i.e., recurrent multiple word sequences. You can easily create an n-gram frequency list using unnest_tokens(): corp_us_trigram &lt;- corp_us_df %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigram We then can examine which n-grams were most often used by each President: corp_us_trigram %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) Exercise 3.3 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram would be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by different Presidents? So now let’s compute the dispersion of the n-grams in our corp_us_df. Here we define the dispersion of an n-gram as the number of documents where it occurs. corp_us_trigram %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) # # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) Therefore, usually lexical bundles or n-grams are defined based on distrubtional patterns of these multiword units. In particular, cut-off values are often determined to select a list of meaningful lexical bundles. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. "],
["parts-of-speech-tagging.html", "Chapter 4 Parts-of-Speech Tagging 4.1 Parts-of-Speech Tagging 4.2 Metalingusitic Analysis 4.3 Saving POS-tagged Texts", " Chapter 4 Parts-of-Speech Tagging library(tidyverse) library(tidytext) In many textual analysis, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. 4.1 Parts-of-Speech Tagging # install spacyR # devtools::install_github(&quot;quanteda/spacyr&quot;, build_vignettes = FALSE) library(spacyr) #spacy_install() spacy_initialize() txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt,pos = T, tag = T, lemma = T) parsedtxt Two tagsets are included in the output of spacy_parse: pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set. library(quanteda) library(tidytext) corp_us_df &lt;- data_corpus_inaugural %&gt;% corpus %&gt;% tidy corp_us_df corp_us_df$text[1] %&gt;% spacy_parse() %&gt;% unnest_tokens(word, token) One trick here. If the input text character vecotr for spacy_parse() does not specify names() attributes for each text, then by default in the column doc_id of the output, it will use an autoamtic number to refer to each text. If we specify the names() of all our texts, then we can keep the meta information of each text. documents &lt;- corp_us_df$text names(documents)&lt;-str_c(corp_us_df$Year, corp_us_df$FirstName, corp_us_df$President, sep=&quot;_&quot;) corp_us_word_tag &lt;- documents %&gt;% spacy_parse(pos=T) %&gt;% unnest_tokens(word,token) 4.2 Metalingusitic Analysis In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_word_tag and first generate the frequencies of verbs, and number of words for each presidential speech text. corp_us_word_tag_2 &lt;-corp_us_word_tag %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) corp_us_word_tag_2 With the syntactic complexity of each president, we can plot the tendency: corp_us_word_tag_2 %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 4.1 Please add a regression/smooth line to the above plot to indicate the downward trend? 4.3 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time when we process the data, it would be more convenient if we save the tokenized texts with the POS tags in the hard drive. Next time we can import those files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. documents %&gt;% spacy_parse(tag=T) %&gt;% unnest_tokens(word, token, to_lower = F, strip_punct = F) -&gt; corp_us_word_tag_3 spacy_finalize() "],
["keyword-analysis.html", "Chapter 5 Keyword Analysis", " Chapter 5 Keyword Analysis G2 \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Relative Frequency Ratio \\[ RFR = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] Difference Coefficient \\[ DC = \\frac{a-b}{a+b} \\] library(tidyverse) library(tidytext) library(readtext) library(quanteda) #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) corpus corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% count(word, textid) %&gt;% tidyr::spread(textid, n, fill = 0) %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) "],
["constructions-and-idioms.html", "Chapter 6 Constructions and Idioms 6.1 Chinese Four-character Idioms 6.2 Dictionary Entries 6.3 Case Study: X來Y去 6.4 Exercises", " Chapter 6 Constructions and Idioms library(tidyverse) 6.1 Chinese Four-character Idioms Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. This chapter will provide a exploratory analysis of four-character idioms in Chinese. 6.2 Dictionary Entries In our demo_data directory, there is a file dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first import the idioms in the file. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;) head(all_idioms) ## [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; tail(all_idioms) ## [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; length(all_idioms) ## [1] 56536 In order to make use of the tidy structure in R, we convert the data into a tibble: idiom &lt;- tibble(string = all_idioms) 6.3 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來X去: idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we may need to extract the X and Y in the schema: idiom_laiqu &lt;-idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% mutate(pattern = str_replace(string, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(pattern, into = c(&quot;w1&quot;, &quot;w2&quot;), sep = &quot;_&quot;) idiom_laiqu One empirical question is how many of these idioms are of the pattern X=Y (e.g., 想來想去, 直來直去) and how many are of X!=Y (e.g., 說來道去, 朝來暮去): idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() 6.4 Exercises Exercise 6.1 Please use idiom and extract the idioms with the schema of 一X一Y. Exercise 6.2 Also with the idiom as our data source, now if we are interested in all idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Also, provide the distribution of the two types. Exercise 6.3 Following Exercise 6.2, for each type of the idioms, please provide their respective proportions of X=Y vs. X!=Y. Exercise 6.4 Folloing Exercise 6.3, please identify the character that is duplicated in the idioms. One follow-up analysis would be to look at the distribution of these pivotal characters. Can you reproduce a graph as shown below as closely as possible? "],
["chinese-text-processing.html", "Chapter 7 Chinese Text Processing 7.1 Chinese Word Segmenter jiebaR 7.2 Case Study 1: Word Frequency and Wordcloud 7.3 Case Study 2: Patterns 7.4 Case Study 3: Lexical Bundles", " Chapter 7 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss tokenization in @ref{tokenization}, it is easy to do the word tokenization in English as the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. This chapter is devoted to Chinese text processing. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 7.1 Chinese Word Segmenter jiebaR First, you haven’t installed the library jiebaR, please install: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) Now let us take a look at a quick example. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment a text, you first initialize a segmenter seg1 using worker() and use this segmenter to segment() texts. There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (default is FALSE) bylines = FALSE: Whether to return each word one line at a time From the above examples, it is clear to see that some of the words are not correctly identified by the current segmenter: 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. seg3 &lt;- worker(stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣民眾&quot; &quot;黨&quot; ## [25] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; &quot;哲&quot; &quot;7&quot; ## [31] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; ## [37] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; So far we did not see the parts-of-speech tag provided by the word segmenter. If you need the tags of the words, you need to specify this need when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; The following table lists the annotations of the POS tagsets used in jiebaR: You can check the dictionaries being used in your current enviroment: dir(show_dictpath()) ## [1] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/jiebaRD/dict&quot; ## [1] &quot;backup.rda&quot; &quot;hmm_model.utf8&quot; &quot;hmm_model.zip&quot; &quot;idf.utf8&quot; ## [5] &quot;idf.zip&quot; &quot;jieba.dict.utf8&quot; &quot;jieba.dict.zip&quot; &quot;model.rda&quot; ## [9] &quot;README.md&quot; &quot;stop_words.utf8&quot; &quot;user.dict.utf8&quot; scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, what=character(),nlines=50,sep=&#39;\\n&#39;, encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) ## [1] &quot;\\&quot;&quot; &quot;.&quot; &quot;。&quot; &quot;,&quot; &quot;、&quot; &quot;！&quot; &quot;？&quot; &quot;：&quot; &quot;；&quot; &quot;`&quot; &quot;﹑&quot; &quot;•&quot; ## [13] &quot;＂&quot; &quot;^&quot; &quot;…&quot; &quot;‘&quot; &quot;’&quot; &quot;“&quot; &quot;”&quot; &quot;〝&quot; &quot;〞&quot; &quot;~&quot; &quot;\\\\&quot; &quot;∕&quot; ## [25] &quot;|&quot; &quot;¦&quot; &quot;‖&quot; &quot;— &quot; &quot;(&quot; &quot;)&quot; &quot;〈&quot; &quot;〉&quot; &quot;﹞&quot; &quot;﹝&quot; &quot;「&quot; &quot;」&quot; ## [37] &quot;‹&quot; &quot;›&quot; &quot;〖&quot; &quot;〗&quot; &quot;】&quot; &quot;【&quot; &quot;»&quot; &quot;«&quot; &quot;』&quot; &quot;『&quot; &quot;〕&quot; &quot;〔&quot; ## [49] &quot;》&quot; &quot;《&quot; When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a list of text vectors as input and return a list of word vectors as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; rm(seg1, seg2, seg3, seg4) 7.2 Case Study 1: Word Frequency and Wordcloud # loading the corpus # NB: this may take some time apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% as_tibble() %&gt;% filter(text !=&quot;&quot;) %&gt;% mutate(doc_id = row_number()) apple_df # tokenization segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T) apple_word &lt;- apple_df %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = segmenter)) apple_word library(showtext) # font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) # `wordcloud` version # require(wordcloud) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 100) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 7.3 Case Study 2: Patterns # define a function # to concatenate the jieba pos results into: w1_tag1 w2_tag2 w3_tag3 ... sequence my_segmenter &lt;- function(txt, seg){ txt_tag &lt;- segment(txt, seg) str_c(txt_tag, names(txt_tag), collapse=&quot; &quot;, sep=&quot;_&quot;) } tagger &lt;- worker(type=&quot;tag&quot;, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = FALSE) my_segmenter(text, tagger) ## [1] &quot;綠黨_n 桃園市_x 議員_n 王浩宇_x 爆料_n ，_x 指民眾_x 黨_n 不_d 分區_n 被_p 提名_v 人_n 蔡壁如_x 、_x 黃_zg 瀞_x 瑩_zg ，_x 在昨_x （_x 6_x ）_x 日_m 才_d 請辭_v 是_v 為領_x 年終獎金_n 。_x 台灣_x 民眾_x 黨_n 主席_n 、_x 台北_x 市長_x 柯文_nz 哲_n 7_x 日_m 受訪_v 時則_x 說_zg ，_x 都_d 是_v 按_p 流程_n 走_v ，_x 不要_df 把_p 人家_n 想得_x 這麼_x 壞_a 。_x&quot; my_segmenter(txt = &quot;我是在測試一個句子&quot;, seg = tagger) ## [1] &quot;我_r 是_v 在_p 測試_vn 一個_x 句子_n&quot; # IPU tokenization apple_ipu &lt;-apple_df %&gt;% unnest_tokens(IPU, text, token = function(x) str_split(x, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;))%&gt;% #IPU tokenization filter(IPU!=&quot;&quot;) %&gt;% # remove empty IPU mutate(IPU_tag = map_chr(IPU, function(x) my_segmenter(txt=x, seg = tagger))) # tag each IPU apple_ipu For more information related to the unicode ranage for the punctuations in CJK languages, please see this SO discussion thread. After we segment our corpus into inter-punctuation units (IPU), we can make use of the words as well as their parts-of-speech tags to extract the target pattern we are interested: 被 + ... constructions. # Extract BEI + WORD apple_bei &lt;-apple_ipu %&gt;% filter(str_detect(IPU_tag, pattern = &quot;\\\\b被_p\\\\b&quot;)) %&gt;% mutate(PATTERN = str_extract(IPU_tag, pattern = &quot;\\\\b被_p\\\\s([^_]+_[^\\\\s]+\\\\s)*?[^_]+_v&quot;)) %&gt;% filter(PATTERN !=&quot;&quot;) %&gt;% mutate(VERB = str_extract(PATTERN, pattern = &quot;\\\\s[^_]+_v&quot;) %&gt;% str_replace_all(&quot;_v&quot;,&quot;&quot;)) # Calculate WORD frequency require(wordcloud2) apple_bei %&gt;% count(VERB) %&gt;% arrange(desc(n)) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.6) # Statistical Assoication Exercise 7.1 Please use the apple_ipu as your corpus and extract Chinese particle constructions of ... 外/內/中. Usually a particle construction like this consists of a landmark NP and the space particle. For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively believe that the word directly preceding the space particle is our landmark NP head noun. Please extract all concordance lines with these space particles and at the same time identify their SPC and LM, as shown below. Exercise 7.2 Following Exercise 7.1, please generate a frequency list of the LMs for each spac particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Exercise 7.3 Following Exercise 7.2, for each space particle, please create a word cloud of its co-occuring LMs based on the top 200 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. 7.4 Case Study 3: Lexical Bundles With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at recurrent four-grams. As we discussed in Chapter @ref(#tokenization), a multiword unit can be defined based on at least two metrics: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) As the default tokenization in unnest_tokens() only works with the English data, we start this task by defining our own ngram_chi() for Chinese n-grams. # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes a text (scalar) as an input, and return a vector of n-grams. Most importantly, this function assumes that in the text string, each word token is delimited by a whitespace. s &lt;- &quot;這 是 一個 測試 的 句子 。&quot; ngram_chi(text = s, n = 2, delimiter = &quot;_&quot;) ## [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; &quot;句子_。&quot; ngram_chi(text = s, n = 4, delimiter = &quot;_&quot;) ## [1] &quot;這_是_一個_測試&quot; &quot;是_一個_測試_的&quot; &quot;一個_測試_的_句子&quot; ## [4] &quot;測試_的_句子_。&quot; ngram_chi(text = s, n = 5, delimiter = &quot; &quot;) ## [1] &quot;這 是 一個 測試 的&quot; &quot;是 一個 測試 的 句子&quot; &quot;一個 測試 的 句子 。&quot; We vectorize the function ngram_chi(). # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) #system.time(parallel::pvec(x&lt;-apple_ipu$IPU_tag,function(x) vngram_chi(x, n=4))) # 47s Vectorized functions are a very useful feature of R, but programmers who are used to other languages often have trouble with this concept at first. A vectorized function works not just on a single value, but on a whole vector of values at the same time. In our first defined ngram_chi function, it takes one text vector as an input and process it one at a time. However, we would expect ngram_chi to process a vector of texts (i.e., multiple texts) at the same time and return a list of resulting ngrams vectors at the same time. Therefore, we use Vectorize() as a wrapper to vectorize our function and specifically tell R that the argument text is vectorized, i.e., process each value in the text vector in the same way. Now we can tokenize our corpus into n-grams using our own tokenization function vngram_chi() and the unnest_tokens(). In this case study, we demonstrate the analysis of four-grams in our Apple News corpus. We begin by first creating a sentence ID for each IPU of the article. Then we remove all POS tags, and these cleaned versions of IPU are sent to vngram_chi() to extract four-grams. apple_ngram &lt;-apple_ipu %&gt;% group_by(doc_id) %&gt;% mutate(sentID = row_number()) %&gt;% ungroup %&gt;% mutate(IPU_word = str_replace_all(IPU_tag, &quot;_[^ ]+&quot;,&quot;&quot;)) %&gt;% unnest_tokens(ngram, IPU_word, token = function(x) vngram_chi(text = x, n= 4)) %&gt;% filter(ngram!=&quot;&quot;) apple_ngram Now we have all the four-grams in each article, we can create the frequency list of four-grams. In particular, we compute the frequency of the four-grams as well as their dispersion. apple_ngram_dist &lt;- apple_ngram %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) "],
["structured-corpus.html", "Chapter 8 Structured Corpus 8.1 NCCU Spoken Mandarin 8.2 Connecting SPID to Metadata 8.3 More Socialinguistic Analyses", " Chapter 8 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for lingustic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. 8.1 NCCU Spoken Mandarin CHILDES format 8.1.1 Loading the Corpus NCCU &lt;- readtext(&quot;demo_data/NCCU_SPOKEN.tar.gz&quot;) %&gt;% as_tibble 8.1.2 Line Segmentation NCCU_lines &lt;- NCCU %&gt;% unnest_tokens(line, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) 8.1.3 Metadata vs. Transcript NCCU_lines_meta &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^@&quot;)) NCCU_lines_data &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^[^@]&quot;)) %&gt;% group_by(doc_id) %&gt;% mutate(lineID = row_number()) %&gt;% ungroup %&gt;% separate(line, into = c(&quot;SPID&quot;,&quot;line&quot;), sep=&quot;\\t&quot;) %&gt;% mutate(line2 = line %&gt;% str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) NCCU_lines_data 8.1.4 Word Tokenization NCCU_words &lt;- NCCU_lines_data %&gt;% unnest_tokens(word, line2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words 8.1.5 Word frequencies and Wordcloud NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% select(word, freq) %&gt;% #mutate(freq = log(freq)) %&gt;% wordcloud2::wordcloud2(minSize = 0.5, size=1, shape=&quot;diamonds&quot;) 8.1.6 Concordances # extracting particular patterns NCCU_lines_data %&gt;% filter(str_detect(line2, &quot;覺得&quot;)) 8.1.7 N-grams (Lexical Bundles) ########################## # Chinse ngrams functin # ########################## # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) NCCU_ngrams &lt;- NCCU_lines_data %&gt;% select(-line, -SPID) %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 5, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq 8.2 Connecting SPID to Metadata NCCU_lines_meta NCCU_lines_data # Self-defined function fill_spid &lt;- function(vec){ vec_filled &lt;-vec for(i in 1:length(vec_filled)){ if(vec_filled[i]==&quot;&quot;){ vec_filled[i]&lt;-vec_filled[i-1] }else{ i &lt;- i+1 } #endif }#endfor return(vec_filled) }#endfunc # Please check M005.cha NCCU_lines_data %&gt;% group_by(doc_id) %&gt;% filter(lineID == 1 &amp; SPID==&quot;&quot;) # Remove the typo case NCCU_lines_data_filled &lt;- NCCU_lines_data %&gt;% filter(!(doc_id ==&quot;M005.cha&quot; &amp; lineID==1)) %&gt;% group_by(doc_id) %&gt;% mutate(SPID = str_replace_all(SPID, &quot;[*:]&quot;,&quot;&quot;)) %&gt;% mutate(SPID_FILLED = fill_spid(SPID)) %&gt;% mutate(DOC_SPID = str_c(doc_id, SPID_FILLED, sep=&quot;_&quot;)) %&gt;% ungroup %&gt;% select(doc_id, lineID, line2, DOC_SPID) NCCU_lines_data_filled Based on the metadata of each file hedaer, we can extract demographic information related to each speaker, including their ID, age, gender, etc. NCCU_meta &lt;- NCCU_lines_meta %&gt;% filter(str_detect(line, &quot;^@(id)&quot;)) %&gt;% separate(line, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 8.3 More Socialinguistic Analyses 8.3.1 Check Ngram Distribution By Age Groups NCCU_ngram_with_meta &lt;- NCCU_lines_data_filled %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 3, delimiter = &quot;_&quot;)) %&gt;% filter(ngram!=&quot;&quot;) %&gt;% filter(!(str_detect(ngram,&quot;[&lt;a-z:]&quot;))) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_ngram_by_age &lt;- NCCU_ngram_with_meta %&gt;% count(ngram,AGE_GROUP, DOC_SPID) %&gt;% group_by(ngram, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) Below20 Word Cloud Order ggplot barplots by factor frequencies require(wordcloud2) NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(ngram,freq) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_age_ordered &lt;- NCCU_ngram_by_age %&gt;% group_by(AGE_GROUP) %&gt;% top_n(20, freq) %&gt;% ungroup() %&gt;% arrange(AGE_GROUP, freq) %&gt;% mutate(order=row_number()) # transparency indicates dispersion NCCU_ngram_by_age_ordered %&gt;% ggplot(aes(order, freq, fill = AGE_GROUP, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ AGE_GROUP, scales = &quot;free&quot;) + labs(y = &quot;Ngram Frequency&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_age_ordered$order, labels=NCCU_ngram_by_age_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) 8.3.2 Check Word Distribution of different genders NCCU_ngram_by_gender &lt;- NCCU_ngram_with_meta %&gt;% count(ngram, GENDER, DOC_SPID) %&gt;% group_by(ngram, GENDER) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_gender NCCU_ngram_by_gender %&gt;% #filter(dispersion &gt; 10) %&gt;% count(GENDER) %&gt;% ggplot(aes(x=GENDER, y = n, fill=GENDER))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;male&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;female&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_gender_ordered &lt;- NCCU_ngram_by_gender %&gt;% group_by(GENDER) %&gt;% top_n(20, dispersion) %&gt;% ungroup() %&gt;% arrange(GENDER,dispersion) %&gt;% mutate(order=row_number()) NCCU_ngram_by_gender_ordered %&gt;% ggplot(aes(order, dispersion, fill = GENDER, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ GENDER, scales = &quot;free&quot;) + labs(y = &quot;Ngram Dispersion&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_gender_ordered$order, labels=NCCU_ngram_by_gender_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) "],
["vector-space-representation.html", "Chapter 9 Vector Space Representation", " Chapter 9 Vector Space Representation See Chapter 3 "],
["references.html", "Chapter 10 References", " Chapter 10 References "]
]
