[["structured-corpus.html", "Chapter 10 Structured Corpus 10.1 NCCU Spoken Mandarin 10.2 CHILDES Format 10.3 Loading the Corpus 10.4 From Text-based to Turn-based DF 10.5 Metadata vs. Utterances 10.6 Word-based DF and Frequency List 10.7 Concordances 10.8 Collocations (Bigrams) 10.9 N-grams (Lexical Bundles) 10.10 Connecting SPID to Metadata 10.11 Corpus Headers 10.12 Sociolinguistic Analyses", " Chapter 10 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for linguistic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. To facilitate the sharing of corpus data, the corpus linguistic community has now settled on a few common schemes for textual data storage and exchange. In particular, I would like to talk about two common types of corpus data representation: CHILDES (this chapter) and XML (Chapter 11). 10.1 NCCU Spoken Mandarin In this demonstration, I will use the dataset of Taiwan Mandarin Corpus for illustration. This dataset, collected by Prof. Kawai Chui and Prof. Huei-ling Lai at National Cheng-Chi University (Chui et al., 2017; Chui &amp; Lai, 2008), includes spontaneous face-to-face conversations of Taiwan Mandarin. The data transcription conventions can be found on the NCCU Corpus Official Website. Generally, the NCCU corpus transcripts follow the conventions of CHILDES format. In computational text analytics, the first step is always to analyze the structure of the textual data. 10.2 CHILDES Format The following is an excerpt from the file demo_data/data-nccu-M001.cha from the NCCU Corpus of Taiwan Mandarin. The conventions of CHILDES transcription include: The lines with header information begin with @ The lines with utterances begin with * The indented lines refer to the utterances of the continuing speaker turn Words are separated by spaces (i.e., a word-segmented corpus) The meanings of transcription symbols used in the corpus can be found in the documention of the corpus. 10.3 Loading the Corpus The corpus data is available in our demo_data/corp-NCCU-SPOKEN.tar.gz, which is a zipped archived file, i.e., one zipped tar file including all the corpus documents. We can use the readtext::readtext() to load the data. In this step, we treat all the *.cha files as if they are normal text files (i.e. .txt) and load the entire corpus into a data frame with two columns: doc_id and text (The warning messages only warn you that by default readtext() takes only .txt files). NCCU &lt;- readtext(&quot;demo_data/corp-NCCU-SPOKEN.tar.gz&quot;) %&gt;% as_tibble 10.4 From Text-based to Turn-based DF Now the data frame NCCU is a text-based one, where each row refers to one transcript file in the corpus. Before we do further tokenization, we first need to concatenate all same-turn utterances (i.e., utterances with no speaker ID at the initial of the line) with their initial utterance of the speaker turn, and then we use unnest_tokens() to transform the text-based DF into a turn-based DF. NCCU_turns &lt;- NCCU %&gt;% mutate(text = str_replace_all(text,&quot;\\n\\t&quot;,&quot; &quot;)) %&gt;% # deal with same-speaker-turn utterances unnest_tokens(turn, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) # inspect first file NCCU_turns %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.5 Metadata vs. Utterances Lines starting with @ are the headers of the transcript while lines starting with * are the utterances of the conversation. We split our NCCU_turns into: NCCU_turns_meta: a DF with all header lines NCCU_turns_utterance: a DF with all utterance lines # Metadata NCCU_turns_meta &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^@&quot;)) # extract all lines starting with `@` # Utterance NCCU_turns_utterance &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^\\\\*&quot;)) %&gt;% # extract all lines starting with `*` group_by(doc_id) %&gt;% mutate(turn_id = row_number()) %&gt;% ungroup %&gt;% tidyr::separate(col=&quot;turn&quot;, # extract SPID into = c(&quot;SPID&quot;, &quot;turn&quot;), sep = &quot;:\\t&quot;) %&gt;% mutate(turn2 = turn %&gt;% # clean up utterances str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) When extracting all the utterances of the speaker turns, we perform data preprocessing as well. Specifically, we: Replace all pause tags with &lt;PAUSE&gt; Replace all extralinguistic tags with &lt;EXTRACLING&gt; Remove all overlapping talk tags Remove all code-switching tags Remove duplicate/trailing/leading spaces The turn-based DF, NCCU_turns_utterance, includes the utterance of each speark turn as well as the doc_id, turn_id and the SPID. All this metadata information can help us connect each utterance back to the original conversation. # Turns of `M001.cha` NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.6 Word-based DF and Frequency List Because the NCCU corpus has been word segmented, we can easily transform the turn-based DF into a word-based DF using unnest_tokens(). The key is that we specify our own tokenization function token =.... The tokenization method is simple: tokenize the utterance into words based on the delimiter of whitespaces. NCCU_words &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(word, turn2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words %&gt;% head(100) With the word-based DF, we can create a word frequency list of the NCCU corpus. NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) NCCU_words_freq %&gt;% head(100) With word frequencies, we can generate a word cloud to have a quick overview of the word distributions in NCCU corpus. # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% # remove annotations/tags select(word, freq) %&gt;% top_n(150, freq) %&gt;% mutate(freq = log(freq)) %&gt;% # deal with Zipfian distribution wordcloud2::wordcloud2(size=0.2, shape=&quot;diamonds&quot;) 10.7 Concordances If we need to identify turns with a particular linguistic unit, we can make use of the data wrangling tricks to easily extract speaker turns with the target pattern. You can of course make use of regular expressions to extract more complex constructions and patterns from the utterances. # extracting particular patterns NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;覺得&quot;)) NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;這樣子&quot;)) Exercise 10.1 If we are interested in the use of the verb 覺得. After we extract all the speaker turns with the verb 覺得, we may need to know the subjects that often go with the verb. Please identify the word before the verb for each concordance token as one independent column of the resulting data frame (see below). Please note that one speaker turn may have more than one use of 覺得. Please create a barplot as shown below to summarize the distribution of the top 10 frequent words that directly precedes 覺得. Among the top 10 words, you would see “的 覺得” combinations, which are counter-intuitive. Please examine these tokens and explain why. Alternatively, we can also create a tokens object and apply the kwic() from quanteda for concordance lines: # `tokens` object NCCU_tokens&lt;- NCCU_turns_utterance$turn2 %&gt;% str_split(&quot;\\\\s+&quot;) %&gt;% map(function(x) x[nzchar(x)]) %&gt;% set_names(str_c(NCCU_turns_utterance$doc_id, NCCU_turns_utterance$turn_id, sep=&quot;-&quot;)) %&gt;% as.tokens # check token numbers sum(sapply(NCCU_tokens, length)) # based on `tokens` [1] 194670 nrow(NCCU_words) # based on `unnest_tokens` [1] 194670 # We can add docvars to `tokens` docvars(NCCU_tokens)&lt;- NCCU_turns_utterance[,c(&quot;doc_id&quot;,&quot;SPID&quot;, &quot;turn_id&quot;)] %&gt;% mutate(Text = doc_id) # KWIC kwic(NCCU_tokens, &quot;覺得&quot;, 8) kwic(NCCU_tokens, &quot;機車&quot;, 8) 10.8 Collocations (Bigrams) Now we extend our analysis beyond single words. Please recall the ngram_chi() function we have defined and used several times in previous chapters. # functions from ch Chinese Text Processing ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s|\\u3000&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc We use the self-defined tokenization function together with unnest_tokens() to transform the turn-based DF into a bigram-based DF. system.time( NCCU_bigrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(bigrams, turn2, token = function(x) map(x, ngram_chi, n = 2)) %&gt;% filter(bigrams!=&quot;&quot;) ) user system elapsed 2.455 0.052 3.051 NCCU_bigrams %&gt;% filter(doc_id == &quot;M001.cha&quot;) Please note that when we perform the n-gram tokenization, we take each speaker turn as our input. This step is important because this would make sure that we don’t get bigrams that span different speaker turns. To determine significant collocations in conversation, we can compute the relevant distributional statistics for each bigram type, including: frequencies dispersion collocation strength (lexical associations) We first compute the frequencies and dispersions of bigrams. NCCU_bigrams_freq &lt;- NCCU_bigrams %&gt;% count(bigrams, doc_id) %&gt;% group_by(bigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% top_n(100, freq) Exercise 10.2 In the above example, we compute the dispersion based on the number of documents where the bigram occurs. Please note that the dispersion can be defined on the basis of the speakers as well, i.e., the number of speakers who use the bigram at least once in the corpus. How do we get dispersion statistics like this? Please show the top frequent 100 bigrams and their SPID-based dispersion statistics. To compute the lexical associations, we need to: remove bigrams with para-linguistic tags exclude bigrams of low dispersion get necessary observed frequencies (e.g., w1 and w2 frequencies) get expected frequencies (for more advanced lexical association metrics) NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% # exclude bigrams with para tags filter(dispersion &gt;= 5) %&gt;% # set bigram dispersion cut-off rename(O11 = freq) %&gt;% tidyr::separate(col=&quot;bigrams&quot;, c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;_&quot;) %&gt;% # split bigrams into two columns mutate(R1 = NCCU_words_freq$freq[match(w1, NCCU_words_freq$word)], C1 = NCCU_words_freq$freq[match(w2, NCCU_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), # compute associations t = (O11 - E11)/sqrt(E11)) %&gt;% mutate_if(is.double, round,2) -&gt; NCCU_collocations NCCU_collocations %&gt;% arrange(desc(dispersion), desc(MI)) # sorting by MI NCCU_collocations %&gt;% arrange(desc(dispersion), desc(t)) # sorting by t Exercise 10.3 Please compute the lexical associations of the bigrams using the log-likelihood ratios. 10.9 N-grams (Lexical Bundles) We can also extend our analysis to n-grams of larger sizes, i.e., the lexical bundles. system.time( NCCU_ngrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(ngram, turn2, token = function(x) map(x, ngram_chi, n = 4, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) ) user system elapsed 2.339 0.032 2.945 NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion), desc(freq)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq %&gt;% filter(dispersion &gt;= 5) 10.10 Connecting SPID to Metadata So far the previous analyses have not used any information of the headers. In other words, the connection between the utterances and their corresponding speakers’ profiles are not transparent in our current corpus analysis. However, for socio-linguists, the headers of the transcripts can be very informative. For example, in the NCCU_turns_meta, we have more demographic information of the speakers, which allows us to further examine the linguistic variations on various social factors (e.g., areas, ages, gender etc.) NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) NCCU_turns_meta %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.11 Corpus Headers In this section, I would like to demonstrate how to extract speaker-related information from the headers (i.e., NCCU_turns_meta) and link these speaker profiles to our corpus data (i.e., NCCU_turns_utterance). Based on the metadata of each file header, we can extract demographic information related to each speaker, including their ID, age, gender, etc. In the headers of each transcript, the demographic profiles of each speaker are provided in the lines starting with @id:\\t; and each piece of information is separated by a pipe sign | in the line. All speakers’ profiles in the corpus follow the same structure. NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) To parse the data of the speaker profiles in the turn column of NCCU_turns_meta: we extract all lines starting with @id separate the column into several columns using | select relevant columns (speaker profiles) rename the columns create unique IDs for each speaker of each transcript NCCU_meta &lt;- NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) %&gt;% separate(col=&quot;turn&quot;, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 10.12 Sociolinguistic Analyses Now with NCCU_meta and NCCU_turns_utterance, we can now connect each utterance to a particular speaker (via SPID in NCCU_turns_utterance and DOC_SPID in NCCU_meta) and therefore study the linguistic variation across speakers of varying sub-groups/communities. The steps are as follows: We first extract the patterns we are interested in from NCCU_turns_utterance; We then connect the concordance tokens to their corresponding SPID profiles in NCCU_meta; We analyze how the patterns vary according to speakers of different profiles. NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.12.1 Check Bigrams Distribution By Age Groups For example, we can look at bigrams used by speakers of varying age groups. The analysis requires the following steps: we retrieve target bigrams from NCCU_bigrams we generate DOC_SPID for all bigrams tokens extracted we map the DOC_SPID to NCCU_meta to get the speaker profiles of each bigram token we recode the speaker’s age into a three-level factor for more comprehensive analysis (i.e., AGE_GROUP) for each age group, we compute the bigram frequencies and dispersion NCCU_bigrams_with_meta &lt;- NCCU_bigrams %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% mutate(DOC_SPID = str_c(doc_id, str_replace_all(SPID,&quot;\\\\*&quot;,&quot;&quot;), sep=&quot;_&quot;)) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_bigrams_by_age &lt;- NCCU_bigrams_with_meta %&gt;% count(bigrams,AGE_GROUP, DOC_SPID) %&gt;% group_by(bigrams, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt;= 5) %&gt;% ungroup NCCU_bigrams_by_age 10.12.2 Numbers of Bigrams above Cut-off by Age We can examine the type frequencies of the bigrams that are used by at least five different speakers for each age group. NCCU_bigrams_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Type Frequency \\nof Bigrams of Dispersion &gt;= 5 (Speaker)&quot;) We can also analyze the token frequencies of the bigrams above the cut-off dispersion value. NCCU_bigrams_by_age %&gt;% group_by(AGE_GROUP) %&gt;% summarize(freq = sum(freq)) %&gt;% ggplot(aes(x=AGE_GROUP, y = freq, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Token Frequencies \\nof Bigrams of Dispersion &gt;= 5 (Speaker)&quot;) 10.12.3 Bigram Word clouds by Age require(wordcloud2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(bigrams, freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2(size = 0.6, shape=&quot;diamond&quot;, rotateRatio = 0.2, minRotation = -pi/2, maxRotation = -pi/2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(bigrams, freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2(size = 0.6, shape=&quot;diamond&quot;, rotateRatio = 0.2, minRotation = -pi/2, maxRotation = -pi/2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(bigrams,freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2(size = 0.6, shape=&quot;diamond&quot;, rotateRatio = 0.2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 10.4 Please create a barplot, showing the top 20 bigrams ranked according to the bigram frequencies for each age group. Also, in the bar graph please include the information of dispersion for each bigram, using the transparency of the bars. The more transparent, the less dispersed (See below). Exercise 10.5 Please create a barplot, showing the top 20 bigrams ranked according to the bigram token frequencies for each male and female speakers. Also, in the graph please include the information of dispersion for each bigram, using the transparency of the bars. The more transparent, the less dispersed (See below). Exercise 10.6 Please analyze the bigram variations in terms of speaker relations and present your findings/observations in your own way. In NCCU Corpus, speaker relations include 14 different relations. You may consider collapsing these levels into larger categories in order to identify the general tendencies. Observations may differ depending on how you analyze your data:) References Chui, Kawai, &amp; Lai, H. (2008). The NCCU corpus of spoken Chinese: Mandarin, Hakka, and Southern Min. Taiwan Journal of Linguistics, 6(2). Chui, Kawai, Lai, Huei-ling, &amp; Chen, H.-C. (2017). The Taiwan Spoken Chinese Corpus [Book Section]. In R. Sybesma (Ed.), Encyclopedia of chinese language and linguistic (pp. 257–259). Brill. "]]
