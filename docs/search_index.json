[
["chinese-text-processing.html", "Chapter 8 Chinese Text Processing 8.1 Chinese Word Segmenter jiebaR 8.2 Chinese Text Analytics Pipeline 8.3 Comparing Tokenization Methods 8.4 Data 8.5 Loading Text Data 8.6 quanteda::tokens() vs. jiebaR::segment() 8.7 Case Study 1: Word Frequency and Wordcloud 8.8 Case Study 2: Patterns 8.9 Case Study 3: Lexical Bundles 8.10 Afterwords", " Chapter 8 Chinese Text Processing In this chapter, we will turn to the topic of Chinese text processing. In particular, we will discuss one of the most important issues in Chinese language processing, i.e., word segmentation. When we discuss English parts-of-speech tagging in Chapter 5, it is easy to do word tokenization in English because the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. In later Chapter 10, we will introduce another segmenter developed by the CKIP Group at the Academia Sinica. The CKIP Tagger seems to be the state-of-art tagger for Taiwan Mandarin, i.e., with more additional functionalities. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 8.1 Chinese Word Segmenter jiebaR 8.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) ## [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: Initilzie a jiebar object using worker() Tokenize the texts into words using the function segment() with the designated jiebar object created earlier seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(seg1) ## [1] &quot;jiebar&quot; &quot;segment&quot; &quot;jieba&quot; To word-tokenize the document, text, you first initialize a jiebar object, i.e., seg1, using worker() and feed this jiebar to segment(jiebar = seg1)and tokenize text into words. 8.1.2 Parameters Setting There are many different parameters you can specify when you initialize the jiebar object. You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) Exercise 8.1 In our earlier example, when we created the jiebar object named seg1, we did not specify any arguments for worker(). Can you tell what the default settings are for the parameters of worker()? Please try to create worker() with different settings (e.g., symbols = T, bylines = T) and see how the tokenization results differ from each other. 8.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when tokenizing your texts because different corpora may have their own unique vocabulary (i.e., domain-specific lexicon). This can be done with the argument user = ... when you initialize the jiebar object, i.e, worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a Chinese txt file created by Notepad may not be UTF-8. (Usually, it is encoded in big-5.) Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 8.1.4 Stopwords When you initialize the jiebar, you can also specify a stopword list, i.e., words that you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative, thus often excluded in the process of preprocessing. seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; ## [19] &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; ## [25] &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; ## [31] &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; ## [37] &quot;這麼&quot; &quot;壞&quot; Exercise 8.2 How do we quickly check which words in segment(text, seg2) were removed as compared to the results of segment(text, seg3)? (Note: seg2 and seg3 only differ in the stop_word=... argument.) ## [1] &quot;日&quot; &quot;是&quot; &quot;都&quot; 8.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = &quot;tag&quot; when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;, symbol = T) segment(text, seg4) ## n ns n x n x n ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;，&quot; &quot;指&quot; ## x x p v n x x ## &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;、&quot; ## x x x x x x d ## &quot;黃瀞瑩&quot; &quot;，&quot; &quot;在昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; &quot;才&quot; ## v x n x x x n ## &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## x ns n x x v x ## &quot;、&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg x p n v x df ## &quot;說&quot; &quot;，&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; &quot;不要&quot; ## p n x r a x ## &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;。&quot; The returned object is a named character vector, i.e., the POS tags of the words are included in the names of the vectors. Every POS tagger has its own predefined tagset. The following table lists the annotations of the POS tagset used in jiebaR: Exercise 8.3 How do we convert the named word vector with POS tags returned by segment(text, seg4) into a long string as shown below? ## [1] &quot;綠黨/n 桃園市/ns 議員/n 王浩宇/x 爆料/n ，/x 指/n 民眾黨/x 不分區/x 被/p 提名/v 人/n 蔡壁如/x 、/x 黃瀞瑩/x ，/x 在昨/x （/x 6/x ）/x 才/d 請辭/v 為領/x 年終獎金/n 。/x 台灣/x 民眾黨/x 主席/n 、/x 台北/ns 市長/n 柯文哲/x 7/x 受訪/v 時則/x 說/zg ，/x 按/p 流程/n 走/v ，/x 不要/df 把/p 人家/n 想得/x 這麼/r 壞/a 。/x&quot; 8.1.6 Default Word Lists in JiebaR You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: # show files under `dictpath` dir(show_dictpath()) # Check the default stop_words list # Please change the path to your default dict path # scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, # what=character(),nlines=50,sep=&#39;\\n&#39;, # encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) readLines(&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, n = 50) 8.1.7 Reminders When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and returns a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; 8.2 Chinese Text Analytics Pipeline In Chapter 5, we have talked about the pipeline for English texts processing, as shown below: Figure 8.1: English Text Analytics Flowchart For Chinese texts, the pipeline is similar. In the following Chinese Text Analytics Flowchart (Figure 8.2), I have highlighted the steps that are crucial to Chinese processing. It is not recommended to use quanteda::summary() and quanteda::kwic() directly on the Chinese corpus object because the word tokenization in quanteda is not ideal (cf. dashed arrows in Figure 8.2). It is recommended to use self-defined word segmenter for analysis. For processing under tidy structure framework, use own segmenter in unnest_tokens(); for processing under quanteda framework, create the tokens object, which is defined in quanteda as well. Figure 8.2: Chinese Text Analytics Flowchart It is important to note that when we specify a self-defined unnest_tokens(…,token=…) function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument worker(…, byline = TRUE). 8.2.1 Creating a Corpus Object So based on our simple corpus example above, we first transform the character vector text into a corpus object—text_corpus. With this, like with the English data, we can apply quanteda::summary() and quanteda::kwic() with the corpus object. text_corpus &lt;- text %&gt;% corpus summary(text_corpus) kwic(text_corpus, pattern = &quot;柯文哲&quot;) kwic(text_corpus, pattern = &quot;柯&quot;) Exercise 8.4 Do you know why there are no tokens of concordance lines from kwic(text_corpus, pattern = &quot;柯文哲&quot;)? 8.2.2 Tidy Structure Framework We can now transform the corpus object into a text-based TIBBLE using tidy(). Also, we generate an unique index for each row using row_number(). # a text-based tidy corpus text_corpus_tidy &lt;-text_corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) text_corpus_tidy For word segmentation, we initialize the jiebar object using worker(). # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) Finally, we use unnest_tokens() to tokenize the text-based TIBBLE text_corpus_tidy into a word-based TIBBLE text_corpus_tidy_word. That is, texts included in the text column are tokenized into words, which are unnested into rows of the word column in the new TIBBLE. # tokenization text_corpus_tidy_word &lt;- text_corpus_tidy %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) text_corpus_tidy_word In the above example, we specify our own tokenization function, function(x) segment(x, jiebar = my_seg). This is called anonymous functions. It is a real function object, which just happens to have not been assigned to any symbol before being used. You may check R language documentation for more detail on Writing Functions. Generally functions are assigned to symbols but they don’t need to be. The value returned by the call to function is a function. If this is not given a name it is referred to as an anonymous function. Anonymous functions are most frequently used as arguments to other functions such as the apply family or outer. 8.2.3 Quanteda Framework Under the quanteda framework, we can also create the tokens object of the corpus and do kwic() search. Most of the functions that work with corpus object can also work with tokens object in quanteda. text_tokens &lt;- text_corpus_tidy$text %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens kwic(text_tokens, pattern = &quot;柯文哲&quot;) 8.3 Comparing Tokenization Methods quanteda also provides its own default word tokenization for Chinese texts. However, its default tokenization method does not allow us to add our own dictionary to the segmentation process, which renders the results less reliable. We can compare the the two results. we can use quanteda::tokens() to see how quanteda tokenizes Chinese texts. The function returns a tokens object. # create TOKENS object using quanteda default text_corpus %&gt;% tokens -&gt; text_tokens_qtd we can also use our own tokenization function segment() and convert the list to a tokens object using as.tokens(). (This of course will give us the same tokenization result as we get in the earlier unnest_tokens() because we are using the same segmenter my_seg.) # create TOKENS object manually text_corpus %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens -&gt; text_tokens_jb Now let’s compare the two resulting tokens objects: These are the tokens based on self-defined segmenter: # compare our tokenization with quanteda tokenization text_tokens_jb[[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;，&quot; ## [7] &quot;指&quot; &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;、&quot; &quot;黃瀞瑩&quot; &quot;，&quot; &quot;在昨&quot; &quot;（&quot; ## [19] &quot;6&quot; &quot;）&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; ## [25] &quot;為領&quot; &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [31] &quot;、&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; ## [37] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;，&quot; &quot;都&quot; &quot;是&quot; ## [43] &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; &quot;不要&quot; &quot;把&quot; ## [49] &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;。&quot; These are the tokens based on default quanteda tokenizer: text_tokens_qtd[[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; ## [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; ## [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; ## [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; ## [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; ## [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; ## [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; ## [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; ## [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; ## [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; ## [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; ## [67] &quot;。&quot; Therefore, for linguistic analysis, I would suggest to define own Chinese word segmenter using jiebaR, which is tailored to specific tasks/corpora. 8.4 Data In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. (This dataset was collected by Meng-Chen Wu when he was working on his MA thesis project with me years ago. The demo data here was a random sample of the original Apple News Corpus.) 8.5 Loading Text Data When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext. The main function in this package, readtext(), which takes a file or a directory name from the disk or a URL, and returns a type of data.frame that can be used directly with the corpus() constructor function in quanteda, to create a quanteda corpus object. In other words, the output from readtext can be directly passed on to the processing in the tidy structure framework (i.e., tidytext::unnest_tokens()). The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. The corpus constructor command corpus() works directly on: a vector of character objects, for instance that you have already loaded into the workspace using other tools; a data.frame containing a text column and any other document-level metadata the output of readtext::readtext() # loading the corpus # NB: this may take some time apple_corpus &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% corpus summary(apple_corpus, 10) 8.6 quanteda::tokens() vs. jiebaR::segment() In Chapter 4, we’ve seen that after we create a corpus object, we can apply kwic() to get the concordance lines of a particular word. At that time, we emphasized that this worked because quanteda underlyingly tokenized the texts behind the scene. We can do the same the with Chinese texts as well: kwic(apple_corpus, &quot;勝率&quot;) In Section 8.3, I have made it clear that quanteda does have its own tokenization method (i.e., tokens()) for Chinese texts. It uses the tokenizer, stringi::stri_split_boundaries, which utilizes a library called ICU (International Components for Unicode) and the library uses dictionaries for segmentation of texts in Chinese. The biggest problem is that we cannot add our own dictionary when using the default tokenization tokens() (at least I don’t know how). In other words, when we apply kwic() to apple_corpus, quanteda tokenizes the Chinese texts using its default tokenizer (i.e., tokens())and perform the keyword-in-context search. Like we did in Section 8.3, we can compare the word segmentation results between quanteda defaults and jiebaR (with own dictionary) with our current news corpus. First we tokenize all texts in apple_corpus using jiebaR::segment() and the jiebar initilized with user-defined dictionary. Second, we convert the returned list from segment() into a tokens object using as.tokens(). On the other hand, we use quanteda default tokens() to convert the corpus object into tokens object. # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) # Tokenization using jiebaR apple_corpus %&gt;% segment(jiebar = segmenter) %&gt;% as.tokens -&gt; apple_tokens # Tokenization using qunateda::tokens() apple_corpus %&gt;% tokens -&gt; apple_tokens_qtd Now we can compare the two versions of word segmentation. Let’s take a look at the first document: apple_tokens[[1]] %&gt;% length ## [1] 168 apple_tokens_qtd[[1]] %&gt;% length ## [1] 148 apple_tokens[[1]] %&gt;% as.character ## [1] &quot;《&quot; &quot;蘋果&quot; &quot;體育&quot; &quot;》&quot; &quot;即日起&quot; &quot;進行&quot; &quot;虛擬&quot; &quot;賭盤&quot; ## [9] &quot;擂台&quot; &quot;，&quot; &quot;每名&quot; &quot;受邀&quot; &quot;參賽者&quot; &quot;進行&quot; &quot;勝負&quot; &quot;預測&quot; ## [17] &quot;，&quot; &quot;每周&quot; &quot;結算&quot; &quot;在&quot; &quot;周二&quot; &quot;公布&quot; &quot;，&quot; &quot;累積&quot; ## [25] &quot;勝率&quot; &quot;前&quot; &quot;3&quot; &quot;高&quot; &quot;參賽者&quot; &quot;可&quot; &quot;繼續&quot; &quot;參賽&quot; ## [33] &quot;，&quot; &quot;單周&quot; &quot;勝率&quot; &quot;最高者&quot; &quot;，&quot; &quot;將&quot; &quot;加封&quot; &quot;「&quot; ## [41] &quot;蘋果&quot; &quot;波神&quot; &quot;」&quot; &quot;頭銜&quot; &quot;。&quot; &quot;註&quot; &quot;:&quot; &quot;賭盤&quot; ## [49] &quot;賠率&quot; &quot;如有&quot; &quot;變動&quot; &quot;，&quot; &quot;以&quot; &quot;台灣&quot; &quot;運彩&quot; &quot;為主&quot; ## [57] &quot;。&quot; &quot;\\n&quot; &quot;資料&quot; &quot;來源&quot; &quot;：&quot; &quot;NBA&quot; &quot;官網&quot; &quot;http&quot; ## [65] &quot;:&quot; &quot;/&quot; &quot;/&quot; &quot;www&quot; &quot;.&quot; &quot;nba&quot; &quot;.&quot; &quot;com&quot; ## [73] &quot;\\n&quot; &quot;\\n&quot; &quot;金塊&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;103&quot; ## [81] &quot;：&quot; &quot;92&quot; &quot; &quot; &quot;76&quot; &quot;人&quot; &quot;騎士&quot; &quot;(&quot; &quot;主&quot; ## [89] &quot;)&quot; &quot; &quot; &quot;88&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;快艇&quot; &quot;活塞&quot; ## [97] &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;92&quot; &quot;：&quot; &quot;75&quot; &quot; &quot; ## [105] &quot;公牛&quot; &quot;勇士&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;108&quot; &quot;：&quot; ## [113] &quot;82&quot; &quot; &quot; &quot;灰熊&quot; &quot;熱火&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; ## [121] &quot;103&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;灰狼&quot; &quot;籃網&quot; &quot;(&quot; &quot;客&quot; ## [129] &quot;)&quot; &quot; &quot; &quot;90&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;公鹿&quot; &quot;溜&quot; ## [137] &quot;馬&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;111&quot; &quot;：&quot; &quot;100&quot; ## [145] &quot; &quot; &quot;馬刺&quot; &quot;國王&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;112&quot; ## [153] &quot;：&quot; &quot;102&quot; &quot; &quot; &quot;爵士&quot; &quot;小牛&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; ## [161] &quot; &quot; &quot;108&quot; &quot;：&quot; &quot;106&quot; &quot; &quot; &quot;拓荒者&quot; &quot;\\n&quot; &quot;\\n&quot; apple_tokens_qtd[[1]] %&gt;% as.character ## [1] &quot;《&quot; &quot;蘋果&quot; &quot;體育&quot; ## [4] &quot;》&quot; &quot;即日起&quot; &quot;進行&quot; ## [7] &quot;虛擬&quot; &quot;賭&quot; &quot;盤&quot; ## [10] &quot;擂台&quot; &quot;，&quot; &quot;每名&quot; ## [13] &quot;受邀&quot; &quot;參賽者&quot; &quot;進行&quot; ## [16] &quot;勝負&quot; &quot;預測&quot; &quot;，&quot; ## [19] &quot;每周&quot; &quot;結算&quot; &quot;在&quot; ## [22] &quot;周二&quot; &quot;公布&quot; &quot;，&quot; ## [25] &quot;累積&quot; &quot;勝率&quot; &quot;前&quot; ## [28] &quot;3&quot; &quot;高&quot; &quot;參賽者&quot; ## [31] &quot;可&quot; &quot;繼續&quot; &quot;參賽&quot; ## [34] &quot;，&quot; &quot;單&quot; &quot;周&quot; ## [37] &quot;勝率&quot; &quot;最高&quot; &quot;者&quot; ## [40] &quot;，&quot; &quot;將&quot; &quot;加封&quot; ## [43] &quot;「&quot; &quot;蘋果&quot; &quot;波&quot; ## [46] &quot;神&quot; &quot;」&quot; &quot;頭銜&quot; ## [49] &quot;。&quot; &quot;註&quot; &quot;:&quot; ## [52] &quot;賭&quot; &quot;盤&quot; &quot;賠&quot; ## [55] &quot;率&quot; &quot;如有&quot; &quot;變動&quot; ## [58] &quot;，&quot; &quot;以&quot; &quot;台灣&quot; ## [61] &quot;運&quot; &quot;彩&quot; &quot;為主&quot; ## [64] &quot;。&quot; &quot;資料&quot; &quot;來源&quot; ## [67] &quot;：&quot; &quot;NBA&quot; &quot;官&quot; ## [70] &quot;網&quot; &quot;http://www.nba.com&quot; &quot;金塊&quot; ## [73] &quot;(&quot; &quot;客&quot; &quot;)&quot; ## [76] &quot;103&quot; &quot;：&quot; &quot;92&quot; ## [79] &quot;76&quot; &quot;人&quot; &quot;騎士&quot; ## [82] &quot;(&quot; &quot;主&quot; &quot;)&quot; ## [85] &quot;88&quot; &quot;：&quot; &quot;82&quot; ## [88] &quot;快艇&quot; &quot;活塞&quot; &quot;(&quot; ## [91] &quot;客&quot; &quot;)&quot; &quot;92&quot; ## [94] &quot;：&quot; &quot;75&quot; &quot;公牛&quot; ## [97] &quot;勇士&quot; &quot;(&quot; &quot;客&quot; ## [100] &quot;)&quot; &quot;108&quot; &quot;：&quot; ## [103] &quot;82&quot; &quot;灰&quot; &quot;熊&quot; ## [106] &quot;熱火&quot; &quot;(&quot; &quot;客&quot; ## [109] &quot;)&quot; &quot;103&quot; &quot;：&quot; ## [112] &quot;82&quot; &quot;灰&quot; &quot;狼&quot; ## [115] &quot;籃網&quot; &quot;(&quot; &quot;客&quot; ## [118] &quot;)&quot; &quot;90&quot; &quot;：&quot; ## [121] &quot;82&quot; &quot;公鹿&quot; &quot;溜&quot; ## [124] &quot;馬&quot; &quot;(&quot; &quot;客&quot; ## [127] &quot;)&quot; &quot;111&quot; &quot;：&quot; ## [130] &quot;100&quot; &quot;馬&quot; &quot;刺&quot; ## [133] &quot;國王&quot; &quot;(&quot; &quot;客&quot; ## [136] &quot;)&quot; &quot;112&quot; &quot;：&quot; ## [139] &quot;102&quot; &quot;爵士&quot; &quot;小牛&quot; ## [142] &quot;(&quot; &quot;客&quot; &quot;)&quot; ## [145] &quot;108&quot; &quot;：&quot; &quot;106&quot; ## [148] &quot;拓荒者&quot; kwic(apple_tokens, &quot;勝率&quot;) kwic(apple_tokens_qtd, &quot;勝率&quot;) Any significant differences in the word tokenization? To work with the Chinese texts, if you need to utilize more advanced text-analytic functions provided by quanteda, please perform the word tokenization on the texts using your own word segmenter first and convert the object into a tokens, which can then be properly passed on to other functions in quanteda (e.g., dfm). (In other words, for Chinese text analytics, probably corpus object is less practical; rather, creating a tokens object of your corpus might be more useful.) In the later demonstrations, we will use our own defined segmenter for word segmentation/tokenization. 8.7 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart 8.2 and deal with the Chinese texts using the tidy structure framework: Load the corpus data using readtext() and convert it into an corpus object Create a text-based tidy structure DF apple_corpus_tidy (i.e., a tibble) Intialize a word segmenter using worker() Tokenize the text-based data frame into a word-based tidy data frame using unnest_tokens() # loading corpus apple_df &lt;- apple_corpus %&gt;% tidy %&gt;% filter(text !=&quot;&quot;) %&gt;% #remove empty documents mutate(doc_id = row_number()) # create doccument index # Initialize the `jiebar` segmenter_word &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) # Tokenization: Word-based DF apple_word &lt;- apple_df %&gt;% unnest_tokens(output = word, input= text, token = function(x) segment(x, jiebar = segmenter_word)) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup apple_word %&gt;% head(100) These tokenization results should be the same as our earlier apple_tokens: apple_word %&gt;% filter(doc_id == 1) %&gt;% mutate(word_quanteda_tokens = apple_tokens[[1]]) Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to trace back to the original context where the word, phrase or sentence comes from. With all these unique indices, we can easily keep track of the sources of all tokenized linguistic units. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) With a word-based tidy DF, we can easily generate a word frequency list as well as a wordcloud to have a quick overview of the word distribution in the corpus. stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch.txt&quot;) apple_word_freq &lt;- apple_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% # remove words consisting of digits count(word) %&gt;% arrange(desc(n)) library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 400) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) # clear up memory rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 8.8 Case Study 2: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. Our steps are as follows: Initilize jiebar object Define own function to word-seg and pos-tag each text and combine all tokens, word/tag, into a long string for each text With the text-based apple_df, create a new column, which includes the tokenized version of each text, using mutate() # initilize `jiebar` segmenter_word_pos &lt;- worker(type = &quot;tag&quot;, # get pos user = &quot;demo_data/dict-ch-user.txt&quot;, # use own dict symbol = T, # keep symbols bylines = FALSE) # for `mutate()` use # define own function tag_text &lt;- function(x, jiebar){ segment(x, jiebar) %&gt;% paste(names(.), sep=&quot;/&quot;, collapse=&quot; &quot;) } # apply tagger function to each text system.time(apple_df %&gt;% mutate(text_tag = map_chr(text, tag_text, segmenter_word_pos)) -&gt; apple_df_2) ## user system elapsed ## 7.136 0.120 7.300 8.8.1 BEI Construction This section will show you how we can make use of the POS tags for construction analysis. I would like to illustrate their usefulness with a case study: 被 + ... Construction. The data retrieval process is now very straighforward: we only need to create a regular expression that matches our construction and go through the word-segmented and pos-tagged texts to identify these matches. In the following example, we: define a regular expression \\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v for BEI-Construction, i.e., 被 + VERB use unnest_tokens() and str_extract_all() to extract target patterns # define regex patterns pattern_bei &lt;- &quot;\\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v&quot; # extract patterns from corp apple_df_2 %&gt;% select(-text) %&gt;% # `text` is the column with original raw texts unnest_tokens(output = pat_bei, input = text_tag, token = function(x) str_extract_all(x, pattern=pattern_bei)) -&gt; result_bei result_bei Please check Chapter 5 Parts of Speech Tagging on evaluating the quality of the data retrieved by a regular expression (i.e., precision and recall). To have a more in-depth analysis of BEI construction, we like to automatically extract the verb used in the BEI construction. # Extract BEI + WORD result_bei &lt;- result_bei %&gt;% mutate(VERB = str_replace(pat_bei,&quot;.+\\\\s([^/]+)/v$&quot;,&quot;\\\\1&quot;)) result_bei # Calculate WORD frequency require(wordcloud2) result_bei %&gt;% count(VERB) %&gt;% mutate(n = log(n)) %&gt;% top_n(100, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.3) Exercise 8.5 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native-speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 8.6 To more properly evaluate the quality of the pattern queries, it would be great if we still have the original texts available in the resulting data frame result_bei. How do we keep this information? That is, please have one column in result_bei, which shows the original texts from which the construction token is extracted. Exercise 8.7 Please use the sample corpus, apple_df as your data source and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. Please (a) extract all construction tokens with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 8.8 Following Exercise 8.7, please generate a frequency list of the LMs for each space particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Also, you may visualize the top 10 landmarks that co-occur with each space particle in a bar plot as shown below. Exercise 8.9 Following Exercise 8.8, for each space particle, please create a word cloud of its co-occuring LMs based on the top 100 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 8.10 Based on the word clouds provided in Exercise 8.9, do you find any bizarre cases? Can you tell us why? What would be the problems? Or what did we do wrong in the text preprocessing that may lead to these cases? Please discuss these issues in relation to the steps in our data processing, i.e., word segmentation, POS tagging, and pattern retrievals. 8.9 Case Study 3: Lexical Bundles 8.9.1 N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at the recurrent four-grams in Chinese. As the default n-gram tokenization in unnest_tokens() only works with the English data, we start this task by defining our own tokenization functions. In this section, we define three functions: ngram_chi(): This function takes a word vector and returns a ngram-based vectors tokenizer_ngrams(): This function takes texts vector and returns a list of ngram-based vectors tokenizer_chunks(): This function takes texts vector and returns a list of chunk-based vectors # Generate ngram sequences from a word vector # By default, `word_vec` is assumed to be the word tokens of the text ngram_chi &lt;- function(word_vec, n = 2, delimiter = &quot;_&quot;){ if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes a word vector as an input, and returns a vector of n-grams. In other words, it can take the output of segment() as the input. Before we define the tokenization functions, we first initialize the jiebar object for jiebar. # define `jiebar` for jiebar segmenter_word &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) Now we can define own ngram tokenizer tokenizer_ngrams(): # define own tokenizer for ngrams tokenizer_ngrams &lt;- function(input, jiebar, n, delimiter){ input %&gt;% segment(jiebar) %&gt;% # segment texts into word vectors map(ngram_chi, n, delimiter) # convert word vectors into ngram vectors } # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams(input=texts, jiebar=segmenter_word, n = 2, delimiter = &quot;_&quot;) ## [[1]] ## [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; ## ## [[2]] ## [1] &quot;這_句子&quot; ## ## [[3]] ## [1] &quot;超短_句&quot; ## ## [[4]] ## [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams(input=texts, jiebar=segmenter_word, n = 5, delimiter = &quot;_&quot;) ## [[1]] ## [1] &quot;這是_一個_測試_的_句子&quot; ## ## [[2]] ## [1] &quot;&quot; ## ## [[3]] ## [1] &quot;&quot; ## ## [[4]] ## [1] &quot;最後_一個_超長_的_句子&quot; &quot;一個_超長_的_句子_測試&quot; And then we define own chunk tokenizer tokenizer_chunks(): # define chunk tokenization function tokenizer_chunks &lt;- function(input){ str_split(input, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;) } texts &lt;- apple_df$text[1:2] tokenizer_chunks(input = texts) ## [[1]] ## [1] &quot;&quot; &quot;蘋果體育&quot; ## [3] &quot;即日起進行虛擬賭盤擂台&quot; &quot;每名受邀參賽者進行勝負預測&quot; ## [5] &quot;每周結算在周二公布&quot; &quot;累積勝率前&quot; ## [7] &quot;高參賽者可繼續參賽&quot; &quot;單周勝率最高者&quot; ## [9] &quot;將加封&quot; &quot;蘋果波神&quot; ## [11] &quot;頭銜&quot; &quot;註&quot; ## [13] &quot;賭盤賠率如有變動&quot; &quot;以台灣運彩為主&quot; ## [15] &quot;資料來源&quot; &quot;官網&quot; ## [17] &quot;金塊&quot; &quot;客&quot; ## [19] &quot;人騎士&quot; &quot;主&quot; ## [21] &quot;快艇活塞&quot; &quot;客&quot; ## [23] &quot;公牛勇士&quot; &quot;客&quot; ## [25] &quot;灰熊熱火&quot; &quot;客&quot; ## [27] &quot;灰狼籃網&quot; &quot;客&quot; ## [29] &quot;公鹿溜馬&quot; &quot;客&quot; ## [31] &quot;馬刺國王&quot; &quot;客&quot; ## [33] &quot;爵士小牛&quot; &quot;客&quot; ## [35] &quot;拓荒者&quot; &quot;&quot; ## ## [[2]] ## [1] &quot;&quot; &quot;動新聞&quot; ## [3] &quot;綜合報導&quot; &quot;新北市一名&quot; ## [5] &quot;歲李姓男子&quot; &quot;疑因女友要分手&quot; ## [7] &quot;避不見面也不接電話&quot; &quot;他為見女友一面&quot; ## [9] &quot;挽回感情&quot; &quot;竟學蜘蛛人攀爬鐵窗&quot; ## [11] &quot;欲潛入女友位於&quot; &quot;樓住處&quot; ## [13] &quot;行經&quot; &quot;樓時被住戶發現&quot; ## [15] &quot;他誆稱&quot; &quot;撿鑰匙&quot; ## [17] &quot;矇混過關&quot; &quot;但仍被&quot; ## [19] &quot;樓住戶懷疑是小偷報案&quot; &quot;最後雖成功進入&quot; ## [21] &quot;樓見到女友&quot; &quot;仍無法挽回感情&quot; ## [23] &quot;因侵入住宅罪嫌被帶回警局&quot; &quot;女方家屬不提告作罷&quot; ## [25] &quot;警方指出&quot; &quot;李男為挽回感情&quot; ## [27] &quot;鋌而走險攀爬鐵窗至&quot; &quot;樓&quot; ## [29] &quot;其後方就是一處工地&quot; &quot;萬一失足墜落&quot; ## [31] &quot;後果不堪設想&quot; &quot;經勸說後&quot; ## [33] &quot;讓李男離去&quot; &quot;&quot; In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a working unit (i.e., chunks). This can be controversial to many grammarians and syntacticians. However, in practice, this may not be a bad idea for n-grams extraction. For more information related to the unicode range for the punctuations in CJK languages, please see this SO discussion thread. With all these functions ready, the extraction of n-grams may follow the steps as shown below: We transform the text-based data frame into a chunk-based data frame using unnest_tokens(...) with self-defined tokenization function tokenizer_chunks() We transform the chunk-based data frame into an n-gram-based data frame using unnest_tokens(...) with self-defined tokenization function tokenizer_ngrams() We remove empty n-grams entries (Chunks with less than four words will have NO four-grams extracted.) # text-based to chunk-based apple_df %&gt;% unnest_tokens(chunk, text, token = tokenizer_chunks) %&gt;% filter(nzchar(chunk)) -&gt; apple_chunk apple_chunk %&gt;% head # chunked-based to ngram-based system.time( apple_chunk %&gt;% unnest_tokens(ngram, chunk, token = function(x) tokenizer_ngrams(input=x, jiebar=segmenter_word, n = 4, delimiter=&quot;_&quot;)) %&gt;% filter(nzchar(ngram)) -&gt; apple_ngram) # end system.time ## user system elapsed ## 36.072 0.186 36.286 nrow(apple_ngram) ## [1] 974022 apple_ngram %&gt;% head(20) 8.9.2 Frequency and Dispersion As we discussed in Chapter 4, a multiword unit can be defined based on at least distributional properties: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) Now that we have the four-grams-based DF, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams at: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) system.time( apple_ngram_dist &lt;- apple_ngram %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 5) ) #end system.time ## user system elapsed ## 11.408 0.043 11.456 Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;被&quot;)) %&gt;% arrange(desc(dispersion)) apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;以&quot;)) %&gt;% arrange(desc(dispersion)) Exercise 8.11 In the above example, if we are only interested in the four-grams with the word 以, how can we revise the regular expression so that we can get rid of tokens like ngrams with 以及, 以上 etc. 8.10 Afterwords Figure 8.3: Chinese Word Segmentation and POS Tagging Tokenizations are complex in Chinese text processing. Many factors may need to be taken into account when determining the right tokenization method. While word segmentation is almost a necessary step in Chinese computational text analytics, several important questions may also be relevant to the data processing methods: Do you need the parts-of-speech tags of words in your research? What is the base linguistic unit you would like to work with? Texts? Chunks? Sentences? N-grams? Words? Do you need non-word tokens such as symbols, punctuations, or numbers in your analysis? Your answers to the above questions should help you determine the most effective structure of the tokenization methods for your data. Exercise 8.12 Please scrape the articles on the most recent 10 index pages of the PTT Gossipping board. Analyze all the articles whose titles start with [問卦], [新聞], or [爆卦] (Please ignore all articles that start with Re:). Specifically, please create the word frequency list of these target articles by: including only words that are tagged as nouns or verbs by JiebaR (i.e., all words whose POS tags start with n or v) removing words on the stopword list (cf. demo_data/stopwords-ch.txt) providing both the word frequency and dispersions (i.e., number of articles where it occurs) In addition, please visualize your results with a wordcloud as shown below, showing the recent hot words based on these recently posted target articles on PTT Gossipping. In the wordcloud, please include words whose (a) nchar() &gt;=2, and (b) dispersion &lt;= 5. Note: For Chinese word segementation, you may use the dictionary provided in demo_data/dict-ch-user.txt ## user system elapsed ## 8.101 0.047 56.178 The target articles from PTT Gossipping: Word Frequency List Wordclound "]
]
