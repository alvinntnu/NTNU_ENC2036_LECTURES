[["index.html", "Corpus Linguistics Preface Course Objective Reading Materials Course Requirement Exams Course Website Contributing to the Lecture Notes Course Demo Data Academic Integrity House-keeping Guidelines Necessary Packages Environment Settings", " Corpus Linguistics Alvin Cheng-Hsien Chen 2022-05-24 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to one of the most active sub-fields of applied linguistics, i.e., Corpus Linguistics. Have you decided to embark on a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offering necessary inter-disciplinary skills and knowledge for quantitative and computational analysis of language. This course requires as prerequisite basic-level knowledge of coding skills. Students are expected to have taken ENC2055 or other equivalents of introductory programming courses before taking this course. Please see the FAQ of the course website for more information about the prerequisite. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will guide the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Reading Materials The course lectures will follow the materials provided on the course website. Please preview the lecture notes before the class. Also, for each topic, please review the assigned readings on your own. These readings will be part of the midterm and final exams as well. In particular, we will be referring to two useful reference books as our main reading materials– Stefanowitsch (2019), Gries (2016). In addition, there are a few more reference books listed at the end of the section (See References), which we will refer to for specific topics, including Gries (2021b), Baayen (2008), Brezina (2018), McEnery &amp; Hardie (2011), Wynne (2006), Winter (2020), Hunston (2022), Bird et al. (2009). In particular, assigned readings for each chapter are listed as shown below. Chapter Assigned Readings 1 Gries (2021b), ch1; McEnery &amp; Hardie (2011), ch1; Stefanowitsch (2019), ch1-2 2 Check ENC2055; Wickham &amp; Grolemund (2017) 3 Wynne (2006), ch1-2; Munzert et al. (2014), ch2&amp;9 4 Hunston (2022), Ch2-3; Stefanowitsch (2019), Ch4; Gries (2016), Ch3; Evert (2007) 5 Bird et al. (2009), ch3&amp;5; Gries (2016), Ch3; Stefanowitsch (2019), Ch7-8 6 Stefanowitsch (2019), Ch10 7 Lecture Notes Only 8 Stefanowitsch &amp; Gries (2003); Stefanowitsch &amp; Gries (2005); Gries &amp; Stefanowitsch (2004) 9 Lecture Notes Only 10 Wynne (2006), ch3,4,6; 11 Gries (2016), ch3; Munzert et al. (2014), ch3-4 12 Jurafsky &amp; Martin (2022), ch6 Course Requirement Exams The midterm and final exams will be a timed in-class open-book examination. Questions will include both coding exercises as well as essay questions (on the course materials). All exams will be considered individual work with no inter-personal communication. Please see Academic Integrity below. Course Website We have a course website. You may need a password to access the course materials (i.e., the data sets). If you are an officially enrolled student, please ask the instructor for the pass code. Please read the FAQ of the course website before course registration. Contributing to the Lecture Notes Although I have tried every possible way to make sure that the contents are correct, I may still accidentally make mistakes in the materials. If you spot any errors and would like make suggestions for better solutions, I would really appreciate it. To contribute your ideas, you can annotate the lecture notes using Hypothes.is, which is an amazing tool for website annotations. Go to Hypothes.is, and click the “get-started” on the top-right corner of the homepage. Install the the add-on for chrome, or other browser. (Optional!) To add an annotation, select some text and then click the on the pop-up menu. To see the annotations of others, click the in the upper right-hand corner of the page. Please turn on the Hypothes.is add-on when you are reading the course lecture notes, and you will see all public/shared annotations made by other course participants. See Quick Start Guide for Students and Annotation Tips for Students. At the beginning of the semester, I will share with the class a link to invite all the enrolled students to join a private group for annotation. But one can always provide feedbacks via the public annotations of the website. Course Demo Data Dropbox Demo Data Directory (Password protected) Academic Integrity All coding assignments are to be individual work, but discussion or collaboration with others is allowed. However, direct copying of others’ codes is absolutely forbidden. Any incidents of academic misconduct such as cheating, plagiarism, copying others’ work, or other inappropriate assistance on projects or examinations will be treated with zero tolerance and will result in a grade of “F” for the course. In particular, the midterm and final exams are taken seriously as individual work with absolutely no outside help or assistance. Breaches of academic integrity may also result in other action being taken by the University. House-keeping Guidelines Please direct all your course-related questions to the Discussion Forum on Moodle. Do not send your coding questions to the TA via email. By posting all the questions on Moodle, we can also make sure that those with similar questions would get proper assistance as well. If you need to consult TA (Howard Su) for technical help, please make an appointment with TA first. A recommended session is the hour after each week’s meeting. Please submit your coding assignments on time. Late submissions are subject to points deduction as a late penalty. All assignments must be submitted via Moodle. Please make sure that your access to Moodle is active. It is the student’s responsiblity to keep themselves posted of the most recent announcements. Necessary Packages In this course, we will need the following R packages for tutorials and exercises. library(dplyr) library(ggplot2) library(ggpubr) library(ggrepel) library(gutenbergr) library(htmlwidgets) library(jiebaR) library(kableExtra) library(parallel) library(purrr) library(quanteda) library(RColorBrewer) library(readtext) library(reticulate) library(Rtsne) library(rvest) library(showtext) library(spacyr) library(stringr) library(text2vec) library(textdata) library(tidyr) library(tidytext) library(tidyverse) library(wordcloud) library(wordcloud2) Environment Settings The R Version to produce the lecture notes: R version 4.1.2 (2021-11-01) If your R version is older than the above one, please consider updating your R. Details about updating R can be found in: 3 Methods to Update R &amp; Rstudio (For Windows &amp; Mac) Updating R, Rstudio, and Your Packages References Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction to statistics using R. Cambridge University Press. Bird, S., Klein, E., &amp; Loper, E. (2009). Natural language processing with python: Analyzing text with the natural language toolkit. \" O’Reilly Media, Inc.\". https://www.nltk.org/book/ Brezina, V. (2018). Statistics in corpus linguistics: A practical guide. Cambridge University Press. Evert, S. (2007). Corpora and collocations. https://stephanie-evert.de/PUB/Evert2007HSK_extended_manuscript.pdf Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Gries, S. T. (2021b). Statistics for linguistics with R: A practical introduction. 3rd edition. (3rd ed.). Walter de Gruyter. Gries, S. T., &amp; Stefanowitsch, A. (2004). Extending collostructional analysis: A corpus-based perspective onalternations’. International Journal of Corpus Linguistics, 9(1), 97–129. Hunston, S. (2022). Corpora in applied linguistics (2nd ed.). Cambridge University Press. Jurafsky, D., &amp; Martin, J. H. (2022). Speech and language processing. \" O’Reilly Media, Inc.\". https://web.stanford.edu/~jurafsky/slp3/ McEnery, T., &amp; Hardie, A. (2011). Corpus linguistics: Method, theory and practice. Cambridge University Press. Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Stefanowitsch, A., &amp; Gries, S. T. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. Stefanowitsch, A., &amp; Gries, S. T. (2005). Covarying collexemes. Corpus Linguistics and Linguistic Theory, 1, 1–43. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. Winter, B. (2020). Statistics for linguists: An introduction using R. Routledge. Wynne, M. (Ed.). (2006). Developing linguistic corpora—a guide to good practice. EADH: The European Association for Digital Humanities. https://users.ox.ac.uk/~martinw/dlc/index.htm "],["what-is-corpus-linguistics.html", "Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? 1.2 What is corpus? 1.3 What is a corpus linguistic study? 1.4 Additional Information on CL", " Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? There is an unnecessary dichotomy in linguistics “intuiting” linguistic data Inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality Corpus data Highlight the importance of language use in real context Highlight the linguistic tendency in the population (from a sample) Strengths of Corpus Data Data reliability How sure can we be that other people will arrive at the same observations/patterns/conclusions using the same method? Can others replicate the same logical reasoning in intuiting data? Can others make the same “grammatical judgement”? Data validity How well do we understand the true mental representation that the linguistic data correspond to? Can we know more about the mental representation of grammar based on one man’s constructed sentences and/or his grammatical judgement? Can we better generalize our insights from one man’s intuition or from the group minds (population vs. sample vs. one-man)? Please provide one or two examples that show the differences between one’s grammatical intuition and the corpus-based observations. The examples could be in English or in your native language (e.g., Chinese). 1.2 What is corpus? This can be tricky: different disciplines, different definitions Literature History Sociology Field Linguistics Linguistic Corpus in corpus linguistics (Stefanowitsch, 2019) Authentic Representative Large A few well-received definitions “a collection of texts which have been selected and brought together so that language can be studied on the computer” (Wynne, 2006) “A corpus is a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research.” (John Sinclair in (Wynne, 2006)) “A corpus refers to a machine-readable collection of (spoken or written) texts that were produced in a natural communitive setting, and in which the collection of texts is compiled with the intention (1) to be representative and balanced with respect to a particular linguistic language, variety, register, or genre and (2) to be analyzed linguistically.” (Gries, 2016) 1.3 What is a corpus linguistic study? CL characteristics No general agreement as to what it is Not a very homogeneous methodological framework (Compared to other sub-disciplines in linguistics) It’s quite new Intertwined with many linguistic fields Interactional linguistics, cognitive linguistics, functional syntax, usage-based grammar etc. Stylometry, computational linguistics, NLP, digital humanities, text mining, sentiment analysis Corpus-based vs. Corpus-driven (cf. Tognini-Bonelli, 2001) Corpus-based studies: Typically use corpus data in order to explore a theory or hypothesis, aiming to validate it, refute it or refine it. Take corpus linguistics as a method Corpus-driven studies: Typically reject the characterization of corpus linguistics as a method Claim instead that the corpus itself should be the sole source of our hypotheses about language It is thus claimed that the corpus itself embodies a theory of language (Tognini-Bonelli, 2001, pp. 84–85) The distinction between corpus-based and corpus-driven studies can be subtle. Both of these approaches, however, are often connected to the general usage-based grammar framework. I would highly recommend the chapter on usage-based model (Chapter 11) in Croft &amp; Cruse (2004). An example of corpus-driven grammatical framework is pattern grammar. Please see Hunston &amp; Francis (2000) for a comprehensive introduction. Stefanowitsch (2019) defines Corpus Linguistics as follows: Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus More on Conditional Distribution An exhaustive investigation Text-processing technology Retrieval and coding Regular expressions Common methods KWIC concordances Collocates Frequency lists A systematic investigation The distribution of a linguistic phenomenon under particular conditions (e.g. lexical, syntactic, social, pragmatic etc. contexts) Statistical properties of language Examples When do English speakers use the complementizer that? What are the differences between small and little? When do English speakers choose “He picked up the book” vs. “He picked the book up”? When do English speakers place the adverbial clauses before the matrix clause? Do speakers use different linguistic forms in different genres? Is the word “gay” used differently across different time periods? Do L2 learners use similar collocation patterns as do L1 speakers? Do speakers of different socio-economic classes talk differently? Now please try to think of one or two research questions that may fit into the category of a corpus linguistic study and elaborate on the idea of conditional distribution: You are interested in the linguistic structure of … and would like to see how it varies/changes/develops depending on … 1.4 Additional Information on CL Important Journals in Corpus Linguistics Corpus Linguistics and Linguistic Theory International Journal of Corpus Linguistics Corpora Applied Linguistics Computational Linguistics Digital scholarship in the Humanities Language Teaching Language Learning Journal of Second Language Writing CALL Language Teaching Research ReCALL System Important Conferences on Corpus Linguistics Corpus Linguistics Conference (usually held in UK) International Association of Applied Linguistics American Association for Corpus Linguistics American Association for Applied Linguistics Teaching and Language Corpora Conference International Cognitive Linguistics Conference International Conference on Construction Grammar References Croft, W., &amp; Cruse, D. A. (2004). Cognitive linguistics. Cambridge University Press. Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Hunston, S., &amp; Francis, G. (2000). Pattern grammar: A corpus-driven approach to the lexical grammar of English. John Benjamins Publishing. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Tognini-Bonelli, E. (2001). Corpus linguistics at work. John Benjamins. Wynne, M. (Ed.). (2006). Developing linguistic corpora—a guide to good practice. EADH: The European Association for Digital Humanities. https://users.ox.ac.uk/~martinw/dlc/index.htm "],["r-fundamentals.html", "Chapter 2 R Fundamentals A Quick Note", " Chapter 2 R Fundamentals A Quick Note In this course, we assume that students have a certain level of background knowledge of R. Please review fundamental concepts relating to the R language on your own. Important topics are covered in more detail in my other course, ENC2055. In particular, we assume that students have working knowledge on the following topics covered in the Lecture Notes of ENC2055: Chapter 2: R Fundamentals Chapter 3: Code Format Convention Chapter 4: Subsetting Chapter 6: Data Manipulation Chapter 7: Data Import Chapter 8: String Manipulation Chapter 9: Conditions and Loops Chapter 10: Iterations Please refer Winter (2020), Ch1-2 and Gries (2016), Ch3 for a comprehensive overview of R fundamentals (especially the parts on regular expressions). References Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Winter, B. (2020). Statistics for linguists: An introduction using R. Routledge. "],["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resources 3.6 Final Remarks", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract linguistic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections (cf. Structured Corpus and XML), chances are that sometimes you still need to collect your own data for a particular research question. But please note that when you are creating your own corpus for specific research questions, always pay attention to the three important criteria: representativeness, authenticity, and size. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. If you are new to tidyverse R, please check its official webpage for learning resources. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure The HyperText Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser. 3.1.1 HTML Syntax To illustrate the structure of the HTML, please download the sample html file from: demo_data/data-sample-html.html and first open it with your browser. &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? This is how to get back to the course page: &lt;a href=&quot;https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/&quot;, target=&quot;_blank&quot;&gt;ENC2036&lt;/a&gt;. &lt;/p&gt; &lt;h1&gt; Contents of the Page &lt;/h1&gt; &lt;p&gt; Anything you can say about the page.....&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; An HTML document includes several important elements (cf. Figure 3.1): DTD: document type definition which informs the browser about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= \"index.html\"&gt; Homepage &lt;/a&gt;). They are expressed as name = \"value\" pairs. Figure 3.1: Syntax of An HTML Tag Element An HTML document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the webpage textual contents would go into the &lt;body&gt; part. Most of the web-related codes and metadata (e.g., javascripts, CSS) are often included in the &lt;head&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in Figure 3.2. Figure 3.2: Tree Structure of An HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags and elements. However, in order to scrape the textual data from the Internet, you need to know at least from which parts of HTML elements you need your textual data from on the web pages. Usually, before you scrape the data from the webpage, bear the following questions in mind: From which HTML elements/tags would you like to extract the data for corpus construction? Do you need the textual content of the HTML element? Do you need a specific attribute of the HTML element? 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The idea is that CSS specifies the formats/styles of the HTML elements. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } You probably would wonder how to link a set of CSS style definitions to an HTML document. There are in general three ways: inline, internal and external. You can learn more about this in W3School.com. Here I will show you an example of the internal method. Below is a CSS style definition for &lt;h1&gt;. h1 { color: red; margin-bottom: 2em; } We can embed this within a &lt;style&gt;...&lt;/style&gt; element. Then you put the entire &lt;style&gt; element under &lt;head&gt; of the HTML file you would like to style. &lt;style&gt; h1 { color: red; margin-bottom: 1.5em; } &lt;/style&gt; After you include the &lt;style&gt; in the HTML file, refresh the web page to see if the CSS style works. 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In the following demonstration, the text data scraped from the PTT forum is presented as it is without adjustment. However, please note that the language on PTT may strike some readers as profane, vulgar or even offensive. library(tidyverse) library(rvest) In this tutorial, let’s assume that we like to scrape texts from PTT Forum. In particular, we will demonstrate how to scrape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create an session() (like we open a browser linking to the page) gossiping.session &lt;- session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created session() and create another session. gossiping &lt;- session_submit( x = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html Status: 200 Type: text/html; charset=utf-8 Size: 20275 Now our html session, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest [1] 39055 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% session_jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372214.A.B1A.html&quot; [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372282.A.78A.html&quot; [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372284.A.5F9.html&quot; [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372376.A.4BA.html&quot; [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372380.A.747.html&quot; [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372384.A.153.html&quot; [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372406.A.5B8.html&quot; [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372408.A.A5E.html&quot; [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372443.A.7A7.html&quot; [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372463.A.83C.html&quot; [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372474.A.130.html&quot; [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372528.A.054.html&quot; [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372560.A.183.html&quot; [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372590.A.791.html&quot; [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372600.A.930.html&quot; [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372619.A.724.html&quot; [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372628.A.1F8.html&quot; [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372825.A.667.html&quot; [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372866.A.651.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% session_jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. Because we are interested in the metadata and the contents of each article, now the question is: where are they in the HTML? We need to go back to the source page of the article HTML again: After a closer inspection of the article HTML, we know that: The metadata of the article are included in &lt;span&gt; tag elements, belonging to the class class=\"article-meta-value\" The contents of the article are included in the &lt;div&gt; element, whose ID is ID=\"main-content\" Now we are ready to extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() article.header [1] &quot;kukulee ()&quot; [2] &quot;Gossiping&quot; [3] &quot;[新聞] 飛行教官「空中激戰」正妹學員影片外流！&quot; [4] &quot;Tue May 24 14:03:30 2022&quot; The metadata of each PTT article in fact includes four pieces of information: author, board name, title, post time. The above code retrieves directly the values of these metadata. We can retrieve the tags of these metadata values as well: temp.html %&gt;% html_nodes(&quot;span.article-meta-tag&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() [1] &quot;作者&quot; &quot;看板&quot; &quot;標題&quot; &quot;時間&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author [1] &quot;kukulee&quot; article.title [1] &quot;[新聞] 飛行教官「空中激戰」正妹學員影片外流！&quot; article.datetime [1] &quot;Tue May 24 14:03:30 2022&quot; Now we extract the main contents of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content [1] &quot;1.媒體來源: ETtoday\\n\\n\\n2.記者署名: 張靖榕\\n\\n\\n3.完整新聞標題:\\n\\n\\n飛行教官「空中激戰」正妹學員影片外流！ 以砲換課下場慘\\n\\n\\n4.完整新聞內文:\\n\\n俄羅斯一所飛行學校的一名28歲已婚機師，說服21歲辣妹女學員，只要和他在空中「激戰\\n」，就能免費獲得額外的飛行時數，2人可以在空中繼續徜徉更多時間。2人不僅利用上課\\n時間發生性關係，還將影片上傳到網路上，之後被飛行學校發現，怒將2人開除。\\n\\n綜合外電報導，俄羅斯薩索沃（Sasovo）民航飛行學校的男機師，起先在上課期間試著說服辣妹女學員和他發生性關係，但女學員以他已婚為由拒絕，之後男機師提出「優惠」，表示只要和他性交就可以獲得額外的飛行時數，女學員才終於答應，兩人在機上熱吻之後更展開激烈的「空中打砲」。畫面中2人在塞斯納172型小型飛機的駕駛艙內激戰，底下的風景不斷掠過，素人空中激戰\\n的影片立刻在網路上獲得關注，就連飛行學校的其他職員都看到，而且不只一個人認出男\\n機師就是同事，於是通報學校高層。\\n\\n從事發到學校發現僅僅過了一個月，校方立刻開除男機師和女學員。女學員起先聲稱沒有\\n和男機師發生性關係，但在男機師保證會有額外飛行時數後，她才和對方擁抱和親吻，沒\\n有發生性關係，而且當時飛機有開啟自動飛航模式，但之後又改口2人只做過一次。\\n\\n當地部分媒體指出，影片其實是另外一名學員上傳的，該名學員疑似為了報復女學員，才\\n將影片上傳到網路上，而女學員其實早在知道影片曝光後，就已申請退學。https://cdn2.ettoday.net/images/6358/6358098.jpghttps://cdn2.ettoday.net/images/6358/6358097.jpg5.完整新聞連結 (或短網址)需放媒體原始連結，不可用轉載媒體連結:https://www.ettoday.net/news/20220524/2257459.htm6.備註:\\n\\n空中打炮好刺激R\\n\\n--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt;. These children &lt;div&gt; or &lt;span class=“f2”&gt; of the &lt;div id = “main-content”&gt; include the push comments (推文) of the article, which are not the main content of the article. Now we can combine all information related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push {xml_nodeset (60)} [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [4] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [5] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [6] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [7] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [8] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [9] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [10] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [11] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [12] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [13] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [14] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [15] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [16] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [17] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [18] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [19] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [20] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag [1] &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; [16] &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;→&quot; &quot;推&quot; &quot;噓&quot; [31] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;噓&quot; &quot;推&quot; [46] &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author [1] &quot;Lenney33&quot; &quot;sted0101&quot; &quot;pionaero&quot; &quot;StylishTrade&quot; &quot;gsm60kimo&quot; [6] &quot;ebod221&quot; &quot;deann&quot; &quot;qa1122z&quot; &quot;ah937609&quot; &quot;jim12441&quot; [11] &quot;seiya1201&quot; &quot;amelet&quot; &quot;outsi&quot; &quot;E6300&quot; &quot;WeGoStyle&quot; [16] &quot;qa1122z&quot; &quot;xheath&quot; &quot;milkBK&quot; &quot;Julian9x9x9&quot; &quot;xheath&quot; [21] &quot;makigoto123&quot; &quot;s155260&quot; &quot;SydLrio&quot; &quot;ebv&quot; &quot;newqazwsx&quot; [26] &quot;wasicoelon&quot; &quot;franchy&quot; &quot;skygray2&quot; &quot;ghostl40809&quot; &quot;dear133&quot; [31] &quot;coffee112&quot; &quot;nextpage&quot; &quot;a11011788&quot; &quot;sunnyyoung&quot; &quot;laneog&quot; [36] &quot;DellSale999&quot; &quot;simga&quot; &quot;monkeydpp&quot; &quot;widec&quot; &quot;wwvvkai&quot; [41] &quot;starbear&quot; &quot;widec&quot; &quot;d0922030&quot; &quot;xxxluke&quot; &quot;dick929&quot; [46] &quot;k1314520illy&quot; &quot;babyalley&quot; &quot;coffee112&quot; &quot;ph777&quot; &quot;COLINLIU&quot; [51] &quot;lavendersea&quot; &quot;Gogoro5566&quot; &quot;madeathmao&quot; &quot;beeboombee&quot; &quot;josef15&quot; [56] &quot;allenz78&quot; &quot;winiS&quot; &quot;STerry1986&quot; &quot;s6525480&quot; &quot;horseorange&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content [1] &quot;: 真會玩&quot; [2] &quot;: 正嗎&quot; [3] &quot;: 好爽&quot; [4] &quot;: 好爽喔&quot; [5] &quot;: Top Gun&quot; [6] &quot;: 那不是珍珠港的劇情嗎&quot; [7] &quot;: 戰鬥民族在空中戰鬥 真的高難度&quot; [8] &quot;: 有自動駕駛真的很棒&quot; [9] &quot;: 洋腸&quot; [10] &quot;: 俄羅斯人真會玩&quot; [11] &quot;: 大!&quot; [12] &quot;: 真．速度與激情&quot; [13] &quot;: 禿頭？就&quot; [14] &quot;: 俄羅斯版的伊森杭特&quot; [15] &quot;: 正港a空幹王&quot; [16] &quot;: 為什麼近視就不能考？&quot; [17] &quot;: 前線打炮 後勤也在打炮&quot; [18] &quot;: 飛機上做愛也太刺激&quot; [19] &quot;: 飛飛飛飛機&quot; [20] &quot;: 前線軍人情何以堪&quot; [21] &quot;: 戰鬥民族不同凡響&quot; [22] &quot;: 真。空幹&quot; [23] &quot;: 空幹王XD&quot; [24] &quot;: 我還以為是電影Top Gun耶~阿湯在空中打炮劇情&quot; [25] &quot;: 棒&quot; [26] &quot;: 阿湯哥輸了&quot; [27] &quot;: 空中打炮是不是機師專屬的權利&quot; [28] &quot;: 好刺激&quot; [29] &quot;: 爽到升天&quot; [30] &quot;: 愛錄再錄&quot; [31] &quot;: ....................................&quot; [32] &quot;: 三萬英呎的空中高潮就問爽不爽&quot; [33] &quot;: 需要做研究用途&quot; [34] &quot;: 上傳是智障嗎&quot; [35] &quot;: 懂玩!!這挑戰沒幾個人可以解的&quot; [36] &quot;: 董砲&quot; [37] &quot;: 爽ㄟ~~阿湯哥都沒拍過這XD&quot; [38] &quot;: 這個奶子…&quot; [39] &quot;: 這個屌...竟然是在駕駛艙打炮&quot; [40] &quot;: top gun&quot; [41] &quot;: 懂玩&quot; [42] &quot;: 再頂就要去太空艙打炮才比得過了&quot; [43] &quot;: 大&quot; [44] &quot;: 28歲禿成這樣&quot; [45] &quot;: top gun?&quot; [46] &quot;: 空中使出高難度動作&quot; [47] &quot;: 會玩喔&quot; [48] &quot;: 禿頭又怎樣 人家在飛機上.............&quot; [49] &quot;: 曬太陽不熱嗎&quot; [50] &quot;: 懂玩&quot; [51] &quot;: 俄羅斯人懂玩&quot; [52] &quot;: 所以要去哪裡看==&quot; [53] &quot;: TOP gun(物理)&quot; [54] &quot;: 網址呢&quot; [55] &quot;: 俄羅斯韓導&quot; [56] &quot;: 空幹王!&quot; [57] &quot;: 竟然不是模擬倉，太強了吧&quot; [58] &quot;: 塞斯納不是很小架嗎 還能在上面打炮喔&quot; [59] &quot;: 空中姦慾 幾十年前拍的&quot; [60] &quot;: 所以要怎麼打?&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime [1] &quot;118.165.130.163 05/24 14:03&quot; &quot;111.250.25.118 05/24 14:03&quot; [3] &quot;223.138.162.35 05/24 14:04&quot; &quot;111.249.225.81 05/24 14:04&quot; [5] &quot;61.228.88.20 05/24 14:04&quot; &quot;1.162.32.215 05/24 14:04&quot; [7] &quot;60.250.235.109 05/24 14:04&quot; &quot;223.141.26.31 05/24 14:04&quot; [9] &quot;180.217.235.78 05/24 14:04&quot; &quot;223.138.53.185 05/24 14:04&quot; [11] &quot;219.85.83.97 05/24 14:04&quot; &quot;117.56.51.117 05/24 14:04&quot; [13] &quot;61.222.103.115 05/24 14:04&quot; &quot;223.138.83.141 05/24 14:05&quot; [15] &quot;1.200.178.154 05/24 14:05&quot; &quot;223.141.26.31 05/24 14:05&quot; [17] &quot;42.79.228.215 05/24 14:05&quot; &quot;114.34.189.70 05/24 14:05&quot; [19] &quot;36.230.98.209 05/24 14:05&quot; &quot;42.79.228.215 05/24 14:05&quot; [21] &quot;223.139.97.248 05/24 14:05&quot; &quot;36.225.253.225 05/24 14:05&quot; [23] &quot;42.79.244.69 05/24 14:05&quot; &quot;140.128.67.246 05/24 14:05&quot; [25] &quot;42.73.194.156 05/24 14:05&quot; &quot;42.74.204.132 05/24 14:05&quot; [27] &quot;61.219.19.44 05/24 14:06&quot; &quot;61.216.55.178 05/24 14:07&quot; [29] &quot;1.162.116.11 05/24 14:07&quot; &quot;36.228.208.121 05/24 14:07&quot; [31] &quot;1.174.200.128 05/24 14:07&quot; &quot;36.235.29.80 05/24 14:07&quot; [33] &quot;223.136.74.54 05/24 14:07&quot; &quot;49.216.24.101 05/24 14:08&quot; [35] &quot;27.246.2.56 05/24 14:08&quot; &quot;42.77.20.169 05/24 14:08&quot; [37] &quot;112.104.83.180 05/24 14:08&quot; &quot;114.137.98.41 05/24 14:09&quot; [39] &quot;1.165.44.183 05/24 14:09&quot; &quot;27.52.74.170 05/24 14:10&quot; [41] &quot;118.231.153.7 05/24 14:10&quot; &quot;1.165.44.183 05/24 14:10&quot; [43] &quot;223.137.178.72 05/24 14:10&quot; &quot;101.10.45.104 05/24 14:11&quot; [45] &quot;49.216.32.16 05/24 14:11&quot; &quot;180.217.136.48 05/24 14:11&quot; [47] &quot;111.71.212.223 05/24 14:12&quot; &quot;1.174.200.128 05/24 14:12&quot; [49] &quot;223.141.168.173 05/24 14:12&quot; &quot;111.82.0.30 05/24 14:12&quot; [51] &quot;49.216.169.5 05/24 14:12&quot; &quot;61.218.44.76 05/24 14:13&quot; [53] &quot;60.250.204.170 05/24 14:15&quot; &quot;42.73.40.210 05/24 14:15&quot; [55] &quot;42.73.25.113 05/24 14:16&quot; &quot;123.0.62.125 05/24 14:16&quot; [57] &quot;180.217.66.0 05/24 14:17&quot; &quot;1.163.53.225 05/24 14:18&quot; [59] &quot;223.137.83.246 05/24 14:18&quot; &quot;61.64.0.72 05/24 14:20&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to collect text data in large amounts: For each index page, we need to extract all the article hyperlinks of the page. For each article hyperlink, we need to extract the article content, metadata, and the push comments. So, it would be great if we can wrap these two routines into two functions. 3.3.1 extract_art_links() extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. It returns a vector of article links. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% session_jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } For example, we can extract all the article links from the most recent index page: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Get all article links from the most recent index page cur_art_links &lt;-extract_art_links(cur_index_page, gossiping) cur_art_links [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372214.A.B1A.html&quot; [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372282.A.78A.html&quot; [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372284.A.5F9.html&quot; [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372376.A.4BA.html&quot; [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372380.A.747.html&quot; [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372384.A.153.html&quot; [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372406.A.5B8.html&quot; [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372408.A.A5E.html&quot; [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372443.A.7A7.html&quot; [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372463.A.83C.html&quot; [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372474.A.130.html&quot; [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372528.A.054.html&quot; [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372560.A.183.html&quot; [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372590.A.791.html&quot; [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372600.A.930.html&quot; [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372619.A.724.html&quot; [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372628.A.1F8.html&quot; [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372825.A.667.html&quot; [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1653372866.A.651.html&quot; 3.3.2 extract_article_push_tables() extract_article_push_tables(): This function takes an article link link as the argument and extracts the metadata, textual contents, and pushes of the article. It returns a list of two elements—article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% session_jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge push table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc For example, we can get the article and push tables from the first article link: extract_article_push_tables(cur_art_links[1]) $article.table # A tibble: 1 × 5 datetime title author content url &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Tue May 24 14:03:30 2022 [新聞] 飛行… kukulee &quot;1.媒體來源: ETto… https://www.… $push.table # A tibble: 60 × 5 tag author content datetime url &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 → Lenney33 : 真會玩 118.165.130… https://www.… 2 推 sted0101 : 正嗎 111.250.25.… https://www.… 3 推 pionaero : 好爽 223.138.162… https://www.… 4 推 StylishTrade : 好爽喔 111.249.225… https://www.… 5 → gsm60kimo : Top Gun 61.228.88.2… https://www.… 6 推 ebod221 : 那不是珍珠港的劇情嗎 1.162.32.21… https://www.… 7 推 deann : 戰鬥民族在空中戰鬥 真的高難度 60.250.235.… https://www.… 8 → qa1122z : 有自動駕駛真的很棒 223.141.26.… https://www.… 9 → ah937609 : 洋腸 180.217.235… https://www.… 10 → jim12441 : 俄羅斯人真會玩 223.138.53.… https://www.… # … with 50 more rows 3.3.3 Streamline the Codes Now we can simplify our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scrape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # number of articles on this index page length(ptt_data) [1] 19 # Check the first contents of 1st hyperlink ptt_data[[1]]$article.table ptt_data[[1]]$push.table Finally, the last thing we can do is to combine all article tables from each index page into one; and all push tables into one for later analysis. # Merge all article.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows # Merge all push.tables into one push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph summarizes our work flowchart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scraped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resources Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the sustainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata representation of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Technical parts for corpus creation (cf. Sinclair’s Appendix) 3.6 Final Remarks Please pay attention to the ethical aspects involved in the process of web crawling (esp. with personal private matters). If the website has their own API built specifically for one to gather data, use it instead of scraping. Always read the terms and conditions provided by the website regarding data gathering. Always be gentle with the data scraping (e.g., off-peak hours, spacing out the requests) Value the data you gather and treat the data with respect. Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Your script may look something like: # I define my own `scrapePTT()` function: # ptt_url: specify the board to scrape texts from # num_index_page: specify the number of index pages to be scraped # return: list(article, push) PTT_data &lt;-scrapePTT(ptt_url = &quot;https://www.ptt.cc/bbs/Gossiping&quot;, num_index_page = 3) PTT_data$article %&gt;% head PTT_data$push %&gt;% head # corpus size PTT_data$article$content %&gt;% nchar %&gt;% sum [1] 36971 Exercise 3.3 Please choose a website (other than PTT) you are interested in and demonstrate how you can use R to retrieve textual data from the site. The final scraped text collection could be from only one static web page. The purpose of this exercise is to show that you know how to parse the HTML structure of the web page and retrieve the data you need from the website. References Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. "],["corpus-analysis-a-start.html", "Chapter 4 Corpus Analysis: A Start 4.1 Installing quanteda 4.2 Building a corpus from character vector 4.3 Keyword-in-Context (KWIC) 4.4 KWIC with Regular Expressions 4.5 Tidy Text Format of the Corpus 4.6 Processing Flowchart 4.7 Frequency Lists 4.8 Word Cloud 4.9 Collocations 4.10 Constructions", " Chapter 4 Corpus Analysis: A Start In this chapter, I will demonstrate how to perform a basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 4.1 Installing quanteda There are many packages that are made for computational text analytics in R. You may consult the CRAN Task View: Natural Language Processing for a lot more alternatives. To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. library(tidyverse) library(quanteda) library(readtext) library(tidytext) packageVersion(&quot;quanteda&quot;) [1] &#39;3.2.1&#39; 4.2 Building a corpus from character vector To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. data_corpus_inaugural Corpus consisting of 59 documents and 4 docvars. 1789-Washington : &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot; 1793-Washington : &quot;Fellow citizens, I am again called upon by the voice of my c...&quot; 1797-Adams : &quot;When it was first perceived, in early times, that no middle ...&quot; 1801-Jefferson : &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot; 1805-Jefferson : &quot;Proceeding, fellow citizens, to that qualification which the...&quot; 1809-Madison : &quot;Unwilling to depart from examples of the most revered author...&quot; [ reached max_ndoc ... 53 more documents ] class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; We create a corpus() object with the pre-loaded corpus in quanteda– data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) # save the `corpus` to a short obj name After the corpus is loaded, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. summary(corp_us) If you like to add document-level metadata information to each document in the corpus object, you can use docvars(). These document-level metadata information is referred to as docvars (document variables) in quanteda. docvars(corp_us, &quot;country&quot;) &lt;- &quot;US&quot; The advantages of these docvars is that we can easily subset the corpus based on these document-level variables (i.e., a new corpus can be extracted/created based on logical conditions applied to docvars): corpus_subset(corp_us, Year &gt; 2000) %&gt;% summary require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() Exercise 4.1 Could you reproduce the above line plot and add information of President to the plot as labels of the dots? Hints: Please check ggplot2::geom_text() or more advanced one, ggrepel::geom_text_repel() So the idea is that as long as you can load the text data into a character vector, you can easily create an corpus object with quanteda::corpus(). The library readtext provides a very effective function readtext() for you to load text data from external files. Please check its documentation for more effective usages. For example, if you have downloaded the file gutenberg.zip and stored it in demo_data, you can load in the gutenberg corpus as follows: gutenberg &lt;- readtext(file = &quot;demo_data/gutenberg.zip&quot;) gutenberg_corp &lt;- corpus(gutenberg) summary(gutenberg_corp) 4.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examining how it is being used in a wider context. We first tokenize the corpus using tokens() and then we can use kwic() to perform a search for a word and retrieve its concordances from the corpus: ## word tokenization corp_us_tokens &lt;- tokens(corp_us) ## concordances kwic(corp_us_tokens, &quot;terror&quot;) kwic() returns a data frame, which can be easily exported to a CSV file for later use. Please note that kwic(), when taking a corpus object as the argument, will automatically tokenize the corpus data and do the keyword-in-context search on a word basis. Yet, the recommended way is to tokenize the corpus object first with tokens() before you perform the concordance analysis with kwic(). The pattern you look for cannot be a linguistic pattern across several words. We will talk about how to extract phrasal patterns/constructions later. Also, for languages without explicit word boundaries (e.g., Chinese), this may be a problem with quanteda. We will talk more about this in the later chapter on Chinese Texts Analytics. 4.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can create a regular expression for the concordances. kwic(corp_us_tokens, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) %&gt;% data.frame By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us_tokens, phrase(&quot;our country&quot;)) %&gt;% data.frame It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. After a series of trial and error attempts, it seems that phrase() also supports regular expression search. If we want to extract the concordances of a multiword pattern defined by a regular expression, we can specify the regular expression on a word basis and change the valuetype to be regex. It is less clear to me how we can use the regular expression to retrieve multiword patterns of variable sizes with kwic(). kwic(corp_us_tokens, phrase(&quot;^(the|our)$ ^countr.+?$&quot;), valuetype = &quot;regex&quot;) %&gt;% data.frame kwic(corp_us_tokens, phrase(&quot;^(be|was|were)$ ^(\\\\w+ly)$ ^(\\\\w+ed)$&quot;), valuetype = &quot;regex&quot;) %&gt;% data.frame Exercise 4.2 Please create a bar plot, showing the number of uses of the word country in each president’s address. Please include different variants of the word, e.g., countries, Countries, Country, in your kwic() search. 4.5 Tidy Text Format of the Corpus So far our corpus is a corpus object defined in quanteda. In most of the R standard packages, people normally follow the using tidy data principles to make handling data easier and more effective. As described by Hadley Wickham (Wickham &amp; Grolemund, 2017), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table Essentially, it is an idea of making an abstract object (i.e., corpus) a more intuitive data structure, i.e., a data.frame, which is easier for human readers to work with. With text data like a corpus, we can also define the tidy text format as being a data.frame with one-token-per-row. A token can be any meaningful unit of the text, such as a word that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. In computational text analytics, the token (i.e., each row in the data frame) is most often a single word, but can also be an n-gram, a sentence, or a paragraph. The tidytext package in R is made for the handling of the tidy text format of the corpus data. With a tidy data format of the corpus, we can manipulate the text data with a standard set of tidy tools and packages, including dplyr, tidyr, and ggplot2. The tidytext package includes a function, tidy(), to convert the corpus object from quanteda into a document/text-based data.frame. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) # convert `corpus` to `data.frame` class(corp_us_tidy) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.6 Processing Flowchart Figure 4.1: Computational Text Processing Flowchart 4.7 Frequency Lists 4.7.1 Word (Unigram) To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages (e.g., the semantics of the documents). The tidytext provides a powerful function, unnest_tokens() to tokenize a data frame with larger linguistic units (e.g., texts) into one with smaller units (e.g., words). That is, the unnest_tokens() convert a text-based data frame (each row is a text document) into a token-based data frame(each row is a token splitted from the text). corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(output = word, # new base unit column name input = text, # original base unit column name token = &quot;words&quot;) # tokenization method corp_us_words The unnest_tokens() is optimized for English tokenization of smaller linguistic units, such as words, ngrams, sentences, lines, and paragraphs (check ?unnest_tokens()). To handle Chinese data, however, we need to be more careful. We probably need to define own ways of tokenization method in unnest_tokens(…, token = …). We will discuss the principles for Chinese text processing in a later chapter. Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”, strip_punct = F, strip_numeric = F). Now we can count the word frequencies by making use of the dplyr library: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort = TRUE) corp_us_words_freq 4.7.2 Bigrams Frequency lists can be generated for bigrams or any other multiword combinations as well. The key is we need to convert the text-based data frame into a bigram-based data frame. corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens( output = bigram, # new base unit column name input = text, # original base unit column name token = &quot;ngrams&quot;, # tokenization method n = 2 ) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) # size of unigrams [1] 137939 sum(corp_us_bigrams_freq$n) # size of bigrams [1] 137880 Could you explain the difference between the total numbers of unigrams and bigrams? Exercise 4.3 Based on the bigram-based data frame, how can we create a frequency list showing each president’s uses of bigrams with the first-person plural pronoun we as the first word. In the output, please present the most frequently used bigram in each presential addresss only. If there are ties, present all of them. Arrange the frequency list according to the year, the president’s last name, and the token frequency of the bigram. Exercise 4.4 The function unnest_tokens() does a lot of work behind the scene. Please take a closer look at the outputs of unnest_tokens() and examine how it takes care of the case normalization and punctuations within the sentence. Will these treatments affect the frequency lists we get in any important way? Please elaborate. 4.7.3 Ngrams (Lexical Bundles) corp_us_trigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigrams We then can examine which n-grams were most often used by each President: corp_us_trigrams %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) %&gt;% arrange(President, desc(n)) Exercise 4.5 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. 4.7.4 Frequency and Dispersion When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram can be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by many different Presidents? The degrees of n-gram dispersion has a lot to do with the significance of its frequency. So now let’s compute the dispersion of the n-grams in our corp_us_trigrams. Here we define the dispersion of an n-gram as the number of Presidents who have used the n-gram at least once in his address(es). Dispersion is an important construct and many more sophisticated quantitative metrics have been proposed to more properly operationalize this concept. Please see Gries (2021a). # method 1 corp_us_trigrams %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = sum(n), DISPERSION = n()) %&gt;% filter(DISPERSION &gt;= 5) %&gt;% arrange(desc(DISPERSION)) # method2 corp_us_trigrams %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = n(), DISPERSION = n_distinct(President)) %&gt;% filter(DISPERSION &gt;= 5) %&gt;% arrange(desc(DISPERSION)) # Arrange according to frequency # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) In particular, cut-off values are often used to determine a list of meaningful n-grams. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. A subset of n-grams that are defined and selected based on these distributional criteria (i.e., frequency and dispersion) are often referred to as Lexical bundles (See Biber et al. (2004)). Exercise 4.6 Please create a list of four-word lexical bundles that have been used in at least FIVE different presidential addressess. Arrange the resulting data frame according to the frequency of the four-grams. 4.8 Word Cloud With frequency data, we can visualize important words in the corpus with a Word Cloud. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts. library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 400, min.freq = 30, scale = c(2,0.5), color = brewer.pal(8, &quot;Dark2&quot;), vfont=c(&quot;serif&quot;,&quot;plain&quot;))) Exercise 4.7 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. (Criteria: Frequency &gt;= 20; Max Number of Words Plotted = 400) Hint: Check dplyr::anti_join() require(tidytext) stop_words Exercise 4.8 Get yourself familiar with another R package for creating word clouds, wordcloud2, and re-create a word cloud as requested in Exercise 4.7 but in a fancier format, i.e., a star-shaped one. (Criteria: Frequency &gt;= 20; Max Number of Words Plotted = 400) 4.9 Collocations With unigram and bigram frequencies of the corpus, we can further examine the collocations within the corpus. Collocation refers to a frequent phenomenon where two words tend to co-occur very often in use. This co-occurrence is defined statistically by their lexical associations. 4.9.1 Cooccurrence Table and Observed Frequencies Cooccurrence frequency data for a word pair, w1 and w2, are often organized in a contingency table extracted from a corpus, as shown in Figure 4.2. The cell counts of this contingency table are referred to as the observed frequencies O11, O12, O21, and O22. Figure 4.2: Cooccurrence Frequency Table The sum of all four observed frequencies (called the sample size N) is equal to the total number of bigrams extracted from the corpus. And before we discuss the computation of lexical associations, there are a few terms that we often use when talking about the contingency table. R1 and R2 are the row totals of the observed contingency table, while C1 and C2 are the corresponding column totals. These row and column totals are referred to as marginal frequencies (because they are often written on the margins of the table) The frequency in O11 is referred to as the joint frequency of the two words. 4.9.2 Expected Frequencies For every contingency table as seen above, if one knows the marginal frequencies (i.e., the row and column sums), one can compute the expected frequencies of the four cells accordingly. These expected frequencies would be the expected distribution under the null hypothesis that W1 and W2 are statistically independent. And the idea of lexical association between W1 and W2 is to statistically access to what extent the observed frequencies in the contingency table are different from the expected frequencies (given the current the marginal frequencies). Therefore, equations for different association measures (i.e., mutual information, log-likelihood ratios, chi-square) are often given in terms of the observed frequencies, marginal frequencies, and the expected frequencies E11, …, E22. Please see Stefan Evert’s Computational Approaches to Collocation for a very detailed and comprehensive comparison of various statistical methods for lexical association. The expected frequencies can be computed from the marginal frequencies as shown in Figure 4.3. Figure 4.3: Computing Expected Frequencies Maybe it would be easier for us to illustrate this with a simple example: Figure 4.4: Computing Expected Frequencies How do we compute the expected frequencies of the four cells? Figure 4.5: Computing Expected Frequencies example &lt;- matrix(c(90, 10, 110, 290), byrow=T, nrow=2) Exercise 4.9 Please compute the expected frequencies for the above matrix example in R. 4.9.3 Association Measures The idea of lexical assoication is to measure how much the observed frequencies deviate from the expected. Some of the metrics (e.g., t-statistic, MI) consider only the joint frequency deviation (i.e., O11), while others (e.g., G2, a.k.a Log Likelihood Ratio) consider the deviations of ALL cells. Here I would like to show you how we can compute the most common two asssociation metrics for all the bigrams found in the corpus: t-test statistic and Mutual Information (MI). \\(t = \\frac{O_{11}-E_{11}}{\\sqrt{O_{11}}}\\) \\(MI = log_2\\frac{O_{11}}{E_{11}}\\) \\(G^2 = 2 \\sum_{ij}{O_{ij}log\\frac{O_{ij}}{E_{ij}}}\\) corp_us_bigrams_freq %&gt;% head(10) corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% rename(O11 = n) %&gt;% tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;\\\\s&quot;) %&gt;% # split bigrams into two columns mutate(R1 = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], C1 = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), t = (O11 - E11)/sqrt(O11)) %&gt;% # compute associations arrange(desc(MI)) # sorting corp_us_collocations %&gt;% filter(O11 &gt; 5) # set bigram frequency cut-off Please note that in the above example, we compute the lexical associations for bigrams whose frequency &gt; 5. This is necessary in collocation studies because bigrams of very low frequency would not be informative even though its association can be very strong. However, the cut-off value can be arbitrary, depending on the corpus size or researchers’ considerations. How to compute lexical associations is a non-trivial issue. There are many more ways to compute the association strengths between two words. Please refer to Stefan Evert’s site for a very comprehensive review of lexical association measures. Probably the recommended method is G2 (Stefanowitsch, 2019). The idea of “co-occurrence” can be defined in many different ways. So far, our discussion and computation of bigrams’ collocation strength has been based on a very narrow definition of “co-occurrence”, i.e., w1 and w2 have to be directly adjacent to each other (i.e., contiguous bigrams). The co-occurrence of two words can be defined in more flexible ways: w1 and w2 co-occur within a defined window frame (e.g., -/+ ten-word window frame) w1 and w2 co-occur within a particular syntactic frame (e.g., Adjective + … + Noun) While the operationalization of co-occurrence may lead to different w1 and w2 joint frequencies in the above contingency table, the computation of the collocation metrics for the two words is still the same. Exercise 4.10 Sort the collocation data frame corp_us_collocations according to the t-score and compare the results sorted by MI scores. Please describe the differences between the bigram collocations found with both metrics (i.e., MI and t-score). Exercise 4.11 Based on the formula provided above, please create a new column for corp_us_collocations, which gives the Log-Likelihood Ratios of all the bigrams. When you do the above exercise, you may run into a couple of problems: Some of the bigrams have NaN values in their LLR. This may be due to the issue of NAs produced by integer overflow. Please solve this. After solving the above overflow issue, you may still have a few bigrams with NaN in their LLR, which may be due to the computation of the log value. In Math, how do we define log(1/0) and log(0/1)? Do you know when you would get an undefined value NaN in the computation of log()? To solve the problems, please assign the value 0 if the log returns NaN values. Exercise 4.12 Find the top FIVE bigrams ranked according to MI values for each president. The result would be a data frame as shown below. (Please remove bigrams whose frequency &lt; 5 from before ranking.) Create a plot as shown below to visualize your results. Please note that due to the ties of MI scores, there may be more than five bigrams presented in the results for each presidential address. Please note that when you compute the bigram’s collocation strength, their MI values should be same across different presidential addressess. A quick way to check the accuracy of your numbers is to look at the same bigram used in different presidential addresses and see if it has the same MI values. For example, the above table shows that while in “1801_Jefferson”, there was only one token of the bigram domestic concerns, the bigram occurred 7 times in the entire corpus. 4.10 Constructions We are often interested in the use of linguistic patterns, which are beyond the lexical boundaries. My experience is that usually it is better to work with the corpus on a sentential level. We can use the same tokenization function, unnest_tokens() to convert our text-based corpus data frame, corpus_us_tidy, into a sentence-based tidy structure: corp_us_sents &lt;- corp_us_tidy %&gt;% unnest_tokens(output = sentence, ## nested unit input = text, ## superordinate unit token = &quot;sentences&quot;) # tokenize the `text` column into `sentence` corp_us_sents With each sentence, we can investigate particular constructions in more detail. Let’s assume that we are interested in the use of Perfect aspect in English by different presidents. We can try to extract Perfect constructions (including Present/Past Perfect) from each sentence using the regular expression. Here we make a simple naive assumption: Perfect constructions include all have/has/had + ...-en/ed tokens from the sentences. require(stringr) # Perfect corp_us_sents %&gt;% unnest_tokens( perfect, ## nested unit sentence, ## superordinate unit token = function(x) str_extract_all(x, &quot;ha(d|ve|s) \\\\w+(en|ed)&quot;) ) -&gt; result_perfect result_perfect In the above example, we specify the token = … in unnest_tokens(…, token = …) with a self-defined function. The idea of tokenization in unnest_tokens() is that the token argument can be a function which takes a text-based vector as input (i.e, each element of the input vector may be a document text) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (see Figure below). In our demonstration, we define a tokenization function, which takes sentence as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions in individual sentences in sentence. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Figure 4.6: Intuition for token= in unnest_tokens() And of course we can do an exploratory analysis of the frequencies of Perfect constructions by different presidents: require(tidyr) # table result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) # graph result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) %&gt;% pivot_longer(c(&quot;TOKEN_FREQ&quot;, &quot;TYPE_FREQ&quot;), names_to = &quot;STATISTIC&quot;, values_to = &quot;NUMBER&quot;) %&gt;% ggplot(aes(President, NUMBER, fill = STATISTIC)) + geom_bar(stat = &quot;identity&quot;,position = position_dodge()) + theme(axis.text.x = element_text(angle=90)) There are quite a few things we need to take care of more thoroughly: The auxiliary HAVE and the past participle do not necessarily have to stand next to each other for Perfect constructions. We now lose track of one important information: from which sentence of the Presidential addresses was each construction token extracted? Any ideas how to solve all these issues? The following exercises will be devoted to these two important issues. Exercise 4.13 Please create a better regular expression to retrieve more tokens of English Perfect constructions, where the auxilliary and participle may not stand together. Exercise 4.14 Re-generate a result_perfect data frame, where you can keep track of: From the N-th sentence of the address did the Perfect come? (e.g., SENT_ID column below) From which president’s address did the Perfect come? (e.g., INDEX column below) You may have a data frame as shown below. Exercise 4.15 Re-create the above bar plot in a way that the type and token frequencies are computed based on each address and the x axis should be arranged accordingly (i.e., by the years and presidents). Your resulting graph should look similar to the one below. References Biber, D., Conrad, S., &amp; Cortes, V. (2004). If you look at…: Lexical bundles in university teaching and textbooks. Applied Linguistics, 25(3), 371–405. Gries, S. T. (2021a). Analyzing dispersion. In A practical handbook of corpus linguistics (pp. 99–118). Springer. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. "],["parts-of-speech-tagging.html", "Chapter 5 Parts-of-Speech Tagging 5.1 Installing the Package 5.2 Quick Overview 5.3 Quick Analysis 5.4 Working Pipeline 5.5 Parsing Your Texts 5.6 Metalinguistic Analysis 5.7 Construction Analysis 5.8 Issues on Pattern Retrieval 5.9 Saving POS-tagged Texts 5.10 Finalize spaCy 5.11 Notes on Chinese Processing 5.12 Useful Information", " Chapter 5 Parts-of-Speech Tagging library(tidyverse) library(tidytext) library(quanteda) In this chapter, we will use packages written in Python, which requires a proper Python environment set up in your current operating system. Please read Python Environment for more detail. In many textual analyses, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. In particular, I will introduce a powerful package spacyr, which is an R wrapper to the spaCy— “industrial strength natural language processing” Python library from https://spacy.io. In addition to POS tagging, the package provides other linguistically relevant annotations for more in-depth analysis of the English texts. The spaCy also supports many other languages, including Chinese. In this chapter, we will show you how to utilize spacyr to process Chinese texts. We will talk about Chinese text processing in a later chapter, Chinese Text Processing, in a more comprehensive way. 5.1 Installing the Package Please consult the spacyr github for more instructions on installing the package. There are at least four steps: Install and set up a python environment (e.g., miniconda, or Anaconda) that you would like to use in R. Because spacyr is an R wrapper to a Python package spaCy, we need to install the python module (and the language model files) in your python environment first. You can either install the Python library spacy and its language models with the normal installation of python modules, or install the python library from within R/RStudio (i.e., using spacyr). See below. Install the spacyr R package. install.packages(&quot;spacyr&quot;) It is recommended that you have Python 3.6+ and spacy 3+ at least in your operating system. If you have installed Anaconda and created python conda environments previously, you don’t have to install the python spaCy module within R. It’s better for you to install the python module in a pythonian way. You can install the python spacy and the language models in the python conda environment you would like to use in R. From within RStudio, you should be able to use spacy from that conda environment directly. If you haven’t used Python before and you don’t want to get invovled with Python too much, the easiest way to install Python spaCy is to install it in Rstudio through the R function spacyr::spacy_install(). This function automatically creates a new conda environment called spacy_condaenv by default, as long as some version of conda has been installed on the user’s the system. library(spacyr) spacy_install(version=&#39;3.2.4&#39;) ## Please note that there are big differences between spacy v2 and v3. The default settings of spacy_install() is to: create a stand-alone conda environment including a python executable separate from your system Python (or anaconda python); install the latest version of spaCy (and its required packages); download the English language model. Please check the documentation of spacy_instsall() if you would like to have more specific settings (e.g., spacy version, environment name, python version, language models etc.). To use the python module, spacy, in R can be tricky. The key is you need to have a running python kernel in your OS system. Therefore, Step 1 is very important. If you don’t have any conda version installed in your system, you can install Anaconda https://www.anaconda.com/products/individual or miniconda from https://conda.io/miniconda.html on your own (Choose the 64-bit version). Alternatively, the spacy_install() can also automatically install the miniconda (if there’s no conda installed in your system) for MAC users. Windows users may need to consult the spacyr github for more important instructions on installation (I am not sure). For Windows, you need to run RStudio as an administrator to make installation work properly. To do so, right click the RStudio icon (or R desktop icon) and select “Run as administrator” when launching RStudio. Restart R and Initialize spaCy in R In the following code, I initialize the spacyr python environment with my pre-configured conda environment (e.g., corpling). You may need to either specify your self-defined conda environment (or python virtual environment) or leave all the parameters with default values (i.e., spacy_initialize()). library(spacyr) spacy_initialize(model = &quot;en_core_web_sm&quot;, condaenv = &quot;corpling&quot;) successfully initialized (spaCy Version: 3.3.0, language model: en_core_web_sm) (python options: type = &quot;condaenv&quot;, value = &quot;corpling&quot;) # spacy_initialize() ## if you use the default spacyr settings reticulate::py_config() ## check ur python environment python: /Users/alvinchen/opt/anaconda3/envs/corpling/bin/python libpython: /Users/alvinchen/opt/anaconda3/envs/corpling/lib/libpython3.7m.dylib pythonhome: /Users/alvinchen/opt/anaconda3/envs/corpling:/Users/alvinchen/opt/anaconda3/envs/corpling version: 3.7.13 (default, Mar 28 2022, 07:24:34) [Clang 12.0.0 ] numpy: /Users/alvinchen/opt/anaconda3/envs/corpling/lib/python3.7/site-packages/numpy numpy_version: 1.21.6 NOTE: Python version was forced by use_python function R communicates with Python via the library reticulate. If you have installed the library, you can check the current Python path used in the RStudio: library(reticulate) ## use_condaenv(&quot;corpling&quot;, required=TRUE) py_config() 5.2 Quick Overview The spacyr provides a useful function, spacy_parse(), which allows us to parse an English text in a very convenient way. txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent 2.2 years and US$400,000 dollars in N. Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt, pos = T, # universal POS tag = T, # OntoNotes 5 POS lemma = T, # Lemma entity = T, # Named Entities dependency = T, # Dependency nounphrase = T) # NP Chunking parsedtxt The output of spacy_parse() is a data frame, which includes token-level annotations of the original texts at multiple levels. All texts have been tokenized into words with unique IDs (i.e., doc_id, sentence_id, token_id) The original word form is in the column of token. The lemma of each word token is provided in lemma. The POS tag of each word token is provided in pos and tag. pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that suffices most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set (cf. Penn Treebank Tagset) Depending on the argument setting for spacy_parse(), you can get more annotations, such as named entities (entity) and dependency relations (del_rel). In SpaCy, the English part-of-speech tagger uses the OntoNotes 5 version of the Penn Treebank tag set. It also maps the tags to the simpler Universal Dependencies v2 POS tag set. The following table shows the descriptions of the tag set. 5.3 Quick Analysis ## Entities entity_extract(parsedtxt) ## NP Chunks nounphrase_extract(parsedtxt) ## If you haven&#39;t installed `rsyntax` ## Uncomment and install ## install.packages(&quot;rsyntax&quot;) ## Dependency library(rsyntax) plot_tree( as_tokenindex(parsedtxt), doc_id = &quot;d1&quot;, token, lemma, pos, viewer_size = c(150, 150), textsize =1, spacing = 2, viewer_mode = T ) As you can see, with spacy_parse(), you can get a lot of annotations on the corpus data. However, you probably don’t need all of them. So you can specify only those annotations you need from spacy_parse() to save the computing time. Also, please check the documentation of spacy_parse() for more functions on the parameter additional_attributes =, which allows one to extract additional attributes of tokens from spaCy. For example, additional_attributes = c(\"like_num\", \"like_email\") gives you whether each token is a number/email or not. More details on token attributes can be found in spaCy API documentation. 5.4 Working Pipeline In Chapter 4, we provide a primitive working pipeline for text analytics. Here we like to revise the workflow to satisfy different goals in computational text analytics (See Figure 5.1). After we secure a collection of raw texts as our corpus, if we do not need additional parts-of-speech information, we follow the workflow on the right. If we need additional annotations from spacyr, we follow the workflow on the left. Figure 5.1: English Text Analytics Flowchart 5.5 Parsing Your Texts Now let’s use this spacy_parse() to analyze the presidential addresses we’ve seen in Chapter 4: the data_corpus_inaugural from quanteda. To illustrate the annotation more clearly, let’s parse the first text in data_corpus_inaugural: library(quanteda) library(tidytext) doc1 &lt;- spacy_parse(data_corpus_inaugural[1]) doc1 We can parse the whole corpus collection as well. The spacy_parse() can take a character vector as the input, where each element is a text/document of the corpus. system.time(corp_us_words &lt;- spacy_parse(data_corpus_inaugural)) user system elapsed 18.195 4.663 23.301 The function system.time() is a useful function which gives you the CPU times that the expression in the parenthesis used. In other words, you can put any R expression in the parenthesis of system.time() as its argument and measure the time required for the expression. This is sometimes necessary because some of the data processing can be very time consuming. And we would like to know HOW time-consuming it is in case that we may need to run the prodecure again. corp_us_words Exercise 5.1 In corpus linguistics analysis, we often need to examine constructions on a sentential level. It would be great if we can transform the word-based data frame into a sentence-based one for more efficient later analysis. Also, on the sentential level, it would be great if we can preserve the information of the lexical POS tags. How can you transform the corp_us_words into one as provided below? (You may name the sentence-based data frame as corp_us_sents.) 5.6 Metalinguistic Analysis Now spacy_parse() has enriched our corpus data with more linguistic annotations. We can now utilize the additional POS tags for more analysis. In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntactic complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct syntactic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s\\;C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_words and first generate the frequencies of verbs, and number of words for each presidential speech text. syn_com &lt;- corp_us_words %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) %&gt;% ungroup syn_com With the syntactic complexity of each president, we can plot the tendency: syn_com %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = &quot;none&quot;) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 5.2 Please add a regression/smooth line to the above plot to indicate the downward trend? Exercise 5.3 Extract all the subject-predicate word token pairs from the corpus of the presidential addresses. This relationship is defined as a syntactic dependency relationship of nsubj between two word tokens. The subject token is the dependent and the predicate token is the head. The output includes the subject-predicate token pairs and their relevant information (e.g., From which sentence of which presidential address is the pair extracted?) Please ignore the tagging errors made by the dependency parser in spacyr. 5.7 Construction Analysis Now with parts-of-speech tags, we are able to look at more linguistic patterns or constructions in detail. These POS tags allow us to extract more precisely the target patterns we are interested in. In this section, we will use the output from Exercise 5.1. We assume that now we have a sentence-based corpus data frame, corp_us_sents. Here I like to provide a case study on English Preposition Phrases. ## ###################################### ## If you haven&#39;t finished the exercise, ## the dataset is also available in ## `demo_data/corp_us_sents.RDS ## ###################################### ## Uncomment this line if you dont have `corp_us_sents` # corp_us_sents &lt;- readRDS(&quot;demo_data/corp_us_sents.RDS&quot;) corp_us_sents We can utilize the regular expressions to extract PREP + NOUN combinations from the corpus data. # define regex patterns pattern_pat1 &lt;- &quot;[^/ ]+/ADP [^/]+/NOUN&quot; # extract patterns from corp corp_us_sents %&gt;% unnest_tokens( output = pat_pp, input = sentence_tag, token = function(x) str_extract_all(x, pattern = pattern_pat1) ) -&gt; result_pat1 result_pat1 In the above example, we specify the token= argument in unnest_tokens(..., token = ...) with a self-defined (anonymous1) function. The idea of tokenization with unnest_tokens() is that the token argument can be any function which takes a text-based vector as input (i.e, each element of the input vector is a document string) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (cf. Figure 5.2). Figure 5.2: Intuition for token= in unnest_tokens() In the above example, we define a tokenization function: function(x) str_extract_all(x, pattern = pattern_pat1) function(x) { str_extract_all(x, pattern = pattern_pat1) } This function takes a vector, sentence_tag, as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions from each sentence in sentence_tag. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Exercise 5.4 Create a new column, pat_clean, with all annotations removed in the data frame result_pat1. With these constructional tokens of English PP’s, we can then do further analysis. We first identify the PREP and NOUN for each constructional token. We then clean up the data by removing POS annotations. # extract the prep and head result_pat1 %&gt;% tidyr::separate(col=&quot;pat_pp&quot;, into=c(&quot;PREP&quot;,&quot;NOUN&quot;), sep=&quot;\\\\s+&quot; ) %&gt;% mutate(PREP = str_replace_all(PREP, &quot;/[^ ]+&quot;,&quot;&quot;), NOUN = str_replace_all(NOUN, &quot;/[^ ]+&quot;,&quot;&quot;)) -&gt; result_pat1a result_pat1a Now we are ready to explore the text data. We can look at how each preposition is being used by different presidents: result_pat1a %&gt;% count(doc_id, PREP) %&gt;% arrange(doc_id, desc(n)) We can examine the most frequent NOUN that co-occurs with each PREP: # Most freq NOUN for each PREP result_pat1a %&gt;% count(PREP, NOUN) %&gt;% group_by(PREP) %&gt;% top_n(1,n) %&gt;% arrange(desc(n)) We can also look at a more complex usage pattern: how each president uses the PREP of in terms of their co-occurring NOUNs? # NOUNS for `of` uses across different presidents result_pat1a %&gt;% filter(PREP == &quot;of&quot;) %&gt;% count(doc_id, PREP, NOUN) %&gt;% tidyr::pivot_wider( id_cols = c(&quot;doc_id&quot;), names_from = &quot;NOUN&quot;, values_from = &quot;n&quot;, values_fill = list(n=0)) Exercise 5.5 In our earlier demonstration, we made a naive assumption: Preposition Phrases include only those cases where PREP and NOUN are adjacent to each other. But there are many more tokens where words do come between the PREP and the NOUN (e.g., with greater anxieties, by your order). Please revise the regular expression to improve the retrieval of the English Preposition Phrases from the corpus data corp_us_sents. Specifically, we can define an English PP as a sequence of words, which start with a preposition, and end at the first word after the preposition that is tagged as NOUN, PROPN, or PRON. Exercise 5.6 Based on the output from Exercise 5.5, please identify the PREP and NOUN for each constructional token and save information in two new columns. 5.8 Issues on Pattern Retrieval Any automatic pattern retrieval comes with a price: there are always errors returned by the system. I would like to discuss this issue based on the second text, 1793-Washington. First let’s take a look at the Preposition Phrases extracted by my regular expression used in Exercise 5.5 and 5.6: ## ###################################### ## If you haven&#39;t finished the exercise, ## the dataset is also available in ## `demo_data/result_pat2a.RDS ## ###################################### ## uncomment this line if you dont have `result_pat2a` # result_pat2a &lt;- readRDS(&quot;demo_data/result_pat2a.RDS&quot;) result_pat2a %&gt;% filter(doc_id == &quot;1793-Washington&quot;) My regular expression has identified 20 PP’s from the text. However, if we go through the text carefully and do the PP annotation manually, we may have different results. Figure 5.3: Manual Annotation of English PP’s in 1793-Washington There are two types of errors: False Positives: Patterns identified by the system (i.e., regular expression) but in fact they are not true patterns (cf. blue in Figure 5.3). False Negatives: True patterns in the data but are not successfully identified by the system (cf. green in Figure 5.3). As shown in Figure 5.3, manual annotations have identified 21 PP’s from the text while the regular expression identified 20 tokens. A comparison of the two results shows that: In the regex result, the following returned tokens (rows highlighted in blue) are False Positives—the regular expression identified them as PP but in fact they were NOT PP according to the manual annotations. doc_id sentence_id PREP NOUN pat_pp row_id 1793-Washington 1 by voice by/adp the/det voice/noun 1 1793-Washington 1 of my of/adp my/pron 2 1793-Washington 1 of its of/adp its/pron 3 1793-Washington 2 for it for/adp it/pron 4 1793-Washington 2 of honor of/adp this/det distinguished/adj honor/noun 5 1793-Washington 2 of confidence of/adp the/det confidence/noun 6 1793-Washington 2 in me in/adp me/pron 7 1793-Washington 2 by people by/adp the/det people/noun 8 1793-Washington 2 of united of/adp united/propn 9 1793-Washington 3 to execution to/adp the/det execution/noun 10 1793-Washington 3 of act of/adp any/det official/adj act/noun 11 1793-Washington 3 of president of/adp the/det president/propn 12 1793-Washington 3 of office of/adp office/noun 13 1793-Washington 4 in your in/adp your/pron 14 1793-Washington 4 during my during/adp my/pron 15 1793-Washington 4 of government of/adp the/det government/propn 16 1793-Washington 4 in instance in/adp any/det instance/noun 17 1793-Washington 4 to upbraidings to/adp the/det upbraidings/noun 18 1793-Washington 4 of all of/adp all/pron 19 1793-Washington 4 of ceremony of/adp the/det present/adj solemn/adj ceremony/noun 20 In the above manual annotation (Figure 5.3), phrases highlighted in green are NOT successfully identified by the current regex query, i.e., False Negatives. We can summarize the pattern retrieval results as: Most importantly, we can describe the quality/performance of the pattern retrieval with two important measures. \\(Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Positives}\\) \\(Recall = \\frac{True\\;Positives}{True\\;Positives + False\\;Negatives}\\) In our case: \\(Precision = \\frac{16}{16+4} = 80.00%\\) \\(Recall = \\frac{16}{16 +5} = 76.19%\\) It is always very difficult to reach 100% precision or 100% recall for automatic retrieval of the target patterns. Researchers often need to make a compromise. The following are some heuristics based on my experiences: For small datasets, probably manual annotations give the best result. For moderate-sized datasets, semi-automatic annotations may help. Do the automatic annotations first and follow up with manual checkups. For large datasets, automatic annotations are preferred in order to examine the general tendency. However, it is always good to have a random sample of the data to evaluate the effectiveness of the pattern matching. The more semantics-related the annotations, the more likely one would adopt a manual approach to annotation (e.g., conceptual metaphors, sense distinctions, dialogue acts). Common annotations of corpus data may prefer an automatic approach, such as Chinese word segmentation, POS tagging, named entity recognition, chunking, noun-phrase extractions, or dependency relations(?). In medicine, there are two similar metrics used for the assessment of the diagnostic medical tests—sensitivity (靈敏度) and specificity (特異性). Sensitivity refers to the proportion of true positives that are correctly identified by the medical test. This is indeed the recall rates we introduced earlier. \\(Sensitivity = \\frac{True\\;Positives}{True\\;Positives + False\\;Negatives}\\) Specificity refers to the proportion of true negatives that are correctly identified by the medical test. It is computed as follows: \\(Specificity = \\frac{True\\;Negatives}{False\\;Positives + True\\;Negatives}\\) In plain English, the sensitivity of a medical test indicates the percentage of sick people who are correctly identified as having the disease; the specificity of a medical test indicates the percentage of healthy people who are correctly identified as healthy (i.e., not having the disease). Take the pandemic COVID19 for example. If one aims to develop an effective antigen rapid test for COVID19, which metric would be a more reliable and crucial evaluation metric for the quality of the test? Exercise 5.7 Please extract all English preposition phrases from the presidential addresses by making use of the dependency parsing provided in spacyr. Also, please discuss whether this dependency-based method improves the quality of pattern retrieval with the 1793-Washington text as an example. In particular, please discuss the precision and recall rates of the dependency-based method and our previous POS-based method. Dependency-based results on 1793-Washington are provided below. 5.9 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time we analyze the data, it would be more convenient if we save the tokenized texts with the POS tags in external files. Next time we can directly load these files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. A few suggestions: If you are dealing with a small corpus, I would probably suggest you to save the resulting data frame from spacy_parse() as a csv for later use. If you are dealing with a big corpus, I would probably suggest you to save the parsed output of each text file in an independent csv for later use. write_csv(corp_us_words, &quot;corp-inaugural-word.csv&quot;) 5.10 Finalize spaCy While running spaCy on Python through R, a Python process is always running in the background and R session will take up a lot of memory (typically over 1.5GB). spacy_finalize() terminates the Python process and frees up the memory it was using. spacy_finalize() Also, if you would like to use other spacy language models in the same R session, or change the default Python environment in the same R session, you can first use spacy_finalize() to quit from the current Python environment. 5.11 Notes on Chinese Processing The library spacyr supports Chinese processing as well. The key is you need to download the Chinese language model from the original spaCy and make sure that the language model is accessible in the python environment you are using in R. ############################ ### Chinese Processing ## ############################ library(spacyr) spacy_initialize(model = &quot;zh_core_web_sm&quot;, condaenv = &quot;corpling&quot;) reticulate::py_config() ## check ur python environment python: /Users/alvinchen/opt/anaconda3/envs/corpling/bin/python libpython: /Users/alvinchen/opt/anaconda3/envs/corpling/lib/libpython3.7m.dylib pythonhome: /Users/alvinchen/opt/anaconda3/envs/corpling:/Users/alvinchen/opt/anaconda3/envs/corpling version: 3.7.13 (default, Mar 28 2022, 07:24:34) [Clang 12.0.0 ] numpy: /Users/alvinchen/opt/anaconda3/envs/corpling/lib/python3.7/site-packages/numpy numpy_version: 1.21.6 NOTE: Python version was forced by use_python function txt_ch &lt;- c(d1 = &quot;2022年1月7日 週五 上午4:42·1 分鐘 (閱讀時間) 桃園機場群聚感染案件確診個案增，也讓原本近日展開的尾牙餐會受到波及，桃園市某一家飯店不斷接到退訂，甚至包括住房、圍爐等，也讓飯店人員感嘆好不容易看見有點復甦景象，一下子又被疫情打亂。（李明朝報導）&quot;, d2 = &quot;桃園機場群聚感染，隨著確診個案增加，疫情好像短時間無法終結，原本期待在春節期間能夠買氣恢復的旅宿業者，首當其衝大受波及，桃園市一家飯店人員表示，「1月6日就開始不斷接到消費者打來電話，包括春酒尾牙、圍爐桌席和住房，其中單單一個上午就有約20桌宴席、近百間房取消或延期。&quot;) parsedtxt_ch &lt;- spacy_parse(txt_ch, pos = T, tag = T, # lemma = T, entity = T, dependency = T) parsedtxt_ch With the enriched information of the text, we can perform more in-depth exploration of the text. For example, identify useful semantic information from the named entities in the text. ## Extract named entities (dates, events, and cardinal or ordinal quantities) entity_extract(parsedtxt_ch, type=&quot;all&quot;) ## Compound multiword entities into single tokens entity_consolidate(parsedtxt_ch) spacy_finalize() 5.12 Useful Information In order to make good use of all the functionalities provided by spacy, one may need to have clearer understanding of all the annotations and the meanings of the tags at different linguistic levels (e.g., POS tags, Universal POS Tags, Dependency Tags etc.). Most importantly, the tag sets may vary from language to language. For more detail, please refer to this glossary.py from the original spacy module. In general, here are a few common annotation schemes: Universal POS Tags POS Tags (Chinese): OntoNotes Chinese Penn Treebank POS tags (English): OntoNotes 5 Penn Treebank Dependency Labels (English): ClearNLP / Universal Dependencies Named Entity Recognition: OntoNotes 5 Exercise 5.8 Use the script of PTT Crawler you created in the previous chapter and collect your corpus data from the Gossiping Board. With your corpus data collected, please present the corpus size in terms of word numbers. Also, please create a word cloud for your corpus by including only disyllabic words whose parts of speech fall into the following categories: nouns and verbs. Exercise 5.9 In this exercise, please use the corpus data provided in quanteda.textmodels::data_corpus_moviereviews. This dataset is provided as a corpus object in the package quanteda.textmodels (please install the package on your own). The data_corpus_moviereviews includes 2,000 movie reviews. Please use the spacyr to parse the texts and provide the top 20 adjectives for positive and negative reviews respectively. Adjectives are naively defined as any words whose pos tags start with “J” (please use the fine-grained version of the POS tags. i.e., tag, from spacyr). When computing the word frequencies, please use the lemmas instead of the word forms. Please provide the top 20 words that are content words for positive and negative reviews ranked by a weighted score, which is computed using the formula provided below. Content words are naively defined as any words whose pos tags start with N, V, or J. (In my results below, there is one additional criterion for word selection: words whose first letter starts with a non-alphanumeric character are removed from the frequency list.) \\[Word\\;Frequency \\times log(\\frac{Numbe\\; of \\; Documents}{Word\\;Diserpsion}) \\] For example, if the lemma action occurs 691 times in the negative reviews collection. These occurrences are scattered in 337 different documents. There are 1000 negative texts in the current corpus. Then the weighted score for action is: \\[691 \\times log(\\frac{1000}{337}) = 751.58 \\] summary(quanteda.textmodels::data_corpus_moviereviews) ans1 ans2 In our earlier chapters, we have discussed the issues of word frequencies and their significance in relation to the dispersion of the words in the entire corpus. In terms of identifying important words from a text collection, our assumption is that: if a word is scattered in almost every document in the corpus collection, it is probably less informative. For example, words like a, the would probably be observed in every document in the corpus. Therefore, the high frequencies of these widely-dispersed words may not be as important compared to the high frequencies of those which occur in only a subset of the corpus collection. The word frequency is sometimes referred to as term frequency (tf) in information retrieval; the dispersion of the word is referred to as document frequency (df). In information retrieval, people often use a weighting scheme for word frequencies in order to extract informative words from the text collection. The scheme is as follows: \\[tf \\times log(\\frac{N}{df}) \\] N refers to the total number of documents in the corpus. The \\(log\\frac{N}{df}\\) is referred to as inversed document frequency (idf). This tf.idf weighting scheme is popular in many practical applications. The smaller the df of a word, the higher the idf, the larger the weight for its tf. "],["keyword-analysis.html", "Chapter 6 Keyword Analysis 6.1 About Keywords 6.2 Statistics for Keyness 6.3 Implementation 6.4 Tidy Data (Self-Study) 6.5 Word Frequency Transformation 6.6 Computing Keynesss 6.7 Conclusion", " Chapter 6 Keyword Analysis In this chapter, I would like to talk about the idea of keywords. Keywords in corpus linguistics are defined statistically using different measures of keyness. Keyness can be computed for words occurring in a target corpus by comparing their frequencies (in the target corpus) to the frequencies in a reference corpus. In other words, the idea of “keyness” is to evaluate whether the word occurs more frequently in the target corpus as compared to its occurrence in the reference corpus. If yes, the word may be a key term of the target corpus. We can quantify the relative attraction of each word to the target corpus by means of a statistical association metric. This idea of course can be extended to key phrases as well. Therefore, for keyword analysis, we assume that there is a reference corpus on which the keyness of the words in the target corpus is computed and evaluated. 6.1 About Keywords Qualitative Approach Keywords are important for research on language and ideology. Most researchers draw inspiration from Raymond Williams’s idea of keywords, which he defines as terms presumably carrying socio-cultural meanings characteristic of (Western capitalist) ideologies (Williams, 1976). Keywords in Williams’ study were determined based on the subjective judgement of the socio-cultural meanings of the predefined list of words (e.g., art, bureaucracy, educated, welfare, violence, and many others). Quantitative Approach In contrast to William’s intuition-based approach, recent studies have promoted a bottom-up corpus-based method to discover key terms reflecting the ideological undercurrents of particular text collections. This data-driven approach to keywords is sympathetic to the notion of statistical keywords popularized by Michael Stubbs (Stubbs, 1996, 2003) (with his famous toolkit, Wordsmith) For a comprehensive discussion on the statistical nature of keyword analysis, please see Gabrielatos (2018). 6.2 Statistics for Keyness To compute the keyness of a word w, we need two frequency numbers: the frequency of w in the target corpus vs. the frequency of w in the reference corpus. These frequencies are often included in a contingency table as shown in Figure 6.1: Figure 6.1: Frequency distributions of a word and all other words in two corpora What are the important factors that may be connected to the significance of the frequencies of the word in two corpora? the frequency of the word w in general the sizes of the target/reference corpus In other words, the marginal frequencies of the contingency table are crucial to determining the significance of the word frequencies in two corpora. Different keyness statistics may have different ways to evaluate the relative importance of the co-occurrences of the word w with the target and the reference corpus (i.e., a and b in Figure 6.1) and statistically determine which connection is stronger. In this chapter, I would like to discuss three common statistics used in keyness analysis. This tutorial is based on Gries (2016), Ch. 5.2.6. Log-likelihood Ratio (G2) (Dunning, 1993); \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Difference Coefficient (Leech &amp; Fallon, 1992); \\[ difference\\;coefficient = \\frac{a - b}{a + b} \\] Relative Frequency Ratio (Damerau, 1993) \\[ rfr = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] 6.3 Implementation In this tutorial we will use two documents as our mini reference and target corpus. These two documents are old Wikipedia entries (provided in Gries (2016)) on Perl and Python respectively. First we initialize necessary packages in R library(tidyverse) library(tidytext) library(readtext) library(quanteda) Then we load the corpus data, which are available as two text files in our demo_data, and transform the corpus into a tidy structure flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) %&gt;% select(textid, text) We convert the text-based corpus into a word-based data frame, which allows us to easily extract word frequency information corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% head(100) Exercise 6.1 We mentioned this before. It is always good to keep track of the relative positions of the word in the original text. Please create a column in corpus_word, indicating the the relative position of each word in the text with unique word_id. (NB: Only the first 100 rows are shown here.) Now we need to get the frequencies of each word in the two corpora respectively. corpus_word %&gt;% count(word, textid, sort=T) -&gt; word_freq word_freq As now we are analyzing each word in relation to the two corpora, it would be better for us to have each word type as one independent row, and columns recording their co-occurrence frequencies with the two corpora (i.e., target and reference). How to achieve this? 6.4 Tidy Data (Self-Study) Now I would like to talk about the idea of tidy dataset before we move on. Wickham &amp; Grolemund (2017) suggests that a tidy dataset needs to satisfy the following three interrelated principles: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In our current word_freq, our observation in each row is a word. This is OK because a tidy dataset needs to have every independent word (type) as an independent row in the table. However, there are two additional issues with our current data frame word_freq: Each row in word_freq in fact represents the combination of two factors, i.e., word, textid. In addition, the column n contains the token frequencies for each level combination of these two (nominal) variables. The same word type appears twice in the dataset in the rows (e.g., the, a) This is the real life: we often encounter datasets that are NOT tidy at all. Instead of expecting others to always provide you a perfect tidy dataset for analysis (which is very unlikely), we might as well learn how to deal with messy dataset. Wickham &amp; Grolemund (2017) suggest two common strategies that data scientists often apply: Long-to-Wide: One variable might be spread across multiple columns Create more variables/columns based on one old variable Wide-to-Long: One observation might be scattered across multiple rows Reduce several variables/columns by collapsing them into levels of a new variable 6.4.1 An Long-to-Wide Example Here I would like to illustrate the idea of Long-to-Wide transformation with a simple dataset from Wickham &amp; Grolemund (2017), Chapter 12. people &lt;- tribble( ~name, ~profile, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) people The above dataset people is not tidy because: the column profile contains more than one variable; an observation (e.g., Phillip Woods) is scattered across several rows. To tidy up people, we can apply the Long-to-Wide strategy: One variable might be spread across multiple columns The function tidyr::pivot_wider() is made for this. There are two important parameters in pivot_wider(): names_from = ...: The column from which we create new variable names. Here it’s profile. values_from = ...: The column from which we take values for new columns. Here it’s values. Figure 6.2: From Long to Wide: pivot_wider() We used pivot_wider() to transform people into a wide-format data frame. require(tidyr) people %&gt;% pivot_wider(names_from = profile, values_from = values) 6.4.2 A Wide-to-Long Example Now let’s take a look at an example of the second strategy, Long-to-Wide transformation. When do we need this? This type of data transformation is often needed when you see that some of your columns/variables are in fact connected to the same factor. That is, these columns in fact can be considered levels of another underlying factor. Take the following simple case for instance. preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) preg The dataset preg has three columns: pregnant, male, and female. Of particular interest here are the last two columns. It is clear that male and female can be considered levels of another underlying factor, i.e., gender. More specifically, the above dataset preg can be tidied up as follows: we can have a column gender we can have a column pregnant we can have a column count (representing the number of observations for the combinations of gender and pregnant) In other words, we need the Wide-to-Long transformation: One observation might be scattered across multiple rows. The function tidyr::pivot_longer() is made for this. There are three important parameters: cols = ...: The set of columns whose names are levels of an underlying factor. Here they are male and female. names_to = ...: The name of the new underlying factor. Here it is gender. values_to = ...: The name of the new column for values of level combinations. Here it is count. Figure 6.3: From Wide to Long: pivot_longer() We applied the Wide-to-Long transformation to preg: preg %&gt;% pivot_longer(cols=c(&quot;male&quot;,&quot;female&quot;), names_to = &quot;gender&quot;, values_to = &quot;count&quot;) 6.5 Word Frequency Transformation Now back to our word_freq: word_freq %&gt;% head(10) It is probably clearer to you now what we should do to tidy up word_freq. It is obvious that some words (observations) are scattered across several rows. The column textid can be pivoted into two variables: “perl” vs. “python”. In order words, we need strategy of Long-to-Wide. One variable might be spread across multiple columns We transformed our data accordingly. word_freq_wide &lt;- word_freq %&gt;% pivot_wider(names_from = &quot;textid&quot;, values_from = &quot;n&quot;) head(word_freq_wide) Exercise 6.2 In the above Long-to-Wide transformation, there is still one problem. There are quite a few words that occur in one corpus but not the other. For these words, their frequencies would be a NA because R cannot allocate proper values for these unseen cases. Could you fix this problem by assigning these unseen cases a 0 when transforming the data frame? Please name the updated wide version of the word frequency data frame as contingency_table. Hint: Please check the argument pivot_wider(..., values_fill = ...) Problematic unseen cases in one of the corpora: word_freq_wide %&gt;% filter(is.na(corp_perl.txt) | is.na(corp_python.txt)) Updated contingency_table: contingency_table Before we compute the keyness, we preprocess the data by: including words consisting of alphabets only; renaming the columns to match the cell labels in Figure 6.1 above; creating necessary frequencies (columns) for keyness computation contingency_table %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table 6.6 Computing Keynesss With all necessary frequencies, we can now compute the three keyness statistics for each word. contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) %&gt;% mutate_if(is.numeric, round, 2) -&gt; keyness_table keyness_table Although now we have the keyness values for words, we still don’t know to which corpus (target or reference corpus) the word is more attracted. What to do next? keyness_table %&gt;% mutate(preference = ifelse(a &gt; a.exp, &quot;perl&quot;,&quot;python&quot;)) %&gt;% select(word, preference, everything())-&gt; keyness_table keyness_table keyness_table %&gt;% group_by(preference) %&gt;% top_n(10, G2) %&gt;% select(preference, word, G2) %&gt;% arrange(preference, -G2) 6.7 Conclusion Keyness discussed in this chapter is based on the comparative study of word frequencies in two separate corpora: one as the target and the other as the reference corpus. Therefore, the statistical keyness found in the word distribution is connected to the difference between the target and reference corpus (i.e., their distinctive features). These statistical keywords should be therefore interpreted along with the distinctive feature(s) of the target and reference corpus. If the text collections in the target and reference corpus differ significantly in different their time periods, then the keywords may reflect important terms prominent in specific time periods (i.e., diachronic studies). If the text collections differ significantly in genres/registers, then the keywords may reflect important terms tied to particular genres/registers. If the target corpus is a collection of L2 texts and the reference corpus is a collection of L1 texts, keywords extracted may be indicative of expressions preferred by L1/L2 learners (See Bestgen (2017)). Please note that the keyness discussed in this chapter is different from the tf.idf (we used in the previous chapter), which is designed to reflect how important a word is to a document in a corpus. tf.idf is more often used in information retrieval. Important words of a document are selected based on the tf.idf weighting scheme; the relevance of the document to the user’s query is determined based on how close the search word(s) is to the important words of the document. Exercise 6.3 The CSV in demo_data/data-movie-reviews.csv is the IMDB dataset with 50,000 movie reviews and their sentiment tags (source: Kaggle). The CSV has two columns—the first column review includes the raw texts of each movie review; the second column sentiment provides the sentiment tag for the review. Each review is either positive or negative. We can treat the dataset as two separate corpora: negative and positive corpora. Please find the top 10 keywords for each corpus ranked by the G2 statistics. In the data preprocessing, please use the default tokenization in unnest_tokens(..., token = \"words\"). When computing the keyness, please exclude: words with at least one non-alphanumeric symbols in them (e.g. regex class \\W) words whose frequency is &lt; 10 in each corpus The expected results are provided below for your reference. The first six rows of demo_data/data-movie-reviews.csv: Sample result: References Bestgen, Y. (2017). Beyond single-word measures: L2 writing assessment, lexical richness and formulaic competence. System, 69, 65–78. Damerau, F. J. (1993). Generating and evaluating domain-oriented multi-word terms from texts. Information Processing &amp; Management, 29(4), 433–447. Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 61–74. https://www.aclweb.org/anthology/J93-1003 Gabrielatos, C. (2018). Keyness analysis: Nature, metrics and techniques. In Corpus approaches to discourse (pp. 225–258). Routledge. Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Leech, G., &amp; Fallon, R. (1992). Computer corpora–what do they tell us about culture. ICAME Journal, 16. Stubbs, M. (1996). Text and corpus analysis. Blackwell. Stubbs, M. (2003). Words and phrases. Blackwell. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. Williams, R. (1976). Keywords. Oxford University Press. "],["chinese-text-processing.html", "Chapter 7 Chinese Text Processing 7.1 Chinese Word Segmenter jiebaR 7.2 Chinese Text Analytics Pipeline 7.3 Data 7.4 Loading Text Data 7.5 Quanteda-based Exploration 7.6 Initialize jiebaR 7.7 Case Study 1: Concordances with kwic() 7.8 Case Study 2: Word Frequency and Wordcloud 7.9 Case Study 3: Patterns 7.10 Case Study 4: Lexical Bundles 7.11 Recaps 7.12 More Exercises", " Chapter 7 Chinese Text Processing In this chapter, we will turn to the topic of Chinese text processing. In particular, we will discuss one of the most important issues in Chinese language processing, i.e., word segmentation. When we discuss English parts-of-speech tagging in Chapter 5, it is easy to perform (word) tokenization on English texts because the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. In later Chapter 9, we will introduce another segmenter developed by the CKIP Group at the Academia Sinica. The CKIP Tagger seems to be the state-of-art tagger for Taiwan Mandarin, i.e., with more additional functionalities. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 7.1 Chinese Word Segmenter jiebaR 7.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: Initialize a jiebar object using worker() Tokenize the texts into words using the function segment() with the designated jiebar object created earlier seg1 &lt;- worker() segment(text, jiebar = seg1) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(seg1) [1] &quot;jiebar&quot; &quot;segment&quot; &quot;jieba&quot; To word-tokenize the document, text, you first initialize a jiebar object, i.e., seg1, using worker() and feed this jiebar to segment(jiebar = seg1)and tokenize text into words. 7.1.2 Parameters Setting There are many different parameters you can specify when you initialize the jiebar object. You may get more detail via the documentation ?worker(). Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) Exercise 7.1 In our earlier example, when we created the jiebar object named seg1, we did not specify any arguments for worker(). Can you tell what the default settings are for the parameters of worker()? Please try to create worker() with different settings (e.g., symbols = T, bylines = T) and see how the tokenization results differ from each other. 7.1.3 User-defined dictionary From the above example, it is clear to see that some of the words have not been correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when tokenizing your texts because different corpora may have their own unique vocabulary (i.e., domain-specific lexicon). This can be done with the argument user = ... when you initialize the jiebar object, i.e, worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) segment(text, seg2) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a Chinese txt file created by Notepad may not be UTF-8. (Usually, it is encoded in big-5). Also, files created by MS Office applications tend to be less transparent in terms of their encoding. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the conversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 7.1.4 Stopwords When you initialize the worker(), you can also specify a stopword list, i.e., words that you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative, thus often excluded in the process of preprocessing. seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; [13] &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; [19] &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; [25] &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; [31] &quot;走&quot; &quot;不要&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; Exercise 7.2 How do we quickly check which words in segment(text, seg2) were removed as compared to the results of segment(text, seg3)? (Note: seg2 and seg3 only differ in the stop_word=... argument.) [1] &quot;被&quot; &quot;日&quot; &quot;是&quot; &quot;都&quot; &quot;把&quot; 7.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = \"tag\" when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, #dict = &quot;demo_data/jieba-tw/dict.txt&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;, symbol = F) segment(text, seg4) n ns n n n n n &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; n v n n n x x &quot;不分區&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; d v x n x n n &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ns n n x v x zg &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; p n v df n x r &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; a &quot;壞&quot; The returned object is a named character vector, i.e., the POS tags of the words are included in the names of the vectors. Every POS tagger has its own predefined tag set. The following table lists the annotations of the POS tag set used in jiebaR: Exercise 7.3 How do we convert the named word vector with POS tags returned by segment(text, seg4) into a long string as shown below? [1] &quot;綠黨/n 桃園市/ns 議員/n 王浩宇/n 爆料/n 指/n 民眾黨/n 不分區/n 提名/v 人/n 蔡壁如/n 黃瀞瑩/n 在昨/x 6/x 才/d 請辭/v 為領/x 年終獎金/n 台灣/x 民眾黨/n 主席/n 台北/ns 市長/n 柯文哲/n 7/x 受訪/v 時則/x 說/zg 按/p 流程/n 走/v 不要/df 人家/n 想得/x 這麼/r 壞/a&quot; 7.1.6 Default Word Lists in JiebaR You can check the dictionaries and the stopword list being used by jiebaR in your current environment: # show files under `dictpath` dir(show_dictpath()) [1] &quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/jiebaRD/dict&quot; [1] &quot;backup.rda&quot; &quot;hmm_model.zip&quot; &quot;idf.zip&quot; &quot;jieba.dict.zip&quot; [5] &quot;model.rda&quot; &quot;README.md&quot; &quot;stop_words.utf8&quot; &quot;user.dict.utf8&quot; # Check the default stop_words list # Please change the path to your default dict path # scan(file=&quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, # what=character(),nlines=50,sep=&#39;\\n&#39;, # encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) readLines(&quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, encoding = &quot;UTF-8&quot;, n = 200) [1] &quot;\\&quot;&quot; &quot;.&quot; &quot;。&quot; &quot;,&quot; &quot;、&quot; &quot;！&quot; &quot;？&quot; &quot;：&quot; [9] &quot;；&quot; &quot;`&quot; &quot;﹑&quot; &quot;•&quot; &quot;＂&quot; &quot;^&quot; &quot;…&quot; &quot;‘&quot; [17] &quot;’&quot; &quot;“&quot; &quot;”&quot; &quot;〝&quot; &quot;〞&quot; &quot;~&quot; &quot;\\\\&quot; &quot;∕&quot; [25] &quot;|&quot; &quot;¦&quot; &quot;‖&quot; &quot;— &quot; &quot;(&quot; &quot;)&quot; &quot;〈&quot; &quot;〉&quot; [33] &quot;﹞&quot; &quot;﹝&quot; &quot;「&quot; &quot;」&quot; &quot;‹&quot; &quot;›&quot; &quot;〖&quot; &quot;〗&quot; [41] &quot;】&quot; &quot;【&quot; &quot;»&quot; &quot;«&quot; &quot;』&quot; &quot;『&quot; &quot;〕&quot; &quot;〔&quot; [49] &quot;》&quot; &quot;《&quot; &quot;}&quot; &quot;{&quot; &quot;]&quot; &quot;[&quot; &quot;﹐&quot; &quot;¸&quot; [57] &quot;﹕&quot; &quot;︰&quot; &quot;﹔&quot; &quot;;&quot; &quot;！&quot; &quot;¡&quot; &quot;？&quot; &quot;¿&quot; [65] &quot;﹖&quot; &quot;﹌&quot; &quot;﹏&quot; &quot;﹋&quot; &quot;＇&quot; &quot;´&quot; &quot;ˊ&quot; &quot;ˋ&quot; [73] &quot;-&quot; &quot;―&quot; &quot;﹫&quot; &quot;@&quot; &quot;︳&quot; &quot;︴&quot; &quot;_&quot; &quot;¯&quot; [81] &quot;＿&quot; &quot;￣&quot; &quot;﹢&quot; &quot;+&quot; &quot;﹦&quot; &quot;=&quot; &quot;﹤&quot; &quot;‐&quot; [89] &quot;&lt;&quot; &quot;­&quot; &quot;˜&quot; &quot;~&quot; &quot;﹟&quot; &quot;#&quot; &quot;﹩&quot; &quot;$&quot; [97] &quot;﹠&quot; &quot;&amp;&quot; &quot;﹪&quot; &quot;%&quot; &quot;﹡&quot; &quot;*&quot; &quot;﹨&quot; &quot;\\\\&quot; [105] &quot;﹍&quot; &quot;﹉&quot; &quot;﹎&quot; &quot;﹊&quot; &quot;ˇ&quot; &quot;︵&quot; &quot;︶&quot; &quot;︷&quot; [113] &quot;︸&quot; &quot;︹&quot; &quot;︿&quot; &quot;﹀&quot; &quot;︺&quot; &quot;︽&quot; &quot;︾&quot; &quot;_&quot; [121] &quot;ˉ&quot; &quot;﹁&quot; &quot;﹂&quot; &quot;﹃&quot; &quot;﹄&quot; &quot;︻&quot; &quot;︼&quot; &quot;的&quot; [129] &quot;了&quot; &quot;the&quot; &quot;a&quot; &quot;an&quot; &quot;that&quot; &quot;those&quot; &quot;this&quot; &quot;that&quot; [137] &quot;$&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; [145] &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;?&quot; &quot;_&quot; &quot;“&quot; &quot;”&quot; &quot;、&quot; [153] &quot;。&quot; &quot;《&quot; &quot;》&quot; &quot;一&quot; &quot;一些&quot; &quot;一何&quot; &quot;一切&quot; &quot;一则&quot; [161] &quot;一方面&quot; &quot;一旦&quot; &quot;一来&quot; &quot;一样&quot; &quot;一般&quot; &quot;一转眼&quot; &quot;万一&quot; &quot;上&quot; [169] &quot;上下&quot; &quot;下&quot; &quot;不&quot; &quot;不仅&quot; &quot;不但&quot; &quot;不光&quot; &quot;不单&quot; &quot;不只&quot; [177] &quot;不外乎&quot; &quot;不如&quot; &quot;不妨&quot; &quot;不尽&quot; &quot;不尽然&quot; &quot;不得&quot; &quot;不怕&quot; &quot;不惟&quot; [185] &quot;不成&quot; &quot;不拘&quot; &quot;不料&quot; &quot;不是&quot; &quot;不比&quot; &quot;不然&quot; &quot;不特&quot; &quot;不独&quot; [193] &quot;不管&quot; &quot;不至于&quot; &quot;不若&quot; &quot;不论&quot; &quot;不过&quot; &quot;不问&quot; &quot;与&quot; &quot;与其&quot; By default, when you apply jieba word segmentation, the default stop word list is NOT used. If you like to use the jieba default stop word list, you may need to convert it into a traditional Chinese version before using it. You can also download the traditional Chinese version from demo_data/stopwords-ch-jiebar-zht.txt. The following codes show you how to do the conversion and apply the stop word list to the word segmentation. # use the jiebar default stopword list ## convert jiebar default stopword into traditional zh ## Please uncomment the following line if you need to install the package # devtools::install_github(&quot;Lchiffon/ropencc&quot;) library(ropencc) ccst = converter(S2T) ccst[&quot;开放中文转换&quot;] [1] &quot;開放中文轉換&quot; ccts = converter(T2S) ccts[&quot;開放中文轉換&quot;] [1] &quot;开放中文转换&quot; st_zhs &lt;- readLines(&quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, encoding = &quot;UTF-8&quot;) st_zht&lt;-ccst[st_zhs] con &lt;- file(&quot;demo_data/stopwords-ch-jiebar-zht.txt&quot;, open = &quot;w&quot;, encoding = &quot;utf-8&quot;) writeLines(st_zht, con) close(con) ## Use the default stopword list seg5 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-jiebar-zht.txt&quot; ) segment(text, seg5) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;提名&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; [13] &quot;日&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; [19] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;日&quot; &quot;受訪&quot; [25] &quot;時則&quot; &quot;說&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;想得&quot; [31] &quot;壞&quot; 7.1.7 Reminders When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and returns a list of word-based vectors of the same size as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) [[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) [1] &quot;list&quot; class(text_tag_0) [1] &quot;character&quot; 7.2 Chinese Text Analytics Pipeline In Chapter 5, we have talked about the pipeline for English texts processing, as shown below: Figure 7.1: English Text Analytics Flowchart For Chinese texts, the pipeline is similar. In the following Chinese Text Analytics Flowchart (Figure 7.2), I have highlighted the steps that are crucial to Chinese processing. It is not recommended to use quanteda::summary() and quanteda::kwic() directly on the Chinese corpus object because the word tokenization in quanteda is not ideal (cf. dashed arrows in Figure 7.2). It is recommended to use self-defined word segmenter for analysis. For processing under tidy structure framework, use own segmenter in unnest_tokens(); For processing under quanteda framework, use self-defined word segmenter to create the tokens object, defined in quanteda. Figure 7.2: Chinese Text Analytics Flowchart 7.2.1 Creating a Corpus Object So based on our simple corpus example above, we first transform the character vector text into a corpus object—text_corpus. First, let’s try the default Quanteda-native Chinese word segmentation: With the corpus object, we can apply quanteda::summary(), and the statistics of tokens and types are based on the Quanteda-native word segmentation; Or we can use the Quanteda-native tokenization method, tokens(), to convert the corpus object into tokens object and apply quanteda::kwic() to get concordance lines. ## create corpus object text_corpus &lt;- corpus(text) ## summary summary(text_corpus) ## Create tokens object text_tokens &lt;- tokens(text_corpus) ## Check quanteda-native word tokenization result text_tokens[[1]] [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; [67] &quot;。&quot; ## KWIC kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;柯&quot;) Do you know why there are no tokens of concordance lines from kwic(text_corpus, pattern = \"柯文哲\")? It is clear to see that quite a few word tokens have not been successfully identified by the Quanteda-native word segmentation (e.g., several proper names in the text). This would also have great impact on the effectiveness of kwic() as well. Therefore analysis based on the Quanteda-native segmentation can be very limited. Now let’s improve the word segmentation by using self-defined word segmenter based on jiebaR. 7.2.2 Tidy Structure Framework To perform word tokenization under the tidy structure framework, we first convert the corpus object into a text-based data frame using tidy(). Also, we generate an unique index for each row/text using row_number(). # a text-based tidy corpus text_corpus_tidy &lt;-text_corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) text_corpus_tidy To use self-defined word segmenter jiebar, we first initialize the jiebar object using worker(). # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) Then, we use unnest_tokens() to tokenize the text-based data frame (text_corpus_tidy) into a word-based data frame (text_corpus_tidy_word). Texts included in the text column are tokenized into words, which are unnested into independent rows in the word column of the new TIBBLE. # tokenization text_corpus_tidy_word &lt;- text_corpus_tidy %&gt;% unnest_tokens( word, ## new tokens unnested text, ## original larger units token = function(x) ## self-defined tokenization method segment(x, jiebar = my_seg) ) text_corpus_tidy_word It can be seen that for the token parameter in unnest_tokens(), we use an anonymous function based on jieba and segment() for self-defined Chinese word segmentation. This is called anonymous functions because it has not been assigned to any object name in the current R session. You may check R language documentation for more detail on Writing Functions. It is important to note that when we specify a self-defined unnest_tokens(…,token=…) function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument worker(…, byline = TRUE). 7.2.3 Quanteda Framework To perform word tokenization under the Quanteda framework, we need to create the tokens object using a self-defined tokenization function based on jiebar. With this tokens object, we can apply kiwc() or other Quanteda-supported processing to the corpus data. The steps are very straightforward. We utilize jiebar for word tokenization like before: (1) initialize a jiebar model and (2) use it to tokenize the corpus text with segment(). The key is that we need to convert the output of segment() from a list to a tokens using as.tokens(). ## create tokens based on self-defined segmentation text_tokens &lt;- text_corpus_tidy$text %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens ## kwic on word tokens kwic(text_tokens, pattern = &quot;柯文哲&quot;) kwic(text_tokens, pattern = &quot;.*?[台市].*?&quot;, valuetype = &quot;regex&quot;) In Chapter 9, we will discuss another word segmenter for Taiwan Mandarin, i.e., the CKIP Tagger developed by the CKIP group of Academia Sinica. 7.3 Data In the following sections, we will look at a few more case studies of Chinese text processing with a bigger dataset. We will use the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. This data set was collected by Meng-Chen Wu when he was working on his MA thesis project with me years ago. The demo data here was a random sample of the original Apple News Corpus. 7.4 Loading Text Data When we need to load text data from external files (e.g., csv, txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext, which goes hand in hand with quanteda. The main function in this package, readtext(), which takes a file or a directory name from the disk or a URL, and returns a type of data.frame that can be used directly with the corpus() constructor function in quanteda, to create a quanteda corpus object. In other words, the output from readtext() is a data frame, which can be directly passed on to the processing in the tidy structure framework (i.e., tidytext::unnest_tokens()). The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. The corpus constructor command corpus() works directly on: a text-based character vector; a data.frame containing a text column and any other document-level metadata the output of readtext::readtext() # loading the corpus # NB: this may take some time depending on your hardware ## readtext system.time( apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;, encoding = &quot;UTF-8&quot;) ) # time the loading process user system elapsed 4.667 5.448 10.208 ## check the class of the `readtext()` output class(apple_df) [1] &quot;readtext&quot; &quot;data.frame&quot; ## cleaning apple_df &lt;- apple_df %&gt;% filter(!str_detect(text,&quot;^\\\\s*$&quot;)) %&gt;% ## remove empty documents mutate(filename = doc_id, ## save original text filenames doc_id = row_number()) ## create doc id The dataset demo_data/applenews10000.tar.gz may include a few empty documents, which may have resulted from the errors of web crawling. We remove these documents from our analysis. 7.5 Quanteda-based Exploration While we are not going to use Quanteda-native tokenization, we can utilize the corpus() and summary() to have a quick over view of the corpus. ## corpus apple_corpus &lt;- corpus(apple_df) ## summary corpus summary(apple_corpus, 10) ## Get all summary apple_corpus_overview &lt;- summary(apple_corpus, ndoc(apple_corpus)) ## Quick overview apple_corpus_overview %&gt;% ggplot(aes(Tokens)) + geom_histogram(fill=&quot;white&quot;, color=&quot;lightblue&quot;) Please note that the statistics provided by summary() may be misleading because it is based on the Quanteda-native tokenization. 7.6 Initialize jiebaR Because we use jiebaR for word tokenization, we first need to initialize the jiebaR models. Here we created two jiebaR objects, one for word tokenization only and the other for parts-of-speech tagging. # initialize segmenter ## for word segmentation only my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol = T) ## for POS tagging my_seg_pos &lt;- worker( type = &quot;tag&quot;, bylines = F, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol = T ) Even though we have specified a user-defined dictionary in the initialization of the worker(), we can also add add-hoc new words to the model. This can be very helpful when we spot any weird segmentation results in the output. By default, new_user_word() assigns each new word with a default n tag. #Add customized terms temp_new_words &lt;-c(&quot;蔡英文&quot;, &quot;新北市&quot;, &quot;批踢踢實業坊&quot;, &quot;批踢踢&quot;) new_user_word(my_seg, temp_new_words) [1] TRUE new_user_word(my_seg_pos, temp_new_words) [1] TRUE 7.7 Case Study 1: Concordances with kwic() This is an example of processing the Chinese data under Quanteda framework. Without relying on the Quanteda-native tokenization, we can create the tokens object directly based on the output of the jiebar segment(). With this tokens object, we can perform the concordance analysis with kwic(). It seems that the tokenization function segment() in jiebar works better with shorter texts like sentences. Therefore, I think it might be better to first tokenize texts into smaller chunks before word tokenization. These small chunks could be based on punctuation marks or simply line breaks (see CHUNK_DELIMITER below). CHUNK_DELIMITER &lt;- &quot;[，。！？；：\\n]+&quot; ## create `tokens` object using jiebaR system.time( apple_df$text %&gt;% map(str_split, pattern= CHUNK_DELIMITER, simplify=TRUE) %&gt;% ## line tokenization map(segment, my_seg) %&gt;% ## word tokenization map(unlist) %&gt;% ## list to vec as.tokens -&gt; apple_tokens ) user system elapsed 10.928 0.179 11.114 # add document-level variables docvars(apple_tokens) &lt;- apple_df[c(&quot;doc_id&quot;, &quot;filename&quot;)] ## perform kwic kwic(apple_tokens, pattern= &quot;柯文哲&quot;, window = 10 ) kwic(apple_tokens, pattern=&quot;蔡英文&quot;, window = 10) If you would like to ignore the potential impact of chunk tokenization, you can simplify the above process and create the tokens object as follows: apple_tokens &lt;- as.tokens(segment(apple_df$text, my_seg)) library(&quot;quanteda.textplots&quot;) textplot_xray( kwic(apple_tokens, pattern= &quot;柯文哲&quot;) ) 7.8 Case Study 2: Word Frequency and Wordcloud The following are examples of processing the Chinese texts under the tidy structure framework. Recall the three important steps: Load the corpus data using readtext() and create a text-based data frame of the corpus; Initialize a jieba word segmenter using worker() Tokenize the text-based data frame into a word-based data frame using unnest_tokens() Please note that the output of readtext() is already a text/document-based data frame, i.e., a tidy structure of the corpus. We did not use the corpus object because in this example we did not need it for further processing with other quanteda functions (e.g., kwic()). # Tokenization: lines &gt; words apple_line &lt;- apple_df %&gt;% ## line tokenization unnest_tokens( output = line, input = text, token = function (x) str_split(x, CHUNK_DELIMITER) ) %&gt;% group_by(doc_id) %&gt;% mutate(line_id = row_number()) %&gt;% ungroup apple_line %&gt;% head(20) apple_word &lt;- apple_line %&gt;% ## word tokenization unnest_tokens( output = word, input = line, token = function(x) segment(x, jiebar = my_seg) ) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup apple_word %&gt;% head(100) Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to keep track of the original context of the word, phrase or sentence in the concordances. All these unique indices (as well as the source text filenames) would make things a lot easier. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) for more interesting analysis. With a word-based data frame, we can easily create a word frequency list as well as a word cloud to have a quick overview of the word distribution of the corpus. It should be noted that before creating the frequency list, we need to consider whether to remove unimportant tokens (e.g., stopwords, symbols, punctuation, digits, alphabets.) Regular expressions are effective strategies that can help us quickly identify character sets that we like to include or remove from the analysis. Please check Unicode Regular Expressions for more advanced usages. ## load chinese stopwords stopwords_chi &lt;- readLines(&quot;demo_data/stopwords-ch-jiebar-zht.txt&quot;, encoding = &quot;UTF-8&quot;) ## create word freq list apple_word_freq &lt;- apple_word %&gt;% filter(!word %in% stopwords_chi) %&gt;% # remove stopwords filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% # remove words consisting of digits count(word) %&gt;% arrange(desc(n)) library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 400) %&gt;% filter(nchar(word) &gt;= 2) %&gt;% ## remove monosyllabic tokens wordcloud2(shape = &quot;star&quot;, size = 0.3) 7.9 Case Study 3: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to enrich our corpus data by adding POS tags information to our current tidy corpus design. Our steps are as follows: Initialize a jiebar object, which performs not only word segmentation but also POS tagging; Create a self-defined function to word-seg and pos-tag each text chunk and combine all tokens, word/tag, into a long string for each text; With the line-based data frame apple_line, create a new column, which includes the enriched version of each text chunk of the texts, using mutate() # define a function to word-seg and pos-tag a text tag_text &lt;- function(x, jiebar) { segment(x, jiebar) %&gt;% ## tokenize paste(names(.), sep = &quot;/&quot;, collapse = &quot; &quot;) ## reformat output } # demo of the function `tag_text()` tag_text(apple_line$line[2], my_seg_pos) [1] &quot;每名/x 受邀/v 參賽者/n 進行/v 勝負/v 預測/vn&quot; # apply `tag_text()` function to each text system.time(apple_line %&gt;% mutate(line_tag = map_chr(line, tag_text, my_seg_pos)) -&gt; apple_line) user system elapsed 23.147 2.987 26.149 apple_line %&gt;% head 7.9.1 BEI Construction Now we have obtained an enriched version of the texts, we can make use of the POS tags for construction analysis. Let’s look at the example of 被 + ... Construction. The data retrieval procedure is now very straightforward: we only need to create a regular expression that matches our construction and go through the enriched version of the text chunks (i.e., line_tag column in apple_line) to identify these matches with unnest_tokens(). 1.Define a regular expression \\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v for BEI-Construction, i.e., 被 + VERB 2.Use unnest_tokens() and str_extract_all() to extract target patterns and create a pattern-based data frame. For more sophisticated uses of regular expressions in pattern retrieval, please refer to Gries (2016), Chapter 3 (very important). # define regex patterns pattern_bei &lt;- &quot;\\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v&quot; # extract patterns from corp apple_line %&gt;% select(-line) %&gt;% # `text` is the column with original raw texts unnest_tokens( output = pat_bei, ## pattern name input = line_tag, ## original base linguistic unit token = function(x) str_extract_all(x, pattern = pattern_bei) ) -&gt; result_bei result_bei Please check Chapter 5 Parts of Speech Tagging on evaluating the quality of the data retrieved by a regular expression (i.e., precision and recall). Exercise 7.4 If we go through the results retrieved by the current regular expressions (see the data frame pat_bei), we can see quite a few false positives. Please discuss the causes of these errors and provide any possible solutions to improve the results of the pattern retrieval. To have a more in-depth analysis of BEI construction, we can automatically identify the verb of the BEI construction. # Extract BEI + WORD result_bei &lt;- result_bei %&gt;% mutate(VERB = str_replace(pat_bei, &quot;.+\\\\s([^/]+)/v$&quot;, &quot;\\\\1&quot;)) result_bei ## Exploratory Analysis result_bei %&gt;% count(VERB) %&gt;% top_n(40, n) %&gt;% ggplot(aes(x=reorder(VERB, n), y =n, fill=n)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + labs(x = &quot;Verbs in BEI Constructions&quot;, y = &quot;Frequency&quot;) # Calculate WORD frequency require(wordcloud2) result_bei %&gt;% count(VERB) %&gt;% mutate(n = log(n)) %&gt;% top_n(100, n) %&gt;% wordcloud2(shape = &quot;diamond&quot;, size = 0.3) Exercise 7.5 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which goes counter to our native-speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? How would you revise the regular expression to improve the data retrieval? Exercise 7.6 To more properly evaluate the quality of the pattern queries, it would be great if we can have the original texts available in the resulting data frame result_bei. How do we keep this information in the results? That is, please have one column in result_bei, which shows the original texts from which the construction token is extracted. Exercise 7.7 Please use the sample corpus, apple_df, as your data source and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and a space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. Please (a) extract all construction tokens with these space particles and (b) at the same time identify their respective SP and LM, as shown below. If you are interested in a more in-depth analysis of these Chinese space particles, please refer to Su &amp; Chen (2019). Exercise 7.8 Following Exercise 7.7, please create a frequency list of the LMs for each space particle. Show the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Also, visualize the top 10 landmarks that co-occur with each space particle in a bar plot as shown below. Exercise 7.9 Following Exercise 7.8, for each space particle, please create a word cloud of its co-occuring LMs based on the top 100 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 7.10 Based on the word clouds provided in Exercise 7.9, do you find any bizarre cases? Can you tell us why? What would be the problems? Or what did we do wrong in the text preprocessing that may lead to these cases? Please discuss these issues in relation to the steps in our data processing, i.e., word segmentation, POS tagging, and pattern retrievals, and provide your alternative solutions. 7.10 Case Study 4: Lexical Bundles 7.10.1 N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in Apple News. Here let’s take a look at the recurrent four-grams in our Chinese corpus. As the default n-gram tokenization in unnest_tokens(..., token = \"ngrams\") only works with the English data, we need to define our own ngram tokenization functions. The Chinese ngram tokenization function should: tokenize each text into small chunks; tokenize each chunk into word tokens; create a set of ngrams from the word tokens of each chunk ## self defined ngram tokenizer tokenizer_ngrams &lt;- function(texts, jiebar, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% ## chunks-based char vector segment(jiebar) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } In the above self-defined ngram tokenizer, we make use of tokens_ngrams() in quanteda, which creates a set of ngrams from already tokenized text objects, i.e., tokens. Because this function requires a tokens object as the input, we need to do the class conversion via as.tokens() and as.list(). Take a look at the following examples for a quick overview of tokens_ngrams(): sents &lt;- c(&quot;Jack and Jill went up the hill to fetch a pail of water&quot;, &quot;Jack fell down and broke his crown and Jill came tumbling after&quot;) sents_tokens &lt;- tokens(sents) ## English supported sents_tokens Tokens consisting of 2 documents. text1 : [1] &quot;Jack&quot; &quot;and&quot; &quot;Jill&quot; &quot;went&quot; &quot;up&quot; &quot;the&quot; &quot;hill&quot; &quot;to&quot; &quot;fetch&quot; [10] &quot;a&quot; &quot;pail&quot; &quot;of&quot; [ ... and 1 more ] text2 : [1] &quot;Jack&quot; &quot;fell&quot; &quot;down&quot; &quot;and&quot; &quot;broke&quot; &quot;his&quot; [7] &quot;crown&quot; &quot;and&quot; &quot;Jill&quot; &quot;came&quot; &quot;tumbling&quot; &quot;after&quot; tokens_ngrams(sents_tokens, n = 2, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and&quot; &quot;and_Jill&quot; &quot;Jill_went&quot; &quot;went_up&quot; &quot;up_the&quot; &quot;the_hill&quot; [7] &quot;hill_to&quot; &quot;to_fetch&quot; &quot;fetch_a&quot; &quot;a_pail&quot; &quot;pail_of&quot; &quot;of_water&quot; text2 : [1] &quot;Jack_fell&quot; &quot;fell_down&quot; &quot;down_and&quot; &quot;and_broke&quot; [5] &quot;broke_his&quot; &quot;his_crown&quot; &quot;crown_and&quot; &quot;and_Jill&quot; [9] &quot;Jill_came&quot; &quot;came_tumbling&quot; &quot;tumbling_after&quot; tokens_ngrams(sents_tokens, n = 2, skip = 1) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_Jill&quot; &quot;and_went&quot; &quot;Jill_up&quot; &quot;went_the&quot; &quot;up_hill&quot; [6] &quot;the_to&quot; &quot;hill_fetch&quot; &quot;to_a&quot; &quot;fetch_pail&quot; &quot;a_of&quot; [11] &quot;pail_water&quot; text2 : [1] &quot;Jack_down&quot; &quot;fell_and&quot; &quot;down_broke&quot; &quot;and_his&quot; [5] &quot;broke_crown&quot; &quot;his_and&quot; &quot;crown_Jill&quot; &quot;and_came&quot; [9] &quot;Jill_tumbling&quot; &quot;came_after&quot; tokens_ngrams(sents_tokens, n = 5, skip = 0) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_and_Jill_went_up&quot; &quot;and_Jill_went_up_the&quot; &quot;Jill_went_up_the_hill&quot; [4] &quot;went_up_the_hill_to&quot; &quot;up_the_hill_to_fetch&quot; &quot;the_hill_to_fetch_a&quot; [7] &quot;hill_to_fetch_a_pail&quot; &quot;to_fetch_a_pail_of&quot; &quot;fetch_a_pail_of_water&quot; text2 : [1] &quot;Jack_fell_down_and_broke&quot; &quot;fell_down_and_broke_his&quot; [3] &quot;down_and_broke_his_crown&quot; &quot;and_broke_his_crown_and&quot; [5] &quot;broke_his_crown_and_Jill&quot; &quot;his_crown_and_Jill_came&quot; [7] &quot;crown_and_Jill_came_tumbling&quot; &quot;and_Jill_came_tumbling_after&quot; tokens_ngrams(sents_tokens, n = 5, skip = 1) Tokens consisting of 2 documents. text1 : [1] &quot;Jack_Jill_up_hill_fetch&quot; &quot;and_went_the_to_a&quot; [3] &quot;Jill_up_hill_fetch_pail&quot; &quot;went_the_to_a_of&quot; [5] &quot;up_hill_fetch_pail_water&quot; text2 : [1] &quot;Jack_down_broke_crown_Jill&quot; &quot;fell_and_his_and_came&quot; [3] &quot;down_broke_crown_Jill_tumbling&quot; &quot;and_his_and_came_after&quot; # examples texts &lt;- c(&quot;這是一個測試的句子&quot;, &quot;這句子&quot;, &quot;超短句&quot;, &quot;最後一個超長的句子測試&quot;) tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 0, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; $text2 [1] &quot;這_句子&quot; $text3 [1] &quot;超短_句&quot; $text4 [1] &quot;最後_一個&quot; &quot;一個_超長&quot; &quot;超長_的&quot; &quot;的_句子&quot; &quot;句子_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 2, skip = 1, delimiter = &quot;_&quot; ) $text1 [1] &quot;這是_測試&quot; &quot;一個_的&quot; &quot;測試_句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後_超長&quot; &quot;一個_的&quot; &quot;超長_句子&quot; &quot;的_測試&quot; tokenizer_ngrams( texts = texts, jiebar = my_seg, n = 5, skip=0, delimiter = &quot;/&quot; ) $text1 [1] &quot;這是/一個/測試/的/句子&quot; $text2 character(0) $text3 character(0) $text4 [1] &quot;最後/一個/超長/的/句子&quot; &quot;一個/超長/的/句子/測試&quot; With the self-defined ngram tokenizer, we can now perform the ngram tokenization on our Chinese corpus: We transform the text-based data frame into an line-based data frame using unnest_tokens(...) with the self-defined tokenization function str_split(); We then transform the line-based data frame into an ngram-based data frame using unnest_tokens(...) with the self-defined function tokenizer_ngrams(); We remove empty and unwanted n-grams entries: Empty ngrams due to short texts Ngrams spanning punctuation marks, symbols, redundant white-spaces, or paragraph breaks Ngrams including alphanumeric characters ## from text-based to ngram-based system.time( apple_line %&gt;% unnest_tokens( ngram, line, token = function(x) tokenizer_ngrams( texts = x, jiebar = my_seg, n = 4, skip = 0, delimiter = &quot;_&quot; ) ) -&gt; apple_ngram ) ## end system.time user system elapsed 24.900 1.870 20.211 ## remove unwanted ngrams apple_ngram2 &lt;- apple_ngram %&gt;% filter(nzchar(ngram)) %&gt;% ## empty strings filter(!str_detect(ngram, &quot;[^\\u4E00-\\u9FFF_]&quot;)) ## remove unwanted ngrams We can represent any character in Unicode in the form of \\uXXXX, where the XXXX refers to the coding numbers of the character in Unicode (UTF-8) in hexadecimal format. For example, can you tell which character \\u6211 refers to? How about \\u4f60? In the above regular expression, the Unicode range [\\u4E00-\\u9FFF] includes frequently used Chinese characters. Therefore, the way we remove unwanted ngrams is to identify all the ngrams that include non-Chinese characters that fall outside this Unicode range (as well as the delimiter _). For more information related to the Unicode range for the punctuation marks in CJK languages, please see this SO discussion thread. 7.10.2 Frequency and Dispersion As we have discussed in Chapter 4, a multiword unit can be defined based on at least two important distributional properties (See Biber et al. (2004)): The frequency of the whole multiword unit (i.e., frequency) The number of texts where the multiword unit is observed (i.e., dispersion) Now that we have the ngram-based DF, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams at: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) system.time( apple_ngram_dist &lt;- apple_ngram2 %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 5) ) #end system.time user system elapsed 47.115 0.142 47.278 Please take a look at the four-grams, arranged by frequency and dispersion respectively: # arrange by dispersion apple_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;被&quot;)) %&gt;% arrange(desc(dispersion)) apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;以&quot;)) %&gt;% arrange(desc(dispersion)) Exercise 7.11 In the above example, if we are only interested in the four-grams with the word 以, how can we revise the regular expression so that we can get rid of tokens like ngrams with 以及, 以上 etc. 7.11 Recaps Figure 7.3: Chinese Word Segmentation and POS Tagging Tokenization is an important step in Chinese text processing. We may need to take into account many factors when determining the right tokenization method, including: What is the base unit we would like to work with? Texts? Paragraphs? Chunks? Sentences? N-grams? Words? Do we need an enriched version of the raw texts, e.g., the parts-of-speech tags of words in the later analysis? Do we need to include non-word tokens such as symbols, punctuation marks, digits, or alphabets in the analysis? Do we need to remove semantically irrelevant/unimportant words, i.e., stopwords? Do we have many domain-specific words in our corpus (e.g., terminology, proper names)? The answers to the above questions would give us more clues on how to determine the most effective tokenization methods for the data. 7.12 More Exercises Exercise 7.12 Please scrape the articles on the most recent 10 index pages of the PTT Gossipping board. Analyze all the articles whose titles start with [問卦], [新聞], or [爆卦] (Please ignore all articles that start with Re:). Specifically, please create the word frequency list of these target articles by: including only words that are tagged as nouns or verbs by JiebaR (i.e., all words whose POS tags start with n or v) removing words on the stopword list (cf. demo_data/stopwords-ch.txt) providing both the word frequency and dispersions (i.e., number of articles where it occurs) In addition, please visualize your results with a wordcloud as shown below, showing the recent hot words based on these recently posted target articles on PTT Gossipping. In the wordcloud, please include words whose (a) nchar() &gt;=2, and (b) dispersion &lt;= 5. Note: For Chinese word segementation, you may use the dictionary provided in demo_data/dict-ch-user.txt The target articles from PTT Gossiping: Word Frequency List [1] TRUE [1] TRUE Wordclound Exercise 7.13 If you look at your word frequency list, it is very likely that you will see puzzling tokens. And you may wonder why these tokens have high frequency counts and whether these high frequency numbers are due to the wrong word segmentation. For example, in my results, I have a token 陳 on the top of the frequency list. How can we check the concordance lines of one specific token so that we can decide whether to include new words in the user dictionary before word segmentation? For example, based on the following concordance lines, I can see why I have 陳 in my data, and maybe I can consider including 陳吉仲 in the user defined dictionary. Exercise 7.14 Based on the PTT data you get from the previous exercise, examine the recurrent lexical bundles from these articles that satisfy the following distributional criteria: all four to six contiguous word sequences (i.e., 4-grams to 6-grams) ngrams occurring in more than 5 articles ngrams that do NOT include line/paragraph breaks or symbols (non-Chinese characters) References Biber, D., Conrad, S., &amp; Cortes, V. (2004). If you look at…: Lexical bundles in university teaching and textbooks. Applied Linguistics, 25(3), 371–405. Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Su, H.-K., &amp; Chen, A. C.-H. (2019). Conceptualization of containment in chinese: A corpus-based study of the chinese space particles lǐ, nèi, and zhōng. Concentric, 45(2), 211–245. "],["constructions-and-idioms.html", "Chapter 8 Constructions and Idioms 8.1 Collostruction 8.2 Corpus 8.3 Word Segmentation 8.4 Extract Constructions 8.5 Distributional Information Needed for CA 8.6 Running Collostructional Analysis 8.7 Interpretations 8.8 Exercises", " Chapter 8 Constructions and Idioms library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 8.1 Collostruction In this chapter, I would like to talk about the relationship between a construction and words. Words may co-occur to form collocation patterns. When words co-occur with a particular morphosyntactic pattern, they would form collostruction patterns. Here I would like to introduce a widely-applied method for research on the meanings of constructional schemas—Collostructional Aanalysis (Stefanowitsch &amp; Gries, 2003). This is the major framework in corpus linguistics for the study of the relationship between words and constructions. The idea behind collostructional analysis is simple: the meaning of a morphosyntactic construction can be determined very often by its co-occurring words. In particular, words that are strongly associated (i.e., co-occurring) with the construction are referred to as collexemes of the construction. Collostructional Analysis is an umbrella term, which covers several sub-analyses for constructional semantics: collexeme analysis (cf. Stefanowitsch &amp; Gries (2003)) co-varying collexeme analysis (cf. Stefanowitsch &amp; Gries (2005)) distinctive collexeme analysis (cf. Gries &amp; Stefanowitsch (2004)) This chapter will focus on the first one, collexeme analysis, whose principles can be extended to the other analyses. Also, I will demonstrate how we can conduct a collexeme analysis by using the R script written by Stefan Gries (Collostructional Analysis). 8.2 Corpus In this chapter, I will use the Apple News Corpus from Chapter 7 as our corpus. (It is available in: demo_data/applenews10000.tar.gz.) And in this demonstration, I would like to look at a particular morphosyntactic frame in Chinese, X + 起來. Our goal is simple: in order to find out the semantics of this constructional schema, it would be very informative if we can find out which words tend to strongly occupy this X slot of the constructional schema. That is, we are interested in the collexemes of the construction X + 起來 and their degree of semantic coherence. So our first step is to load the text collections of Apple News into R and create a corpus object. ## Text normalization function ## Define a function normalize_document &lt;- function(texts) { texts %&gt;% str_replace_all(&quot;\\\\p{C}&quot;, &quot; &quot;) %&gt;% ## remove control chars str_replace_all(&quot;\\\\s+&quot;, &quot;\\n&quot;) ## replace whitespaces with linebreak } ## Load Data apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;, encoding = &quot;UTF-8&quot;) %&gt;% ## loading data filter(!str_detect(text, &quot;^\\\\s*$&quot;)) %&gt;% ## removing empty docs mutate(doc_id = row_number()) ## create index ## Example usage of `normalize_document()` ## before cleaning apple_df$text[1] [1] &quot;《蘋果體育》即日起進行虛擬賭盤擂台，每名受邀參賽者進行勝負預測，每周結算在周二公布，累積勝率前3高參賽者可繼續參賽，單周勝率最高者，將加封「蘋果波神」頭銜。註:賭盤賠率如有變動，以台灣運彩為主。\\n資料來源：NBA官網http://www.nba.com\\n\\n金塊(客) 103：92 76人騎士(主) 88：82 快艇活塞(客) 92：75 公牛勇士(客) 108：82 灰熊熱火(客) 103：82 灰狼籃網(客) 90：82 公鹿溜馬(客) 111：100 馬刺國王(客) 112：102 爵士小牛(客) 108：106 拓荒者\\n\\n&quot; ## after cleaning normalize_document(apple_df$text[1]) [1] &quot;《蘋果體育》即日起進行虛擬賭盤擂台，每名受邀參賽者進行勝負預測，每周結算在周二公布，累積勝率前3高參賽者可繼續參賽，單周勝率最高者，將加封「蘋果波神」頭銜。註:賭盤賠率如有變動，以台灣運彩為主。\\n資料來源：NBA官網http://www.nba.com\\n金塊(客)\\n103：92\\n76人騎士(主)\\n88：82\\n快艇活塞(客)\\n92：75\\n公牛勇士(客)\\n108：82\\n灰熊熱火(客)\\n103：82\\n灰狼籃網(客)\\n90：82\\n公鹿溜馬(客)\\n111：100\\n馬刺國王(客)\\n112：102\\n爵士小牛(客)\\n108：106\\n拓荒者\\n&quot; ## Apply cleaning to all docs apple_df$text &lt;- normalize_document(apple_df$text) Raw texts usually include a lot of noise. For example, texts may include invisible control characters, redundant white-spaces, and duplicate line breaks. These redundant characters may have an impact on the word segmentation performance. It is often suggested to clean up the raw texts before tokenization. In the above example, I created a simple function, normalize_text(). You can always create a more sophisticated one that is designed for your own corpus data. 8.3 Word Segmentation We use the self-defined word tokenization method based on jiebar. There are three important steps: We first initialize a jiebar language model using worker(); We then define a function word_seg_text(), which enriches the raw texts with word boundaries and parts-of-speech tags; Then we apply the function word_seg_text() to the original texts and create a new column to our text-based data frame, a column with the enriched versions of the original texts. This new column will serve as the basis for later construction extraction. # Initialize jiebar segmenter &lt;- worker(user = &quot;demo_data/dict-ch-user.txt&quot;, bylines = F, symbol = T) # Define function word_seg_text &lt;- function(text, jiebar) { segment(text, jiebar) %&gt;% # word tokenize str_c(collapse = &quot; &quot;) ## concatenate word tokens into long strings } # Apply the function apple_df &lt;- apple_df %&gt;% mutate(text_tag = map_chr(text, word_seg_text, segmenter)) Our self-defined function word_seg_text() is a simple function to convert a Chinese raw text into am enriched version with word boundaries and POS tags. word_seg_text(apple_df$text[1], segmenter) [1] &quot;《 蘋果 體育 》 即日起 進行 虛擬 賭盤 擂台 ， 每名 受邀 參賽者 進行 勝負 預測 ， 每周 結算 在 周二 公布 ， 累積 勝率 前 3 高 參賽者 可 繼續 參賽 ， 單周 勝率 最高者 ， 將 加封 「 蘋果 波神 」 頭銜 。 註 : 賭盤 賠率 如有 變動 ， 以 台灣 運彩 為主 。 \\n 資料 來源 ： NBA 官網 http : / / www . nba . com \\n 金塊 ( 客 ) \\n 103 ： 92 \\n 76 人 騎士 ( 主 ) \\n 88 ： 82 \\n 快艇 活塞 ( 客 ) \\n 92 ： 75 \\n 公牛 勇士 ( 客 ) \\n 108 ： 82 \\n 灰熊 熱火 ( 客 ) \\n 103 ： 82 \\n 灰狼 籃網 ( 客 ) \\n 90 ： 82 \\n 公鹿 溜 馬 ( 客 ) \\n 111 ： 100 \\n 馬刺 國王 ( 客 ) \\n 112 ： 102 \\n 爵士 小牛 ( 客 ) \\n 108 ： 106 \\n 拓荒者 \\n&quot; Please note that in this example, we did not include the parts-of-speech tagging in order to keep this example as simple as possible. However, in real studies, we often need to rely on the POS tags if we want to improve the quality of the pattern retrieval. Therefore, for your own project, probably you need to revise the word_seg_text() and also the jiebar language model (worker()) if you would like to include the POS tag annotation in the data processing. 8.4 Extract Constructions With the word boundary information, we can now extract our target patterns from the corpus using regular expressions with unnest_tokens(). # Define regex pattern_qilai &lt;- &quot;[^\\\\s]+\\\\s起來\\\\b&quot; # Extract patterns apple_qilai &lt;- apple_df %&gt;% select(-text) %&gt;% ## dont need original texts unnest_tokens( output = construction, ## name for new unit input = text_tag, ## name of old unit token = function(x) ## unesting function str_extract_all(x, pattern = pattern_qilai) ) # Print apple_qilai 8.5 Distributional Information Needed for CA To perform the collexeme analysis, which is essentially a statistical analysis of the association between words and a specific construction, we need to collect necessary distributional information of the words (X) and the construction (X + 起來). In particular, to use Stefan Gries’ R script of Collostructional Analysis, we need the following information: Joint Frequencies of the words and the construction Frequencies of Words in Corpus Corpus Size (total number of words in corpus) Construction Size (total number of the construction tokens in corpus) Take the word 使用 for example. We need the following distributional information: Joint Frequencies: the frequency of 使用＋起來 The frequency of 使用 in Corpus Corpus Size (total number of words in corpus) Construction Size (total number of the construction tokens in corpus) 8.5.1 Word Frequency List Let’s attend to the second distributional information needed for the analysis : the frequencies of words/collexemes. It is easy to get the word frequencies of the entire corpus. With the tokenized texts, we first convert the text-based data frame into a word-based one; then we create the word frequency list via simple data manipulation tricks. ## create word freq list apple_word_freq &lt;- apple_df %&gt;% select(-text) %&gt;% ## dont need original raw texts unnest_tokens( ## tokenization word, ## new unit text_tag, ## old unit token = function(x) ## tokenization function str_split(x, &quot;\\\\s+&quot;) ) %&gt;% filter(nzchar(word)) %&gt;% ## remove empty strings count(word, sort = T) apple_word_freq %&gt;% head(100) In the above example, when we convert our data frame from a text-based to a word-based one, we didn’t use any specific tokenization function in unnest_tokens() because we have already obtained the enriched version of the texts, i.e., texts where each word token is delimited by a white-space. Therefore, the unnest_tokens() here is a lot simpler: we simply tokenize the texts into word tokens based on the known delimiter, i.e., the white-spaces. 8.5.2 Joint Frequencies Now let’s attend to the first distributional information needed for the analysis: the joint frequencies of X and X+起來 construction. With all the pattern-based data frame, apple_qilai, this should be simple. Also, because we have created the word frequency list of the corpus, we can include the frequency of the collexeme in our table as well. ## Joint frequency table apple_qilai_freq &lt;- apple_qilai %&gt;% count(construction, sort = T) %&gt;% ## get joint frequencies tidyr::separate(col = &quot;construction&quot;, ## restructure data frame into = c(&quot;w1&quot;, &quot;construction&quot;), sep = &quot;\\\\s&quot;) %&gt;% ## identify the freq of X in X_起來 mutate(w1_freq = apple_word_freq$n[match(w1, apple_word_freq$word)]) apple_qilai_freq 8.5.3 Input for coll.analysis.r Now we have almost all distributional information needed for the Collostructional Analysis. Let’s see how we can use Stefan Gries’ script, coll.analysis.r, to perform the collostructional analysis on our data set. The script coll.analysis.r expects a particular input format. The input file should be a tsv file, which includes a three-column table: Words Word frequency in the corpus Word joint frequency with the construction ## prepare a tsv ## for coll analysis apple_qilai_freq %&gt;% select(w1, w1_freq, n) %&gt;% write_tsv(&quot;qilai.tsv&quot;) In the later Stefan Gries’ R script, it requires that the input be a tab-delimited file (tsv), not a comma-delimited file (csv). 8.5.4 Other Information In addition to the input file, Stefan Gries’ coll.analysis.r also requires a few general statistics for the computing of association measures. We prepare necessary distributional information for the later collostructional analysis: Corpus size: The total number of words in the corpus Construction size: the total number of the construction tokens in the corpus Later when we run Gries’ script, we need to enter these numbers manually in the terminal. ## corpus information cat(&quot;Corpus Size: &quot;, sum(apple_word_freq$n), &quot;\\n&quot;) Corpus Size: 3209784 cat(&quot;Construction Size: &quot;, sum(apple_qilai_freq$n), &quot;\\n&quot;) Construction Size: 546 Sometimes you may need to keep important information printed in the R console in an external file for later use. There’s a very useful function, sink(), which allows you to easily keep track of the outputs printed in the R console and save these outputs in an external text file. ## save info in a text sink(&quot;qilai_info.txt&quot;) ## start flushing outputs to the file not the terminal cat(&quot;Corpus Size: &quot;, sum(apple_word_freq$n), &quot;\\n&quot;) cat(&quot;Construction Size: &quot;, sum(apple_qilai_freq$n), &quot;\\n&quot;) sink() ## end flushing You should be able to find a newly created file, qilai_info.txt, in your working directory, where you can keep track of the progress reports of your requested information. Therefore, sink() is a useful function that helps you direct necessray terminal outputs to an external file for later use. 8.5.5 Create Output File Stefan Gries’ coll.analysis.r can automatically output the results to an external file. Before running the CA script, we can first create an empty output txt file to keep the results from the CA script. ## Create new file file.create(&quot;qilai_results.txt&quot;) 8.6 Running Collostructional Analysis Stefan Gries’ coll.analysis.r will initialize the analysis by first removing all the objects in your current R session. Please make sure that you have saved all necerssary information/objects in your current R session before you source the script. Finally we are ready to perform the collostructional analysis using Stefan Gries’ coll.analysis.r. We can use source() to run the entire R script. The coll.analysis.r is available on Stefan Gries’ website. We can either save the script onto our laptop and run it offline or source the online version ( coll.analysis.r) directly. ###################################### ## WARNING!!!!!!!!!!!!!!! ## ## The script re-starts a R session ## ###################################### source(&quot;demo_data/coll.analysis.r&quot;) coll.analysis.r is an R script with interactive instructions. When you run the analysis, you will be prompted with guided questions, to which you would need to fill in necessary information/answers in the R terminal. For our current example, the answers to be entered for each prompt include: analysis to perform: 1 name of construction: QILAI corpus size: 3209784 freq of constructions: 546 index of association strength: 1 (=fisher-exact) sorting: 4 (=collostruction strength) decimals: 2 text file with the raw data: &lt;qilai.tsv&gt; Where to save output: 1 (= text file) output file: &lt;qilai_results.txt&gt; If everything works properly, you should get the output of coll.analysis.r as a text file qilai_results.txt in your working directory. The text output from Gries’ script may look as follows. 8.7 Interpretations The output from coll.analysis.r is a text file with both the result data frame (i.e., the data frame with all the statistics) as well as detailed annotations/explanations provided by Stefan Gries. We can extract the result data frame from the text file. A sample output file from the collexeme analysis of QILAI has been made available in demo_data/qilai_results.txt. To extract the result data frame from the script output: We first load the result txt file like a normal text file using readlines() We extract the lines which include the statistics and parse them as a delimited table (i.e., TSV) into a data frame using read_tsv() ## load the output txt results &lt;-readLines(&quot;demo_data/qilai_results.txt&quot;, encoding = &quot;UTF-8&quot;) ## subset lines results&lt;-results[-c(1:17, (length(results)-17):length(results))] ## convert into CSV collo_table&lt;-read_tsv(I(results)) ## auto-print collo_table %&gt;% filter(relation ==&quot;attraction&quot;) %&gt;% arrange(desc(coll.strength)) %&gt;% head(100) %&gt;% select(words, coll.strength, everything()) The most important column is coll.strength, which is a statistical measure indicating the association strength between each collexeme and the construction. Please do check Stefanowitsch &amp; Gries (2003) very carefully on how to interpret these numbers. With the collexeme analysis statistics, we can therefore explore the top N collexemes according to specific association metrics. Here we look at the top 10 collexemes according to four different distributional metrics: obs.freq: the raw joint frequency of the word and construction. delta.p.constr.to.word: the delta P of the construction to the word delta.p.word.to.constr: the delta P of the word to the construction coll.strength: the log-transformed p-value based on Fisher exact test ## from wide to long collo_table %&gt;% filter(relation == &quot;attraction&quot;) %&gt;% filter(obs.freq &gt;=5) %&gt;% select(words, obs.freq, delta.p.constr.to.word, delta.p.word.to.constr, coll.strength) %&gt;% pivot_longer(cols=c(&quot;obs.freq&quot;, &quot;delta.p.constr.to.word&quot;, &quot;delta.p.word.to.constr&quot;, &quot;coll.strength&quot;), names_to = &quot;metric&quot;, values_to = &quot;strength&quot;) %&gt;% mutate(metric = factor(metric, levels = c(&quot;obs.freq&quot;, &quot;delta.p.constr.to.word&quot;, &quot;delta.p.word.to.constr&quot;, &quot;coll.strength&quot;))) %&gt;% group_by(metric) %&gt;% top_n(10, strength) %&gt;% ungroup %&gt;% arrange(metric, desc(strength)) -&gt; coll_table_long ## plot coll_table_long %&gt;% mutate(words = reorder_within(words, strength, metric)) %&gt;% ggplot(aes(words, strength, fill=metric)) + geom_col(show.legend = F) + coord_flip() + facet_wrap(~metric,scales = &quot;free&quot;) + tidytext::scale_x_reordered() + labs(x = &quot;Collexemes&quot;, y = &quot;Strength&quot;, title = &quot;Collexemes Ranked by Different Metrics&quot;) The bar plots above show the top 10 collexemes based on four different metrics: obs.freq, delta.p.contr.to.word, delta.p.word.to.contr, and coll.strength. Please refer to the assigned readings on how to compute the collostrengths. Also, in Stefanowitsch &amp; Gries (2003), please pay special attention to the parts where Stefanowitsch and Gries are arguing for the advantages of the collostrengths based on the Fisher Exact tests over the traditional raw frequency counts. Specifically, delta P is a very unique association measure. It has received increasing attention in psycholinguistic studies. Please see Ellis (2006) and Gries (2013) for more comprehensive discussions on the issues of association’s directionality. I need everyone to have a full understanding of how delta p is computed and how we can interpret this association metric. Exercise 8.1 If we look at the top 10 collexemes ranked by the collostrength, we would see a few puzzling collexemes, such as 一, 了, 不. Please identify these puzzling construction tokens as concordance lines (using quanteda::kwic())and discuss their issues and potential solutions. 8.8 Exercises The following exercises should use the dataset Yet Another Chinese News Dataset from Kaggle. The dataset is available on our dropbox demo_data/corpus-news-collection.csv. The dataset is a collection of news articles in Traditional and Simplified Chinese, including some Internet news outlets that are NOT Chinese state media. Exercise 8.2 Please conduct a collexeme analysis for the aspectual construction “X + 了” in Chinese. Extract all tokens of this consturction from the news corpus and identify all words preceding the aspectual marker. Based on the distributional information, conduct the collexemes analysis using the coll.analysis.r and present the collexemes that significantly co-occur with the construction “X + 了” in the X slot. Rank the collexemes according to the collostrength provided by Stefan Gries’ script. When you tokenize the texts using jiebaR, you may run into an error message as shown below. If you do, please figure out what may have contributed to the issue and solve the problem on your own. It is suggested that you parse/tokenize the corpus data and create two additional columns to the text-based data frame — text_id, and text_tag. The following is an example of the first ten articles. A word frequency list of the top 100 words is attached below (word tokens that are pure whitespaces or empty strings were not considered) After my data preprocessing and tokenization, here is relevant distributional information for coll.analysis.r: Corpus Size: 7898105 Consturction Size: 25569 The output of the Collexeme Analysis (coll.analysis.r) When plotting the results, if you have Inf values in the coll.strength column, please replace all the Infvalues with the maximum numeric value of the coll.strength column. Exercise 8.3 Using the same Chinese news corpus—demo_data/corpus-news-collection.csv, please create a frequency list of all four-character words/idioms that are included in the four-character idiom dictionary demo_data/dict-ch-idiom.txt. Please include both the frequency as well as the dispersion of each four-character idiom in the corpus. Dispersion is defined as the number of articles where it is observed. Please arrange the four-character idioms according to their dispersion. user system elapsed 11.496 0.164 11.663 Exercise 8.4 Let’s assume that we are particularly interested in the idioms of the schema of X_X_, such as “一心一意”, “民脂民膏”, “滿坑滿谷” (i.e., idioms where the first character is the same as the third character). Please find the top 20 frequent idioms of this schema and visualize their frequencies in a bar plot as shown below. Exercise 8.5 Continuing the previous exercise, use the same set of idioms of the schema X_X_ and identify the X. Here we refer to the character X as the pivot of the idiom. Please identify all the pivots for idioms of this schema which have at least two types of constructional variants in the corpus (i.e., its type frequency &gt;= 2) and visualize their type frequencies as shown below. For example, the type frequency of the most productive pivot schema, “不_不_”, is 21 in the news corpus. That is, there are 21 types of constructional variants of this schema with the pivot 不, as shown below: Exercise 8.6 Continuing the previous exercise, to further study the semantic uniqueness of each pivot schema, please identify the top 5 idioms of each pivot schema according to the frequencies of the idioms in the corpus. Please present the results for schemas whose type frequencies &gt;= 5 (i.e., the pivot schema has at least FIVE different idioms as its constructional instances). Please visualize your results as shown below. Exercise 8.7 Let’s assume that we are interested in how different media may use the four-character words differently. Please show the average number of idioms per article by different media and visualize the results in bar plots as shown below. The average number of idioms per article for each media source can be computed based on token frequency (i.e., on average how many idioms were observed in each article?) or type frequency (i.e., on average how many different idiom types were observed in each article?). For example, there are 2529 tokens (1443 types) of idioms observed in the 1756 articles published by “Zaobao”. The average token frequency of idiom uses would be: 2529/1756 = 1.44; the average type frequency of idiom uses would be: 1443/1756 = 0.82. References Ellis, N. C. (2006). Language acquisition as rational contingency learning. Applied Linguistics, 27(1), 1–24. Gries, S. T. (2013). 50-something years of work on collocations: What is or should be next…. International Journal of Corpus Linguistics, 18(1), 137–166. Gries, S. T., &amp; Stefanowitsch, A. (2004). Extending collostructional analysis: A corpus-based perspective onalternations’. International Journal of Corpus Linguistics, 9(1), 97–129. Stefanowitsch, A., &amp; Gries, S. T. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. Stefanowitsch, A., &amp; Gries, S. T. (2005). Covarying collexemes. Corpus Linguistics and Linguistic Theory, 1, 1–43. "],["ckiptagger.html", "Chapter 9 CKIP Tagger 9.1 Installation 9.2 Download the Model Files 9.3 R-Python Communication 9.4 Environment Checking 9.5 Initialization 9.6 Segmenting Texts 9.7 Define Own Dictionary 9.8 Beyond Word Boundaries", " Chapter 9 CKIP Tagger library(tidyverse) library(reticulate) The current state-of-art Chinese segmenter for Taiwan Mandarin available is probably the CKIP tagger, created by Chinese Knowledge and Information Processing (CKIP) group at Academia Sinica. The ckiptagger is released as a python module. In this chapter, I will demonstrate how to use the module for Chinese word segmentation but in an R environment, i.e., how to integrate Python modules in R coherently to perform complex tasks. Alternatively, we can run python codes directly in RStudio. In that case, we don’t need to worry about the Python-to-R interface issues. If you are familiar with Python, you are encouraged to take this option and run the ckiptagger word segmentation directly with Python. 9.1 Installation Because ckiptagger is built in python, we need to have python installed in our working environment. Please install the following applications on your own before you start: Anaconda + Python 3.7+ Create a conda environment Install ckiptagger module in the conda environment Install tensorflow module in the conda environment For example, the following codes: Create a new conda environment corpling (skip this step if you’ve already created a conda environment to be used in RStudio); Activate the new conda environment; Install ckiptagger in the conda environment Install tensorflow in the conda environment The codes for the environment setup need to be executed in the terminal. ## Codes in terminal conda create --name corpling python=3.7 source activate corpling pip install -U ckiptagger pip install tensorflow Check the versions of the Python modules: ## Codes in terminal pip show ckiptagger pip show tensorflow Name: ckiptagger Version: 0.2.1 Summary: Neural implementation of CKIP WS, POS, NER tools Home-page: https://github.com/ckiplab/ckiptagger Author: Peng-Hsuan Li Author-email: jacobvsdanniel@gmail.com License: GPLv3 Location: /Users/alvinchen/opt/anaconda3/envs/corpling/lib/python3.7/site-packages Requires: Required-by: Name: tensorflow Version: 2.8.0 Summary: TensorFlow is an open source machine learning framework for everyone. Home-page: https://www.tensorflow.org/ Author: Google Inc. Author-email: packages@tensorflow.org License: Apache 2.0 Location: /Users/alvinchen/opt/anaconda3/envs/corpling/lib/python3.7/site-packages Requires: typing-extensions, libclang, tf-estimator-nightly, flatbuffers, keras, setuptools, google-pasta, opt-einsum, h5py, tensorflow-io-gcs-filesystem, astunparse, tensorboard, wrapt, keras-preprocessing, gast, termcolor, numpy, absl-py, six, protobuf, grpcio Required-by: Please consult the github of the ckiptagger for more details on installation. 9.2 Download the Model Files All NLP applications have their models behind their fancy performances. To use the tagger provided in ckiptagger, we need to download their pre-trained model files. Please go to the github of CKIP tagger to download the model files, which is provided as a zipped file. (The file is very big. It takes a while.) After you download the zipped file, unzip it under your working directory to the data/ directory. In the unzipped directory, data/, are the required model files for the ckiptagger. 9.3 R-Python Communication In order to call Python functions in R/Rstudio, we need to install an R library in your R. The R-Python communication is made possible through the R library reticulate. Please make sure that you have this library installed in your R. install.packages(&quot;reticulate&quot;) 9.4 Environment Checking Before we proceed, please check if you have everything ready. The following includes the versions of the modules used for this session: Anaconda + Python 3.7+ (Python 3.7.13) A working conda environment (corpling) Python modules: ckiptagger (ckiptagger 0.2.1 + tensorflow 2.8.0) R library: reticulate(1.22) CKIP model files under your working directory ./data If yes, then we are ready to go. 9.5 Initialization We first load the library reticulate and specify in R which Python we will be using in the current R session using path_to_python() (It is highly likely that there is more than one Python environment in your system). Please change the path_to_python() to the target Python kernel, i.e., the conda environment you would like to use in R. library(reticulate) There are three important steps in initialization before you can perform word segmentation in R: Activate a specific conda environment in R Import the ckiptagger module in R Initialize the tagger models ## Activate a specific conda env in R reticulate::use_condaenv(&quot;corpling&quot;, required = TRUE) ## Import ckiptagger module ckip &lt;- reticulate::import(module = &quot;ckiptagger&quot;) ## Initialize model for word segmentation ws &lt;- ckip$WS(&quot;./data&quot;) 9.6 Segmenting Texts The initialized word segmenter object, ws(), can tokenize any input text-based vectors into a list of word-based vectors of the same size. ## Raw text corpus texts &lt;- c(&quot;傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。&quot;, &quot;美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。&quot;, &quot;土地公有政策?？還是土地婆有政策。.&quot;, &quot;… 你確定嗎… 不要再騙了……他來亂的啦&quot;, &quot;最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.&quot;, &quot;科長說:1,坪數對人數為1:3。2,可以再增加。&quot;) words &lt;- ws(texts) words [[1]] [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; &quot;，&quot; &quot;卻&quot; &quot;突然&quot; [9] &quot;爆出&quot; &quot;自己&quot; &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來&quot; &quot;體育台&quot; [17] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; [25] &quot;電視台&quot; &quot;。&quot; [[2]] [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; [37] &quot;。&quot; [[3]] [1] &quot;土地公&quot; &quot;有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地&quot; &quot;婆&quot; [9] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; [[4]] [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; &quot;他&quot; &quot;來&quot; &quot;亂&quot; &quot;的&quot; &quot;啦&quot; [[5]] [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; [[6]] [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; The word segmenter ws() returns a list object, each element of which is a word-based vector of the original text. 9.7 Define Own Dictionary The performance of Chinese word segmenter depends highly on the dictionary. Texts in different disciplines may have very domain-specific vocabulary. To prioritize a set of words in a dictionary, we can further ensure the accuracy of the word segmentation. To create a dictionary for ckiptagger, we need to: Create a named list, i.e., a list with each element’s name = “the new word” and element’s value = “the weight”. Use the python function ckip$construct_dictionary() to create the dictionary Python object Use the dictionary object as the argument for the parameter recommend_dictionary = ... in word segmenter, i.e., ws(..., recommend_dictionary = ...). ## Define new words in own dictionary ## To create a dictionary for `construct_dictionary()` ## We need a list, ## consisting of pairs of `names` = `weights` in the dictionary new_words &lt;- list(&quot;土地公有&quot; = 2, &quot;土地公&quot;=1, &quot;土地婆&quot;=1, &quot;來亂的&quot;=1, &quot;啦&quot;=1, &quot;緯來體育台&quot;=1) ## Create Python `dictionary` object, required by `ckiptagger.wc()` dictionary&lt;-ckip$construct_dictionary(new_words) ## Segment texts using dictionary words_1 &lt;- ws(texts, recommend_dictionary = dictionary) words_1 [[1]] [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; [[2]] [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; [37] &quot;。&quot; [[3]] [1] &quot;土地公有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; [7] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; [[4]] [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; [9] &quot;再&quot; &quot;騙&quot; &quot;了&quot; &quot;…&quot; &quot;…&quot; &quot;他&quot; &quot;來亂的&quot; &quot;啦&quot; [[5]] [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; [[6]] [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; Exercise 9.1 We usually have a list of new words saved in a text file. Can you write a R function, which loads the words in the demo_data/dict-sample.txt into a named list, i.e., new_words, which can easily serve as the input for ckip$construct_dictionary() to create the python dictionary object? (Note: All weights are default to 1) ## Load external file as dictionary new_words&lt;-loadDictionary(input = &quot;demo_data/dict-sample.txt&quot;) dictionary&lt;-ckip$construct_dictionary(new_words) ## Segment texts using dictionary words_2 &lt;- ws(texts, recommend_dictionary = dictionary) words_2 [[1]] [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; [[2]] [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; [37] &quot;。&quot; [[3]] [1] &quot;土地公有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; [7] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; [[4]] [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; [9] &quot;再&quot; &quot;騙&quot; &quot;了&quot; &quot;…&quot; &quot;…&quot; &quot;他&quot; &quot;來亂的&quot; &quot;啦&quot; [[5]] [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; [[6]] [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; Exercise 9.2 Use the ckiptagger word segmentation method to tokenize the text we discuss in Chapter 7 (as repeated below). Please use the tidytext to process the data, tokenize the data with unnest_tokens() and present the results as follows. Please include the following words in the user-defined dictionary: 被提名人, 年終獎金, 受訪, 不分區. Also, compare the results based on jiebar in Chapter 7 and ckiptagger and discuss their respective strengths and weaknesses. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; [[1]] [1] &quot;綠黨&quot; &quot;桃園&quot; &quot;市議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;，&quot; [7] &quot;指&quot; &quot;民眾黨&quot; &quot;不分區&quot; &quot;被提名人&quot; &quot;蔡壁如&quot; &quot;、&quot; [13] &quot;黃瀞瑩&quot; &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（6）&quot; &quot;日&quot; [19] &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; &quot;年終獎金&quot; [25] &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; &quot;台北市長&quot; [31] &quot;柯文哲&quot; &quot;7日&quot; &quot;受訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; [37] &quot;，&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; [43] &quot;，&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想&quot; &quot;得&quot; [49] &quot;這麼&quot; &quot;壞&quot; &quot;。&quot; 9.8 Beyond Word Boundaries In addition to primitive word segmentation, the ckiptagger provides also the parts-of-speech tags for words and named entity recognitions for the texts. The ckiptagger follows the pipeline below for text processing. Load the models To perform these additional tasks, we need to load the necessary models (pre-trained and provided by the CKIP group) first as well. They should all have been included in the model directory you unzipped earlier (cf. ./data). ## Loading pretrained CKIP models system.time((pos &lt;- ckip$POS(&quot;./data&quot;))) ## POS Tagging Model user system elapsed 3.496 2.414 6.591 system.time((ner &lt;- ckip$NER(&quot;./data&quot;))) ## Named Entity Recognition Model user system elapsed 3.511 1.951 5.608 POS tagging and NER # Parts-of-speech Tagging pos_words &lt;- pos(words_1) pos_words [[1]] [1] &quot;Nb&quot; &quot;Nd&quot; &quot;D&quot; &quot;VC&quot; [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; [9] &quot;VJ&quot; &quot;Nh&quot; &quot;Neu&quot; &quot;Nf&quot; [13] &quot;Ng&quot; &quot;P&quot; &quot;Nc&quot; &quot;VC&quot; [17] &quot;COMMACATEGORY&quot; &quot;Nh&quot; &quot;D&quot; &quot;VK&quot; [21] &quot;Nh&quot; &quot;Ncd&quot; &quot;VJ&quot; &quot;Nc&quot; [25] &quot;PERIODCATEGORY&quot; [[2]] [1] &quot;Nc&quot; &quot;Nc&quot; &quot;P&quot; &quot;Nd&quot; [5] &quot;Na&quot; &quot;Nb&quot; &quot;D&quot; &quot;VC&quot; [9] &quot;DE&quot; &quot;Na&quot; &quot;Nb&quot; &quot;VC&quot; [13] &quot;VC&quot; &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;VE&quot; [17] &quot;Nh&quot; &quot;D&quot; &quot;D&quot; &quot;Dfa&quot; [21] &quot;VH&quot; &quot;VC&quot; &quot;Nc&quot; &quot;VC&quot; [25] &quot;COMMACATEGORY&quot; &quot;VG&quot; &quot;Nes&quot; &quot;Nc&quot; [29] &quot;D&quot; &quot;Neu&quot; &quot;Nf&quot; &quot;DE&quot; [33] &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; [37] &quot;PERIODCATEGORY&quot; [[3]] [1] &quot;VH&quot; &quot;Na&quot; &quot;QUESTIONCATEGORY&quot; &quot;QUESTIONCATEGORY&quot; [5] &quot;Caa&quot; &quot;Nb&quot; &quot;V_2&quot; &quot;Na&quot; [9] &quot;PERIODCATEGORY&quot; &quot;PERIODCATEGORY&quot; [[4]] [1] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;Nh&quot; &quot;VK&quot; &quot;T&quot; [6] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;D&quot; &quot;D&quot; &quot;VC&quot; [11] &quot;Di&quot; &quot;ETCCATEGORY&quot; &quot;ETCCATEGORY&quot; &quot;Nh&quot; &quot;VA&quot; [16] &quot;T&quot; [[5]] [1] &quot;VH&quot; &quot;VJ&quot; &quot;Neu&quot; &quot;Nf&quot; [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;Caa&quot; &quot;Neu&quot; [9] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; [13] &quot;D&quot; &quot;VH&quot; &quot;T&quot; &quot;PERIODCATEGORY&quot; [17] &quot;Nep&quot; &quot;SHI&quot; &quot;Na&quot; &quot;DE&quot; [21] &quot;Na&quot; &quot;PERIODCATEGORY&quot; [[6]] [1] &quot;Na&quot; &quot;VE&quot; &quot;Neu&quot; &quot;Na&quot; [5] &quot;P&quot; &quot;Na&quot; &quot;VG&quot; &quot;Neu&quot; [9] &quot;PERIODCATEGORY&quot; &quot;Neu&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; [13] &quot;D&quot; &quot;VHC&quot; &quot;PERIODCATEGORY&quot; # Named Entity Recognition entities &lt;- ner(words_1, pos_words) entities [[1]] {(0, 3, &#39;PERSON&#39;, &#39;傅達仁&#39;), (18, 22, &#39;DATE&#39;, &#39;20年前&#39;), (23, 28, &#39;ORG&#39;, &#39;緯來體育台&#39;)} [[2]] {(7, 9, &#39;DATE&#39;, &#39;今天&#39;), (0, 2, &#39;GPE&#39;, &#39;美國&#39;), (21, 24, &#39;PERSON&#39;, &#39;趙小蘭&#39;), (11, 13, &#39;PERSON&#39;, &#39;布什&#39;), (17, 21, &#39;ORG&#39;, &#39;勞工部長&#39;), (42, 45, &#39;ORG&#39;, &#39;參議院&#39;), (60, 62, &#39;NORP&#39;, &#39;華裔&#39;), (56, 58, &#39;ORDINAL&#39;, &#39;第一&#39;), (2, 5, &#39;ORG&#39;, &#39;參議院&#39;)} [[3]] {(10, 13, &#39;PERSON&#39;, &#39;土地婆&#39;)} [[4]] set() [[5]] {(4, 10, &#39;CARDINAL&#39;, &#39;59,000&#39;), (14, 18, &#39;CARDINAL&#39;, &#39;5.9萬&#39;)} [[6]] {(14, 15, &#39;CARDINAL&#39;, &#39;3&#39;), (16, 17, &#39;CARDINAL&#39;, &#39;2&#39;), (4, 6, &#39;CARDINAL&#39;, &#39;1,&#39;), (12, 13, &#39;CARDINAL&#39;, &#39;1&#39;)} Exercise 9.3 Use the same texts in the above example and the same word tokenization method (i.e., ckiptagger), but please process the Chinese texts using the tidytext framework. That is, use the ckiptagger tokenization method along with the unnest_tokens() and present your token-based information of the corpus as shown below. Your results should include word tokens, their POS tags, and the start and end indices of each word token. Use the user-defined dictionary demo_data/dict-sample.txt in your word segmentation. Exercise 9.4 With a word-based tidy structure of the corpus, it is easy to convert it into a text-based one with both the information of word boundaries and parts-of-speech tag. Please convert the word-based data frame into a text-based data frame, as shown below. Please note that the text column includes an enriched version of the original texts. Exercise 9.5 The outputs of ner() are not very R friendly. How to tidy up the output of ner() by converting it into a more R-compatible data frame? For example, convert the output of ner() from ckiptagger into a data frame like this: Exercise 9.6 Add the named entity annotations to the word-based data frame obtained in the previous exercise. That is, under the tity text framework, parse the corpus data texts by tokenizing the text-based data frame into a word-based data frame, with the following annotations provided for each word: text_id: text ID word_id: word ID start: starting character index end: ending character index word: word tag: CKIP POS tag entity: CKIP Named Entity Tag The above result data frame makes use of the I(O)B format (short for inside, outside, beginning) for the annotations of the named entities. It is a common tagging format for tagging (multiword) tokens in a chunking task in computational linguistics (e.g., NP-chunking, named entitity, semantic roles). The _B suffix after a tag indicates that the tag is the beginning of a chunk. The _I suffix after a tag indicates that the tag is inside a chunk. Some annotation scheme (we don’t have this in our above example) may have a third suffix: The _O tag indicates that a token belongs to no chunk (i.e., outside of all relevant chunks). "],["structured-corpus.html", "Chapter 10 Structured Corpus 10.1 NCCU Spoken Mandarin 10.2 CHILDES Format 10.3 Loading the Corpus 10.4 From Text-based to Turn-based DF 10.5 Metadata vs. Utterances 10.6 Word-based DF and Frequency List 10.7 Concordances 10.8 Collocations (Bigrams) 10.9 N-grams (Lexical Bundles) 10.10 Connecting SPID to Metadata 10.11 Processing Corpus Headers 10.12 Sociolinguistic Analyses", " Chapter 10 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for linguistic studies. Unlike self-collected text corpora, these structured corpora are usually provided in a structured format and the text data are often enriched with annotations. When presenting/distributing the structured corpus data, the corpus provider can determine the ultimate format and structure in which the users can access the text data. Important factors usually include: Web Interface: An online interface for users to access the corpus data Annotation: Additional linguistic annotations at different levels Metadata: Additional demographic information for the corpus data Full-Text Availability: A commercial/non-commercial license to download the full-texts of the corpus data Full-Text Formats: The formats of the full-texts data Web Interface Annotation Metadata Full-TextAvailability Full-TextFormats Capacities GUICQLSyntax POSLEMMAWordSeg Demographic InfoGenre CommercialAcademic License TextsXMLJSONTextgridsXML Sinica Corpus ✓ ✓ ✓ ✓ XML COCT ✓ ✓ ✓ COCA ✓ ✓ ✓ ✓ Raw Texts BNC2014 ✓ ✓ ✓ ✓ XML CHILDES ? ✓ ✓ ✓ CHILDES ICNALE ✓ ✓ Raw Texts This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. To facilitate the sharing of corpus data, the corpus linguistic community has now settled on a few common schemes for textual data storage and exchange. In particular, I would like to talk about two common types of corpus data representation: CHILDES (this chapter) and XML (Chapter 11). 10.1 NCCU Spoken Mandarin In this demonstration, I will use the dataset of Taiwan Mandarin Corpus for illustration. This dataset, collected by Prof. Kawai Chui and Prof. Huei-ling Lai at National Cheng-Chi University (Chui et al., 2017; Chui &amp; Lai, 2008), includes spontaneous face-to-face conversations of Taiwan Mandarin. The data transcription conventions can be found on the NCCU Corpus Official Website. Generally, the NCCU corpus transcripts follow the conventions of CHILDES format. In computational text analytics, the first step is always to analyze the structure of the textual data. 10.2 CHILDES Format The following is an excerpt from the file demo_data/data-nccu-M001.cha from the NCCU Corpus of Taiwan Mandarin. The conventions of CHILDES transcription include: The lines with header information begin with @ The lines with utterances begin with * The indented lines refer to the utterances of the continuing speaker turn Words are separated by white-spaces (i.e., a word-segmented corpus) The meanings of transcription symbols used in the corpus can be found in the documention of the corpus. 10.3 Loading the Corpus The corpus data are available in our demo_data/corp-NCCU-SPOKEN.tar.gz, which is a zipped archived file, i.e., one zipped tar file including all the corpus documents. We can use the readtext::readtext() to load the data. By default, readtext() assumes that the archived file consists of documents in .txt format. In this step, we treat all the *.cha files as if they are normal text files (i.e. .txt) and load the entire corpus into a data frame with two columns: doc_id and text (The warning messages only warn you that by default readtext() takes only .txt files). ## Load corpus NCCU &lt;- as_tibble(readtext(&quot;demo_data/corp-NCCU-SPOKEN.tar.gz&quot;, encoding = &quot;UTF-8&quot;)) 10.4 From Text-based to Turn-based DF Now the data frame NCCU is a text-based one, where each row refers to one transcript file in the corpus. Before we do further tokenization, we first need to concatenate all same-turn utterances (i.e., utterances with no speaker ID at the initial of the line) with their initial utterance of the speaker turn; Then we use unnest_tokens() to transform the text-based DF into a turn-based DF. ## From text-based DF to turn-based DF NCCU_turns &lt;- NCCU %&gt;% mutate(text = str_replace_all(text,&quot;\\n\\t&quot;,&quot; &quot;)) %&gt;% # deal with same-speaker-turn utterances unnest_tokens(turn, ## name for new unit text, ## name for old unit token = function(x) ## self-defined tokenizer str_split(x, pattern = &quot;\\n&quot;)) ## Inspect first file NCCU_turns %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.5 Metadata vs. Utterances Lines starting with @ are the headers of the transcript while lines starting with * are the utterances of the conversation. We split our NCCU_turns into: NCCU_turns_meta: a DF with all header lines NCCU_turns_utterance: a DF with all utterance lines ## Metadata DF NCCU_turns_meta &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^@&quot;)) ## extract all lines starting with `@` When extracting all the utterances of the speaker turns, we perform data preprocessing as well. Specifically, we: Create unique indices for every speaker turn Extract the speaker index of each speaker turn (SPID) Replace all pause tags with &lt;PAUSE&gt; Replace all extra-linguistic tags with &lt;EXTRACLING&gt; Remove all overlapping talk tags Remove all code-switching tags Remove duplicate/trailing/leading spaces ## Utterance DF NCCU_turns_utterance &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^\\\\*&quot;)) %&gt;% ## extract all lines starting with `*` group_by(doc_id) %&gt;% ## create unique index for turns mutate(turn_id = row_number()) %&gt;% ungroup %&gt;% tidyr::separate(col=&quot;turn&quot;, ## split SPID + Utterances into = c(&quot;SPID&quot;, &quot;turn&quot;), sep = &quot;:\\t&quot;) %&gt;% mutate(turn2 = turn %&gt;% ## Clean up utterances str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% ## &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% ## &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% ## overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% ## code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% ## additional white-spaces str_trim()) The turn-based DF, NCCU_turns_utterance, includes the utterances of each speaker turn as well as the doc_id, turn_id and the SPID. All these unique indices can help us connect each utterance back to the original conversation. ## Turns of `M001.cha` NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) %&gt;% select(doc_id, turn_id, SPID, turn, turn2) 10.6 Word-based DF and Frequency List Because the NCCU corpus has been word segmented, we can easily transform the turn-based DF into a word-based DF using unnest_tokens(). The key is that we need to specify our own tokenization function token = .... The word tokenization now is simple – tokenize the utterance into words based on the delimiter of white-spaces. ## From turn DF to word DF NCCU_words &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% ## remove raw text column unnest_tokens(word, ## name for new unit turn2, ## name for old unit token = function(x) ## self-defined tokenizer str_split(x, &quot;\\\\s+&quot;)) ## Check NCCU_words %&gt;% head(100) With the word-based DF, we can create a word frequency list of the NCCU corpus. ## Create word freq list NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) ## Check NCCU_words_freq %&gt;% head(100) With word frequencies, we can generate a word cloud to have a quick overview of the word distributions in NCCU corpus. ## Create Wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% ## remove annotations/tags select(word, freq) %&gt;% top_n(150, freq) %&gt;% ## plot top 150 words mutate(freq = sqrt(freq)) %&gt;% ## deal with Zipfian distribution wordcloud2(size=0.6) 10.7 Concordances If we need to identify turns with a particular linguistic unit, we can make use of the data wrangling tricks to easily extract speaker turns with the target pattern. We can make use of regular expressions again to extract more complex constructions and patterns from the utterances. ## extracting particular patterns NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;覺得&quot;)) NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;這樣子&quot;)) Exercise 10.1 If we are interested in the use of the verb 覺得, after we extract all the speaker turns with the verb 覺得, we may need to know the subjects that often go with the verb. Please identify the word before the verb for each concordance token as one independent column of the resulting data frame (see below). Please note that one speaker turn may have more than one use of 覺得. Please create a barplot as shown below to summarize the distribution of the top 10 frequent words that directly precedes 覺得. Among the top 10 words, you would see “的 覺得” combinations, which are counter-intuitive. Please examine these tokens and explain why. Alternatively, we can also create a tokens object and apply the kwic() from quanteda for concordance lines: ## Quanteda `tokens` object NCCU_tokens&lt;- NCCU_turns_utterance$turn2 %&gt;% str_split(&quot;\\\\s+&quot;) %&gt;% ## split into word-based list as.tokens ## list2tokens ## check token numbers sum(sapply(NCCU_tokens, length)) ## based on `tokens` [1] 194670 nrow(NCCU_words) ## based on `unnest_tokens` [1] 194670 ## We can add docvars to `tokens` docvars(NCCU_tokens)&lt;- NCCU_turns_utterance[,c(&quot;doc_id&quot;,&quot;SPID&quot;, &quot;turn_id&quot;)] %&gt;% mutate(Text = doc_id) ## KWIC kwic(NCCU_tokens, &quot;覺得&quot;, 8) kwic(NCCU_tokens, &quot;機車&quot;, 8) 10.8 Collocations (Bigrams) Now we extend our analysis beyond single words. Please recall the tokenizer_ngrams() function we have defined in Chapter 7. ## self defined ngram tokenizer tokenizer_ngrams &lt;- function(texts, jiebar, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% ## chunks-based char vector segment(jiebar) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } The function tokenizer_ngrams() tokenizes the texts into word vectors using jiebaR and based on the word vectors, it extracts the ngram vectors from each text. In other words, it assumes that the input texts have NOT been word segmented. We can create a similar ngram tokenizer for our current NCCU corpus data: ## Self-defined ngram tokenizer tokenizer_ngrams_v2 &lt;- function(texts, n = 2 , skip = 0, delimiter = &quot;_&quot;) { texts %&gt;% ## chunks-based char vector str_split(pattern = &quot;\\\\s+&quot;) %&gt;% ## word tokenization as.tokens %&gt;% ## list to tokens tokens_ngrams(n, skip, concatenator = delimiter) %&gt;% ## ngram tokenization as.list ## tokens to list } We use the self-defined tokenization function together with unnest_tokens() to transform the turn-based DF into a bigram-based DF. ## From turn DF to bigram DF system.time( NCCU_bigrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% ## remove raw texts unnest_tokens(bigrams, ## name for new unit turn2, ## name for old unit token = function(x) ## self-defined tokenizer tokenizer_ngrams_v2( texts = x, n = 2))%&gt;% filter(bigrams!=&quot;&quot;) ) user system elapsed 0.552 0.100 0.368 NCCU_bigrams %&gt;% filter(doc_id == &quot;M001.cha&quot;) Please note that when we perform the n-gram tokenization, we take each speaker turn as our input. This step is important because this would make sure that we don’t get bigrams that span different speaker turns. To determine significant collocations in conversation, we can compute the relevant distributional statistics for each bigram type, including: Frequencies Dispersion Collocation Strength (Lexical Associations) We first compute the frequencies and dispersion of all bigrams types: ## Bigram Joint Frequency &amp; Dispersion NCCU_bigrams_freq &lt;- NCCU_bigrams %&gt;% count(bigrams, doc_id) %&gt;% group_by(bigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) ## Check NCCU_bigrams_freq %&gt;% top_n(100, freq) ## Check (para removed) NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% top_n(100, freq) Exercise 10.2 In the above example, we compute the dispersion based on the number of documents where the bigram occurs. Please note that the dispersion can be defined on the basis of the speakers as well, i.e., the number of speakers who use the bigram at least once in the corpus. How do we get dispersion statistics like this? Please show the top frequent 100 bigrams and their SPID-based dispersion statistics. To compute the lexical associations, we need to: remove bigrams with para-linguistic tags exclude bigrams of low dispersion get necessary observed frequencies (e.g., w1 and w2 frequencies) get expected frequencies (for more advanced lexical association metrics) ## Computing lexical associations NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% ## remove bigrams with para tags filter(dispersion &gt;= 5) %&gt;% ## set bigram dispersion cut-off rename(O11 = freq) %&gt;% tidyr::separate(col=&quot;bigrams&quot;, ## split bigrams into two columns c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;_&quot;) %&gt;% ## Obtain w1 w2 freqs mutate(R1 = NCCU_words_freq$freq[match(w1, NCCU_words_freq$word)], C1 = NCCU_words_freq$freq[match(w2, NCCU_words_freq$word)]) %&gt;% ## compute expected freq of bigrams mutate(E11 = (R1*C1)/sum(O11)) %&gt;% ## Compute lexical assoc mutate(MI = log2(O11/E11), t = (O11 - E11)/sqrt(E11)) %&gt;% mutate_if(is.double, round,2) -&gt; NCCU_collocations ## Check NCCU_collocations %&gt;% arrange(desc(dispersion), desc(MI)) # sorting by MI NCCU_collocations %&gt;% arrange(desc(dispersion), desc(t)) # sorting by t Exercise 10.3 Please compute the lexical associations of the bigrams using the log-likelihood ratios. 10.9 N-grams (Lexical Bundles) We can also extend our analysis to n-grams of larger sizes, i.e., the lexical bundles. ## Turn to 4-gram DF system.time( NCCU_ngrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% ## remove raw texts unnest_tokens(ngram, ## name for new unit turn2, ## name for old unit token = function(x) ## self-defined tokenizer tokenizer_ngrams_v2( texts = x, n = 4))%&gt;% filter(ngram != &quot;&quot;) ## remove empty tokens (due to the short lines) ) user system elapsed 0.953 0.145 0.560 ## 4-gram Frequency List NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion), desc(freq)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq ## Check NCCU_ngrams_freq %&gt;% filter(dispersion &gt;= 5) 10.10 Connecting SPID to Metadata So far the previous analyses have not used any information of the transcripts’ headers (metadata). In other words, the connection between the utterances and their corresponding speakers’ profiles are not transparent in our current corpus analysis. However, for socio-linguists, the headers of the transcripts can be very informative. For example, in the NCCU_turns_meta, we have more demographic information of the speakers, which allows us to further examine the linguistic variations on various social factors (e.g., areas, ages, gender etc.) ## Utterances of `M001.cha` NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) ## Metadata information of speakers in `M001.cha` NCCU_turns_meta %&gt;% filter(doc_id == &quot;M001.cha&quot;) 10.11 Processing Corpus Headers In this section, I would like to demonstrate how to extract speaker-related information from the headers (i.e., NCCU_turns_meta) and link these speaker profiles to our corpus data (i.e., NCCU_turns_utterance). In the headers of each transcript, the demographic profiles of each speaker are provided in the lines starting with @id:\\t; Each piece of information is separated by a pipe sign | in the line. All speakers’ profiles in the corpus follow the same structure. NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) To parse the demographic data of the speaker profiles in the turn column of NCCU_turns_meta, we can: Extract all lines starting with @id Separate the metadata string into several columns using | Select relevant columns (speaker profiles) Rename the columns Create unique IDs for each speaker of each transcript ## Extract metadata for all speakers NCCU_meta &lt;- NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) %&gt;% ## extract all ID lines separate(col=&quot;turn&quot;, ## split SP info into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% ## choose relevant info mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% ## unique SPID rename(AGE = V4, ## renaming columns GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) %&gt;% ## remove irrelevant column mutate(AGE = as.integer(str_replace(AGE,&quot;;&quot;,&quot;&quot;))) ## chr to integer NCCU_meta With the above demographic data frame of all speakers in NCCU, we can now explore the demographic distributions of the corpus. ## Gender by age distribution NCCU_meta %&gt;% ggplot(aes(GENDER, AGE, fill = GENDER)) + geom_boxplot() + theme_light() ## Relation distribution NCCU_meta %&gt;% count(RELATION) %&gt;% ggplot(aes(reorder(RELATION, n), n, fill = n)) + geom_col() + coord_flip() + labs(y = &quot;Number of Speakers&quot;, x = &quot;Relation&quot;) + scale_fill_gradient(guide = &quot;none&quot;) + theme_light() 10.12 Sociolinguistic Analyses Now with NCCU_meta and NCCU_turns_utterance, we can now connect each utterance to a particular speaker (via `doc_id and SPID in NCCU_turns_utterance and DOC_SPID in NCCU_meta) and therefore study the linguistic variation across speakers of different demographic backgrounds. The steps are as follows: We first extract the patterns we are interested in from NCCU_turns_utterance; We then connect the concordance tokens to their corresponding SPID profiles in NCCU_meta; We analyze how the patterns vary according to speakers of different profiles. NCCU_turns_utterance %&gt;% filter(doc_id == &quot;M001.cha&quot;) For example, we can look at bigram usage patterns by speakers of varying age groups. The analysis requires the following steps: We retrieve target bigrams from NCCU_bigrams We generate new indices DOC_SPID for all bigram tokens extracted We map the DOC_SPID to NCCU_meta to get the speaker profiles of each bigram token using left_join() We recode the speaker’s age into a three-level factor for more comprehensive analysis (i.e., AGE_GROUP) For each age group, we compute the bigram frequencies and dispersion (i.e., the number of speakers using the bigram) ## Analyzing bigram use by age NCCU_bigrams_with_meta &lt;- NCCU_bigrams %&gt;% ## bigram-based DF filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% ## remove bigrams with para tags mutate(DOC_SPID = str_c(doc_id, ## Create `DOC_SPID` str_replace_all(SPID, &quot;\\\\*&quot;, &quot;&quot;), sep = &quot;_&quot;)) %&gt;% left_join(NCCU_meta, ## Link to meta DF by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE_GROUP = cut( ## relevel AGE AGE, breaks = c(0, 20, 40, 60), label = c(&quot;Below_20&quot;, &quot;AGE_20_40&quot;, &quot;AGE_40_60&quot;) )) ## Check head(NCCU_bigrams_with_meta,50) ## Bigram Frequency &amp; Dispersion List by Age_Group NCCU_bigrams_by_age &lt;- NCCU_bigrams_with_meta %&gt;% count(bigrams, AGE_GROUP, DOC_SPID) %&gt;% group_by(bigrams, AGE_GROUP) %&gt;% summarize(freq = sum(n), ## frequency dispersion = n()) %&gt;% ## dispersion (speakers) filter(dispersion &gt;= 5) %&gt;% ## dispersion cutoff ungroup ## Bigram freq &amp; dispersion by age NCCU_bigrams_by_age Exercise 10.4 Based on the above bigrams from each age group (whose dispersion &gt;= 5), please identify the top 20 distinctive bigrams from each age group and visualize them in a bar plot. The distinctive bigrams are defined as follows: For each bigram type, identity their observed frequencies in respective age groups. Compute each bigram’s expected frequencies in each age group. We know that the total number of bigram tokens from each age group is: 3366 (Below_20), 37981 (AGE_20_40), 1287 (AGE_40_60). Therefore, the bigram sample sizes of different age groups vary a lot. We therefore expect that each bigram’s frequency would be distributed in these three age groups according to the proportions of each age group’s bigram sample size. So, the expected frequency of a bigram is calculated as follow (\\(N_i\\) refers to the total number of the bigram \\(i\\)): \\[\\text{Below_20_E} = N_i \\times \\frac{3366}{3366+37981+1287}\\] \\[\\text{AGE_20_40_E} = N_i \\times \\frac{37981}{3366+37981+1287}\\] \\[\\text{AGE_40_60_E} = N_i \\times \\frac{1287}{3366+37981+1287}\\] Calculate each bigram’s sum of normalized squared differences between the observed and the expected frequencies using the following formula ( \\(O_i\\) and \\(E_i\\) refers to the bigram’s observed/expected frequency of each age group): \\[ χ^2 = \\sum{\\frac{(O_i – E_i)^2}{E_i}} \\] The above \\(\\chi^2\\) statistics (i.e., the sum of all normalized squared differences) is known as Chi-square statistic. Use this metric to rank the bigrams for each age group and identity the top 20 distinctive bigrams from each age group. That is, identity the top 20 bigrams according to the bigrams’ Chi-square values for each age group. Determine each bigram’s preferred age group by two criteria: (a) the normalized squared differences in a specific age group (i.e., \\(\\frac{(O_i – E_i)^2}{E_i}\\)) indicates how much the observed frequency deviates from the expected; (b) the raw difference between \\(O_i\\) and \\(E_i\\) indicates whether the age group is a preferred one or a repulsed one (i.e., if in the age group \\(O_i\\) is greater than \\(E_i\\), it is likely a preferred age group). In other words, the preferred age group is defined as the age group (a) where \\(O_i\\) is greater than \\(E_i\\), AND (b) its normalized squared difference is the greatest among all. Visualize these distinctive bigrams in bar plots as shown below. In the bar plot, please show the bigram’s observed frequency in its preferred age group. The sample data frame below shows the expected frequencies (i.e., the columns of Below_20_E, AGE_20_40_E, AGE_40_60_E) and the Chi-square values (i.e., X2) and the PREFERRED_AGE for each bigram. The table is sorted by X2. Exercise 10.5 Please create a barplot, showing the top 20 distinctive trigrams of male or female speakers. Similar to the bigram example in the lecture notes, please consider only trigrams: that do not include paralinguistic tags; that have been used by at least FIVE different male or female speakers. After removing the above irrelevant trigrams, the total numbers of trigram tokens are : 5396 for female speakers, and 512 for male speakers. Follow the same principles specified in the previous exercise and identify the top 20 distinctive trigrams of each gender ranked according to the Chi-square values. Visualize these distinctive trigrams in bar plots as shown below. In the bar plot, please show the trigram’s observed frequency in its preferred gender group. The sample data frame below shows the expected frequencies (i.e., the columns of female_E, male_E) and the Chi-square values (i.e., X2) for each trigram The table is sorted by X2. References Chui, Kawai, &amp; Lai, H. (2008). The NCCU corpus of spoken Chinese: Mandarin, Hakka, and Southern Min. Taiwan Journal of Linguistics, 6(2). Chui, Kawai, Lai, Huei-ling, &amp; Chen, H.-C. (2017). The Taiwan Spoken Chinese Corpus [Book Section]. In R. Sybesma (Ed.), Encyclopedia of chinese language and linguistic (pp. 257–259). Brill. "],["xml.html", "Chapter 11 XML 11.1 BNC Spoken 2014 11.2 Processing one XML file 11.3 Process the Whole Directory of BNC2014 Sample 11.4 Metadata 11.5 BNC2014 for Socio-linguistic Variation 11.6 Lexical Analysis 11.7 Constructions Analysis", " Chapter 11 XML library(tidyverse) library(readtext) library(tidytext) library(quanteda) library(xml2) This chapter shows you how to process the recently released BNC 2014, which is by far the largest representative collection of spoken English collected in UK. For the purpose of our in-class tutorials, I have included a small sample of the BNC2014 in our demo_data. However, the whole dataset is now available via the official website: British National Corpus 2014. Please sign up for the complete access to the corpus if you need this corpus for your own research. 11.1 BNC Spoken 2014 XML (the eXtensible Markup Language) is an effective format for text data storage, where the information of markup and annotation is added to the written texts in a structured way. We can open an XML file with Google Chrome to take a look at its hierarchical structure. Before we process the data, we need to understand the structure of the XML tags in the files. Usually we would start from the documentation of the corpus. Please read The BNC 2014: User Manual amd Reference Guide for more detail. Other than that, the steps are pretty much similar to what we have learned before. In this chapter, we will use xml2 to process XML files. Please see Chapter 3.8.2 Some Notes on Handling XML Data in Gries (2016) for more discussions on XML processing. There are many R libraries supporting XML processing. The library rvest we use in Chapter 3 can also be used to process XML data. In addition to xml2 covered in this chapter, XML is another alternative for XML processing in R. 11.2 Processing one XML file Each BNC XML file is like a tree structure. It starts with text node. Each text consists of utterance nodes u. Each utterance node consists of word nodes w or other para-linguistic feature nodes (e.g., pauses or vocal features). Let’s see how we can process one single XML file from BNC2014 first. The steps include: Parse the XML file using read_xml(); Identity the XML root node using xml_root(); Extract all utterance nodes using xml_find_all(); Extract lexical units (word nodes) from each utterance node as well as their relevant annotations. Output everything as a data frame. First, we read and parse an XML file using read_xml(): ## read one XML corp_bnc&lt;-read_xml(x = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;, encoding = &quot;UTF-8&quot;) Second, we identity the XML root using xml_root(): ## Identity the XML root root&lt;- xml_root(corp_bnc) ## Name of xml root xml_name(root) [1] &quot;text&quot; Third, we extract all utterance nodes from the root using xml_find_all(), which supports the xpath query language for XML elements: XPath is a query language that allows us to effectively identify specific elements from HTML/XML documents. We have used this in the task of web crawling in Chapter 3. Please read Chapter 4 XPath in Munzert et al. (2014) very carefully to make sure that you know how to use XPath. ## Extract &lt;u&gt; from XML all_utterances &lt;- xml_find_all(root, xpath = &quot;//u&quot;) ## Check print.AsIs(all_utterances[1]) [[1]] {xml_node} &lt;u n=&quot;1&quot; who=&quot;S0024&quot; trans=&quot;nonoverlap&quot; whoConfidence=&quot;high&quot;&gt; [1] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;an&lt;/w&gt; [2] &lt;w pos=&quot;NNT1&quot; lemma=&quot;hour&quot; class=&quot;SUBST&quot; usas=&quot;T1:3&quot;&gt;hour&lt;/w&gt; [3] &lt;w pos=&quot;RRR&quot; lemma=&quot;later&quot; class=&quot;ADV&quot; usas=&quot;T4&quot;&gt;later&lt;/w&gt; [4] &lt;pause dur=&quot;short&quot;/&gt; [5] &lt;w pos=&quot;VV0&quot; lemma=&quot;hope&quot; class=&quot;VERB&quot; usas=&quot;X2:6&quot;&gt;hope&lt;/w&gt; [6] &lt;w pos=&quot;PPHS1&quot; lemma=&quot;she&quot; class=&quot;PRON&quot; usas=&quot;Z8&quot;&gt;she&lt;/w&gt; [7] &lt;w pos=&quot;VVZ&quot; lemma=&quot;stay&quot; class=&quot;VERB&quot; usas=&quot;M8&quot;&gt;stays&lt;/w&gt; [8] &lt;w pos=&quot;RP&quot; lemma=&quot;down&quot; class=&quot;ADV&quot; usas=&quot;Z5&quot;&gt;down&lt;/w&gt; [9] &lt;pause dur=&quot;short&quot;/&gt; [10] &lt;w pos=&quot;RG&quot; lemma=&quot;rather&quot; class=&quot;ADV&quot; usas=&quot;A13:5&quot;&gt;rather&lt;/w&gt; [11] &lt;w pos=&quot;JJ&quot; lemma=&quot;late&quot; class=&quot;ADJ&quot; usas=&quot;T4&quot;&gt;late&lt;/w&gt; attr(,&quot;class&quot;) [1] &quot;xml_nodeset&quot; print.AsIs(all_utterances[36]) [[1]] {xml_node} &lt;u n=&quot;36&quot; who=&quot;S0144&quot; trans=&quot;nonoverlap&quot; whoConfidence=&quot;high&quot;&gt; [1] &lt;vocal desc=&quot;laugh&quot;/&gt; [2] &lt;pause dur=&quot;short&quot;/&gt; [3] &lt;w pos=&quot;DD1&quot; lemma=&quot;that&quot; class=&quot;ADJ&quot; usas=&quot;Z8&quot;&gt;that&lt;/w&gt; [4] &lt;w pos=&quot;VBDZ&quot; lemma=&quot;be&quot; class=&quot;VERB&quot; usas=&quot;A3&quot;&gt;was&lt;/w&gt; [5] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;a&lt;/w&gt; [6] &lt;w pos=&quot;JJ&quot; lemma=&quot;funny&quot; class=&quot;ADJ&quot; usas=&quot;E4:1&quot;&gt;funny&lt;/w&gt; [7] &lt;w pos=&quot;NN1&quot; lemma=&quot;noise&quot; class=&quot;SUBST&quot; usas=&quot;X3:2&quot;&gt;noise&lt;/w&gt; attr(,&quot;class&quot;) [1] &quot;xml_nodeset&quot; From the above output, we can see that under the &lt;u&gt; node, we may expect not only word nodes &lt;w&gt; but also non-word tokens, such as &lt;pause .../&gt;, &lt;desc .../&gt; (and many more). Fourth, we need to extract all the children nodes (i.e., word and non-word nodes) from each utterance. This can be more complicated because each word node has a lot of annotations as attribute-value pairs in the start tag &lt;w ATTRIBUTE = \"VALUE\"&gt; XXX &lt;/w&gt;. Here let’s use the first utterance as an example. We can do the following: Extract all children nodes of the current utterance using xml_children(); Extract the tag name of each children node using xml_name(); Extract the corpus texts of each children node using xml_text(); Extract annotations (i.e., attributes) from each children node using xml_attr(); ## Example print.AsIs(all_utterances[1]) [[1]] {xml_node} &lt;u n=&quot;1&quot; who=&quot;S0024&quot; trans=&quot;nonoverlap&quot; whoConfidence=&quot;high&quot;&gt; [1] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;an&lt;/w&gt; [2] &lt;w pos=&quot;NNT1&quot; lemma=&quot;hour&quot; class=&quot;SUBST&quot; usas=&quot;T1:3&quot;&gt;hour&lt;/w&gt; [3] &lt;w pos=&quot;RRR&quot; lemma=&quot;later&quot; class=&quot;ADV&quot; usas=&quot;T4&quot;&gt;later&lt;/w&gt; [4] &lt;pause dur=&quot;short&quot;/&gt; [5] &lt;w pos=&quot;VV0&quot; lemma=&quot;hope&quot; class=&quot;VERB&quot; usas=&quot;X2:6&quot;&gt;hope&lt;/w&gt; [6] &lt;w pos=&quot;PPHS1&quot; lemma=&quot;she&quot; class=&quot;PRON&quot; usas=&quot;Z8&quot;&gt;she&lt;/w&gt; [7] &lt;w pos=&quot;VVZ&quot; lemma=&quot;stay&quot; class=&quot;VERB&quot; usas=&quot;M8&quot;&gt;stays&lt;/w&gt; [8] &lt;w pos=&quot;RP&quot; lemma=&quot;down&quot; class=&quot;ADV&quot; usas=&quot;Z5&quot;&gt;down&lt;/w&gt; [9] &lt;pause dur=&quot;short&quot;/&gt; [10] &lt;w pos=&quot;RG&quot; lemma=&quot;rather&quot; class=&quot;ADV&quot; usas=&quot;A13:5&quot;&gt;rather&lt;/w&gt; [11] &lt;w pos=&quot;JJ&quot; lemma=&quot;late&quot; class=&quot;ADJ&quot; usas=&quot;T4&quot;&gt;late&lt;/w&gt; attr(,&quot;class&quot;) [1] &quot;xml_nodeset&quot; ## Extract all children nodes (all_children &lt;- xml_children(all_utterances[1])) {xml_nodeset (11)} [1] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;an&lt;/w&gt; [2] &lt;w pos=&quot;NNT1&quot; lemma=&quot;hour&quot; class=&quot;SUBST&quot; usas=&quot;T1:3&quot;&gt;hour&lt;/w&gt; [3] &lt;w pos=&quot;RRR&quot; lemma=&quot;later&quot; class=&quot;ADV&quot; usas=&quot;T4&quot;&gt;later&lt;/w&gt; [4] &lt;pause dur=&quot;short&quot;/&gt; [5] &lt;w pos=&quot;VV0&quot; lemma=&quot;hope&quot; class=&quot;VERB&quot; usas=&quot;X2:6&quot;&gt;hope&lt;/w&gt; [6] &lt;w pos=&quot;PPHS1&quot; lemma=&quot;she&quot; class=&quot;PRON&quot; usas=&quot;Z8&quot;&gt;she&lt;/w&gt; [7] &lt;w pos=&quot;VVZ&quot; lemma=&quot;stay&quot; class=&quot;VERB&quot; usas=&quot;M8&quot;&gt;stays&lt;/w&gt; [8] &lt;w pos=&quot;RP&quot; lemma=&quot;down&quot; class=&quot;ADV&quot; usas=&quot;Z5&quot;&gt;down&lt;/w&gt; [9] &lt;pause dur=&quot;short&quot;/&gt; [10] &lt;w pos=&quot;RG&quot; lemma=&quot;rather&quot; class=&quot;ADV&quot; usas=&quot;A13:5&quot;&gt;rather&lt;/w&gt; [11] &lt;w pos=&quot;JJ&quot; lemma=&quot;late&quot; class=&quot;ADJ&quot; usas=&quot;T4&quot;&gt;late&lt;/w&gt; ## Extract tag names of all children nodes (token_names &lt;- xml_name(all_children)) [1] &quot;w&quot; &quot;w&quot; &quot;w&quot; &quot;pause&quot; &quot;w&quot; &quot;w&quot; &quot;w&quot; &quot;w&quot; &quot;pause&quot; [10] &quot;w&quot; &quot;w&quot; ## Extract texts of all children nodes (token_texts &lt;- xml_text(all_children)) [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; [9] &quot;&quot; &quot;rather&quot; &quot;late&quot; ## Extract annotations of all children nodes (token_pos &lt;- xml_attr(all_children, attr=&quot;pos&quot;)) ## POS [1] &quot;AT1&quot; &quot;NNT1&quot; &quot;RRR&quot; NA &quot;VV0&quot; &quot;PPHS1&quot; &quot;VVZ&quot; &quot;RP&quot; NA [10] &quot;RG&quot; &quot;JJ&quot; (token_lemma &lt;- xml_attr(all_children, attr=&quot;lemma&quot;)) ## lemma [1] &quot;a&quot; &quot;hour&quot; &quot;later&quot; NA &quot;hope&quot; &quot;she&quot; &quot;stay&quot; &quot;down&quot; [9] NA &quot;rather&quot; &quot;late&quot; (token_class &lt;- xml_attr(all_children, attr=&quot;class&quot;)) ## POS [1] &quot;ART&quot; &quot;SUBST&quot; &quot;ADV&quot; NA &quot;VERB&quot; &quot;PRON&quot; &quot;VERB&quot; &quot;ADV&quot; NA [10] &quot;ADV&quot; &quot;ADJ&quot; (token_usas &lt;- xml_attr(all_children, attr=&quot;usas&quot;)) ## semantic tag [1] &quot;Z5&quot; &quot;T1:3&quot; &quot;T4&quot; NA &quot;X2:6&quot; &quot;Z8&quot; &quot;M8&quot; &quot;Z5&quot; NA [10] &quot;A13:5&quot; &quot;T4&quot; ## Create DF cur_utterance_df &lt;- data.frame( names = token_names, texts = token_texts, pos = token_pos, lemma = token_lemma, class = token_class, usas = token_usas ) cur_utterance_df In the result data frame above, we can see that there are quite a few rows which have NA values across all columns. Do you know why? Take the first utterance node of the XML document for example. Each utterance node includes words as well as non-word tokens (i.e., para-linguistic annotations &lt;pause .../&gt;). We can retrieve: Strings of words in an utterance Lemma forms of all words in the utterance POS tags of all words in the utterance (BNC2014 uses UCREL CLAWS6 Tagset) Paralinguistic tags in the utterance With the above example, it seems that we have completed the processing of the first utterance node and extracted most information we need from the first utterance. But there are still more we can extract from the utterance node: We can extract more information about the para-linguistic tags (e.g., &lt;pause .../&gt;, &lt;vocal .../&gt;), such as the pause duration, or other vocal descriptions. ## Check again the paralinguistic tags print.AsIs(all_utterances[1]) [[1]] {xml_node} &lt;u n=&quot;1&quot; who=&quot;S0024&quot; trans=&quot;nonoverlap&quot; whoConfidence=&quot;high&quot;&gt; [1] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;an&lt;/w&gt; [2] &lt;w pos=&quot;NNT1&quot; lemma=&quot;hour&quot; class=&quot;SUBST&quot; usas=&quot;T1:3&quot;&gt;hour&lt;/w&gt; [3] &lt;w pos=&quot;RRR&quot; lemma=&quot;later&quot; class=&quot;ADV&quot; usas=&quot;T4&quot;&gt;later&lt;/w&gt; [4] &lt;pause dur=&quot;short&quot;/&gt; [5] &lt;w pos=&quot;VV0&quot; lemma=&quot;hope&quot; class=&quot;VERB&quot; usas=&quot;X2:6&quot;&gt;hope&lt;/w&gt; [6] &lt;w pos=&quot;PPHS1&quot; lemma=&quot;she&quot; class=&quot;PRON&quot; usas=&quot;Z8&quot;&gt;she&lt;/w&gt; [7] &lt;w pos=&quot;VVZ&quot; lemma=&quot;stay&quot; class=&quot;VERB&quot; usas=&quot;M8&quot;&gt;stays&lt;/w&gt; [8] &lt;w pos=&quot;RP&quot; lemma=&quot;down&quot; class=&quot;ADV&quot; usas=&quot;Z5&quot;&gt;down&lt;/w&gt; [9] &lt;pause dur=&quot;short&quot;/&gt; [10] &lt;w pos=&quot;RG&quot; lemma=&quot;rather&quot; class=&quot;ADV&quot; usas=&quot;A13:5&quot;&gt;rather&lt;/w&gt; [11] &lt;w pos=&quot;JJ&quot; lemma=&quot;late&quot; class=&quot;ADJ&quot; usas=&quot;T4&quot;&gt;late&lt;/w&gt; attr(,&quot;class&quot;) [1] &quot;xml_nodeset&quot; print.AsIs(all_utterances[36]) [[1]] {xml_node} &lt;u n=&quot;36&quot; who=&quot;S0144&quot; trans=&quot;nonoverlap&quot; whoConfidence=&quot;high&quot;&gt; [1] &lt;vocal desc=&quot;laugh&quot;/&gt; [2] &lt;pause dur=&quot;short&quot;/&gt; [3] &lt;w pos=&quot;DD1&quot; lemma=&quot;that&quot; class=&quot;ADJ&quot; usas=&quot;Z8&quot;&gt;that&lt;/w&gt; [4] &lt;w pos=&quot;VBDZ&quot; lemma=&quot;be&quot; class=&quot;VERB&quot; usas=&quot;A3&quot;&gt;was&lt;/w&gt; [5] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;a&lt;/w&gt; [6] &lt;w pos=&quot;JJ&quot; lemma=&quot;funny&quot; class=&quot;ADJ&quot; usas=&quot;E4:1&quot;&gt;funny&lt;/w&gt; [7] &lt;w pos=&quot;NN1&quot; lemma=&quot;noise&quot; class=&quot;SUBST&quot; usas=&quot;X3:2&quot;&gt;noise&lt;/w&gt; attr(,&quot;class&quot;) [1] &quot;xml_nodeset&quot; We can extract utterance-level metadata as well (i.e., the attributes of the &lt;u&gt; node). its unique index (n) speaker id (who) transition type (trans, i.e., whether or not the transition between turns was overlapping) attribution confidence (whoconfidence, whether or not the transcriber was confident that they had correctly identified the speaker of the turn) ## node-level attributes xml_attrs(all_utterances[1]) [[1]] n who trans whoConfidence &quot;1&quot; &quot;S0024&quot; &quot;nonoverlap&quot; &quot;high&quot; Exercise 11.1 Now we know how to extract token-level information and utterance-level annotation from each utterance. Please come up with a way to extract all relevant linguistic data from all utterances in the file demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml, including their word and non-word tokens as well as their annotations and utterance-level metadata. Ideally, the resulting data frame should consist of rows being the tokens of the utterances, and columns including the attributes and metadata of each token. Specifically, for the word tokens, the data frame should include not only the word strings, but also the word-level annotations of part-of-speech tags, lemmas, and semantic tags (i.e., usas). For non-word tokens (paralinguistic tags), the data frame should also include their information of attributes in a column, called notes. Also, each token is connected to the utterance-level metadata, such as the utterance ID, speaker ID etc. A sample data frame of the XML file is provided below (Only No 1 and 36 utterance nodes are included in the data frame for your reference). 11.3 Process the Whole Directory of BNC2014 Sample 11.3.1 Define Function In Section 11.1, if you have figured out how to extract the token-based data frame from all utterances in an XML file, you can easily wrap the whole procedure as one function. With this function, we can repeat the same procedure for every XML file in BNC2014. For example, let’s assume that we have defined a function: read_xml_bnc2014 &lt;- function(xml){ ... ... ... } # endfunc This function takes one xml file as an argument and returns a token-based data frame, consisting of token strings and other relevant utterance-level and token-level information from the XML. word_df &lt;- read_xml_bnc2014( xmlfile = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) word_df %&gt;% filter(n %in% c(&quot;1&quot;,&quot;36&quot;)) Exercise 11.2 Now your job is to create this function, read_xml_bnc2014(xmlfile = \" \"). This function should take the path to one XML file as the input and return a token-based data frame of the XML file as the output. 11.3.2 Apply Function Now we can utilize the self-defined function, read_xml_bnc2014(), and process all XML files in the demo_data/corp-bnc-spoken2014-sample/. Then we combine the individual data.frame returned from each XML into a bigger one, i.e., corp_bnc_token_df: Please note that the following code chunk assumes that you have created the function read_xml_bnc2014() from the previous exercise. ## Get all XML filenames bnc_flist &lt;- dir(&quot;demo_data/corp-bnc-spoken2014-sample/&quot;,full.names = T) ## Extract token-based df from each XML system.time(corp_bnc_list &lt;- map(bnc_flist, read_xml_bnc2014)) ## Combine all df&#39;s corp_bnc_token_df &lt;- bind_rows(corp_bnc_list) ## save file write_csv(corp_bnc_token_df, file= &quot;demo_data/data-corp-token-bnc2014.csv&quot;) It takes a while to process/parse all the files included in the sample directory because we parse the entire XML file and extract almost everything (annotations + metadata) from the file. You may store this corp_bnc_token_df data frame output for later use so that you don’t have to process the XML files every time you work with BNC2014. The parsed token-based data frame of the BNC2014 is available in our demo_data/data-corp-token-bnc2014.csv. You can check if your output is the same as the CSV in the demo_data. ## Loading corp_bnc_token_df &lt;- read_csv(file = &quot;demo_data/data-corp-token-bnc2014.csv&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) ## Checking corp_bnc_token_df %&gt;% filter(xml_id == &quot;S2A5-tgd.xml&quot; &amp; n %in% c(&quot;1&quot;,&quot;36&quot;)) Also, in addition to &lt;pause&gt; and &lt;vocal&gt;, there are many other non-word tokens under the utterance nodes: corp_bnc_token_df %&gt;% count(name, sort = TRUE) In the current design of read_xml_bnc2014(), we keep track of the annotations included in these non-word tokens in the notes column. ## Random sample of non-word tokens corp_bnc_token_df %&gt;% filter(!name %in% c(&quot;w&quot;)) %&gt;% group_by(name) %&gt;% sample_n(4) You may check the BNC2014 documentation for more detail about the meanings of these XML tags. 11.4 Metadata The best thing about BNC2014 is its rich demographic information related to the settings and speakers of the conversations. The whole corpus comes with two metadata sets: bnc2014spoken-textdata.tsv: metadata for each text transcript bnc2014spoken-speakerdata.tsv: metadata for each speaker ID These two metadata sets allow us to get more information about each transcript as well as the speakers within those transcripts. 11.4.1 Text Metadata There are two files that are relevant to the text metadata: bnc2014spoken-textdata.tsv: This file includes the metadata information of each text file metadata-fields-text.txt: This file includes the column names/meanings of the previous text metadata tsv, i.e., bnc2014spoken-textdata.tsv. ## text metadata bnc_text_meta &lt;- read_tsv( file = &quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-textdata.tsv&quot;, col_names = FALSE, locale = locale(encoding = &quot;UTF-8&quot;) ) bnc_text_meta ## columns about text metadata bnc_text_meta_names &lt;- read_tsv( file = &quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-text.txt&quot;, skip = 1, col_names = TRUE, locale = locale(encoding = &quot;UTF-8&quot;) ) bnc_text_meta_names ## Rename the columns of text metadata names(bnc_text_meta) &lt;- c(&quot;textid&quot;, bnc_text_meta_names$`XML tag`) bnc_text_meta Exercise 11.3 With the text metadata bnc_text_meta, please compute: the total amount of recording length for the current subset of BNC2014 (See the column rec_length); the average recording length of a transcript. [1] &quot;Total duration of the BNC2014 subset: 41 Days 03 Hours 50 Minutes 18 Seconds&quot; [1] &quot;On average, each transcript is about 47 Minutes 22.70 Seconds&quot; 11.4.2 Speaker Metadata There are two files that are relevant to the speaker metadata: bnc2014spoken-speakerdata.tsv: This file includes the demographic information of each speaker metadata-fields-speaker.txt: This file includes the column names/meanings of the previous speaker metadata tsv, i.e., bnc2014spoken-speakerdata.tsv. ## speaker metadata bnc_sp_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-speakerdata.tsv&quot;, col_names = FALSE, locale = locale(encoding = &quot;UTF-8&quot;)) bnc_sp_meta ## columns about speaker metadata bnc_sp_meta_names &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-speaker.txt&quot;, skip = 1, col_names = TRUE, locale = locale(encoding = &quot;UTF-8&quot;)) bnc_sp_meta_names names(bnc_sp_meta) &lt;- c(&quot;spid&quot;, bnc_sp_meta_names$`XML tag`) bnc_sp_meta Exercise 11.4 With the speaker metadata bnc_sp_meta, create a bar plot showing the age distribution (cf. agerange column) of the speakers by different genders in the current BNC2014 subset as shown below. (Disregard cases where the speaker’s age is unknown.) 11.5 BNC2014 for Socio-linguistic Variation Now with both the text-level and speaker-level metadata, bnc_text_meta and bnc_sp_meta, we can easily connect the utterances to speaker and text profiles using their unique ID’s. BNC2014 was born for the study of socio-linguistic variation. Here I would like to show you some naive examples, but you can get the ideas of what one can do with BNC2014. 11.6 Lexical Analysis With the token-based data frame, we can perform lexical analysis on the lexical variations on specific social dimensions. 11.6.1 Word Frequency vs. Gender In this section, I would like to demonstrate how to explore the gender differences in language. Let’s assume that we like to know which adjectives are most frequently used by men and women. ## Extract adjectives and connect to speaker metadata corp_bnc_adj_gender &lt;- corp_bnc_token_df %&gt;% ## token-based DF filter(str_detect(pos, &quot;^(JJ[RT]?$)&quot;)) %&gt;% ## adjectives left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) %&gt;% ## link to metadata mutate(gender = factor(gender, levels=c(&quot;F&quot;,&quot;M&quot;))) %&gt;% ## factor gender filter(!is.na(gender)) ## remove gender-undefined tokens ## Check corp_bnc_adj_gender %&gt;% head(100) 11.6.2 Frequency and Keyword Analysis After we extract word tokens that are adjectives, we can create a frequency list for each gender: ## create freq list by gender freq_adj_by_gender &lt;- corp_bnc_adj_gender %&gt;% count(gender, lemma, sort = T) ## Show top 10 adjectives for each gender freq_adj_by_gender %&gt;% group_by(gender) %&gt;% top_n(10, n) %&gt;% ungroup %&gt;% arrange(gender, desc(n)) Female wordcloud require(wordcloud2) ## Create female adjective word cloud freq_adj_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% top_n(100,n) %&gt;% select(lemma, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Male wordcloud ## Create male adjective word cloud freq_adj_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% top_n(100,n) %&gt;% select(lemma, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 11.5 Which adjectives are more often used by male and female speakers? This should be a statistical problem. We can in fact extend our keyword analysis (cf. Chapter 6) to this question. Please use the statistics of keyword analysis to find out the top 20 adjectives that are strongly attracted to female and male speakers according to G2 statistics. Please include in the analysis words whose frequencies &gt;= 20 in the entire corpus. Also, please note the problem of the NaN values out of the log(). The top 50 adjectives by female speakers based on keyness The top 50 adjectives by male speakers based on keyness 11.7 Constructions Analysis 11.7.1 From Token-based to Turn-based Data Frame We can also conduct analysis of specific constructions. We know that constructions often span word boundaries, but what we have right now is a token-based data frame of BNC2014. For construction or multiword-unit analysis, we can convert the token-based DF into a turn-based DF, but keep necessary token-level annotations needed for your research project. In this demonstration, I will show you how to convert the token-based DF into a turn-based DF, and keep the strings of word forms as well as parts-of-speech tags of words for each token. First, we transform the token-based data frame into a utterance-based token by nesting the token-level information of each utterance into a sub data frame. ## Load processed CSV of BNC2014 corp_bnc_token_df &lt;- read_csv(&quot;demo_data/data-corp-token-bnc2014.csv&quot;, locale = locale(encoding = &quot;UTF-8&quot;)) ## Check head(corp_bnc_token_df, 50) ## From token to utterance DF corp_bnc_utterance_df &lt;- corp_bnc_token_df %&gt;% select(-trans, -whoconfidence) %&gt;% ## remove irrelevant columns group_by(xml_id, n, who) %&gt;% ## collapse utterance-level columns nest %&gt;% ## nest each utterance&#39;s token DF into sub-DF ungroup ## check corp_bnc_utterance_df ## check a sub DF of the first utterance corp_bnc_utterance_df$data[[1]] This is our first time using nest() from tidyr, along with the group_by() function. The idea is simple: the grouping variables (i.e., utterance-level variables in our case) remain in the outer data frame and the others are nested (i.e., token-level variables in our case). I hope that the following animation provides a more comprehensive visual intuition of this nesting transformation. Second, we create a function to process each utterance’s token-level data frame (the data column). In this self-defined function, we: Extract the word forms (texts) and POS of each token Extract para-linguistic tag names. For non-word tokens, we use the name of the extra-linguistic XML tag, enclosed by &lt; and &gt;, to represent the nature of the extra-linguistic annotations in the utterance. Concatenate each utterance’s token-level information (texts + annotations + para-linguistic tags) into a long string ## Define a function ## to process the utterance&#39;s token-level data extract_wordtag_string &lt;- function(u_df){ utterance_df &lt;- u_df cur_text &lt;-utterance_df$text ## all word forms cur_pos &lt;- utterance_df$pos ## all pos tag_index &lt;-which(is.na(cur_text)) ## check paralinguistic index if(length(tag_index)&gt;0){ ## retrieve paralinguistic cur_pos[tag_index]&lt;- cur_text[tag_index] &lt;- paste0(&quot;&lt;&quot;,utterance_df$name[tag_index],&quot;&gt;&quot;, sep =&quot;&quot;) } ## concatenate and output paste(cur_text, cur_pos, sep = &quot;_&quot;, collapse=&quot; &quot;) } ## endfunc ## Example use extract_wordtag_string(corp_bnc_utterance_df$data[[1]]) [1] &quot;an_AT1 hour_NNT1 later_RRR &lt;pause&gt;_&lt;pause&gt; hope_VV0 she_PPHS1 stays_VVZ down_RP &lt;pause&gt;_&lt;pause&gt; rather_RG late_JJ&quot; extract_wordtag_string(corp_bnc_utterance_df$data[[2]]) [1] &quot;well_RR she_PPHS1 had_VHD those_DD2 two_MC hours_NNT2 earlier_RRR&quot; Finally, in the utterance-based data frame, we can apply the self-defined function extract_wordtag_string() to each utterance’s token DF to obtain an enriched version of the utterance’s full texts. ## Applying function corp_bnc_utterance_df %&gt;% mutate(utterance = map_chr(data, extract_wordtag_string)) %&gt;% select(-data) -&gt; corp_bnc_utterance_df ## Check corp_bnc_utterance_df %&gt;% head(50) If you would like to make use of more token-level information provided by BNC2014 (e.g. semantic annotations), you can revise the function extract_wordtag_string() and include these annotations before the string concatenation. With the above utterance-based DF of the corpus, we can extract constructions or morphosyntactic patterns from the utterance column, utilizing the parts-of-speech tags provided by BNC2014. 11.7.2 Degree ADV + ADJ In this section let’s look at the adjectives that are emphasized in conversations (e.g., too bad, very good, quite cheap) and examine how these emphatic adjectives may differ in speakers of different genders. Here we define our patterns, utilizing the POS tags and the regular expressions: [^_]+_RG [^_]+_JJ. First, we extract the target patterns by converting the utterance-based DF into a pattern-based DF. We at the same time link each match with the speaker metadata. ## from utterance to pattern ## and linking speaker metadata corp_bnc_pat_gender &lt;- corp_bnc_utterance_df %&gt;% unnest_tokens(pattern, ## new unit utterance, ## old unit token = function(x) str_extract_all(x, &quot;[^_ ]+_RG [^_ ]+_JJ&quot;), to_lower = FALSE) %&gt;% left_join(bnc_sp_meta, by = c(&quot;who&quot; = &quot;spid&quot;)) ## Check corp_bnc_pat_gender %&gt;% head(100) Second, we can create pattern frequencies by genders. ## Create frequency list freq_pat_by_gender &lt;- corp_bnc_pat_gender %&gt;% mutate(pattern = str_replace_all(pattern, &quot;_[^_ ]+&quot;,&quot;&quot;)) %&gt;% # remove pos tags select(gender, pattern) %&gt;% count(gender, pattern, sort=T) ## print top 100 by gender freq_pat_by_gender %&gt;% group_by(gender) %&gt;% top_n(10,n) %&gt;% ungroup %&gt;% arrange(gender, desc(n)) We can also create wordclouds for the patterns by gender ## word clouds freq_pat_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% top_n(100, n) %&gt;% select(pattern, n) %&gt;% wordcloud2(size = 0.9) freq_pat_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% top_n(100, n) %&gt;% select(pattern, n) %&gt;% wordcloud2(size = 0.9) Exercise 11.6 In the previous task, we have got the frequency list of the patterns (i.e., “Adverb + Adjective”) by gender, i.e., freq_pat_by_gender. Please create a wide version of the frequency list, where each row is a pattern type and the columns include the frequencies of the patterns in male and female speakers, as well as the dispersion of the pattern in male and female speakers. A sample has been provided below. Dispersion is defined as the number of (different) speakers who use the pattern at least once. Exercise 11.7 So far we have been looking at the constructional schema of ADV + ADJ. Now let’s examine further how adjectives that are emphasized differ among speakers of different genders. That is, do male speakers tend to emphasize adjectives that are different from those that are emphasized by females? Now it should be clear to you that which adjectives are more likely to be emphasized by ADV in male and female utterances should be a statistical question. Please use the statistics G2 from keyword analysis to find out the top 10 Adjectives that are strongly attracted to female and male speakers according to G2 statistics. Please include in the analysis adjectives whose dispersion &gt;= 2 in the respective corpus, i.e., adjectives that have been used by at least TWO different male or female speakers. Also, please note the problem of the NaN values out of the log(). You first need to get the frequency list of the adjectives that occur in this constructional schema, ADV + ADJ: Then you convert the frequency list from a long format into a wide format for keyness computation. With the above distributional information, you can compute the keyness of the adjectives. Exercise 11.8 Please analyze the verbs that co-occur with the first-person pronoun I in BNC2014 in terms of speakers of different genders. Please create a frequency list of the verbs that follow the first person pronoun I in demo_data/corp-bnc-spoken2014-sample. Verbs are defined as any words whose POS tag starts with VV. There can be up to two word tokens in-between the pronoun I and the verb (VV) (e.g., “I-canelled” pair in I_PPIS1 've_VH0 cancelled_VVN, “I-think” pair in I_PPIS1 do_VD0 n't_XX think_VVI). Visualize the top 20 co-occurring verbs in a bar plot as shown below. All verb types on the top 100 lists of male and female speakers: Exercise 11.9 Please analyze the recurrent four-grams used by male and female speakers by showing the top 20 four-grams used by males and females respectively ranked according to their frequencies. Please disregard : Four-grams whose dispersion is \\(\\leq\\) 10. Dispersion of four-grams is defined as the number of speakers who have used the four-gram in the corpus. Four-grams that include the non-word tokens (e.g., pauses or extralinguistic tags). References Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. "],["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Distributional Semantics 12.2 Vector Space Model for Documents 12.3 Vector Space Model for Words (Self-Study) 12.4 Exercises", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(showtext) showtext_auto(enable = T) In this chapter, I would like to talk about the idea of distributional semantics, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized computationally and quantified using the distributional data of the linguistic units in the corpus. Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts. 12.1 Distributional Semantics Distributional approach to semantics was first formulated by John Firth in his famous quotation: You shall know a word by the company it keeps (Firth, 1957, p. 11). In other words, words that occur in the same contexts tend to have similar meanings (Z. Harris, 1954). [D]ifference of meaning correlates with difference of distribution. (Z. S. Harris, 1970, p. 785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves. (De Deyne et al., 2016) In computational linguistics, this idea has been implemented in the modeling of lexical semantics and documents topics. The lexical meanings of words or topics of documents can be computationally represented by the distributional information of their co-occurring words. Document Semantics/Topics This distributional model can be applied to the semantic representation of documents in corpus as well. One can extract the distributional information of target documents automatically from large corpora, i.e., their co-occurring contextual features. The co-occurrence frequencies between target documents and contextual features can also be combined in long vectors, which can be utilized to computationally measure the inter-document similarity/difference. Lexical Semantics We can extract the distributional information of target words automatically from large corpora, i.e., their co-occurring contextual features. These co-occurrence frequencies (raw or weighted) between target words and contextual features can be combined in long vectors, which can be utilized to computationally measure the semantic distance/similarity in-between the target words. Therefore, this distributional approach to meanings is sometimes referred to as Vector Space Semantics. In short, we can represent the semantics of a linguistic unit based on its co-occurrence patterns with specific contextual features. If two words co-occur with similar sets of contextual features, they are likely to be semantically similar. With the vectorized representations of words, we can compute their semanitc distances. 12.2 Vector Space Model for Documents Now I would like to demonstrate how we can adopt this vector space model to study the semantics of documents. 12.2.1 Data Processing Flowchart In Chapter 5, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of the corpus documents. Most importantly, a new object class is introduced in Figure 12.1, i.e., the dfm object in quanteda. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterize the documents. The cells in the matrix are the co-occurrence statistics between each document and the feature. Different ways of operationalizing the features and the cell values may lead to different types of dfm. In this section, I would like to show you how to create dfm of a corpus and what are the common ways to define features and cell values for the analysis of document semantics via vector space representation. Figure 12.1: English Text Analytics Flowchart (v2) 12.2.2 Document-Feature Matrix (dfm) To create a dfm, i.e., Dcument-Feature-Matrix, of your corpus data, there are generally three steps: Create an corpus object of your data; Tokenize the corpus object into a tokens object; Create the dfm object based on the tokens object In this tutorial, I will use the same English dataset as we discussed in Chapter 5, the data_corpus_inaugural, preloaded in the package quanteda. For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). ## `corpus` corp_us &lt;- data_corpus_inaugural ## `tokens` corp_us_tokens &lt;- tokens(corp_us) ## `dfm` corp_us_dfm &lt;- dfm(corp_us_tokens) ## check dfm corp_us_dfm Document-feature matrix of: 59 documents, 9,439 features (91.84% sparse) and 4 docvars. features docs fellow-citizens of the senate and house representatives : 1789-Washington 1 71 116 1 48 2 2 1 1793-Washington 0 11 13 0 2 0 0 1 1797-Adams 3 140 163 1 130 0 2 0 1801-Jefferson 2 104 130 0 81 0 0 1 1805-Jefferson 0 101 143 0 93 0 0 0 1809-Madison 1 69 104 0 43 0 0 0 features docs among vicissitudes 1789-Washington 1 1 1793-Washington 0 0 1797-Adams 4 0 1801-Jefferson 1 0 1805-Jefferson 7 0 1809-Madison 0 0 [ reached max_ndoc ... 53 more documents, reached max_nfeat ... 9,429 more features ] ## Check object class class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; class(corp_us) [1] &quot;corpus&quot; &quot;character&quot; class(corp_us_dfm) [1] &quot;dfm&quot; attr(,&quot;package&quot;) [1] &quot;quanteda&quot; 12.2.3 Intuition for DFM What is dfm anyway? A document-feature-matrix is a simple co-occurrence table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s) (i.e., the contextual features in the vector space model). If the contextual feature is a word, then this dfm would be a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the contextual feature is an n-gram, then this dfm would be a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the values in the cells of dfm? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the contextual feature (i.e., column) in a particular document (i.e., row). For example, in the corpus data_corpus_inaugural, based on the corp_us_dfm created earlier, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of “representatives”, 48 occurrences of “and”. Bag of Words Representation By default, the dfm() would create a document-by-word matrix, i.e., generating words as the contextual features. A dfm with words as the contextual features is the simplest way to characterize the documents in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each document as bags of words. That is, how the words are arranged relative to each other is ignored (i.e., the morpho-syntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word dfm should be the most naive characterization of the texts. In many computational tasks, however, it turns out that this simple bag-of-words model is very effective in modeling the semantics of the documents. 12.2.4 More sophisticated DFM The dfm() with the default settings creates a document-by-word matrix. We can create more sophisticated DFM by refining the contextual features. A document-by-ngram matrix can be more informative because the contextual features take into account (partial &amp; limited) sequential information between words. In quanteda, to obtain an ngram-based dfm: Create an ngram-based tokens using tokens_ngrams() first; Create an ngram-based dfm based on the ngram-based tokens; ## create ngrams tokens corp_us_ngrams &lt;- tokens_ngrams(corp_us_tokens, n = 2:3) ## create ngram-based dfm corp_us_ngrams_dfm &lt;- dfm(corp_us_ngrams) 12.2.5 Distance/Similarity Metrics The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with the other documents (i.e., the other rows). For example, now the document 1789-Washington can be represented as a series of numeric values: ## contextual feautres of `1789-Washington` corp_us_dfm[1,] Document-feature matrix of: 1 document, 9,439 features (93.61% sparse) and 4 docvars. features docs fellow-citizens of the senate and house representatives : 1789-Washington 1 71 116 1 48 2 2 1 features docs among vicissitudes 1789-Washington 1 1 [ reached max_nfeat ... 9,429 more features ] The idea is that if two documents co-occur with similar sets of contextual features, they are more likely to be similar in their semantics as well. And now with the numeric representation of the documents, we can quantify these similarities. Take a two-dimensional space for instance. Let’s assume that we have three simple documents: \\(x\\), \\(y\\), \\(z\\), and each document is vectorized as a vector of two numeric values. x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) If we visualize these document vectors in a two-dimensional space, we can compute their distance/similarity mathematically. In Math, there are in general two types of metrics to measure the relationship between vectors: distance-based vs. similarity-based metrics. Figure 12.2: Vector Representation Distance-based Metrics Many distance measures of vectors are based on the following formula and differ in the parameter \\(k\\). \\[\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^k}\\big)^{\\frac{1}{k}}\\] The n in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.) When k is set to 2, it computes the famous Euclidean distance of two vectors, i.e., the direct spatial distance between two points on the n-dimensional space (Remember Pythagorean Theorem? ) \\[\\sqrt{\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^2}\\big)}\\] When \\(k = 1\\), the distance is referred to as Manhattan Distance or City Block Distance: \\[\\sum_{i = 1}^{n}{|x_i - y_i|}\\] When \\(k\\geq3\\), the distance is a generalized form, called, Minkowski Distance at the order of \\(k\\). In other words, \\(k\\) represents the order of norm. The Minkowski Distance of the order 1 is the Manhattan Distance; the Minkowski Distance of the order 2 is the Euclidean Distance. ## Create vectors x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) ## computing pairwise euclidean distance sum(abs(x-y)^2)^(1/2) # XY distance [1] 6 sum(abs(y-z)^2)^(1/2) # YZ distnace [1] 4.472136 sum(abs(x-z)^2)^(1/2) # XZ distnace [1] 8.944272 The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure 12.3) Figure 12.3: Distance-based Metric: Euclidean Distance Similarity-based Metrics In addition to distance-based metrics, we can measure the similarity of vectors using a similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is Cosine Similarity, which can be computed as follows: \\[cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\] # comuting pairwise cosine similarity sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2))) # xy [1] 0.9778024 sum(y*z)/(sqrt(sum(y^2))*sqrt(sum(z^2))) # yz [1] 0.4961389 sum(x*z)/(sqrt(sum(x^2))*sqrt(sum(z^2))) # xz [1] 0.3032037 The Cosine Similarity ranges from -1 (= the least similar) to 1 (= the most similar). The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the greater their cosine similarity, the smaller the arcs, the closer they are. Cosine Similarity is related to Pearson Correlation. If you would like to know more about their differences, take a look at this comprehensive blog post, Cosine Similarity VS Pearson Correlation Coefficient. Therefore, it is clear to see that cosine similarity highlights the documents similarities in terms of whether their values on all the dimensions (contextual features) vary in the same directions. However, distance-based metrics would highlight the document similarities in terms of how much their values on all the dimensions differ. Computing pairwise distance/similarity using quanteda In quanteda.textstats library, there are two main functions that can help us compute pairwise similarities/distances between vectors using two useful functions: textstat_simil(): similarity-based metrics textstat_dist(): distance-based metrics The expected input argument of these two functions is a dfm and they compute all pairwise distance/similarity metrics in-between the rows of the DFM (i.e., the documents). ## Create a simple DFM xyz_dfm &lt;- as.dfm(matrix(c(1,9,1,3,5,1), byrow=T, ncol=2)) ## Rename documents docnames(xyz_dfm) &lt;- c(&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;) ## Check xyz_dfm Document-feature matrix of: 3 documents, 2 features (0.00% sparse) and 0 docvars. features docs feat1 feat2 X 1 9 Y 1 3 Z 5 1 ## Computing cosine similarity textstat_simil(xyz_dfm, method=&quot;cosine&quot;) textstat_simil object; method = &quot;cosine&quot; X Y Z X 1.000 0.978 0.303 Y 0.978 1.000 0.496 Z 0.303 0.496 1.000 ## Computing euclidean distance textstat_dist(xyz_dfm, method = &quot;euclidean&quot;) textstat_dist object; method = &quot;euclidean&quot; X Y Z X 0 6.00 8.94 Y 6.00 0 4.47 Z 8.94 4.47 0 There are other useful functions in R that can compute the pairwise distance/similarity metrics on a matrix. When using these functions, please pay attention to whether they provide distance or similarity metrics because these two are very different in meanings. For example, amap::Dist() provides cosine-based distance, not similarity. ## Compute cosine distance with amap::Dist() amap::Dist(xyz_dfm, method = &quot;pearson&quot;) X Y Y 0.02219759 Z 0.69679634 0.50386106 ## Compute cosine similarity with amap::Dist() (1- amap::Dist(xyz_dfm, method=&quot;pearson&quot;)) X Y Y 0.9778024 Z 0.3032037 0.4961389 Interim Summary The Euclidean Distance metric is a distance-based metric: the larger the value, the more distant the two vectors. The Cosine Similarity metric is a similarity-based metric: the larger the value, the more similar the two vectors. Based on our computations of the metrics for the three vectors, now in terms of the Euclidean Distance, y and z are closer; in terms of Cosine Similarity, x and y are closer. Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the documents/words: The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors) The relative increase/decrease of the values on each dimension (i.e., the curvatures of vectors) There are many other distance-based or similarity-based metrics available. For more detail, please see Manning &amp; Schütze (1999) Ch15.2.2. and Jurafsky &amp; Martin (2020) Ch6: Vector Semantics and Embeddings. 12.2.6 Multidimensiona Space Back to our example of corp_us_dfm, it is essentially the same vector representation, but in a multidimensional space (cf. Figure 12.4). The document in each row is represented as a vector of N dimensional space. The size of N depends on the number of contextual features that are included in the analysis of the dfm. Figure 12.4: Example of Document-Feature Matrix 12.2.7 Feature Selection A dfm may not be as informative as we have expected. To better capture the document’s semantics/topics, there are several important factors that need to be more carefully considered with respect to the contextual features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features Granularity In our previous example, we include only words, i.e., unigrams, as our contextual features in the corp_us_dfm. We can in fact include linguistic units at multiple granularities: raw word forms (dfm() default contextual features) (skipped) n-grams lemmas/stems Specific syntactic categories (e.g., lexical words) For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: from corpus to tokens from tokens to ngram-based tokens from ngram-based tokens to dfm ## Create DFM based on bigrams corp_us_dfm_bigram &lt;- dfm(tokens_ngrams(corp_us_tokens, n =2)) Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: ## Create DFM based on word stems corp_us_dfm_unigram_stem &lt;- dfm_wordstem(corp_us_dfm) We can of course create DFM based on stemmed bigrams: ## Create DFM based on stemmed bigrams corp_us_dfm_bigram_stem &lt;- corp_us_tokens %&gt;% ## tokens tokens_ngrams(n = 2) %&gt;% ## bigram tokens dfm() %&gt;% ## dfm dfm_wordstem() ## stem You need to decide which types of contextual features are more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, these are only heuristics, not rules. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) Informativeness There are words that are not so informative in telling us the similarity and difference between the documents because they almost appear in every document of the corpus, but carry little (referential) semantic contents. Typical tokens include: Stopwords Numbers Symbols Control characters Punctuation marks URLs The library quanteda has defined a default English stopword list, i.e., stopwords(\"en\"). ## English stopword stopwords(&quot;en&quot;) %&gt;% head(50) [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; length(stopwords(&quot;en&quot;)) [1] 175 To remove stopwords from the dfm, we can use dfm_remove() on the dfm object. To remove numbers, symbols, or punctuation marks, we can further specify a few parameters for the function tokens(): remove_punct = TRUE: remove all characters in the Unicode “Punctuation” [P] class remove_symbols = TRUE: remove all characters in the Unicode “Symbol” [S] class remove_url = TRUE: find and eliminate URLs beginning with http(s) remove_separators = TRUE: remove separators and separator characters (Unicode “Separator” [Z] and “Control” [C] categories) ## Create new DFM ## w/o non-word tokens and stopwords corp_us_dfm_unigram_stop_punct &lt;- corp_us %&gt;% tokens(remove_punct = TRUE, ## remove non-word tokens remove_symbols = TRUE, remove_numbers = TRUE, remove_url = TRUE) %&gt;% dfm() %&gt;% ## Create DFM dfm_remove(stopwords(&quot;en&quot;)) ## Remove stopwords from DFM We can see that the number of features varies a lot when we operationalize the contextual features differently: ## Changes of Contextual Feature Numbers nfeat(corp_us_dfm) ## default unigram version [1] 9439 nfeat(corp_us_dfm_unigram_stem) ## unigram + stem [1] 5596 nfeat(corp_us_dfm_bigram) ## bigram [1] 64442 nfeat(corp_us_dfm_bigram_stem) ## bigram + stem [1] 58045 nfeat(corp_us_dfm_unigram_stop_punct) ## unigram removing non-words/puncs [1] 9212 Distributional Properties Finally, we can also select contextual features based on their distributional properties. For example: (Term) Frequency: the contextual feature’s frequency counts in the corpus Set a cut-off minimum to avoid hapax legomenon or highly idiosyncratic words Set a cut-off maximum to remove high-frequency function words, carrying little semantic content. Dispersion (Document Frequency) Set a cut-off minimum to avoid idiosyncratic or domain-specific words; Set a cut-off maximum to avoid stopwords; Other Self-defined Weights We can utilize association-based metrics to more precisely represent the association between a contextual feature and a document (e.g., PMI, LLR, TF-IDF). In quanteda, we can easily specify our distributional cutoffs for contextual features: Frequency: dfm_trim(DFM, min_termfreq = ..., max_termfreq = ...) Dispersion: dfm_trim(DFM, min_docfreq = ..., max_docfreq = ...) Weights: dfm_weight(DFM, scheme = ...) or dfm_tfidf(DFM) Exercise 12.2 Please read the documentations of dfm_trim(), dfm_weight(), dfm_tfidf() very carefully and make sure you know how to use them. In the following demo, we adopt a few strategies to refine the contextual features of the dfm: we create a simple unigram dfm based on the word-forms we remove stopwords, punctuation marks, numbers, and symbols we include contextual words whose termfreq \\(\\geq\\) 10 and docfreq \\(\\geq\\) 3 ## Create trimmed DFM corp_us_dfm_trimmed &lt;- corp_us %&gt;% ## corpus tokens( ## tokens remove_punct = T, remove_numbers = T, remove_symbols = T ) %&gt;% dfm() %&gt;% ## dfm dfm_remove(stopwords(&quot;en&quot;)) %&gt;% ## remove stopwords dfm_trim( min_termfreq = 10, ## frequency max_termfreq = NULL, termfreq_type = &quot;count&quot;, min_docfreq = 3, ## dispersion max_docfreq = NULL, docfreq_type = &quot;count&quot; ) nfeat(corp_us_dfm_trimmed) [1] 1401 In dfm_trim(), we can specify the cutoff values of min_termfreq/max_termfreq and min_docfreq/max_docfreq in different ways. The numbers could refer to: \"count\": Raw termfreq/docfreq counts; \"prop\": Normalized termfreq/docfreq (percentages); \"rank\": Inverted ranking of the features in terms of overall termfreq/docfreq; \"quantile\": Quantiles of termfreq/docfreq. 12.2.8 Exploratory Analysis of dfm We can check the top features in the current corpus: ## Check top important contextual features topfeatures(corp_us_dfm_trimmed, n = 20) people government us can must upon great 584 564 505 487 376 371 344 may states world shall country nation every 343 334 319 316 308 305 300 one peace new power now public 267 258 250 241 229 225 We can visualize the top features using a word cloud: ## Create wordcloud ## Set random seed ## (to make sure consistent results for every run) set.seed(100) ## Color brewer require(RColorBrewer) ## Plot wordcloud textplot_wordcloud( corp_us_dfm_trimmed, max_words = 200, random_order = FALSE, rotation = .25, color = brewer.pal(7, &quot;Dark2&quot;) ) 12.2.9 Document Similarity As shown in 12.4, with the N-dimensional vector representation of each document, we can compute the mathematical distances/similarities between two documents. In Section 12.2.5, we introduced two important metrics: Distance-based metric: Euclidean Distance Similarity-based metric: Cosine Similarity quanteda provides useful functions to compute these metrics (as well as other alternatives): textstat_simil() and textstat_dist() Before computing the document similarity/distance, we usually convert the frequency counts in dfm into more sophisticated metrics using normalization or weighting schemes. There are two common schemes: Normalized Term Frequencies TF-IDF Weighting Scheme Normalized Term Frequencies We can normalize these term frequencies into percentages to reduce the impact of the document size (i.e., marginal frequencies) on the significance of the co-occurrence frequencies. ## Intuition for Normalized Frequencies ## Raw Frequency Counts corp_us_dfm_trimmed[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 1 1 2 2 1 ## Normalized Frequencies dfm_weight(corp_us_dfm_trimmed, scheme=&quot;prop&quot;)[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives 1789-Washington 0.002427184 0.002427184 0.004854369 0.004854369 features docs among 1789-Washington 0.002427184 ## Intuition corp_us_dfm_trimmed[1,1:5]/sum(corp_us_dfm_trimmed[1,]) Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives 1789-Washington 0.002427184 0.002427184 0.004854369 0.004854369 features docs among 1789-Washington 0.002427184 TF-IDF Weighting SCheme A more sophisticated weighting scheme is TF-IDF scheme. We can weight the significance of term frequencies based on the term’s IDF. Inverse Document Frequencies For each contextual feature, we can also assess their distinctive power in terms of their dispersion across the entire corpus. In addition to document frequency counts, there is a more effective metric, Inverse Document Frequency(IDF) (often used in Information Retrieval), to capture the distinctiveness of the features. The IDF of a term (\\(w_i\\)) in a corpus (\\(D\\)) is defined as the log-transformed ratio of the corpus size (\\(|D|\\)) to the term’s document frequency (\\(df_i\\)). The more widely dispersed the contextual feature is, the lower its IDF; the less dispersed the contextual feature, the higher its IDF. \\[ IDF(w_i, D) = log_{10}{\\frac{|D|}{df_i}} \\] ## Intuition for Inverse Document Frequency ## Docfreq of the first ten features docfreq(corp_us_dfm_trimmed)[1:10] fellow-citizens senate house representatives among 19 9 8 14 43 life event greater order received 49 9 29 29 10 ## Inverse Doc docfreq(corp_us_dfm_trimmed, scheme = &quot;inverse&quot;, base = 10)[1:10] fellow-citizens senate house representatives among 0.49209841 0.81660950 0.86776202 0.62472398 0.13738356 life event greater order received 0.08065593 0.81660950 0.30845401 0.30845401 0.77085201 ## Intuition corpus_size &lt;- ndoc(corp_us_dfm_trimmed) log(corpus_size/docfreq(corp_us_dfm_trimmed), 10)[1:10] fellow-citizens senate house representatives among 0.49209841 0.81660950 0.86776202 0.62472398 0.13738356 life event greater order received 0.08065593 0.81660950 0.30845401 0.30845401 0.77085201 The TFIDF of a term (\\(w_i\\)) in a document (\\(d_j\\)) is the product of the word’s term frequency (TF) in the document (\\(tf_{ij}\\)) and the IDF of the term (\\(log\\frac{|N|}{df_i}\\)). \\[ TFIDF(w_i, d_j) = tf_{ij} \\times log\\frac{|N|}{df_i} \\] ## Intuition for TFIDF ## Raw Frequency Counts corp_us_dfm_trimmed[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 1 1 2 2 1 ## TF-IDF dfm_tfidf(corp_us_dfm_trimmed)[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 0.4920984 0.8166095 1.735524 1.249448 0.1373836 ## Intuition TF_ex &lt;- corp_us_dfm_trimmed[1,] IDF_ex &lt;- docfreq(corp_us_dfm_trimmed, scheme= &quot;inverse&quot;) TFIDF_ex &lt;- TF_ex*IDF_ex TFIDF_ex[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 0.4920984 0.8166095 1.735524 1.249448 0.1373836 We weight the dfm with the TF-IDF scheme before the document similarity analysis. ## weight DFM corp_us_dfm_trimmed_tfidf &lt;- dfm_tfidf(corp_us_dfm_trimmed) ## top features of count-based DFM topfeatures(corp_us_dfm_trimmed, 20) people government us can must upon great 584 564 505 487 376 371 344 may states world shall country nation every 343 334 319 316 308 305 300 one peace new power now public 267 258 250 241 229 225 ## top features of tfidf-based DFM topfeatures(corp_us_dfm_trimmed_tfidf, 20) america union congress constitution freedom upon 56.45704 50.48339 40.09902 39.93330 39.69166 36.63779 americans democracy revenue laws federal public 35.55963 35.05941 34.57065 34.54126 33.35736 33.21061 states executive business today government let 32.98389 32.93036 32.48151 32.38767 30.93465 28.76598 policy necessary 28.49188 27.83804 Distance-based Results (Euclidean Distance) ## Distance-based: Euclidean corp_us_euclidean &lt;- textstat_dist(corp_us_dfm_trimmed_tfidf, method = &quot;euclidean&quot;) Cosine-based Results (Cosine Similarity) ## Similarity-based: Cosine corp_us_cosine &lt;- textstat_simil(corp_us_dfm_trimmed_tfidf, method = &quot;cosine&quot;) As we have discussed in the previous sections, different distance/similarity metrics may give very different results. As an analyst, we can go back to the original documents and evaluate these quantitative results in a qualitative way. For example, let’s manually check the document that is the most similar to 1789-Washington according to Euclidean Distance and Cosine Similarity: ## Closest document based on euclidean which.min(corp_us_euclidean[-1,1]) 1793-Washington 1 ## Closest document based on cosine which.max(corp_us_cosine[-1,1]) 1841-Harrison 13 ## You can further inspect the texts as.character(corp_us[&quot;1789-Washington&quot;]) %&gt;% strtrim(400) 1789-Washington &quot;Fellow-Citizens of the Senate and of the House of Representatives:\\n\\nAmong the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat wh&quot; as.character(corp_us[&quot;1793-Washington&quot;]) %&gt;% strtrim(400) 1793-Washington &quot;Fellow citizens, I am again called upon by the voice of my country to execute the functions of its Chief Magistrate. When the occasion proper for it shall arrive, I shall endeavor to express the high sense I entertain of this distinguished honor, and of the confidence which has been reposed in me by the people of united America.\\n\\nPrevious to the execution of any official act of the President the Con&quot; as.character(corp_us[&quot;1841-Harrison&quot;]) %&gt;% strtrim(400) 1841-Harrison &quot;Called from a retirement which I had supposed was to continue for the residue of my life to fill the chief executive office of this great and free nation, I appear before you, fellow-citizens, to take the oaths which the Constitution prescribes as a necessary qualification for the performance of its duties; and in obedience to a custom coeval with our Government and what I believe to be your expec&quot; 12.2.10 Cluster Analysis The pairise distance/similarity matrices are sometimes less comprehensive. We can visulize the document distances in a more comprehensive way by an exploratory technique called hierarchical cluster analysis. ## distance-based corp_us_hist_euclidean &lt;- corp_us_euclidean %&gt;% as.dist %&gt;% ## DFM to dist hclust ## cluster analysis ## Plot dendrogram # plot(corp_us_hist_euclidean, hang = -1, cex = 0.6) require(&quot;ggdendro&quot;) ggdendrogram(corp_us_hist_euclidean, rotate = TRUE, theme_dendro = TRUE) ## similarity corp_us_hist_cosine &lt;- (1 - corp_us_cosine) %&gt;% ## similarity to distance as.dist %&gt;% hclust ## Plot dendrogram # plot(corp_us_hist_cosine, hang = -1, cex = 0.6) ggdendrogram(corp_us_hist_cosine, rotate = TRUE, theme_dendro = TRUE) Please note that textstat_simil() gives us the similarity matrix. In other words, the numbers in the matrix indicate how similar the documents are. However, for hierarchical cluster analysis, the function hclust() expects a distance-based matrix, namely one indicating how dissimilar the documents are. Therefore, we need to use (1 - corp_us_cosine) in the cosine example before performing the cluster analysis. Cluster anlaysis is a very useful exploratory technique to examine the emerging structure of a large dataset. For more detail introduction to this statistical method, I would recommend Gries (2013) Ch 5.6 and the very nice introductory book, Kaufman &amp; Rousseeuw (1990). For more information on vector-space semantics, I would highly recommend the chapter of Vector Semantics and Embeddings, by Dan Jurafsky and James Martin. 12.3 Vector Space Model for Words (Self-Study) So far, we have been talking about applying the vector space model to study the document semantics. Now let’s take a look at how this distributional semantic approach can facilitate a lexical semantic analysis. With a corpus, we can also study the distribution, or contextual features, of (target) words based on their co-occurring (contextual) words. Now I would like to introduce another object defined in quanteda, i.e., the Feature-Cooccurrence Matrix fcm. 12.3.1 Feature-Coocurrence Matrix (fcm) A Feature-Cooccurrence Matrix is essentially a word co-occurrence matrix (i.e., a square matrix). There are two ways to create a fcm: from tokens to fcm from dfm to fcm 12.3.2 From tokens to fcm We can create a word co-occurrence matrix fcm from the tokens object. We can further operationalize our contextual features for target words in two ways: Window-based: Only words co-occurring within a defined window size of the target word will be included as its contextual features Document-based: All words co-occurring in the same document as the target word will be included as its contextual features. Window-based Contextual Features ## tokens object corp_us_tokens &lt;- tokens( corp_us, what = &quot;word&quot;, remove_punct = TRUE, remove_symbol = TRUE, remove_numbers = TRUE ) ## window-based FCM corp_fcm_win &lt;- fcm( corp_us_tokens, ## tokens context = &quot;window&quot;, ## context type window = 1) # window size ## Check corp_fcm_win Feature co-occurrence matrix of: 10,075 by 10,075 features. features features Fellow-Citizens of the Senate and House Representatives Fellow-Citizens 0 2 0 0 0 0 0 of 0 0 1766 0 87 6 4 the 0 0 0 8 463 5 0 Senate 0 0 0 0 3 0 0 and 0 0 0 0 2 2 0 House 0 0 0 0 0 0 0 Representatives 0 0 0 0 0 0 0 Among 0 0 0 0 0 0 0 vicissitudes 0 0 0 0 0 0 0 incident 0 0 0 0 0 0 0 features features Among vicissitudes incident Fellow-Citizens 0 0 0 of 0 1 1 the 1 2 2 Senate 0 0 0 and 0 2 0 House 0 0 0 Representatives 1 0 0 Among 0 0 0 vicissitudes 0 0 1 incident 0 0 0 [ reached max_feat ... 10,065 more features, reached max_nfeat ... 10,065 more features ] ## check top 50 features topfeatures(corp_fcm_win, n = 50) our the in we is to 1978 1766 1359 1217 1062 884 be a all for We people 875 777 749 740 715 679 will their are and it that 661 652 608 555 554 546 been this us States by not 530 527 498 467 444 434 upon as do its world those 433 428 427 426 426 412 must should peace Government It them 397 392 390 386 377 367 great power Constitution any only freedom 361 355 339 325 321 314 may government with they shall nation 302 301 299 297 289 288 I so 283 274 A look at the corp_fcm_win: ## Check top 10 contextual features ## for specific target worods corp_fcm_win[&quot;our&quot;,] %&gt;% topfeatures(10) national institutions upon common Constitution Union 38 26 22 21 20 19 children Nation fathers within 19 19 18 15 corp_fcm_win[&quot;must&quot;,] %&gt;% topfeatures(10) We America do go continue There citizen keep 46 9 7 5 5 4 4 4 carry realize 4 4 Document-based Contextual Features ## Document-based FCM corp_fcm_doc &lt;- fcm( corp_us_tokens, context = &quot;document&quot;) ## check corp_fcm_doc Feature co-occurrence matrix of: 10,075 by 10,075 features. features features Fellow-Citizens of the Senate and House Fellow-Citizens 0 710 1057 2 507 2 of 0 681011 1810708 2042 862543 1408 the 0 0 1210315 2749 1148145 1968 Senate 0 0 0 3 1418 12 and 0 0 0 0 297873 909 House 0 0 0 0 0 3 Representatives 0 0 0 0 0 0 Among 0 0 0 0 0 0 vicissitudes 0 0 0 0 0 0 incident 0 0 0 0 0 0 features features Representatives Among vicissitudes incident Fellow-Citizens 2 1 1 2 of 996 514 701 1509 the 1350 688 897 2052 Senate 4 1 1 2 and 503 348 463 1108 House 6 2 2 2 Representatives 1 2 2 2 Among 0 0 2 1 vicissitudes 0 0 0 1 incident 0 0 0 2 [ reached max_feat ... 10,065 more features, reached max_nfeat ... 10,065 more features ] ## Check top 50 contextual features topfeatures(corp_fcm_doc, n = 50) our the to in and a 3176109 3022080 2619459 2586044 2310486 1982691 be is we for their that 1923444 1828034 1671039 1449786 1286232 1254497 are it will The not as 1212962 1185232 1140058 1136470 1081138 993573 We by all which has its 979336 923827 908249 861352 857148 835208 people upon or this us been 831527 822480 816930 809940 728218 716072 of I should It must any 681721 678465 672000 668992 641750 641596 have with them so States great 638159 626747 615322 600749 575961 574693 Government power may they only at 570366 556246 544947 525462 522589 518657 from Constitution 515615 491481 ## Check top 10 contextual features ## for specific target words corp_fcm_doc[&quot;our&quot;,] %&gt;% topfeatures(10) we our We us must upon should world any power 54260 47064 24405 21543 17170 15701 13697 12791 11170 10692 corp_fcm_doc[&quot;must&quot;,] %&gt;% topfeatures(10) We us upon do any must only America peace when 5147 3914 3258 2126 2015 1996 1995 1956 1846 1506 In the above examples of fcm, we tokenize our corpus into tokens first and then use it to create the fcm. The advantage of our current method is that we can have a full control of the word tokenization, i.e., what tokens we would like to include in the fcm. This can be particularly important when we deal with Chinese data. 12.3.3 From dfm to fcm A fcm can also be created from dfm. The limitation is that we can only create a document-based fcm from dfm. But we can make use of the feature selection (e.g., dfm_select(), dfm_remove(), dfm_trim()) discussed in the previous sections to remove irrelevant contextual features before we create the fcm. ## Create FCM from DFM corp_fcm_dfm &lt;- fcm(corp_us_dfm_trimmed) ## Check top 50 features topfeatures(corp_fcm_dfm,n = 50) upon us must peace freedom power 179044 151948 127748 116086 106320 104929 government constitution part spirit law people 100582 98820 95195 89563 88701 79380 laws business congress war state shall 78924 77645 76174 74119 73947 73720 today best make within union world 72941 72190 71803 71153 70608 70446 work let progress political come great 68762 68484 67070 65616 65338 65214 always america purpose states god revenue 63302 61248 61052 60565 60324 59702 americans force justice rights institutions countrymen 57926 57685 56809 56506 55619 54818 republic true federal yet foreign others 54714 54148 54024 53330 52400 52114 long action 51463 51055 ## Check top 10 features ## for specific target words corp_fcm_dfm[&quot;people&quot;,] %&gt;% topfeatures(10) government upon us states great must 8094 5850 5292 5279 4658 4483 people power shall constitution 4217 4060 3891 3881 corp_fcm_win[&quot;people&quot;,] %&gt;% topfeatures(10) our American The are free themselves great 79 40 27 19 12 8 8 It we Our 8 8 8 corp_fcm_doc[&quot;people&quot;,] %&gt;% topfeatures(10) our we their are The We upon States us It 24668 12532 10826 9777 9064 5785 5759 5217 5202 5182 12.3.4 Lexical Similarity fcm is an interesting structure because, similar to dfm, we can now examine the pairwise relationships in-between all target words. In particular, based on this FCM, we can further analyze which target words tend to co-occur with similar sets of contextual words. And based on these quantitative pairwise similarities in-between target words, we can visualize their lexical relations with a network or a cluster dendrogram. ## Create window size 5 FCM corp_fcm_win_5 &lt;- fcm(corp_us_tokens, context = &quot;window&quot;, window = 5) ## Remove stopwords corp_fcm_win_5_select &lt;- corp_fcm_win_5 %&gt;% fcm_remove( pattern = stopwords(), case_insensitive = T ) We first identify a list of top 50/100 contextual feature words ## Find top features (corp_fcm_win_5_top50 &lt;- topfeatures(corp_fcm_win_5_select, 50)) us upon peace must freedom Americans 582 568 425 412 403 330 today work great make States best 292 282 281 279 277 277 God among law come power world 270 249 248 247 243 239 laws spirit Nation Union America know 237 236 236 233 231 231 shall war progress always business Let 229 225 224 217 216 215 help purpose need part within let 215 213 212 211 206 203 American century people Constitution seek Congress 200 197 195 190 188 184 free political strength economic democracy justice 183 183 183 181 180 178 man trade 178 176 (corp_fcm_win_5_top100 &lt;-topfeatures(corp_fcm_win_5_select, 100)) us upon peace must freedom 582 568 425 412 403 Americans today work great make 330 292 282 281 279 States best God among law 277 277 270 249 248 come power world laws spirit 247 243 239 237 236 Nation Union America know shall 236 233 231 231 229 war progress always business Let 225 224 217 216 215 help purpose need part within 215 213 212 211 206 let American century people Constitution 203 200 197 195 190 seek Congress free political strength 188 184 183 183 183 economic democracy justice man trade 181 180 178 178 176 government action home responsibility new 170 167 166 165 163 old America&#39;s common foreign force 162 161 158 157 156 also rights unity support interest 155 154 153 152 152 Federal true race believe promise 151 151 150 149 148 Government long now whether ago 147 145 144 143 143 change faith even peoples history 142 140 140 139 138 better may liberty whole countrymen 137 136 136 136 136 nation find done revenue opportunity 135 135 135 134 134 others strong control office stand 133 133 133 132 132 women land protection forward national 132 131 131 131 129 We subset the FCM where the target and contextual words are on the top 50 important contextual features. We then create a network of these top 50 important features. ## Subset fcm for plot fcm4network &lt;- fcm_select(corp_fcm_win_5_select, pattern = names(corp_fcm_win_5_top50), selection = &quot;keep&quot;, valuetype = &quot;fixed&quot;) fcm4network Feature co-occurrence matrix of: 86 by 86 features. features features Among people States Great nation free shall great world union Among 0 0 0 0 0 0 0 0 0 0 people 0 8 30 0 10 17 8 22 14 0 States 0 0 10 2 3 5 3 5 1 5 Great 0 0 0 0 0 0 0 1 2 0 nation 0 0 0 0 2 4 2 19 5 1 free 0 0 0 0 0 32 8 6 10 1 shall 0 0 0 0 0 0 12 3 4 0 great 0 0 0 0 0 0 0 14 6 1 world 0 0 0 0 0 0 0 0 14 0 union 0 0 0 0 0 0 0 0 0 4 [ reached max_feat ... 76 more features, reached max_nfeat ... 76 more features ] ## plot semantic network of top features require(scales) textplot_network( fcm4network, min_freq = 5, vertex_labelcolor = c(&quot;grey40&quot;), vertex_labelsize = 2 * scales::rescale(rowSums(fcm4network + 1), to = c(1.5, 5)) ) Exercise 12.3 In the above example, our original goal was to subset the FCM by including only the target words (rows) and contextual features (columns) that are on the list of the top 50 contextual features (names(corp_fcm_win_5_top50)). In other words, the subset FCM should have been a 50 by 50 matrix? But it’s not. Why? In addition to a network, we can also create a dendrogram showing the semantic relations in-between a set of contextual features. (For example, the following shows a cluster analysis of the top 100 contextual features.) ## plot the dendrogram ## compute cosine similarity fcm4network &lt;- corp_fcm_win_5_select %&gt;% fcm_keep(pattern = names(corp_fcm_win_5_top100)) ## Check dimensions fcm4network Feature co-occurrence matrix of: 163 by 163 features. features features Among may people States Government Great nation government new Among 0 0 0 0 0 0 0 0 0 may 0 4 9 3 8 0 0 5 4 people 0 0 8 30 6 0 10 20 2 States 0 0 0 10 13 2 3 6 7 Government 0 0 0 0 4 0 2 1 6 Great 0 0 0 0 0 0 0 1 0 nation 0 0 0 0 0 0 2 3 3 government 0 0 0 0 0 0 0 10 9 new 0 0 0 0 0 0 0 0 36 free 0 0 0 0 0 0 0 0 0 features features free Among 0 may 2 people 17 States 5 Government 1 Great 0 nation 4 government 14 new 4 free 32 [ reached max_feat ... 153 more features, reached max_nfeat ... 153 more features ] ## network textplot_network( fcm4network, min_freq = 5, vertex_labelcolor = c(&quot;grey40&quot;), vertex_labelsize = 2 * scales::rescale(rowSums(fcm4network + 1), to = c(1.5, 5)) ) ## Remove words with no co-occurrence data fcm4cluster &lt;- corp_fcm_win_5_select[names(corp_fcm_win_5_top100),] ## check fcm4cluster Feature co-occurrence matrix of: 100 by 9,833 features. features features Fellow-Citizens Senate House Representatives Among vicissitudes us 0 0 0 0 0 0 upon 0 0 0 0 0 0 peace 0 0 0 0 0 0 must 0 0 0 0 0 0 freedom 0 0 0 0 0 0 Americans 0 0 0 0 0 0 today 0 0 0 0 0 0 work 0 0 0 0 0 0 great 0 0 0 0 0 0 make 0 0 0 0 0 0 features features incident life event filled us 0 0 0 0 upon 0 0 0 0 peace 0 0 0 0 must 0 0 0 0 freedom 0 0 0 0 Americans 0 0 0 0 today 0 0 0 0 work 0 0 0 0 great 0 0 0 0 make 0 0 0 0 [ reached max_feat ... 90 more features, reached max_nfeat ... 9,823 more features ] ## cosine fcm4cluster_cosine &lt;- textstat_simil(fcm4cluster, method = &quot;cosine&quot;) ## create hclust fcm_hclust&lt;- hclust(as.dist((1 - fcm4cluster_cosine))) ## plot dendrogram v1 plot(as.dendrogram(fcm_hclust, hang = 0.3), horiz = T, cex = 0.6) ## plot dendrogram v2 ggdendrogram(fcm_hclust, rotate = TRUE, theme_dendro = TRUE, cex = 0.7) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. 12.4 Exercises Exercise 12.4 In this exercise, please create a dendrogram of the documents included in corp_us according to their similarities in trigram uses. Specific steps are as follows: Please create a dfm, where the contextual features are the trigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only trigrams consisting of \\\\w characters Include only trigrams whose frequencies are larger than 2. Include only trigrams whose document frequencies are larger than 2 (i.e., used in at least two different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trigram-based dfm (after the trimming according to the above distributional cut-off, the total number of trigrams in the dfm is: 7748): Example of the dendrogram based on the trigram-based dfm: Exercise 12.5 Based on the corp_us, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corp_us according to their similarities in their co-occurring words. Specific steps are as follows: Please create a tokens object of corpus by removing punctuations, symbols, and numbers first. Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in quanteda::stopwords() from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (after removing the stopwords, there are 9833 features in the fcm): The dimension of the input matrix for textstats_simil() should be: 50 rows and 9833 columns. Example of the dendrogram of the top 50 features in fcm: Exercise 12.6 In this exercise, please create a dendrogram of the Taiwan Presidential Addresses included in demo_data/TW_President.tar.gz according to their similarities in ngram uses. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus Before word segmentation, normalize the texts by removing all white-spaces and line-breaks. During the word-tokenization, please remove symbols by setting worker(..., symbols=F) Please add the following terms into the self-defined dictionary of jiebaR: c(&quot;馬英九&quot;, &quot;英九&quot;, &quot;蔣中正&quot;, &quot;蔣經國&quot;, &quot;李登輝&quot;, &quot;登輝&quot; ,&quot;陳水扁&quot;, &quot;阿扁&quot;, &quot;蔡英文&quot;) Create the dfm of the corpus, where the contextual features are (contiguous) unigrams, bigrams and trigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only ngrams whose frequencies &gt;= 5. Include only ngrams whose document frequencies &gt;= 3 (i.e., used in at least three different presidential addresses) Convert the count-based DFM into TF-IDF DFM using dfm_tfidf() Please use the cosine-based distance for cluster analysis A Sub-sample of the trimmed dfm (Count-based; Number of features: 1183): A Sub-sample of the trimmed dfm (TFIDF-based; Number of features: 1183): Example of the dendrogram: Exercise 12.7 Based on the Taiwan Presidential Addresses Corpus included in demo_data/TW_President.tar.gz, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corpus according to their similarities in their co-occurring words. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus Follow the principles discussed in the previous exercise about text normalization and word segmentation. Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 2 as the contextual words. Please remove all the stopwords included in demo_data/stopwords-ch.txt from the fcm After you get the fcm, create a newtork for the FCM’s top 50 features using textplot_network(). Create a dendrogram for the top 100 important features offcm using the cosine-based distance metrics. When clustering the top 100 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 100 features according to all their co-occurring words within the window size. A sub-sample of the fcm (After removing stopwords, the FCM dimensions are 4903 by 4903: Network of the top 50 features (Try to play with the aesthetic specifications of the network so that the node sizes reflect the total frequency of each word in the FCM): The dimension of the input FCM for textstats_simil() should be: 100 rows and 4903 columns. Example of the dendrogram: References De Deyne, S., Verheyen, S., &amp; Storms, G. (2016). Structure and organization of the mental lexicon: A network approach derived from syntactic dependency relations and word associations [Book Section]. In A. Mehler, A. Lücking, S. Banisch, P. Blanchard, &amp; B. Job (Eds.), Towards a theoretical framework for analyzing complex linguistic networks (pp. 47–79). Springer. https://doi.org/10.1007/978-3-662-47238-5_3 Firth, J. R. (1957). A synopsis of linguistic theory 1930-1955. In J. R. Firth (Ed.), Studies in linguistic analysis (pp. 1–32). Oxford: Blackwell. Gries, S. T. (2013). 50-something years of work on collocations: What is or should be next…. International Journal of Corpus Linguistics, 18(1), 137–166. Harris, Z. (1954). Distributional structure. Word, 10(2/3), 146–162. Harris, Z. S. (1970). Papers in structural and transformational linguistics [Book]. Reidel. Jurafsky, D., &amp; Martin, J. H. (2020). Speech &amp; language processing 3rd. Available at https://web.stanford.edu/~jurafsky/slp3/ (Accessed on 2020/04/01). Kaufman, L., &amp; Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis. New York: Wiley-Interscience. Manning, C. D., &amp; Schütze, H. (1999). Foundations of statistical natural language processing. MIT press. "],["python-environment.html", "Chapter 13 Python Environment Install the python with Anaconda Create Conda Environment Install Python Modules", " Chapter 13 Python Environment All the codes starting with $ should be run in the terminal or Anaconda Prompt. For Windows users, we assume that you run the following steps in Anaconda Powershell Prompt. For Mac users, we assume that you run the following steps in Mac terminal. In this course, we use the Anaconda Python version. Necessary installation steps include: Install the python with Anaconda Please go to the official website of Anaconda for more information. If you have installed Anaconda before, please make sure that you know where your default Anaconda Python interpreter path is. To check this: Windows Start Anaconda Prompt Run $ conda activate environment-name (If you could like to check the python path of a particular conda environment) Run $ where python macOS Open a terminal window. Run $ source activate environment-name (If you could like to check the python path of a particular conda environment) Run $ which python Your default path should be similar to the following formats, where ALVIN would be your username and MYENV would be your conda environment name: Windows default path: C:\\Users\\ALVIN\\Anaconda3\\python.exe or C:\\Users\\ALVIN\\Anaconda3\\envs\\MYENV\\python.exe macOS default path: /Users/ALVIN/anaconda/bin/python Create Conda Environment Create a new conda environment with python 3.6+ to be used in R/Rstudio. $ conda create --name XXX python=3.7 You need to specify the conda environment name XXX on your own. Install Python Modules Install all required python modules in this self-defined conda environment. There are usually two procedures: Activate the conda environment named XXX in your terminal; ## Mac $ source activate XXX ## Window $ conda activate XXX Install the module in the activated conda environment XXX in terminal. In this course, we will need spacy and also the language models of English and Chinese. Please refer to spacy’s documentations of models for more details. $ conda install spacy $ python -m spacy download en_core_web_sm $ python -m spacy download zh_core_web_sm To deactivate the conda environment: $ conda deactivate XXX This is for my reference. If you run into error messages when you try to use a specific conda environment with use_condaenv(), it is probably due to the issue of reticulate failing to find the right conda path. To fix this, there are two solutions: For temporary setting: options(reticulate.conda_binary=&quot;~/opt/anaconda3/bin/conda&quot;) For permanent setting, include the following line in the .Renviron of your root directory ~/: RETICULATE_CONDA=~/opt/anaconda3/bin/conda Also, in RStudio, we can specify the default conda environment to be used in R. For each project, we can also specify a project-specific conda. This can be useful because we don’t have to use_condaenv() in the project then. "],["references.html", "Chapter 14 References", " Chapter 14 References Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction to statistics using R. Cambridge University Press. Bestgen, Y. (2017). Beyond single-word measures: L2 writing assessment, lexical richness and formulaic competence. System, 69, 65–78. Biber, D., Conrad, S., &amp; Cortes, V. (2004). If you look at…: Lexical bundles in university teaching and textbooks. Applied Linguistics, 25(3), 371–405. Bird, S., Klein, E., &amp; Loper, E. (2009). Natural language processing with python: Analyzing text with the natural language toolkit. \" O’Reilly Media, Inc.\". https://www.nltk.org/book/ Brezina, V. (2018). Statistics in corpus linguistics: A practical guide. Cambridge University Press. Chui, Kawai, &amp; Lai, H. (2008). The NCCU corpus of spoken Chinese: Mandarin, Hakka, and Southern Min. Taiwan Journal of Linguistics, 6(2). Chui, Kawai, Lai, Huei-ling, &amp; Chen, H.-C. (2017). The Taiwan Spoken Chinese Corpus [Book Section]. In R. Sybesma (Ed.), Encyclopedia of chinese language and linguistic (pp. 257–259). Brill. Croft, W., &amp; Cruse, D. A. (2004). Cognitive linguistics. Cambridge University Press. Damerau, F. J. (1993). Generating and evaluating domain-oriented multi-word terms from texts. Information Processing &amp; Management, 29(4), 433–447. De Deyne, S., Verheyen, S., &amp; Storms, G. (2016). Structure and organization of the mental lexicon: A network approach derived from syntactic dependency relations and word associations [Book Section]. In A. Mehler, A. Lücking, S. Banisch, P. Blanchard, &amp; B. Job (Eds.), Towards a theoretical framework for analyzing complex linguistic networks (pp. 47–79). Springer. https://doi.org/10.1007/978-3-662-47238-5_3 Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 61–74. https://www.aclweb.org/anthology/J93-1003 Ellis, N. C. (2006). Language acquisition as rational contingency learning. Applied Linguistics, 27(1), 1–24. Evert, S. (2007). Corpora and collocations. https://stephanie-evert.de/PUB/Evert2007HSK_extended_manuscript.pdf Firth, J. R. (1957). A synopsis of linguistic theory 1930-1955. In J. R. Firth (Ed.), Studies in linguistic analysis (pp. 1–32). Oxford: Blackwell. Gabrielatos, C. (2018). Keyness analysis: Nature, metrics and techniques. In Corpus approaches to discourse (pp. 225–258). Routledge. Gries, S. T. (2013). 50-something years of work on collocations: What is or should be next…. International Journal of Corpus Linguistics, 18(1), 137–166. Gries, S. T. (2016). Quantitative corpus linguistics with R: A practical introduction (2nd ed.). Routledge. Gries, S. T. (2021a). Analyzing dispersion. In A practical handbook of corpus linguistics (pp. 99–118). Springer. Gries, S. T. (2021b). Statistics for linguistics with R: A practical introduction. 3rd edition. (3rd ed.). Walter de Gruyter. Gries, S. T., &amp; Stefanowitsch, A. (2004). Extending collostructional analysis: A corpus-based perspective onalternations’. International Journal of Corpus Linguistics, 9(1), 97–129. Harris, Z. (1954). Distributional structure. Word, 10(2/3), 146–162. Harris, Z. S. (1970). Papers in structural and transformational linguistics [Book]. Reidel. Hunston, S. (2022). Corpora in applied linguistics (2nd ed.). Cambridge University Press. Hunston, S., &amp; Francis, G. (2000). Pattern grammar: A corpus-driven approach to the lexical grammar of English. John Benjamins Publishing. Jurafsky, D., &amp; Martin, J. H. (2020). Speech &amp; language processing 3rd. Available at https://web.stanford.edu/~jurafsky/slp3/ (Accessed on 2020/04/01). Jurafsky, D., &amp; Martin, J. H. (2022). Speech and language processing. \" O’Reilly Media, Inc.\". https://web.stanford.edu/~jurafsky/slp3/ Kaufman, L., &amp; Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis. New York: Wiley-Interscience. Leech, G., &amp; Fallon, R. (1992). Computer corpora–what do they tell us about culture. ICAME Journal, 16. Manning, C. D., &amp; Schütze, H. (1999). Foundations of statistical natural language processing. MIT press. McEnery, T., &amp; Hardie, A. (2011). Corpus linguistics: Method, theory and practice. Cambridge University Press. Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. Stefanowitsch, A. (2019). Corpus linguistics: A guide to the methodology. Language Science Press. http://langsci-press.org/catalog/book/148 Stefanowitsch, A., &amp; Gries, S. T. (2003). Collostructions: Investigating the interaction of words and constructions. International Journal of Corpus Linguistics, 8(2), 209–243. Stefanowitsch, A., &amp; Gries, S. T. (2005). Covarying collexemes. Corpus Linguistics and Linguistic Theory, 1, 1–43. Stubbs, M. (1996). Text and corpus analysis. Blackwell. Stubbs, M. (2003). Words and phrases. Blackwell. Su, H.-K., &amp; Chen, A. C.-H. (2019). Conceptualization of containment in chinese: A corpus-based study of the chinese space particles lǐ, nèi, and zhōng. Concentric, 45(2), 211–245. Tognini-Bonelli, E. (2001). Corpus linguistics at work. John Benjamins. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. Williams, R. (1976). Keywords. Oxford University Press. Winter, B. (2020). Statistics for linguists: An introduction using R. Routledge. Wynne, M. (Ed.). (2006). Developing linguistic corpora—a guide to good practice. EADH: The European Association for Digital Humanities. https://users.ox.ac.uk/~martinw/dlc/index.htm It is an anonymous function because it has not be defined with an object name in the R session.↩︎ "]]
