[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2020-03-21 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark on a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offerring necessary inter-disciplinary skills and knowledge. This course requires as prerequisite basic knoweldge of computational coding. It is highly recommended for students to have taken ENC2055 or other equivalents before taking this course. Please see the FAQ of the course webiste for more information about the prerequisite. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will lead the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our reference material for the course. However, we will stress the hands-on implementation of the ideas and methods covered in the book. Also, there are a few more reference books listed at the end of the section, which I would highly recommend (e.g., Gries (2018), Baayen (2008), Brezina (2018), McEnery and Hardie (2011)). Course Website We have a course website. You may need a password to access the course materials. If you are an officially enrolled student, please ask the instructor for the passcode. Please read the FAQ of the course website before course registration. Course Demo Data Dropbox Demo Data Directory References "],
["what-is-corpus-linguistics.html", "Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? 1.2 What is corpus? 1.3 What is a corpus linguistic study? 1.4 Additional Information on CL", " Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? There is an unnecessary dichotomy in linguistics “intuiting” linguistic data Inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality Corpus data Highlight the importance of language use in real context Highlight the linguistic tendency in the population (from a sample) Strengths of Corpus Data Data reliability How sure can we be that other people will arrive at the same observations/patterns/conclusions using the same method? Can others replicate the same logical reasoning in intuiting data? Can others make the same “grammatical judgement”? Data validity How well do we understand what real world phenomenon that the data correspond to? Can we know more about language based on one man’s constructed sentences or his grammatical judgement? Can we better generalize our insights from one man’s intuition or the group minds (population vs. sample vs. one-man)? 1.2 What is corpus? This can be tricky: different disciplines, different definitions Literature History Sociology Field Linguistics Linguistic Corpus in corpus linguistics (Stefanowitch 2019) Authentic Representative Large A few well-received definitions “a collection of texts which have been selected and brought together so that language can be studied on the computer” (Wynne 2005) “A corpus is a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research.” (John Sinclair in (Wynne 2005)) “A corpus refers to a machine-readable collection of (spoken or written) texts that were produced in a natural communitive setting, and in which the collection of texts is compiled with the intention (1) to be representative and balanced with respect to a particular linguistic language, variety, register, or genre and (2) to be analyzed linguistically.” (Gries 2018) 1.3 What is a corpus linguistic study? CL characteristics No general agreement as to what it is Not a very homogenous methodological framework (Compared to other sub-disciplines in linguistics) It’s quite new Intertwined with many linguistic fields Interactional linguistics, cognitive linguistics, functional syntax, usage-based grammar etc. Stylometry, computational linguistics, NLP, digital humanities, text mining, sentiment analysis Corpus-based vs. Corpus-driven (cf. Tognini-Bonelli 2001) Corpus-based studies: Typically use corpus data in order to explore a theory or hypothesis, aiming to validate it, refute it or refine it. Take corpus linguistics as a method Corpus-driven studies: Typically reject the characterization of corpus linguistics as a method Claim instead that the corpus itself should be the sole source of our hypotheses about language It is thus claimed that the corpus itself embodies a theory of language (Tognini-Bonelli 2001, 84–85) Stefanowitch (2019) defines Corpus Linguistics as follows: Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus More on Conditional Distribution An exhaustive investigation Text-processing technology Retrieval and coding Regular expressions Common methods KWIC concordances Collocates Frequency lists A systematic investigation The distribution of a linguistic phenomenon under particular conditions (e.g. lexical, syntactic, social, pragmatic etc. contexts) Statistical properties of language Examples When do English speakers use the complementizer that? What are the differences between small and little? When do English speaker choose “He picked up the book” vs. “He picked the book up”? When do English speaker place the adverbial clauses before the matrix clause? Do speakers use different phrases in different genres? Is the word “gay” used differently across different time periods? Do L2 learners use similar collocation patterns as do L1 speakers? Do speakers of different socio-economic classes talk differently? 1.4 Additional Information on CL Important Journals in Corpus Linguistics Corpus Linguistics and Linguistic Theory International Journal of Corpus Linguistics Corpora Applied Linguistics Computational Linguistics Digital scholarship in the Humanities Language Teaching Language Learning Journal of Second Language Writing CALL Language Teaching Research ReCALL System References "],
["r-fundamentals.html", "Chapter 2 R Fundamentals A Quick Note", " Chapter 2 R Fundamentals A Quick Note This course assumes that students have a certain level of background knowledge of R. We will have a quick overview of several fundamental concepts relating to the R language. These topics will be covered in more detail in my other course, ENC2055. Therefore, in the following weeks, we will go over (or review) some of the important chapters in the Lecture Notes of ENC2055, including: Chapter 2: R Fundamentals Chapter 3: Code Format Convention Chapter 4: Subsetting Chapter 6: Data Manipulation Chapter 7: Data Import Chapter 8: String Manipulation Chapter 9: Conditions and Loops Chpater 10: Iterations Please refer Winter (2020) Chapter 1 and Chapter 2 for a comprehensive overview of R fundamentals. References "],
["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resourcess", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure 3.1.1 HTML Syntax To illustrate the structure of the HTML, please download the sample html file from: demo_data/data-sample-html.html and first open the with your browser. &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; An HTML document includes several important elements (cf. 3.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 3.1: Syntax of An HTML Tag Element An HTML document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in Figure 3.2. Figure 3.2: Tree Structure of An HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The idea is that CSS specifies the formats/styles of the HTML elements. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In the following demonstration, the text data scraped from the PTT forum is presented as it is without adjustment. However, please note that the language on PTT may strike some readers as profane, vulgar or even offensive. In this tutorial, let’s assume that we like to scape texts from PTT Forum. In particular, we will demonstrate how to scape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html_session() (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created html_session and create another session. gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 26226 Now our html sesseion, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest ## [1] 39650 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758025.A.A7D.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758071.A.D78.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758085.A.40F.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758097.A.3F5.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758098.A.841.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758131.A.042.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758131.A.16F.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758162.A.B4D.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758162.A.A6B.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758196.A.597.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758208.A.417.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758243.A.F4B.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758317.A.EF6.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758330.A.E67.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758364.A.6B8.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758372.A.C06.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758437.A.D54.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584758449.A.A5B.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. Because we are interested in the metadata and the contents of each articule, now the question is: where are they in the HTML? We need to go back to the source page of the article HTML again: After a closer inspection of the article HTML, we know that: The metadata of the article are included in &lt;span&gt; tag elements, belonging to the class class=&quot;article-meta-value&quot; The contents of the article are included in the &lt;div&gt; element, whose ID is ID=&quot;main-content&quot; Now we are ready to extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() article.header ## [1] &quot;clubbox (史上最強7788 )&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;[問卦] 我你各位今天是不是要買爆大賣場了&quot; ## [4] &quot;Sat Mar 21 10:33:43 2020&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;clubbox&quot; article.title ## [1] &quot;[問卦] 我你各位今天是不是要買爆大賣場了&quot; article.datetime ## [1] &quot;Sat Mar 21 10:33:43 2020&quot; Now we extract the main contents of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content ## [1] &quot;政府叫我們買到爆\\n\\n好市多停車場滿了\\n！\\n等等要去買貝果果促進經濟當好國民\\n\\n我你各位準備好了沒\\n\\n--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push ## {xml_nodeset (8)} ## [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f3 ... ## [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... ## [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... ## [4] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... ## [5] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... ## [6] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... ## [7] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... ## [8] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag ## [1] &quot;噓&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author ## [1] &quot;KLGlikeshit&quot; &quot;rsarsazz&quot; &quot;silentlee&quot; &quot;amfive&quot; &quot;schi0301&quot; ## [6] &quot;vendan5566&quot; &quot;maesww&quot; &quot;maesww&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content ## [1] &quot;: 沒空&quot; ## [2] &quot;: 推，刺激經濟&quot; ## [3] &quot;: 盡量買，台灣經濟需要各位的幫忙&quot; ## [4] &quot;: 急時鮮大出貨&quot; ## [5] &quot;: 人家是叫你買爆菜市場&quot; ## [6] &quot;: 好市多停車場滿不是很正常的事情嗎？&quot; ## [7] &quot;: 夏天來了天氣熱爆 肺炎也要退 你快去買爆&quot; ## [8] &quot;: 到了夏天 人對\\&quot;冬天的肺炎 抵抗力超強好&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime ## [1] &quot;115.82.161.129 03/21 10:34&quot; &quot;101.137.193.196 03/21 10:34&quot; ## [3] &quot;42.74.155.132 03/21 10:34&quot; &quot;110.50.188.17 03/21 10:35&quot; ## [5] &quot;180.217.162.221 03/21 10:38&quot; &quot;180.217.104.232 03/21 10:39&quot; ## [7] &quot;114.136.252.213 03/21 10:42&quot; &quot;114.136.252.213 03/21 10:45&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. It returns a vector of article links. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables(): This function takes an article link link as the argument and extract the metadata, contents, and pushes of the article. It returns a list of two elements–article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows # Merge all push.tables into one push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph provides a flow chart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resourcess Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the substainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata representation of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Teachnical parts for corpus creation (cf. Sinclair’s Appendix) Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Exercise 3.3 Now you should have basic ideas how we can crawl data from the Internet. Sometimes, we not only collect texts but statistics as well. Please try to collect the statistics of COVID-19 outbreak from the Wikipedia 2019–20 coronavirus pandemic. Specifically, write a short script to automatically get the table included in the Wiki page, where the numbers of confirmed cases, deaths, and recoveries for each country are recorded. Your script should output a data frame as follows. Please name the columns of your data frame as follows as well. (Note: The numbers may vary because of the constant updates of the wiki page.) References "],
["corpus-analysis-a-start.html", "Chapter 4 Corpus Analysis: A Start 4.1 Installing quanteda 4.2 Building a corpus from character vector 4.3 Keyword-in-Context (KWIC) 4.4 KWIC with Regular Expressions 4.5 Tidy Text Format of the Corpus 4.6 Frequency Lists 4.7 Word Cloud 4.8 Collocations 4.9 Constructions", " Chapter 4 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 4.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 4.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. data_corpus_inaugural ## Corpus consisting of 58 documents and 4 docvars. ## 1789-Washington : ## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot; ## ## 1793-Washington : ## &quot;Fellow citizens, I am again called upon by the voice of my c...&quot; ## ## 1797-Adams : ## &quot;When it was first perceived, in early times, that no middle ...&quot; ## ## 1801-Jefferson : ## &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot; ## ## 1805-Jefferson : ## &quot;Proceeding, fellow citizens, to that qualification which the...&quot; ## ## 1809-Madison : ## &quot;Unwilling to depart from examples of the most revered author...&quot; ## ## [ reached max_ndoc ... 52 more documents ] class(data_corpus_inaugural) ## [1] &quot;corpus&quot; We create a corpus() object with the pre-loaded corpus in quanteda– data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) # save the `corpus` to a short obj name summary(corp_us) After the corpus is loaded, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() Exercise 4.1 Could you reproduce the above line plot and add information of President to the plot as labels of the dots? Hints: Please check ggplot2::geom_text() or more advanced one, ggrepel::geom_text_repel() 4.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. Please note that kwic(), when taking a corpus object as the argument, will automatically tokenizethe corpus data and do the keyword-in-context search on a word basis. In other words, the pattern you look for cannot be a linguistic pattern across several words. We will talk about how to extract constructions later. 4.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. Exercise 4.2 Please create a bar plot, showing the number of uses of the word country in each president’s address. Please include different variants of the word, e.g., countries, Countries, Country, in your kwic() search. 4.5 Tidy Text Format of the Corpus So far our corpus is a corpus object defined in quanteda. In most of the R standard packages, people normally follow the using tidy data principles to make handling data easier and more effective. As described by Hadley Wickham (Wickham and Grolemund 2017), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table With text data like a corpus, we can also define the tidy text format as being a data.frame with one-token-per-row. A token is a meaningful unit of text, such as a word that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. In computational text analytics, the token (i.e., each row in the data frame) is most often a single word, but can also be an n-gram, sentence, or paragraph. The tidytext package in R is made for the handling of the tidy text format of the corpus data. Tidy datasets allow manipulation with a standard set of tidy tools, including popular packages such as dplyr, tidyr, and ggplot2. The tidytext package includes functions to tidy() objects from quanteda. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) # convert `corpus` to `data.frame` class(corp_us_tidy) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.6 Frequency Lists 4.6.1 Word (Unigram) To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. The tidytext provides a powerful function, unnest_tokens() to tokenize a data frame with larger linguistic units (e.g., texts) into one with smaller units (e.g., words). corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(output = word, input = text, token = &quot;words&quot;) # tokenize the `text` column into `word` corp_us_words The unnest_tokens() is optimized for English tokenization of other linguistic units, such as words, ngrams, sentences, lines, and paragraphs (check ?unnest_tokens()). To handle Chinese data, however, we need to define own ways of tokenization unnest_tokens(…, token = …). We will discuss the principles for Chinese text processing in a later chapter. Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq 4.6.2 Bigrams Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) # size of unigrams ## [1] 135562 sum(corp_us_bigrams_freq$n) # size of bigrams ## [1] 135504 Exercise 4.3 The function unnest_tokens() does a lot of work behind the scene. Please take a closer look at the outputs of unnest_tokens() and examine how it takes care of the case normalization and punctuations within the sentence. Will these treatments affect the frequency lists we get in any important way? Please elaborate. 4.6.3 Ngrams (Lexical Bundles) corp_us_trigrams &lt;- corp_us_tidy%&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigrams We then can examine which n-grams were most often used by each President: corp_us_trigrams %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) Exercise 4.4 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. 4.6.4 Frequency and Dispersion When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram can be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by different many different Presidents? So now let’s compute the dispersion of the n-grams in our corp_us_tidy. Here we define the dispersion of an n-gram as the number of documents where it occurs at least once. corp_us_trigrams %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = sum(n), DISPERSION = n()) %&gt;% arrange(desc(DISPERSION)) # # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) In particular, cut-off values are often determined to select a list of meaningful lexical bundles. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. A subset of n-grams that are defined and selected based on these distributional criteria (i.e., frequency and dispersion) are often referred to as Lexical bundles. Exercise 4.5 Please create a list of four-grams lexical bundles that have been used in at least FIVE different presidential addressess. Arrange the resulting data frame according to the frequency of the four-grams. 4.7 Word Cloud With frequency data, we can visualize important words in the corpus with a Word Cloud. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts. library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 400, min.freq = 20, scale = c(2,0.5), color = brewer.pal(8, &quot;Dark2&quot;), vfont=c(&quot;serif&quot;,&quot;plain&quot;))) Exercise 4.6 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. (Criteria: Frequency &gt;= 10; Max Number of Words Plotted = 400) require(tidytext) stop_words Exercise 4.7 Get yourself familiar with another R package for creating word clouds, wordcloud2, and re-create a word cloud as Exercise 4.6 but in a fancier format, i.e., a star-shaped one. (Criteria: Frequency &gt;= 15) 4.8 Collocations With unigram and bigram frequencies of the corpus, we can further examine the collocations within the corpus. Collocation refers to a frequent phenomenon where two words tend to co-occur very often in use. This co-occurrence is defined statistically by their lexical associations. 4.8.1 Cooccurrence Table and Observed Frequencies Cooccurrence frequency data for a word pair, w1 and w2, are often organized in a contingency table extracted from a corpus, as shown in Figure 4.1. The cell counts of this contingency table are called the observed frequencies O11, O12, O21, and O22. Figure 4.1: Cooccurrence Freqeucny Table The sum of all four observed frequencies (called the sample size N) is equal to the total number of bigrams extracted from the corpus. R1 and R2 are the row totals of the observed contingency table, while C1 and C2 are the corresponding column totals. The row and column totals are also called marginal frequencies, being written in the margins of the table, and O11 is called the joint frequency. 4.8.2 Expected Frequencies Equations for all association measures are given in terms of the observed frequencies, marginal frequencies, and the expected frequencies E11, …, E22 (under the null hypothesis that W1 and W2 are statistically independent). The expected frequencies can easily be computed from the marginal frequencies as shown in Figure 4.2. Figure 4.2: Computing Expected Frequencies 4.8.3 Association Measures The idea of lexical assoication is to measure how much the observed frequencies deviate from the expected. Some of the metrics (e.g., t-statistic, MI) consider only the joint frequency deviation (i.e., O11), while others (e.g., G2, a.k.a Log Likelihood Ratio) consider the deviations of ALL cells. Here I would like to show you how we can compute the most common two asssociation metrics for all the bigrams found in the corpus–t-test statistic and Mutual Information (MI). \\(t = \\frac{O_{11}-E_{11}}{\\sqrt{E_{11}}}\\) \\(MI = log_2\\frac{O_{11}}{E_{11}}\\) \\(G^2 = 2 \\sum_{ij}{O_{ij}log\\frac{O_{ij}}{E_{ij}}}\\) corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off rename(O11 = n) %&gt;% tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% # split bigrams into two columns mutate(R1 = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], C1 = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), t = (O11 - E11)/sqrt(E11)) %&gt;% # compute associations arrange(desc(MI)) # sorting corp_us_collocations Please note that in the above example, we compute the lexical associations for bigrams whose frequency &gt; 5. This is necessary in collocation studies because bigrams of very low frequency would not be informative even though its association can be very strong. How to compute lexical assoications is a non-trivial issue. There are many more ways to compute the association strengths between two words. Please refer to Stefan Evert’s site for a very comprehensive review of lexical assoication meaasures. Exercise 4.8 Sort the collocation data frame corp_us_collocations according to the t-score and compare the results sorted by MI scores. Exercise 4.9 Based on the formula provided above, please create a new column for corp_us_collocations, which gives the Log-Likelihood Ratios of all the bigrams. When you do the above exercise, you may run into a couple of problems: Some of the bigrams have NaN values in their LLR. This may be due to the issue of NAs produced by integer overflow. Please solve this. After solving the above overflow issue, you may still have a few bigrams with NaN in their LLR, which may be due to the computation of the log value. In Math, log(1/0)= Inf and log(0/1) = -Inf. Do you know when you would get an undefined value NaN in the computation of log()? Exercise 4.10 Find the top FIVE bigrams ranked according to MI values for each president. The result would be a data frame as shown below. Create a plot as shown below to visualize your results. 4.9 Constructions We are often interested in the use of linguistic patterns, which are beyond the lexical boundaries. My experience is that usually it is better to work with the corpus on a sentential level. We can use the same tokenization function, unnest_tokens() to convert our text-based corpus data frame, corpus_us_tidy, into a sentence-based tidy structure: corp_us_sents &lt;- corp_us_tidy %&gt;% unnest_tokens(output = sentence, input = text, token = &quot;sentences&quot;) # tokenize the `text` column into `sentence` corp_us_sents With each sentence, we can investigate particular constructions in more detail. Let’s assume that we are interested in the use of Perfect aspect in English by different presidents. We can try to extract Perfect constructions (including Present/Past Perfect) from each sentence using the regular expression. Here we make a simple naive assumption: Perfect constructions include all have/has/had + VERB-en/ed tokens from the sentences. require(stringr) # Perfect corp_us_sents %&gt;% mutate(pattern = str_extract_all(sentence, &quot;ha[d|ve|s] \\\\w+(en|ed)&quot;)) %&gt;% unnest_tokens(perfect, pattern,token = c) %&gt;% select(-sentence) -&gt; result_perfect result_perfect And of course we can do an exploratory analysis of the frequencies of Perfect constructions by different presidents: require(tidyr) # table result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) # graph result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) %&gt;% pivot_longer(c(&quot;TOKEN_FREQ&quot;, &quot;TYPE_FREQ&quot;), names_to = &quot;STATISTIC&quot;, values_to = &quot;NUMBER&quot;) %&gt;% ggplot(aes(President, NUMBER, fill = STATISTIC)) + geom_bar(stat = &quot;identity&quot;,position = position_dodge()) + theme(axis.text.x = element_text(angle=90)) There are quite a few things we need to take care of more thoroughly: The auxilliary HAVE and the past participle do not necessarily have to stand next to each other for Perfect constructions. We now lose track of one important information: from which sentence of the Presidental addressess did we collect each Perfect constructional token? Any ideas how to solve all these issues? Exercise 4.11 Please create a better regular expression to retrieve more tokens of English Perfect constructions, where the auxilliary and participle may not stand together. Exercise 4.12 Re-generate a result_perfect data frame, where you can keep track of: From the N-th sentence of the address did the Perfect come? From which address did the Perfect come? You may have a data frame as shown below. Exercise 4.13 Re-create the above bar plot in a way that the type and token frequencies are computed based on each address and the x axis should be arranged accordingly. Your resulting graph should look similar to the one below. References "],
["parts-of-speech-tagging.html", "Chapter 5 Parts-of-Speech Tagging 5.1 Installing the Package 5.2 Quick Overview 5.3 Working Pipeline 5.4 Parsing Your Text 5.5 Metalingusitic Analysis 5.6 Construction Analysis 5.7 Issues on Pattern Retrieval 5.8 Saving POS-tagged Texts 5.9 Finalize spaCy", " Chapter 5 Parts-of-Speech Tagging library(tidyverse) library(tidytext) library(quanteda) In many textual analysis, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. In particular, I will introduce a powerful package spacyr, which is an R wrapper to the spaCy “industrial strength natural language processing” Python library from https://spacy.io. In addition to POS tagging, the package provides other linguistically relevant annotations for more in-depth analysis of the text. 5.1 Installing the Package Please consult the spacyr github for more instructions on installing the package. There are at least four steps: Install miniconda (or any other conda version for Python) Install the spacyr R package install.packages(&quot;spacyr&quot;) Because spacyr is an R wrapper to a Python pacakge, you need to have Python installed in your OS system as well. The easiest way to install spaCy and spacyr is through the spacyr function spacy_install(). This function by default creates a new conda environment called spacy_condaenv, as long as some version of conda is installed on the user’s the system. (spacyr uses Python 3.6.x and spaCy 2.2.3+) library(spacyr) spacy_install(version=&#39;2.2.3&#39;) The spacy_install() will create a stand-alone conda environment including a python executable separate from your system Python (or anaconda python), install the latest version of spaCy (and its required packages), and download English language model. If you don’t have any conda version installed on your system, you can install miniconda from [https://conda.io/miniconda.html]https://conda.io/miniconda.html. (Choose the 64-bit version, or alternatively, run to the computer store now and purchase a 64-bit system to replace your ancient 32-bit platform.) Also, the spacy_install() will automatically install the miniconda (if there’s no conda installed on the system) for MAC users. Windows users may need to consult the spacyr github for more important instructions on installation. For Windows, you need to run R as an administrator to make installation work properly. To do so, right click the RStudio icon (or R desktop icon) and select “Run as administrator” when launching R. Initializing spaCy in R library(spacyr) spacy_initialize() ## Found &#39;spacy_condaenv&#39;. spacyr will use this environment ## successfully initialized (spaCy Version: 2.2.3, language model: en_core_web_sm) ## (python options: type = &quot;condaenv&quot;, value = &quot;spacy_condaenv&quot;) 5.2 Quick Overview The spacyr provides a useful function, spacy_parse(), which allows us to parse an English text in a very convenient way. txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt, pos = T, tag = T, lemma = T, entity = T, dependency = T) parsedtxt The output parsedtext is a data frame, which includes annotations of the original texts at multiple granularities. All texts have been tokenized into words with each word, sentence, and text given an unique ID (i.e., doc_id, sentence_id, token_id) Lemmatization is also done (i.e., lemma) POS Tags can also be found (i.e., pos and tag) pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set. Depending on the argument setting for spacy_parse(), you can get more annotations, such as named entities (entity) and dependency relations (del_rel). 5.3 Working Pipeline In Chapter ??, we provide a working pipeline for text analytics. Here we like to revise the workflow to satisfy different goals in computational text analytics. After we secure a collection of raw texts as our corpus, if we do not need additional parts-of-speech information, we follow the workflow on the right. If we need additional annotations from spacyr, we follow the workflow on the left. 5.4 Parsing Your Text Now let’s use this spacy_parse() to analyze the presidential addresses we’ve seen in Chapter 4: the data_corpus_inaugural from quanteda. To illustrate the annotation more clearly, let’s parse the first text in data_corpus_inaugural: library(quanteda) library(tidytext) doc1 &lt;- spacy_parse(data_corpus_inaugural[1]) doc1 We can parse the whole corpus collection as well: we first apply the spacy_parse to each text in data_corpus_inaugural using map() and then rbind() individual resulting data frames into one using do.call(). system.time( corp_us_words &lt;- data_corpus_inaugural %&gt;% map(spacy_parse, tag = T) %&gt;% do.call(rbind, .)) ## user system elapsed ## 13.575 1.112 14.688 corp_us_words Before we move on, we need to clean up the doc_id column of corp_us_words. corp_us_words &lt;-corp_us_words %&gt;% mutate(doc_id = str_replace(row.names(.), &quot;\\\\.\\\\d+$&quot;,&quot;&quot;)) Exercise 5.1 In corpus linguistics analysis, we often need to examine constructions on a sentential level. It would be great if we can transform the word-based data frame into a sentence-based one for more efficient later analysis. Also, on the sentential level, it would be great if we can preserve the information of the lexical POS tags. How can you transform the corp_us_words into one as provided below? 5.5 Metalingusitic Analysis Now spacy_parse() has enriched our corpus data with more linguistic annotations. We can now utilize the additional tags for more analysis. In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_words and first generate the frequencies of verbs, and number of words for each presidential speech text. syn_com &lt;- corp_us_words %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) %&gt;% ungroup syn_com With the syntactic complexity of each president, we can plot the tendency: syn_com %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 5.2 Please add a regression/smooth line to the above plot to indicate the downward trend? 5.6 Construction Analysis Now with parts-of-speech tags, we are able to look at more linguistic patterns or constructions in detail. These POS tags allow us to extract more precisely the target patterns we are interested in. In this section, we will use the output from Exercise 5.1. We assume that now we have a sentence-based corpus data frame, corp_us_sents. Here I like to provide a case study on English Preposition Phrases. corp_us_sents We can utilize the regular expressions to extract PREP + NOUN combinations from the corpus data. # define regex patterns pattern_pat1 &lt;- &quot;[^/ ]+/ADP [^/]+/NOUN&quot; # extract patterns from corp corp_us_sents %&gt;% unnest_tokens(output = pat_pp, input = sentence_tag, token = function(x) str_extract_all(x, pattern=pattern_pat1)) -&gt; result_pat1 result_pat1 In the above example, we specify the token= argument in unnest_tokens(..., token = ...) with a self-defined function. The idea of tokenization in unnest_tokens() is that the token argument should be a function which takes a text-based vector as input (i.e, each element of the input vector may be a document text) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (cf. Figure 5.1). Figure 5.1: Intuition for token= in unnest_tokens() In our demonstration, we define a tokenization function, which takes sentence_tag as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions in individual sentences in sentence_tag. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Exercise 5.3 Create a new column, pat_clean, with all annotations removed in the data frame result_pat1. With these constructional tokens of English PP’s, we can then do further analysis. We first identify the PREP and NOUN for each constructional token. We then clean up the data by removing POS annotations. # extract the prep and head result_pat1 %&gt;% tidyr::separate(col=&quot;pat_pp&quot;, into=c(&quot;PREP&quot;,&quot;NOUN&quot;), sep=&quot;\\\\s+&quot; ) %&gt;% mutate(PREP = str_replace_all(PREP, &quot;/[^ ]+&quot;,&quot;&quot;), NOUN = str_replace_all(NOUN, &quot;/[^ ]+&quot;,&quot;&quot;)) -&gt; result_pat1a result_pat1a Now we are ready to explore the text data. We can look at how each preposition is being used by different presidents: # President Top 2 prep result_pat1a %&gt;% count(doc_id, PREP) %&gt;% arrange(doc_id, desc(n)) We can examine the most frequent NOUN that co-occurrs with each PREP: # Most freq NOUN for each PREP result_pat1a %&gt;% count(PREP, NOUN) %&gt;% group_by(PREP) %&gt;% top_n(1,n) %&gt;% arrange(desc(n)) We can also look at a more complex usage pattern: how each president uses the PREP of in terms of their co-occurring NOUNs? # NOUNS for `of` uses across different presidents result_pat1a %&gt;% filter(PREP == &quot;of&quot;) %&gt;% count(doc_id, PREP, NOUN) %&gt;% tidyr::pivot_wider( id_cols = c(&quot;doc_id&quot;), names_from = &quot;NOUN&quot;, values_from = &quot;n&quot;, values_fill = list(n=0)) Exercise 5.4 In our earlier demonstration, we made a naive assumption: Preposition Phrases include only those cases where PREP and NOUN are adjacent to each other. But there are many more tokens where words do come between the PREP and the NOUN (e.g., with greater anxieties, by your order). Please revise the regular expression to improve the retrieval of the English Preposition Phrases from the corpus data corp_us_sents. Specifically, we can define an English PP as a sequence of words, which start with a preposition, and end at the first word after the preposition that is tagged as NOUN, PROPN, or PRON. Exercise 5.5 Based on the output from Exercise 5.4, please identify the PREP and NOUN for each constructional token and save information in two new columns. 5.7 Issues on Pattern Retrieval Any automatic pattern retrieval comes with a price: there are always errors returned by the system. I would like to discuss this issue based on the second text, 1793-Washington. First let’s take a look at the Preposition Phrases extracted by my regular expression used in Exercise 5.4 and 5.5: ## If you haven&#39;t finished the exercise, the dataset is also available in `demo_data/result_pat2a.RDS # result_pat2a &lt;- readRDS(&quot;demo_data/result_pat2a.RDS&quot;) # uncomment this line if you dont have `result_pat2a` result_pat2a %&gt;% filter(doc_id == &quot;1793-Washington&quot;) My regular expression has identified 20 PP’s from the text. However, if we go through the text carefully and do the PP annotation manually, we may have different results. Figure 5.2: Manual Annotation of English PP’s in 1793-Washington There are two types of errors: False Positives: Patterns identified by the system but in fact they are not true patterns. False Negatives: True patterns in the data but are not successfully identified by the system. As shown in Figure 5.2, manual annotations have identified 21 PP’s from the text while the regular expression identified 20 tokens. A comparison of the two results shows that: In the above regex result, the following returned tokens (rows highlighted) are false, i.e., False Positives. doc_id sentence_id PREP NOUN pat_pp row_id 1793-Washington 1 by voice by/adp the/det voice/noun 1 1793-Washington 1 of country of/adp my/det country/noun 2 1793-Washington 1 of chief of/adp its/det chief/propn 3 1793-Washington 2 for it for/adp it/pron 4 1793-Washington 2 of honor of/adp this/det distinguished/adj honor/noun 5 1793-Washington 2 of confidence of/adp the/det confidence/noun 6 1793-Washington 2 in me in/adp me/pron 7 1793-Washington 2 by people by/adp the/det people/noun 8 1793-Washington 2 of united of/adp united/propn 9 1793-Washington 3 to execution to/adp the/det execution/noun 10 1793-Washington 3 of act of/adp any/det official/adj act/noun 11 1793-Washington 3 of president of/adp the/det president/propn 12 1793-Washington 3 of office of/adp office/noun 13 1793-Washington 4 in presence in/adp your/det presence/noun 14 1793-Washington 4 during administration during/adp my/det administration/noun 15 1793-Washington 4 of government of/adp the/det government/propn 16 1793-Washington 4 in instance in/adp any/det instance/noun 17 1793-Washington 5 to upbraidings to/adp the/det upbraidings/noun 18 1793-Washington 5 of who of/adp all/det who/pron 19 1793-Washington 5 of ceremony of/adp the/det present/adj solemn/adj ceremony/noun 20 In the above manual annotation (Figure 5.2), phrases highlighted in red are NOT successfully identified by the current regex query, i.e., False Negatives. We can summarize the pattern retrieval results as: Most importantly, we can describe the quality of the pattern retrieval with two important measures. \\(Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Positives}\\) \\(Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Negatives}\\) In our case: \\(Precision = \\frac{18}{18+2} = 90%\\) \\(Precision = \\frac{18}{18 + 3} = 85.71%\\) It is always very difficult to reach 100% precision or 100% recall for automatic retrieval of the target patterns. Researchers often need to make a compromise. The following are some heuristics based on my experiences: For small datasets, probably manual annotations give the best result. For moderate-sized dataset, semi-automatic annotations may help. Do the automatic annotations first and follow up with manual checkups. For large datasets, automatic annotations are preferred in order to examine the general tendency. However, it is always good to have a random sample of the data to check the query performance. The more semantics-related the annotations, the more likely one would adopt a manual approach to annotation (e.g., conceptual metaphors, sense distinctions, dialogue acts). Common annotations of corpus data may prefer an automatic approach, such as Chinese word segmentation, POS tagging, named entity recognition, chunking, noun-phrase extractions, or dependency relations(?). 5.8 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time when we process the data, it would be more convenient if we save the tokenized texts with the POS tags in the hard drive. Next time we can import those files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. A few suggestions: If you are dealing with a small corpus, I would probably suggest you to save the resulting data frame from spacy_parse() as a csv for later use. If you are dealing with a big corpus, I would probably suggest you to save the parsed output of each text file in an independent csv for later use. write_csv(corp_us_words, &quot;corp-inaugural-word.csv&quot;) 5.9 Finalize spaCy While running spaCy on Python through R, a Python process is always running in the background and Rsession will take up a lot of memory (typically over 1.5GB). spacy_finalize() terminates the Python process and frees up the memory it was using. spacy_finalize() "],
["keyword-analysis.html", "Chapter 6 Keyword Analysis 6.1 About Keywords 6.2 Statistics for Keyness 6.3 Implementation", " Chapter 6 Keyword Analysis In this chapter, I would like to talk about the idea of “keyness”. Keywords in corpus linguistics are defined statistically using different measures of keyness. Keyness can be computed for words occurring in a target corpus by comparing their frequencies in that target corpus to that of a reference corpus and quantifying the relative attraction of each word to the target corpus by means of a statistical association metric. This idea of course can be easily extended to key phrases as well. In other words, for keyword analysis, we assume that there is a reference corpus on which the keyness of the words in the target corpus is computed. 6.1 About Keywords Keywords are important for research on languagea and ideology. To discover distinct groups through ideological inclinations expressed in writings, most researchers draw inspiration from Raymond Williams’s idea of keywords, terms presumably carrying sociocultural meanings characteristic of Western capitalist ideologies (Williams 1976). In contrast to William’s intuition-based approach, recent studies have promoted a bottom-up corpus-based method to discover terminology reflecting the ideological undercurrents of the text collections. This data-driven approach to keywords is sympathetic to the notion of statistical keywords popularized by Michael Stubbs (Stubbs 1996, 2003). 6.2 Statistics for Keyness To compute the keyness of a word w, we often need a contingency table as shown in Figure 6.1: Figure 6.1: Frequency distributions of a word and all other words in two corpora Different keyness statistics may have different ways to evaluate the relative importance of the co-occurrences of the word w with the target and the reference corpus (i.e., a and b in Figure 6.1) and statistically determine which connection is stronger. In this chapter, I would like to discuss two common statistics used in keyness analysis. This tutorial is based on Gries (2018), Ch. 5.2.6 the log-likelihood ratio G2 (Dunning 1993); \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] difference coefficient (Leech and Fallon 1992); \\[ difference\\;coefficient = \\frac{a - b}{a + b} \\] relative frequency ratio (Damerau 1993) \\[ rfr = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] 6.3 Implementation In this tutorial we will use two documents as our mini reference and target corpus. These two documents are old Wikipedia entries (provided in Gries (2018)) on Perl and Python respectively. First we initialize necessary packages in R library(tidyverse) library(tidytext) library(readtext) library(quanteda) Then we load the corpus data, which are available as two text files in our demo_data, and transform the corpus into a tidy structure #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) %&gt;% select(textid, text) We convert the text-based corpus into a word-based one for word frequency information corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) #%&gt;% #group_by(textid) %&gt;% #mutate(word_id = row_number()) %&gt;% #ungroup corpus_word %&gt;% head(100) Now we need to get the frequencies of each word in the two documents respectively. corpus_word %&gt;% count(word, textid, sort = T) -&gt; word_freq word_freq As now we are analyzing each word in relation to the two corpora, it would be better for us to have each word type as one independent row, and columns recording their co-occurrence frequencies with the two corpora. How do we get this? With all necessary frequencies, we can compute the three keyness statistics for each word contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) -&gt; keyness_table keyness_table Although now we have the keyness values for words, we still don’t know to which corpus the word is more attracted. What to do next? keyness_table %&gt;% mutate(preference = ifelse(a &gt; a.exp, &quot;perl&quot;,&quot;python&quot;)) %&gt;% select(word, preference, everything())-&gt; keyness_table keyness_table Exercise 6.1 The CSV in demo_data/data-movie-reviews.csv is the IMDB dataset with 50,000 movie reviews and their sentiment tags (source: Kaggle). The CSV has two columns–the first column review includes the raw texts of each movie review; the second column sentiment provides the sentiment tag for the review. Each review is either positive or negative. We can treat the dataset as two separate corpora: negative and positive corpora. Please find the top 10 keywords for each corpus ranked by the G2 statistics. In the data preprocessing, please use the default tokenization in unnest_tokens(..., token = &quot;words&quot;). When computing the keyness, please exclude: words with at least one non-alphanumeric symbols in them (e.g. regex class \\W) words whose frequency is &lt; 10 in each corpus The expected results are provided below for your reference. References "],
["constructions-and-idioms.html", "Chapter 7 Constructions and Idioms 7.1 Chinese Four-character Idioms 7.2 Dictionary Entries 7.3 Case Study: X來Y去 7.4 Exercises", " Chapter 7 Constructions and Idioms library(tidyverse) 7.1 Chinese Four-character Idioms Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. This chapter will provide a exploratory analysis of four-character idioms in Chinese. 7.2 Dictionary Entries In our demo_data directory, there is a file dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first import the idioms in the file. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;) head(all_idioms) ## [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; tail(all_idioms) ## [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; length(all_idioms) ## [1] 56536 In order to make use of the tidy structure in R, we convert the data into a tibble: idiom &lt;- tibble(string = all_idioms) 7.3 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來X去: idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we may need to extract the X and Y in the schema: idiom_laiqu &lt;-idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% mutate(pattern = str_replace(string, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(pattern, into = c(&quot;w1&quot;, &quot;w2&quot;), sep = &quot;_&quot;) idiom_laiqu One empirical question is how many of these idioms are of the pattern X=Y (e.g., 想來想去, 直來直去) and how many are of X!=Y (e.g., 說來道去, 朝來暮去): idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() 7.4 Exercises Exercise 7.1 Please use idiom and extract the idioms with the schema of 一X一Y. Exercise 7.2 Also with the idiom as our data source, now if we are interested in all idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Also, provide the distribution of the two types. Exercise 7.3 Following Exercise 7.2, for each type of the idioms, please provide their respective proportions of X=Y vs. X!=Y. Exercise 7.4 Folloing Exercise 7.3, please identify the character that is duplicated in the idioms. One follow-up analysis would be to look at the distribution of these pivotal characters. Can you reproduce a graph as shown below as closely as possible? "],
["chinese-text-processing.html", "Chapter 8 Chinese Text Processing 8.1 Chinese Word Segmenter jiebaR 8.2 Chinese Text Analytics Pipeline 8.3 Loading Text Data 8.4 Case Study 1: Word Frequency and Wordcloud 8.5 Case Study 2: Patterns 8.6 Case Study 3: Lexical Bundles", " Chapter 8 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss English parts-of-speech tagging in Chapter 5, it is easy to do the word tokenization in English as the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. This chapter is devoted to Chinese text processing. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. In later Chapter 9, we will introduce another segmenter for Taiwan Mandarin, i.e., the CKIP Tagger, which comes with more functionalities. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 8.1 Chinese Word Segmenter jiebaR 8.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) ## [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: initilzie a word segmenter object using worker() segment the texts using segment() seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment the document, text, you first initialize a segmenter seg1 using worker() and feed this segmenter to segment(jiebar = seg1)and segment text into words. 8.1.2 Settings There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) Exercise 8.1 In our earlier example, when we created the segmenter seg1, we did not specify any arguments for worker(). Can youo tell what are the default settings for the parameters of worker()? Please try to create work() with different settings and see how the segmented results differ from each other. 8.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. This can be done when you initialize the segmenter using worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 8.1.4 Stopwords When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative. seg3 &lt;- worker(stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣民眾&quot; &quot;黨&quot; ## [25] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; &quot;哲&quot; &quot;7&quot; ## [31] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; ## [37] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; 8.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = &quot;tag&quot; when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; The following table lists the annotations of the POS tagsets used in jiebaR: 8.1.6 Default You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: # show files under `dictpath` dir(show_dictpath()) # Check the default stop_words list # Please change the path to your default dict path scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, what=character(),nlines=50,sep=&#39;\\n&#39;, encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) 8.1.7 Reminder When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and returns a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; 8.2 Chinese Text Analytics Pipeline In Chapter 4, we have talked about the work pipeline for normal English texts processing, as shown below: Figure 8.1: Chinese Text Analytics Flowchart For Chinese texts, the work flow is pretty much the same. Because the current Chinese word segmenter jiebaR does not return the results in a tidy structure format, the most important trick is that when tokenizing the raw texts using unnest_tokens(), we need to specify our own tokenzier for the argument token = ... in the unnest_tokens(). It is important to note that when we specify a self-defined token function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument byline = TRUE for worker(byline = TRUE). So based on our simple-corpus example above, we first transform the character vector text into a tidy structure corpus data frame. Also, we generate an unique index for each row using row_number(). # a text-based tidy corpus a_small_tidy_corpus &lt;- text %&gt;% corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) a_small_tidy_corpus Second, we initialize the work() for tokenization. # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;) Finally, we use unnest_tokens() to tokenize texts into words. Specifically, we tokenize the texts included in the text column and unnest the tokens in the word column. # tokenization a_small_tidy_corpus_by_word &lt;- a_small_tidy_corpus %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) a_small_tidy_corpus_by_word In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. 8.3 Loading Text Data When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext. The main function in this package, readtext(), which takes a file or a directory name from disk or a URL, and returns a type of data.frame that can be used directly with the corpus() constructor function in quanteda, to create a quanteda corpus object. In other words, the output from readtext can be directly passed on to the processing in the tidy structure framework (i.e., tidytext::unnest_tokens()). The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. The corpus constructor command corpus() works directly on: a vector of character objects, for instance that you have already loaded into the workspace using other tools; a data.frame containing a text column and any other document-level metadata 8.4 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart 8.1: Load the corpus data using readtext() Create a text-based tidy data frame apple_df (i.e., a tibble) Intialize a word segmenter using worker() Tokenize the text-based data frame into a word-based tidy data frame using unnest_tokens() NB: readtext() returns a data.frame. It is not necessary to convert the data.frame to tibble. # loading the corpus # NB: this may take some time apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% as_tibble() %&gt;% filter(text !=&quot;&quot;) %&gt;% #remove empty documents mutate(doc_id = row_number()) # create doccument index apple_df %&gt;% head(10) # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T) # Tokenization apple_word &lt;- apple_df %&gt;% unnest_tokens(output = word, input= text, token = function(x) segment(x, jiebar = segmenter)) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup apple_word %&gt;% head(100) Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to trace back the original context where the word, phrase or sentence comes from. With all these unique indices, we can easily keep track of the sources of all tokenized linguistic units. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) With a word-based corpus tibble, we can easily generate a word frequency list as well as a wordcloud to have a quick overview of the word distribution in the corpus. # The font setting is necessary for Mac, not sure for Windows. library(showtext) # font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots ## If loading showtext throws errors, install: https://www.xquartz.org/ showtext_auto(enable = TRUE) apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) # `wordcloud` version # require(wordcloud) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 400) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) # clear up memory rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 8.5 Case Study 2: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. 8.5.1 Define Own Tokenization Functions We define two tokenization functions: chinese_chunk_tokenizer(): This function tokenizes a document text into a series of inter-punctuation units. We refer to these units as sentence-like chunks. chinese_word_tokenizer(): This function tokenizes a text into a vector of “word/tag” tokens. # word tokenizer chinese_word_tokenizer&lt;- function(text, tagger){ segment(text, tagger) %&gt;% map(function(x) paste(x, names(x), sep=&quot;/&quot;)) } # Chunk tokenizer chinese_chunk_tokenizer &lt;- function(text){ str_split(text, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;) } Initialize worker() When initilizing the word segmenter worker(), remember to specify the argument type = &quot;tag&quot; to get POS tags. Also, we specify own dictionary (user = ...) and remove symbols (symbol=T) when doing the word tokenization. # Testing code postagger &lt;-worker(type = &quot;tag&quot;,user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = TRUE) We can try our self-defined functions with one text from the corpus: apple_df$text[1] ## [1] &quot;《蘋果體育》即日起進行虛擬賭盤擂台，每名受邀參賽者進行勝負預測，每周結算在周二公布，累積勝率前3高參賽者可繼續參賽，單周勝率最高者，將加封「蘋果波神」頭銜。註:賭盤賠率如有變動，以台灣運彩為主。\\n資料來源：NBA官網http://www.nba.com\\n\\n金塊(客) 103：92 76人騎士(主) 88：82 快艇活塞(客) 92：75 公牛勇士(客) 108：82 灰熊熱火(客) 103：82 灰狼籃網(客) 90：82 公鹿溜馬(客) 111：100 馬刺國王(客) 112：102 爵士小牛(客) 108：106 拓荒者\\n\\n&quot; apple_df$text[1] %&gt;% chinese_chunk_tokenizer() ## [[1]] ## [1] &quot;&quot; &quot;蘋果體育&quot; ## [3] &quot;即日起進行虛擬賭盤擂台&quot; &quot;每名受邀參賽者進行勝負預測&quot; ## [5] &quot;每周結算在周二公布&quot; &quot;累積勝率前&quot; ## [7] &quot;高參賽者可繼續參賽&quot; &quot;單周勝率最高者&quot; ## [9] &quot;將加封&quot; &quot;蘋果波神&quot; ## [11] &quot;頭銜&quot; &quot;註&quot; ## [13] &quot;賭盤賠率如有變動&quot; &quot;以台灣運彩為主&quot; ## [15] &quot;資料來源&quot; &quot;官網&quot; ## [17] &quot;金塊&quot; &quot;客&quot; ## [19] &quot;人騎士&quot; &quot;主&quot; ## [21] &quot;快艇活塞&quot; &quot;客&quot; ## [23] &quot;公牛勇士&quot; &quot;客&quot; ## [25] &quot;灰熊熱火&quot; &quot;客&quot; ## [27] &quot;灰狼籃網&quot; &quot;客&quot; ## [29] &quot;公鹿溜馬&quot; &quot;客&quot; ## [31] &quot;馬刺國王&quot; &quot;客&quot; ## [33] &quot;爵士小牛&quot; &quot;客&quot; ## [35] &quot;拓荒者&quot; &quot;&quot; apple_df$text[1] %&gt;% chinese_word_tokenizer(postagger) ## [[1]] ## [1] &quot;《/x&quot; &quot;蘋果/n&quot; &quot;體育/vn&quot; &quot;》/x&quot; &quot;即日起/l&quot; &quot;進行/v&quot; ## [7] &quot;虛擬/v&quot; &quot;賭盤/x&quot; &quot;擂台/v&quot; &quot;，/x&quot; &quot;每名/x&quot; &quot;受邀/v&quot; ## [13] &quot;參賽者/n&quot; &quot;進行/v&quot; &quot;勝負/v&quot; &quot;預測/vn&quot; &quot;，/x&quot; &quot;每周/r&quot; ## [19] &quot;結算/v&quot; &quot;在/p&quot; &quot;周二/t&quot; &quot;公布/v&quot; &quot;，/x&quot; &quot;累積/v&quot; ## [25] &quot;勝率/n&quot; &quot;前/f&quot; &quot;3/x&quot; &quot;高/a&quot; &quot;參賽者/n&quot; &quot;可/v&quot; ## [31] &quot;繼續/v&quot; &quot;參賽/n&quot; &quot;，/x&quot; &quot;單周/x&quot; &quot;勝率/n&quot; &quot;最高者/n&quot; ## [37] &quot;，/x&quot; &quot;將/zg&quot; &quot;加封/v&quot; &quot;「/x&quot; &quot;蘋果/n&quot; &quot;波神/x&quot; ## [43] &quot;」/x&quot; &quot;頭銜/n&quot; &quot;。/x&quot; &quot;註/x&quot; &quot;:/x&quot; &quot;賭盤/x&quot; ## [49] &quot;賠率/n&quot; &quot;如有/c&quot; &quot;變動/vn&quot; &quot;，/x&quot; &quot;以/p&quot; &quot;台灣/x&quot; ## [55] &quot;運彩/x&quot; &quot;為主/x&quot; &quot;。/x&quot; &quot;\\n/x&quot; &quot;資料/n&quot; &quot;來源/n&quot; ## [61] &quot;：/x&quot; &quot;NBA/eng&quot; &quot;官網/x&quot; &quot;http/eng&quot; &quot;:/x&quot; &quot;//x&quot; ## [67] &quot;//x&quot; &quot;www/eng&quot; &quot;./x&quot; &quot;nba/eng&quot; &quot;./x&quot; &quot;com/eng&quot; ## [73] &quot;\\n/x&quot; &quot;\\n/x&quot; &quot;金塊/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; ## [79] &quot; /x&quot; &quot;103/m&quot; &quot;：/x&quot; &quot;92/m&quot; &quot; /x&quot; &quot;76/m&quot; ## [85] &quot;人/n&quot; &quot;騎士/n&quot; &quot;(/x&quot; &quot;主/b&quot; &quot;)/x&quot; &quot; /x&quot; ## [91] &quot;88/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; &quot;快艇/n&quot; &quot;活塞/vn&quot; ## [97] &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;92/m&quot; &quot;：/x&quot; ## [103] &quot;75/m&quot; &quot; /x&quot; &quot;公牛/n&quot; &quot;勇士/n&quot; &quot;(/x&quot; &quot;客/n&quot; ## [109] &quot;)/x&quot; &quot; /x&quot; &quot;108/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; ## [115] &quot;灰熊/x&quot; &quot;熱火/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; ## [121] &quot;103/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; &quot;灰狼/n&quot; &quot;籃網/n&quot; ## [127] &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;90/m&quot; &quot;：/x&quot; ## [133] &quot;82/m&quot; &quot; /x&quot; &quot;公鹿/n&quot; &quot;溜/v&quot; &quot;馬/n&quot; &quot;(/x&quot; ## [139] &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;111/m&quot; &quot;：/x&quot; &quot;100/m&quot; ## [145] &quot; /x&quot; &quot;馬刺/nr&quot; &quot;國王/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; ## [151] &quot; /x&quot; &quot;112/m&quot; &quot;：/x&quot; &quot;102/m&quot; &quot; /x&quot; &quot;爵士/n&quot; ## [157] &quot;小牛/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;108/m&quot; ## [163] &quot;：/x&quot; &quot;106/m&quot; &quot; /x&quot; &quot;拓荒者/nr&quot; &quot;\\n/x&quot; &quot;\\n/x&quot; In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a possible sentence-like unit. This can be controversial to many grammarians and syntaticians. However, in practice, this may not be a bad choice as it will become obvious when we extract patterns. For more information related to the unicode ranage for the punctuations in CJK languages, please see this SO discussion thread. 8.5.2 Transform Text-Based to Token-Based Data Frame Now we can apply our self-defined tokenization functions to the text-based corpus data frame apple_df. We first unnest_tokens() the text-based corpus into a chunk-based data frame using the tokenizer chinese_chunk_tokenizer(). Then we transform the chunk-based data into a word based data frame using chinese_word_tokenizer(). system.time( apple_df %&gt;% unnest_tokens(output = chunk, input = text, token = chinese_chunk_tokenizer) %&gt;% filter(nchar(chunk)&gt;1) %&gt;% # remove one-char chunk group_by(doc_id) %&gt;% mutate(chunk_id = row_number()) %&gt;% # create chunk_id ungroup %&gt;% unnest_tokens(output = word, input = chunk, token = function(x) chinese_word_tokenizer(x, postagger)) %&gt;% group_by(chunk_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word_id ungroup -&gt; apple_word_df ) # end sytem.time ## user system elapsed ## 13.531 0.295 13.827 apple_word_df %&gt;% head dim(apple_word_df) ## [1] 2375657 4 The word-based data frame now has parts-of-speech tags for every word in the corpus. Based on the word_id, chunk_id, and doc_id, we can easily keep track of their source documents as well. Now based on the word-based data frame, we create a chunk-based data frame again by concatenating all word/tag in a chunk into a long string. In our earlier chunk tokenization, we only split texts into chunks without performing the word segmentation and POS tagging yet. The word boundary and POS information is only available when we perform the word tokenization using chinese_word_tokenizer. Therefore, to get a chunk with both words and POS tags, we can concatenate “word/tag” tokens into a long string on a chunk basis. system.time( apple_chunk_df &lt;- apple_word_df %&gt;% group_by(doc_id, chunk_id) %&gt;% summarize(chunk = str_c(word, collapse=&quot;\\u3000&quot;)) %&gt;% ungroup ) ## user system elapsed ## 3.187 0.132 3.318 apple_chunk_df %&gt;% head(200) dim(apple_chunk_df) ## [1] 550519 3 But why? Why do we need to combine “word/tag” tokens into a longer string AGAIN?? The chunk-based data frame would be useful for further construction/pattern analysis. 8.5.3 BEI Construction This section will show you how we can make use of the chunk-based corpus data frame. I would like to illustrate its usefulness with a case study: 被 + ... Construction. After we tokenize the text-based tidy corpus into a inter-punctuation-unit-based (IPU), i.e., chunk-based data frame, we can make use of the words as well as their parts-of-speech tags to extract the target pattern we are interested: 被 + ... Construction. The data retrieval process is now very straighforward: we only need to go through all the chunks in the corpus object and see if our target pattern matches any of these chunks. The assumption is that: the BEI-Construction will NOT span different chunks. In the following example, we: define a regular expression \\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v for BEI-Construction, i.e., 被 + VERB use unnest_tokens() and str_extract_all() to extract target patterns # define regex patterns pattern_bei &lt;- &quot;\\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v&quot; # extract patterns from corp apple_chunk_df %&gt;% unnest_tokens(output = pat_bei, input = chunk, token = function(x) str_extract_all(x, pattern=pattern_bei)) -&gt; result_bei result_bei Please check Chapter 5 Parts of Speech Tagging on evaluating the quality of the data retrieved by a regular expression (i.e., precision and recall). To have a more in-depth analysis of BEI construction, we like to automatically extract the verb used in the BEI construction. # Extract BEI + WORD result_bei &lt;- result_bei %&gt;% mutate(VERB = str_replace(pat_bei,&quot;.+\\u3000([^/]+)/v$&quot;,&quot;\\\\1&quot;)) result_bei # Calculate WORD frequency require(wordcloud2) result_bei %&gt;% count(VERB) %&gt;% mutate(n = log(n)) %&gt;% top_n(100, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.3) Exercise 8.2 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 8.3 To more properly evaluate the quality of the pattern queries, it would be great if we still have the original chunk texts available in the resulting data frame result_bei. How do we keep this information? That is, please have one column in result_bei, which shows the original chunk texts from which the construction token is extracted. Exercise 8.4 Please use the apple_chunk_df as your tidy corpus and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. So please (a) extract all concordance lines with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 8.5 Following Exercise 8.4, please generate a frequency list of the LMs for each spac particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Exercise 8.6 Following Exercise 8.5, for each space particle, please create a word cloud of its co-occuring LMs based on the top 100 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 8.7 From the above provided in Exercise 8.5, do you find any bizarre cases? Can you tell us why? What would be problems? Or what did we do wrong in the previous processing? Please discuss these issues in relation to each step in our data processing, i.e., word segmentation, POS tagging, and pattern retrievals. 8.6 Case Study 3: Lexical Bundles 8.6.1 N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at recurrent four-grams. As we discussed in Chapter 4, a multiword unit can be defined based on at least two metrics: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) As the default tokenization in unnest_tokens() only works with the English data, we start this task by defining our own token function ngram_chi() to extract Chinese n-grams. # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s|\\u3000&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes ONE text (scalar) as an input, and returns a vector of n-grams. Most importantly, this function assumes that in the text string, each word token is delimited by a whitespace (i.e., a word-segmented text!!) s &lt;- &quot;這 是 一個 測試 的 句子&quot; ngram_chi(text = s, n = 2, delimiter = &quot;_&quot;) ## [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; ngram_chi(text = s, n = 4, delimiter = &quot;_&quot;) ## [1] &quot;這_是_一個_測試&quot; &quot;是_一個_測試_的&quot; &quot;一個_測試_的_句子&quot; ngram_chi(text = s, n = 5, delimiter = &quot; &quot;) ## [1] &quot;這 是 一個 測試 的&quot; &quot;是 一個 測試 的 句子&quot; ngram_chi(text = s, n = 7, delimiter = &quot;_&quot;) # empty string ## [1] &quot;&quot; We vectorize the function ngram_chi(). This step is important because in unnest_tokens() the self-defined token function should take a text-based vector as input and return a list of token-based vectors of the same length as the output (cf. Section 8.2). # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) Vectorized functions are a very useful feature of R, but programmers who are used to other languages often have trouble with this concept at first. A vectorized function works not just on a single value, but on a whole vector of values at the same time. In our first defined ngram_chi function, it takes one text vector as an input and processes it one at a time. However, we would expect ngram_chi to process a vector of texts (i.e., multiple texts) at the same time and return a list of resulting ngrams vectors at the same time. Therefore, we use Vectorize() as a wrapper to vectorize our function and specifically tell R that the argument text is vectorized, i.e., process each value in the text vector in the same way. Now we can tokenize our corpus into n-grams using our own token function vngram_chi() and the unnest_tokens(). In this case study, we demonstrate the analysis of four-grams in our Apple News corpus. We first remove all POS tags in apple_chunk_df$chunk because n-grams do not need the POS tags We then transform the chunk-based data frame apple_chunk_df into a n-gram-based data frame using unnest_tokens(...) with self-defined token function We remove chunks with no target n-grams extracted (Chunks with less than four words will have NO four-grams extracted.) system.time( apple_ngram &lt;-apple_chunk_df %&gt;% mutate(chunk = str_replace_all(chunk, &quot;/[^/]+(\\u3000|$)&quot;,&quot;\\\\1&quot;)) %&gt;% # remove pos tags unnest_tokens(ngram, chunk, token = function(x) vngram_chi(text = x, n= 4)) %&gt;% filter(ngram!=&quot;&quot;)) ## user system elapsed ## 55.161 0.142 55.305 apple_ngram %&gt;% head(20) dim(apple_ngram) ## [1] 973985 3 Exercise 8.8 Because n-grams extraction often requires no POS tags, it is not necessary (or redundant) to perform the POS tagging first and then remove the tags again indeed. For this task, we can split the raw text corpus into chunks and then do the word segmentation as well as n-grams extraction at the same time. It is also possible to create a self-defined function like chinese_ngram_tokenizer, which takes a simpler word segmenter, and directly get the apple_ngram from apple_df Please define a function chinese_ngram_tokenizer to make the following chunk possible so that we can generate ngrams directly from apple_df, using the simpler segmenter wordsegmenter (see below). The following code chunk should produce the same result as apple_ngram. # Simpler word segmenter wordsegmenter &lt;-worker(user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = TRUE) # N-grams Extraction system.time( apple_df %&gt;% unnest_tokens(output = chunk, input = text, token = chinese_chunk_tokenizer) %&gt;% filter(nchar(chunk)&gt;1) %&gt;% # remove one-char chunk group_by(doc_id) %&gt;% mutate(chunk_id = row_number()) %&gt;% # create chunk_id ungroup %&gt;% unnest_tokens(output = ngram, input = chunk, token = function(x) chinese_ngram_tokenizer(x,wordsegmenter, n = 4)) %&gt;% filter(ngram!=&quot;&quot;) -&gt; apple_ngram_2 ) # end sytem.time ## user system elapsed ## 23.802 0.145 23.946 nrow(apple_ngram_2) ## [1] 973985 nrow(apple_ngram) ## [1] 973985 apple_ngram_2 %&gt;% head(20) 8.6.2 Frequency and Dispersion Now that we have the four-grams-based tidy corpus object, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) apple_ngram_dist &lt;- apple_ngram %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 5) Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;被&quot;)) %&gt;% arrange(desc(dispersion)) apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;以&quot;)) %&gt;% arrange(desc(dispersion)) Exercise 8.9 In the above example, if we are only interested in the four-grams with the word 以, how can we revise the regular expression so that we can get rid of tokens like ngrams with 以及, 以上 etc. "],
["ckiptagger.html", "Chapter 9 CKIP Tagger 9.1 Installation 9.2 Download the Model Files 9.3 R-Python Communication 9.4 Word Segmentation in R 9.5 R Environment Setting 9.6 Loading Python Modules 9.7 Segmenting Texts 9.8 Define Own Dictionary 9.9 Beyond Word Boundaries 9.10 Tidy Up the Results", " Chapter 9 CKIP Tagger The current state-of-art Chinese segmenter for Taiwan Mandarin available is probably the CKIP tagger, created by the Chinese Knowledge and Information Processing (CKIP) group at the Academia Sinica. The ckiptagger is released as a python module. In this chpater, I will demonstrate how to use the module for Chinese word segmentation but in an R environment, i.e., how to integrate Python modules in R coherently to perform complex tasks. 9.1 Installation Because ckiptagger is built in python, we need to have python installed in our working environment. Please install the following applications on your own before you start: Anaconda + Python 3.7+ ckiptagger module in Python (Please install the module using the Anaconda Navigator) (Please consult the github of the ckiptagger for more details on installation.) For some reasons, the module ckiptagger cannot be found in the base channel. In Anaconda Navigator, please add specifically the following channel to the environment so that your Anaconda can find ckiptagger module: https://conda.anaconda.org/roccqqck 9.2 Download the Model Files All NLP applications have their models behind their fancy performances. To use the tagger provided in ckiptagger, we need to download their trained model files. Please go to the github of CKIP tagger to download the model files. (The file is very big. It takes a while.) After you download the zip file, unzip the data/ directory to your working directory. 9.3 R-Python Communication In order to call Python functions in R/Rstudio, we need to install an R library in your R. The R-Python communication is made possible through the R library reticulate. Please make sure that you have this library installed in your R. install.packages(&quot;reticulate&quot;) 9.4 Word Segmentation in R Before we proceed, please check if you have everything ready: Anaconda + Python 3.7+ Python module: ckiptagger R library: reticulate CKIP model files under your working directory ./data If yes, then we are ready to go. 9.5 R Environment Setting We first load the library reticulate and specify in R which Python we will be using in the current R(It is highly likely that there is more than one Python version installed in your system). Please change the path_to_python to your own path, which includes the Anaconda Python you just installed. library(reticulate) # spacy_initialize() would set the default python to miniconda/spacy_condaenviron # install ckiptagger and tensorflow-1.13.1 in spacy_condaenviron # path_to_python &lt;- &quot;/Users/Alvin/opt/anaconda3/envs/r-reticulate/bin/python&quot; # use_python(path_to_python, required = T) 9.6 Loading Python Modules ## If you like to run ckiptagger in a new environment: #conda_create(&quot;r-reticulate&quot;, conda = &quot;/Users/Alvin/opt/anaconda3/bin/conda&quot;) use_condaenv(&quot;r-reticulate&quot;)#, conda = &quot;/Users/Alvin/opt/anaconda3/bin/conda&quot;) ## Import ckiptagger module ckip &lt;- reticulate::import(module = &quot;ckiptagger&quot;) ## Intialize models ws &lt;- ckip$WS(&quot;./data&quot;) 9.7 Segmenting Texts ## Raw text corpus (sentences) sents &lt;- c(&quot;傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。&quot;, &quot;美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。&quot;, &quot;土地公有政策?？還是土地婆有政策。.&quot;, &quot;… 你確定嗎… 不要再騙了……&quot;, &quot;最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.&quot;, &quot;科長說:1,坪數對人數為1:3。2,可以再增加。&quot;) words &lt;- ws(sents) words ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; &quot;，&quot; &quot;卻&quot; &quot;突然&quot; ## [9] &quot;爆出&quot; &quot;自己&quot; &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來&quot; &quot;體育台&quot; ## [17] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; ## [25] &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公&quot; &quot;有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地&quot; &quot;婆&quot; ## [9] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; The word segmenter ws() returns a list object, each element of which is a word-based vector of the original sentence. 9.8 Define Own Dictionary The performance of Chinese word segmenter depends highly on the dictionary. Texts in different disciplinces may have verry different vocabulary. To prioritize a set of words in a dictionary, we can further ensure the accuracy of the word segmentation. To create a dictionary for ckiptagger, we need to create a list with names = “the new words” and elements = “the weights”. Then we use the python function ckip$construct_dictionary() to create the dictionary Python object, which is the input argument for word segmenter ws(..., recommend_dictionary = ...). # Define new words in own dictionary new_words &lt;- c(&quot;土地婆&quot;,&quot;土地公有政策&quot;,&quot;緯來體育台&quot;) # Transform the `vector` into `list` for Python new_words_py &lt;- as.list(rep(1,length(new_words))) # cf. `list(rep, 1 , length(new_words))` names(new_words_py) &lt;- new_words # To create a dictionary for `construct_dictionary()` # We need a list, with names as the words and list elements as the weights in the dictionary # Create Python `dictionary` object, required by `ckiptagger.wc()` dictionary&lt;-ckip$construct_dictionary(new_words_py) # Segment texts using dictionary words_1 &lt;- ws(sents, recommend_dictionary = dictionary) words_1 ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; ## [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; ## [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; ## [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; ## [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公有政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; ## [6] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; Exercise 9.1 We usually have a list of new words saved in a text file. Can you write a R function, which loads the words in the demo_data/dict-sample.txt into a list named new_words, which can easily serve as the input for ckip$construct_dictionary() to create the python dictionary object? (Note: All weights are default to 1) new_words&lt;-loadDictionary(input = &quot;demo_data/dict-sample.txt&quot;) dictionary&lt;-ckip$construct_dictionary(new_words) # Segment texts using dictionary words_2 &lt;- ws(sents, recommend_dictionary = dictionary) words_2 ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; ## [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; ## [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; ## [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; ## [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公有政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; ## [6] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; 9.9 Beyond Word Boundaries In addition to primitive word segmentation, the ckiptagger provides also the parts-of-speech tags for words and named entity recognitions for the texts. The ckiptagger follows the pipeline below for text processing. Load the models To perform these additional tasks, we need to load the necessary models (pretrained and provided by the CKIP) first as well. They should all have been included in the model directory you unzipped earlier (cf. ./data). # loading other necessary models system.time((pos &lt;- ckip$POS(&quot;./data&quot;))) # 詞性 495s ## user system elapsed ## 4.940 1.406 5.408 system.time((ner &lt;- ckip$NER(&quot;./data&quot;))) # 實體辨識 426s ## user system elapsed ## 5.029 1.844 6.341 POS tagging and NER # Parts-of-speech Tagging pos_words &lt;- pos(words_1) pos_words ## [[1]] ## [1] &quot;Nb&quot; &quot;Nd&quot; &quot;D&quot; &quot;VC&quot; ## [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; ## [9] &quot;VJ&quot; &quot;Nh&quot; &quot;Neu&quot; &quot;Nf&quot; ## [13] &quot;Ng&quot; &quot;P&quot; &quot;Nc&quot; &quot;VC&quot; ## [17] &quot;COMMACATEGORY&quot; &quot;Nh&quot; &quot;D&quot; &quot;VK&quot; ## [21] &quot;Nh&quot; &quot;Ncd&quot; &quot;VJ&quot; &quot;Nc&quot; ## [25] &quot;PERIODCATEGORY&quot; ## ## [[2]] ## [1] &quot;Nc&quot; &quot;Nc&quot; &quot;P&quot; &quot;Nd&quot; ## [5] &quot;Na&quot; &quot;Nb&quot; &quot;D&quot; &quot;VC&quot; ## [9] &quot;DE&quot; &quot;Na&quot; &quot;Nb&quot; &quot;VC&quot; ## [13] &quot;VC&quot; &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;VE&quot; ## [17] &quot;Nh&quot; &quot;D&quot; &quot;D&quot; &quot;Dfa&quot; ## [21] &quot;VH&quot; &quot;VC&quot; &quot;Nc&quot; &quot;VC&quot; ## [25] &quot;COMMACATEGORY&quot; &quot;VG&quot; &quot;Nes&quot; &quot;Nc&quot; ## [29] &quot;D&quot; &quot;Neu&quot; &quot;Nf&quot; &quot;DE&quot; ## [33] &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; ## [37] &quot;PERIODCATEGORY&quot; ## ## [[3]] ## [1] &quot;VH&quot; &quot;QUESTIONCATEGORY&quot; &quot;QUESTIONCATEGORY&quot; &quot;Caa&quot; ## [5] &quot;Nb&quot; &quot;V_2&quot; &quot;Na&quot; &quot;PERIODCATEGORY&quot; ## [9] &quot;PERIODCATEGORY&quot; ## ## [[4]] ## [1] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;Nh&quot; &quot;VK&quot; &quot;T&quot; ## [6] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;D&quot; &quot;D&quot; &quot;VC&quot; ## [11] &quot;Di&quot; &quot;ETCCATEGORY&quot; &quot;ETCCATEGORY&quot; ## ## [[5]] ## [1] &quot;VH&quot; &quot;VJ&quot; &quot;Neu&quot; &quot;Nf&quot; ## [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;Caa&quot; &quot;Neu&quot; ## [9] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; ## [13] &quot;D&quot; &quot;VH&quot; &quot;T&quot; &quot;PERIODCATEGORY&quot; ## [17] &quot;Nep&quot; &quot;SHI&quot; &quot;Na&quot; &quot;DE&quot; ## [21] &quot;Na&quot; &quot;PERIODCATEGORY&quot; ## ## [[6]] ## [1] &quot;Na&quot; &quot;VE&quot; &quot;Neu&quot; &quot;Na&quot; ## [5] &quot;P&quot; &quot;Na&quot; &quot;VG&quot; &quot;Neu&quot; ## [9] &quot;PERIODCATEGORY&quot; &quot;Neu&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; ## [13] &quot;D&quot; &quot;VHC&quot; &quot;PERIODCATEGORY&quot; # Named Entity Recognition ner &lt;- ner(words_1, pos_words) ner ## [[1]] ## {(0, 3, &#39;PERSON&#39;, &#39;傅達仁&#39;), (23, 28, &#39;ORG&#39;, &#39;緯來體育台&#39;), (18, 22, &#39;DATE&#39;, &#39;20年前&#39;)} ## ## [[2]] ## {(0, 2, &#39;GPE&#39;, &#39;美國&#39;), (42, 45, &#39;ORG&#39;, &#39;參議院&#39;), (7, 9, &#39;DATE&#39;, &#39;今天&#39;), (56, 58, &#39;ORDINAL&#39;, &#39;第一&#39;), (2, 5, &#39;ORG&#39;, &#39;參議院&#39;), (17, 21, &#39;ORG&#39;, &#39;勞工部長&#39;), (60, 62, &#39;NORP&#39;, &#39;華裔&#39;), (21, 24, &#39;PERSON&#39;, &#39;趙小蘭&#39;), (11, 13, &#39;PERSON&#39;, &#39;布什&#39;)} ## ## [[3]] ## {(10, 13, &#39;PERSON&#39;, &#39;土地婆&#39;)} ## ## [[4]] ## set() ## ## [[5]] ## {(14, 18, &#39;CARDINAL&#39;, &#39;5.9萬&#39;), (4, 10, &#39;CARDINAL&#39;, &#39;59,000&#39;)} ## ## [[6]] ## {(14, 15, &#39;CARDINAL&#39;, &#39;3&#39;), (4, 6, &#39;CARDINAL&#39;, &#39;1,&#39;), (16, 17, &#39;CARDINAL&#39;, &#39;2&#39;), (12, 13, &#39;CARDINAL&#39;, &#39;1&#39;)} 9.10 Tidy Up the Results sent_corp &lt;- data.frame(id = mapply(rep, c(1:length(sents)), sapply(words_1, length)) %&gt;% unlist, words = do.call(c, words_1), pos = do.call(c, pos_words)) sent_corp Exercise 9.2 How to tidy up the results of ner so that we can include the recognized named entities in the same data frame sent_corp? "],
["structured-corpus.html", "Chapter 10 Structured Corpus 10.1 NCCU Spoken Mandarin 10.2 Connecting SPID to Metadata 10.3 More Socialinguistic Analyses", " Chapter 10 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for lingustic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. 10.1 NCCU Spoken Mandarin CHILDES format 10.1.1 Loading the Corpus NCCU &lt;- readtext(&quot;demo_data/NCCU_SPOKEN.tar.gz&quot;) %&gt;% as_tibble 10.1.2 Line Segmentation NCCU_lines &lt;- NCCU %&gt;% unnest_tokens(line, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) 10.1.3 Metadata vs. Transcript NCCU_lines_meta &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^@&quot;)) NCCU_lines_data &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^[^@]&quot;)) %&gt;% group_by(doc_id) %&gt;% mutate(lineID = row_number()) %&gt;% ungroup %&gt;% separate(line, into = c(&quot;SPID&quot;,&quot;line&quot;), sep=&quot;\\t&quot;) %&gt;% mutate(line2 = line %&gt;% str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) NCCU_lines_data 10.1.4 Word Tokenization NCCU_words &lt;- NCCU_lines_data %&gt;% unnest_tokens(word, line2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words 10.1.5 Word frequencies and Wordcloud NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% select(word, freq) %&gt;% #mutate(freq = log(freq)) %&gt;% wordcloud2::wordcloud2(minSize = 0.5, size=1, shape=&quot;diamonds&quot;) 10.1.6 Concordances # extracting particular patterns NCCU_lines_data %&gt;% filter(str_detect(line2, &quot;覺得&quot;)) 10.1.7 N-grams (Lexical Bundles) ########################## # Chinse ngrams functin # ########################## # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) NCCU_ngrams &lt;- NCCU_lines_data %&gt;% select(-line, -SPID) %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 5, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq 10.2 Connecting SPID to Metadata NCCU_lines_meta NCCU_lines_data # Self-defined function fill_spid &lt;- function(vec){ vec_filled &lt;-vec for(i in 1:length(vec_filled)){ if(vec_filled[i]==&quot;&quot;){ vec_filled[i]&lt;-vec_filled[i-1] }else{ i &lt;- i+1 } #endif }#endfor return(vec_filled) }#endfunc # Please check M005.cha NCCU_lines_data %&gt;% group_by(doc_id) %&gt;% filter(lineID == 1 &amp; SPID==&quot;&quot;) # Remove the typo case NCCU_lines_data_filled &lt;- NCCU_lines_data %&gt;% filter(!(doc_id ==&quot;M005.cha&quot; &amp; lineID==1)) %&gt;% group_by(doc_id) %&gt;% mutate(SPID = str_replace_all(SPID, &quot;[*:]&quot;,&quot;&quot;)) %&gt;% mutate(SPID_FILLED = fill_spid(SPID)) %&gt;% mutate(DOC_SPID = str_c(doc_id, SPID_FILLED, sep=&quot;_&quot;)) %&gt;% ungroup %&gt;% select(doc_id, lineID, line2, DOC_SPID) NCCU_lines_data_filled Based on the metadata of each file hedaer, we can extract demographic information related to each speaker, including their ID, age, gender, etc. NCCU_meta &lt;- NCCU_lines_meta %&gt;% filter(str_detect(line, &quot;^@(id)&quot;)) %&gt;% separate(line, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 10.3 More Socialinguistic Analyses 10.3.1 Check Ngram Distribution By Age Groups NCCU_ngram_with_meta &lt;- NCCU_lines_data_filled %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 3, delimiter = &quot;_&quot;)) %&gt;% filter(ngram!=&quot;&quot;) %&gt;% filter(!(str_detect(ngram,&quot;[&lt;a-z:]&quot;))) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_ngram_by_age &lt;- NCCU_ngram_with_meta %&gt;% count(ngram,AGE_GROUP, DOC_SPID) %&gt;% group_by(ngram, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) Below20 Word Cloud Order ggplot barplots by factor frequencies require(wordcloud2) NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(ngram,freq) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_age_ordered &lt;- NCCU_ngram_by_age %&gt;% group_by(AGE_GROUP) %&gt;% top_n(20, freq) %&gt;% ungroup() %&gt;% arrange(AGE_GROUP, freq) %&gt;% mutate(order=row_number()) # transparency indicates dispersion NCCU_ngram_by_age_ordered %&gt;% ggplot(aes(order, freq, fill = AGE_GROUP, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ AGE_GROUP, scales = &quot;free&quot;) + labs(y = &quot;Ngram Frequency&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_age_ordered$order, labels=NCCU_ngram_by_age_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) 10.3.2 Check Word Distribution of different genders NCCU_ngram_by_gender &lt;- NCCU_ngram_with_meta %&gt;% count(ngram, GENDER, DOC_SPID) %&gt;% group_by(ngram, GENDER) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_gender NCCU_ngram_by_gender %&gt;% #filter(dispersion &gt; 10) %&gt;% count(GENDER) %&gt;% ggplot(aes(x=GENDER, y = n, fill=GENDER))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;male&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;female&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_gender_ordered &lt;- NCCU_ngram_by_gender %&gt;% group_by(GENDER) %&gt;% top_n(20, dispersion) %&gt;% ungroup() %&gt;% arrange(GENDER,dispersion) %&gt;% mutate(order=row_number()) NCCU_ngram_by_gender_ordered %&gt;% ggplot(aes(order, dispersion, fill = GENDER, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ GENDER, scales = &quot;free&quot;) + labs(y = &quot;Ngram Dispersion&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_gender_ordered$order, labels=NCCU_ngram_by_gender_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) "],
["xml.html", "Chapter 11 XML 11.1 BNC Spoken 2014 11.2 Process the Whole Directory of BNC2014 Sample 11.3 Metadata 11.4 BNC2014 for Socialinguistic Variation", " Chapter 11 XML library(tidyverse) library(readtext) library(rvest) library(tidytext) library(quanteda) This chapter shows you how to process the recently released BNC 2014, which is by far the largest representative collection of spoken English collected in UK. For the purpose of our in-class tutorials, I have included a small sample of the BNC2014 in our demo_data. However, the whole dataset is now available via the official website: British National Corpus 2014. Please sign up for the complete access to the corpus if you need this corpus for your own research. 11.1 BNC Spoken 2014 XML is similar to HTML. Before you process the data, you need to understand the structure of the XML tags in the files. Other than that, the steps are pretty much similar to what we have done before. First, we read the XML using read_html(): # read one file at a time corp_bnc&lt;-read_html(&quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Now it is intuitive that our next step is to extract all utterances (with the tag of &lt;u&gt;...&lt;/u&gt;) in the XML file. So you may want to do the following: corp_bnc %&gt;% html_nodes(xpath = &quot;//u&quot;) %&gt;% html_text %&gt;% head ## [1] &quot;\\r\\nanhourlaterhopeshestaysdownratherlate&quot; ## [2] &quot;\\r\\nwellshehadthosetwohoursearlier&quot; ## [3] &quot;\\r\\nyeahIknowbutthat&#39;swhywe&#39;reanhourlateisn&#39;tit?mmI&#39;mtirednow&quot; ## [4] &quot;\\r\\n&quot; ## [5] &quot;\\r\\ndidyoutext--ANONnameM&quot; ## [6] &quot;\\r\\nyeahyeahhewrotebacknobotherlad&quot; See the problem? Using the above method, you lose the word boundary information from the corpus. What if you do the following? corp_bnc %&gt;% html_nodes(xpath = &quot;//w&quot;) %&gt;% html_text %&gt;% head(20) ## [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; ## [8] &quot;rather&quot; &quot;late&quot; &quot;well&quot; &quot;she&quot; &quot;had&quot; &quot;those&quot; &quot;two&quot; ## [15] &quot;hours&quot; &quot;earlier&quot; &quot;yeah&quot; &quot;I&quot; &quot;know&quot; &quot;but&quot; At the first sight, probably it seems that we have solved the problem but we don’t. There are even more problems created: Our second method does not extract non-word tokens within each utterance (e.g., &lt;pause .../&gt;, &lt;vocal .../&gt;) Our second method loses the utterance information (i.e., we don’t know which utterance each word belongs to) Exercise 11.1 Please come up with a way to extract both words and non-word tokens from each utterance. Ideally, the resulting data frame would consist of rows being the utterances, and columns recording the attributes of each autterances. Most importantly, the data frame should record not only the tokens of the utterance but at the same time the token-level attributes of each word/non-word token as well, e.g., the parts-of-speech, duration of pause etc. # xml_to_df function read_xml_bnc2014 &lt;- function(xmlfile){ # read one file at a time corp_bnc&lt;-read_html(xmlfile) # word/pause token nodes node_w &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;(//w)|(//pause)&quot;) # u nodes node_u &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;//u&quot;) node_u_id &lt;- node_u %&gt;% html_attr(name = &quot;n&quot;) node_u_who &lt;- node_u %&gt;% html_attr(name =&quot;who&quot;) node_u_trans &lt;- node_u %&gt;% html_attr(name=&quot;trans&quot;) node_u_whoConfidence &lt;- node_u %&gt;% html_attr(name=&quot;whoConfidence&quot;) # Define function # to extract word-based data frame from each u nodeset extractWords &lt;- function(u_node){ cur_w_nodeset &lt;- u_node %&gt;% html_children() corp_df &lt;- tibble( word = cur_w_nodeset %&gt;% html_text, lemma = cur_w_nodeset %&gt;% html_attr(name=&quot;lemma&quot;, default=&quot;@&quot;), pos = cur_w_nodeset %&gt;% html_attr(name=&quot;pos&quot;, default=&quot;@&quot;), usas = cur_w_nodeset %&gt;% html_attr(name = &quot;usas&quot;, default=&quot;@&quot;), dur = cur_w_nodeset %&gt;% html_attr(name = &quot;dur&quot;, default=&quot;@&quot;)) return(corp_df %&gt;% mutate(word = ifelse(word==&quot;&quot;,&quot;&lt;PAUSE&gt;&quot;,word))) }# endfunc extractWords(node_u[[1]]) -&gt;x x x$word[4]==&quot;&quot; # create tidy word-based df for current xml file corp_df &lt;- tibble( node_u_id, node_u_who, node_u_trans, node_w = map(node_u, extractWords) ) %&gt;% unnest_tokens(word, node_w, token = function(x) map(x, function(y) str_c(y$word, y$lemma, y$pos, y$usas, y$dur, sep=&quot;_&quot;))) %&gt;% mutate(xmlid = basename(xmlfile)) %&gt;% separate(word, into = c(&quot;word&quot;,&quot;lemma&quot;,&quot;pos&quot;,&quot;usas&quot;,&quot;dur&quot;), sep=&quot;_&quot;) corp_df } # endfunc 11.2 Process the Whole Directory of BNC2014 Sample 11.2.1 Define Function In Section 11.1, if you have figured how to extract utterances as well as token-based information from the xml file, you can easily wrap the whole procedure as one function. With this function, we can perform the same procedure to all the xml files of the BNC2014. For example, let’s assume that we have defined a function: read_xml_bnc2014 &lt;- function(xml){ ... } This function takes one xml file as an argument and return a data frame, consisting of utterances and other relevant information from the xml. read_xml_bnc2014(xmlfile = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Exercise 11.2 Now your job is to write this function, read_xml_BNC2014(). 11.2.2 Process the all files in the Directory Now we utilize the self-defined function, read_xml_BNC2014(), and process all xml files in the demo_data/corp-bnc-spoken2014-sample/. Also, we combine the data.frame returned from each xml into a bigger one, i.e., corp_bnc_df: s.t &lt;- Sys.time() bnc_flist &lt;- dir(&quot;demo_data/corp-bnc-spoken2014-sample/&quot;,full.names = T) corp_bnc_df &lt;- map(bnc_flist, function(x) read_xml_bnc2014(x)) %&gt;% do.call(rbind, .) Sys.time()-s.t ## Time difference of 1.184391 mins It takes about one and half minute to process the sample directory. You may store this corp_bnc_df data frame output for later use so that you don’t have to process the XML files every time you work on BNC2014. write_csv(corp_bnc_df, &quot;demo_data/corp_bnc_df.csv&quot;,col_names = T) 11.3 Metadata The best thing about BNC2014 is its rich demographic information relating to the settings and speakers of the conversations collected. The whole corpus comes with two metadata sets: bnc2014spoken-textdata.tsv: metadata for each text transcript bnc2014spoken-speakerdata.tsv: metadata for each speaker ID These two metadata sets allow us to get more information about each transcript as well as the speakers in those transcripts. 11.3.1 Text Metadata bnc_text_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-textdata.tsv&quot;, col_names = FALSE) bnc_text_meta_names &lt;-read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-text.txt&quot;, skip =2, col_names = F) names(bnc_text_meta) &lt;- c(&quot;textid&quot;, bnc_text_meta_names$X2) bnc_text_meta 11.3.2 Speaker Metadata bnc_sp_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-speakerdata.tsv&quot;, col_names = F) bnc_sp_meta_names &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-speaker.txt&quot;, skip = 3, col_names = F) names(bnc_sp_meta) &lt;- c(&quot;spid&quot;, bnc_sp_meta_names$X2) bnc_sp_meta 11.4 BNC2014 for Socialinguistic Variation BNC2014 was born for the study of socialinguistic variation. Here we show you some naitve examples, but you should get the ideas. 11.4.1 Word Frequency vs. Gender Now we are ready to explore the gender differences in language. 11.4.1.1 Preprocessing To begin with, there are some utterances with no words at all. We probably like to remove these tokens. #corp_bnc_df &lt;- read_csv(&quot;demo_data/corp_bnc_df.csv&quot;) corp_bnc_df &lt;- corp_bnc_df %&gt;% filter(!is.na(utterance)) corp_bnc_df 11.4.1.2 Target Structures Let’s assume that we like to know which verbs are most frequently used by men and women. corp_bnc_verb_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame corp_bnc_verb_gender ## Problems ### Use own tokenization function ### Default tokenization increase the number of tokens quite a bit word_by_gender &lt;- corp_bnc_verb_gender %&gt;% unnest_tokens(word, utterance, to_lower = F,token = function(x) strsplit(x, split = &quot;\\\\s&quot;)) %&gt;% # tokenize utterance into words filter(str_detect(word, &quot;[^_]+_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # include VERB only mutate(word = str_replace(word, &quot;_(JJ)|(JJR)|(JJT)&quot;,&quot;&quot;)) %&gt;% # remove pos tags #filter(!str_detect(word, &quot;^&#39;&quot;)) %&gt;% # remove contraction #filter(!word %in% stopwords(&quot;en&quot;)) %&gt;% count(gender, word) %&gt;% group_by(gender) %&gt;% top_n(200,n) %&gt;% ungroup Female wordcloud require(wordcloud2) word_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) Male wordcloud word_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) 11.4.2 Degree ADV + ADJ corp_bnc_pattern_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;[^_]+_RG [^_]+_JJ&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame pattern_by_gender &lt;- corp_bnc_pattern_gender %&gt;% unnest_tokens(pattern, utterance, to_lower = F, token = function(x) str_extract_all(x, &quot;[^_ ]+_RG [^_ ]+_JJ &quot;)) %&gt;% mutate(pattern = pattern %&gt;% str_trim %&gt;% str_replace_all(&quot;_[^_ ]+&quot;,&quot;&quot;)) %&gt;% # remove pos tags separate(pattern, into = c(&quot;ADV&quot;,&quot;ADJ&quot;), sep = &quot;\\\\s&quot;) %&gt;% count(gender, ADJ) %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup corp_bnc_pattern_gender %&gt;% #select(n, utterance) %&gt;% filter(n == 235) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) 11.4.3 Trigrams trigram_by_gender &lt;- corp_bnc_df %&gt;% mutate(utterance = utterance %&gt;% str_replace_all(&quot;_[^ ]+&quot;,&quot;&quot;)) %&gt;% unnest_tokens(trigram, utterance, token = &quot;ngrams&quot;, n =3, ngram_delim = &quot;_&quot;) %&gt;% filter(!is.na(trigram)) %&gt;% left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) %&gt;% count(gender, trigram) %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup Exercise 11.3 Remove stopwords from the frequency list of words (unigrams). Exercise 11.4 Include dispersion metrics in the n-gram freuency list. trigram_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(trigram, n) %&gt;% wordcloud2(minSize = 0.5, size = 3, rotateRatio = 0.3) trigram_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(trigram, n) %&gt;% wordcloud2(minSize = 0.5, size = 1.5, rotateRatio = 0.8) "],
["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Data Processing Flowchart 12.2 Document-Feature Matrix (dfm) 12.3 Defining Feature in dfm 12.4 Feature Selection 12.5 Applying DFM", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) 12.1 Data Processing Flowchart 12.2 Document-Feature Matrix (dfm) Two ways to create Dcument-Feature-Matrix: create dfm based on an corpus object create dfm based on an token object For English data, quanteda can take care of the word tokenization fairly well so you can create dfm directly from corpus. However, for Chinese data, it is suggested to create your own corpus token object first, and then feed it to dfm() to create dfm for your corpus. In this section, we demonstrate the document-feature-matrix using the English data we discussed in Chapter ??, the data_corpus_inaugural preloaded in the library quanteda. corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% dfm For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) ## [1] &quot;corpus&quot; 12.3 Defining Feature in dfm What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the following example, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and corp_us_dfm[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] Distributional properties like co-occurrences are very important information in corpus linguistics. Most of the studies in corpus linguistics adopt an implicit distributional hypothesis, which can be illustrated by a few famous quotes: You shall know a word by the comany it keeps. (Firth, 1957, p.11) [D]ifference of meaning correlates with difference of distribution. (Harris, 1970, p.785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves (De Deyne et al. 2016) So in our current context, the idea is that if two documents have similar sets of linguistic units popping up in them, they are more likely to be similar in meaning as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with other documents (i.e., other rows). This is essentially a vector computation (cf. Figure 12.1): the document in each row is represented as a vector of N dimensional space. The size of N depends on the number of linguistic units that are included in the analysis. Figure 12.1: Example of Document-Feature Matrix 12.4 Feature Selection A dfm may be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered when creating a dfm: The granularity of the linguistic unit Stopwords The distributional cut-offs of the linguistic unit 12.4.1 Determining Linguistic Granularity In our previous example, we include only words, i.e., unigrams, as our features in the dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: corp_us_dfm_ngram &lt;- corp_us %&gt;% dfm(ngrams = 2) corp_us_dfm_ngram[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: corp_us_dfm_stem &lt;- corp_us %&gt;% dfm(stem = T) corp_us_dfm_stem[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (38.0% sparse) and 4 docvars. ## features ## docs fellow-citizen of the senat and hous repres : among ## 1789-Washington 1 71 116 1 48 2 2 1 1 ## 1793-Washington 0 11 13 0 2 0 0 1 0 ## 1797-Adams 3 140 163 1 130 3 3 0 4 ## 1801-Jefferson 2 104 130 0 81 0 1 1 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 7 ## 1809-Madison 1 69 104 0 43 0 1 0 0 ## features ## docs vicissitud ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 1 ## [ reached max_ndoc ... 4 more documents ] You need to decide which type of linguistic units is more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, there is no rule for how to do this. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 12.4.2 Stopwords There are words that are not so informative in telling us the similarity and difference between the documents because they almost occur in every document of the corpus, but carray little (refential) semantic contents. These words are usually the function words, such as and, the, of. Also, there are tokens that usually carry limited semantic contents, such as numbers and punctuation. Therefore, it is not uncommon that analysts sometimes create a list of words to be removed from the dfm. These words are referred to as stopwords. The library quanteda has determined a default English stopword list, i.e., stopwords(&quot;en&quot;). When creating the dfm object, we can further specify a few parameters: remove_punct: remove all punctuation tokens remove: remove all words specified in the character vector here corp_us_dfm_stp &lt;- corp_us %&gt;% dfm(remove_punct = T, remove = stopwords(&quot;en&quot;)) corp_us_dfm_stp[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (60.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among ## 1789-Washington 1 1 2 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 ## 1801-Jefferson 2 0 0 0 1 ## 1805-Jefferson 0 0 0 0 7 ## 1809-Madison 1 0 0 0 0 ## features ## docs vicissitudes incident life event filled ## 1789-Washington 1 1 1 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 0 0 2 0 0 ## 1801-Jefferson 0 0 1 0 0 ## 1805-Jefferson 0 0 2 0 0 ## 1809-Madison 0 0 1 0 1 ## [ reached max_ndoc ... 4 more documents ] We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm) ## [1] 9399 nfeat(corp_us_dfm_ngram) ## [1] 9399 nfeat(corp_us_dfm_stem) ## [1] 5584 nfeat(corp_us_dfm_stp) ## [1] 9248 12.4.3 Distributional Cut-offs for Features Depending on the granularity of the linguistic units you consider, you may get a considerable number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the word occurs only once in the corpus (i.e., hapax legomenon), these words can be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurrs in all documents, they won’t help much as well. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate something else. Therefore, sometimes we can control the document frequency of the features (i.e., in how many different texts does the feature occur?) Other self-defined weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a text d, the significance of this n may be connected to: the document size of d the total number of w Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. In the following demo, we adopt a few simple distrubtional criteria: we remove stopwords and punctuations we remove words whose freqency &lt; 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% dfm(remove = stopwords(&quot;en&quot;), remove_punct = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) corp_us_dfm_trimmed[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (52.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among life event ## 1789-Washington 1 1 2 2 1 1 2 ## 1793-Washington 0 0 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 2 0 ## 1801-Jefferson 2 0 0 0 1 1 0 ## 1805-Jefferson 0 0 0 0 7 2 0 ## 1809-Madison 1 0 0 0 0 1 0 ## features ## docs greater order received ## 1789-Washington 1 2 1 ## 1793-Washington 0 0 0 ## 1797-Adams 0 4 0 ## 1801-Jefferson 1 1 0 ## 1805-Jefferson 0 3 0 ## 1809-Madison 0 0 0 ## [ reached max_ndoc ... 4 more documents ] 12.5 Applying DFM 12.5.1 Wordcloud With a dfm of a corpus, we can quickly explore the nature of this corpus by examining the top features of this corpus: topfeatures(corp_us_dfm_trimmed) ## people government us can upon must great ## 574 564 478 471 371 366 340 ## may states shall ## 338 333 314 Or we can visualize the distrubtion of these top features using the wordcloud: set.seed(123) corp_us_dfm_trimmed %&gt;% textplot_wordcloud(min_count = 60, rotation = .35) 12.5.2 Document Similarity As shown in 12.1, with the N-dimensional vector representation of each document, we can easily compute the mathematical similarities between two documents. Based on the similarities, we can further examine how different documents may cluster together in terms of their lexical similarities. corp_us_dist &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist() corp_us_hist &lt;- corp_us_dist %&gt;% as.dist %&gt;% hclust plot(corp_us_hist,hang = -1, cex = 0.7) 12.5.3 Feature Similarity What if we transpose a document-feature matrix? A transposed dfm would be a feature-document matrix. This is an interesting structure because we then can do the same tricks with all the features in the corpus. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent words are similar. # convert `dfm` to `fcm` corp_us_fcm &lt;- corp_us_dfm_trimmed %&gt;% fcm # select top 30 features corp_us_topfeatures &lt;- names(topfeatures(corp_us_fcm, 30)) # plot network fcm_select(corp_us_fcm, pattern = corp_us_topfeatures) %&gt;% textplot_network(min_freq = 0.5) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. Exercise 12.2 Please create a network of the top 30 bigrams based on the corpus corp_us. The criteria for bigrams selection are as follows: Include bigrams that consist of alphanumeric characters only (no punctuations) Include bigrams whose frequency &gt;= 10 and docfreq &gt;= 5 but &lt;= half number of the corpus "],
["vector-space-representation-ii.html", "Chapter 13 Vector Space Representation II 13.1 A Quick View 13.2 Loading the Corpus 13.3 Semgentation 13.4 Corpus Metadata 13.5 Document-Feature Matrix 13.6 Wordcloud 13.7 Document Similarity 13.8 Feature Similarity", " Chapter 13 Vector Space Representation II library(tidyverse) library(quanteda) library(readtext) library(jiebaR) Figure 13.1: Corpus Processing Flowchart In Chapter 12, we have demonstrated the potential of a vector representation of documents with the English data. Here, we would like to look at the Chinese data in more detail. In the corpus data processing flowchart, as repeated below (Figure 13.1), we need to deal with the word segmentation with the Chinese data. This prevents us from creating a dfm directly from a corpus object because the default internal word tokenization in quanteda is not optimized for non-English languages. In this chapter, we will be using the dataset TaiwanPresidentalSpeech.zip in our demo_data directory. Please make sure that you have downloaded the dataset from demo_data. 13.1 A Quick View For Chinese data, the major preprocessing steps have been highlighted in Figure 13.1: First read in the corpus using readtext() and create corpus data.frame object Subset the text column of the corpus data.frame for word segmentation Tokenize the texts using segment() in jiebaRand convert the output into a token object using as.token(). A token object is properly defined in quanteda, with many similar functions as a corpus object. This is the most important trick with the Chinese data. When you utilize many different libaries in R for your tasks, one thing you need to keep in your mind is that you need to fully understand what kind of objects you are dealing with. That is, you need to keep track of every variable you create in your script in terms of their object type/class. A vector is different from a list; a list is different from a token. Also, some of the object classes are predefined in R (e.g., vector, data.frame) while others are defined in specific libararies (e.g., corpus, token, dfm). As a habit, always check your object class (i.e., class()). 13.2 Loading the Corpus corp_tw &lt;- readtext(file=&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% as_tibble class(corp_tw) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; corp_tw %&gt;% mutate(text = str_sub(text, 1,20)) NB: readtext() creates a readtext or data.frame object. Following the tidy principle, we convert everything into tibble. 13.3 Semgentation Three important sub-steps in this part: initialize word segmenter, where a user dictionary is defined (Always use own dictionary to improve the performance of word segmentation) subset the text column of corp_tw tokenize the texts and convert the output into a quanteda-compatible object, token # initialize segmenter chi_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user.txt&quot;) corp_tw_tokens &lt;- corp_tw$text %&gt;% segment(jiebar = chi_seg) %&gt;% as.tokens class(corp_tw_tokens) ## [1] &quot;tokens&quot; corp_tw_tokens$text14[1:10] ## NULL 13.4 Corpus Metadata When we subset the texts from corp_tw for word segmentation, all the metadata connected to the texts did not go with the texts. So the corp_tw_tokens did not have any metadata information. All you have is some arbitrary index to each text. docvars(corp_tw_tokens) So here we extract metadata information from the original filenames of each text stored in the corp_tw, and attach this metadata to corp_tw_tokens. corp_tw_meta &lt;-corp_tw %&gt;% dplyr::select(-text) %&gt;% separate(doc_id, into = c(&quot;YEAR&quot;,&quot;TERM&quot;,&quot;PRESIDENT&quot;),sep = &quot;_&quot;) %&gt;% mutate(PRESIDENT = str_replace(PRESIDENT, &quot;.txt&quot;,&quot;&quot;)) corp_tw_meta docvars(corp_tw_tokens) &lt;- corp_tw_meta 13.5 Document-Feature Matrix Now that we have a token version of our corpus, we can create dfm using the dfm() in quanteda. Also, we can take care of the feature selection (cf. 12.4) using functions like dfm_trim(), dfm_select(). corp_tw_dfm &lt;- corp_tw_tokens %&gt;% dfm(reove_punc = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 2, max_docfreq = 10, docfreq_type = &quot;count&quot;) class(corp_tw_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; corp_tw_dfm[1:5, 1:10] ## Document-feature matrix of: 5 documents, 10 features (32.0% sparse) and 3 docvars. ## features ## docs 中正 國民大會 國父 環境 以及 到 以來 知道 自己 此 ## text1 5 1 3 2 2 1 1 2 1 2 ## text2 4 4 1 1 2 2 0 3 0 0 ## text3 5 1 1 0 0 5 0 4 4 0 ## text4 7 3 7 0 0 0 1 1 2 1 ## text5 5 1 1 0 0 0 0 0 0 3 13.6 Wordcloud require(wordcloud2) require(wordcloud) top_features &lt;- corp_tw_dfm %&gt;% topfeatures(100) word_freq &lt;- data.frame(word = names(top_features), freq = top_features) word_freq %&gt;% wordcloud2() 13.7 Document Similarity corp_tw_dist &lt;- corp_tw_dfm %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist corp_tw_hist &lt;- corp_tw_dist %&gt;% as.dist %&gt;% hclust hist_labels &lt;- str_c(docvars(corp_tw_dfm,&quot;YEAR&quot;), docvars(corp_tw_dfm,&quot;PRESIDENT&quot;), sep=&quot;_&quot;) plot(corp_tw_hist, hang = -1, cex = 1.2, label = hist_labels) 13.8 Feature Similarity # convert `dfm` to `fcm` corp_tw_fcm &lt;- corp_tw_dfm %&gt;% fcm # select top 30 features corp_tw_topfeatures &lt;- names(topfeatures(corp_tw_fcm, 50)) # plot network library(showtext) font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) #par(family = &quot;Arial Unicode MS&quot;) fcm_select(corp_tw_fcm, pattern = corp_tw_topfeatures) %&gt;% textplot_network(min_freq = 0.5) -&gt;g ggsave(&quot;test.png&quot;, g) Exercise 13.1 Create the network of top 30 bigrams for the corpus corp_tw. The critera for bigrams selection are as follows: include bigrams whose frequency &gt;= 10 and docfreq &gt;=5 Exercise 13.2 There is an interesting application. When we analyze the document similarity, we create the graph of a dendrogram using hierarchical cluster analysis. In fact, document relations can also be represented by a network as well, as we do with the features in 12.5.3. How could you make use of the function textplot_network() in quanteda to create a network of the presidents? Please create a similar president network as shown below. "],
["vector-space-representation-iii.html", "Chapter 14 Vector Space Representation III 14.1 Library 14.2 Text Collection 14.3 Tokenization and Vocabulary 14.4 Pruning 14.5 Term-Cooccurrence Matrix 14.6 Fitting Model 14.7 Averaging Word Vectors 14.8 Semantic Space 14.9 Visualizing Multi-dimensional Space", " Chapter 14 Vector Space Representation III In this chapter, we will discuss the idea of representing the semantic space of words via unsupervised learning. The word vectors learned from the large collection of text data are referred to as word embeddings. This tutorial is based on Dmitriy Selivanov’s website. 14.1 Library library(text2vec) 14.2 Text Collection library(text2vec) wikitext &lt;- &quot;demo_data/corp-wikipedia&quot; wiki &lt;- readLines(wikitext, n = 1, warn = FALSE) substr(wiki, 1, 400) ## [1] &quot; anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word a&quot; 14.3 Tokenization and Vocabulary # Create iterator over tokens tokens &lt;- space_tokenizer(wiki) # Create vocabulary. Terms will be unigrams (simple words). it &lt;- itoken(tokens, progressbar = FALSE) vocab &lt;- create_vocabulary(it) 14.4 Pruning vocab &lt;- prune_vocabulary(vocab, term_count_min = 5L) 14.5 Term-Cooccurrence Matrix # Use our filtered vocabulary vectorizer &lt;- vocab_vectorizer(vocab) # use window of 5 for context words tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L) 14.6 Fitting Model glove &lt;- GlobalVectors$new(rank = 50, x_max = 10) wv_main &lt;- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8) # main words vectors dim(wv_main) wv_context &lt;- glove$components dim(wv_context) 14.7 Averaging Word Vectors word_vectors &lt;- wv_main + t(wv_context) 14.8 Semantic Space berlin = word_vectors[&quot;paris&quot;, , drop = FALSE] - word_vectors[&quot;france&quot;, , drop = FALSE] + word_vectors[&quot;germany&quot;, , drop = FALSE] cos_sim = sim2(x = word_vectors, y = berlin, method = &quot;cosine&quot;, norm = &quot;l2&quot;) head(sort(cos_sim[,1], decreasing = TRUE), 5) ## berlin paris germany munich leipzig ## 0.7791436 0.7701525 0.6789727 0.6365748 0.6265751 14.9 Visualizing Multi-dimensional Space Please read How to Use t-SNE Effectively for more information. # TSNE library(Rtsne) library(dplyr) library(ggplot2) Visualizae the semantic distances of the top 100 content words in corpus Here I demonstrate how to visualize the semantic distances of the 2000 words included in the General Servise Word List using the word embeddings. The procedures are described as follows: We load the Google Analogy Dataset from the web. We clean up the data and retrieve a random sample set for visualization We use t-SNE for multidimensional scaling. We obtain two-dimensional cordinates from t-SNE for visualization require(stringr) google_analogy &lt;- readLines(&quot;http://download.tensorflow.org/data/questions-words.txt&quot;) google_analogy[1:10] ## [1] &quot;: capital-common-countries&quot; &quot;Athens Greece Baghdad Iraq&quot; ## [3] &quot;Athens Greece Bangkok Thailand&quot; &quot;Athens Greece Beijing China&quot; ## [5] &quot;Athens Greece Berlin Germany&quot; &quot;Athens Greece Bern Switzerland&quot; ## [7] &quot;Athens Greece Cairo Egypt&quot; &quot;Athens Greece Canberra Australia&quot; ## [9] &quot;Athens Greece Hanoi Vietnam&quot; &quot;Athens Greece Havana Cuba&quot; google_analogy[str_starts(google_analogy,&quot;\\\\:&quot;)] ## [1] &quot;: capital-common-countries&quot; &quot;: capital-world&quot; ## [3] &quot;: currency&quot; &quot;: city-in-state&quot; ## [5] &quot;: family&quot; &quot;: gram1-adjective-to-adverb&quot; ## [7] &quot;: gram2-opposite&quot; &quot;: gram3-comparative&quot; ## [9] &quot;: gram4-superlative&quot; &quot;: gram5-present-participle&quot; ## [11] &quot;: gram6-nationality-adjective&quot; &quot;: gram7-past-tense&quot; ## [13] &quot;: gram8-plural&quot; &quot;: gram9-plural-verbs&quot; google_analogy_df &lt;- data.frame(analogy = str_to_lower(google_analogy)) %&gt;% filter(str_detect(analogy,&quot;^[^\\\\:]&quot;)) %&gt;% tidyr::separate(analogy, into = c(&quot;w1&quot;,&quot;w2&quot;,&quot;w3&quot;,&quot;w4&quot;)) # random sample set.seed(12) words &lt;- google_analogy_df %&gt;% sample_n(20,replace = F) %&gt;% unlist %&gt;% as.vector # Include only seen cases words_seen &lt;-words[words %in% row.names(word_vectors)] %&gt;% unique tsne &lt;- Rtsne(word_vectors[words_seen,], perplexity = (length(words_seen)-1)/3 , pca = FALSE) # tsne_df &lt;- tsne$Y %&gt;% as.data.frame() %&gt;% mutate(word = words_seen) tsne_df tsne_df %&gt;% ggplot(aes(x = V1, y = V2, label = word)) + geom_point(size = 0.7, alpha = 0.7) + geom_text(size = 3, family = &quot;Arial Unicode MS&quot;, vjust = 1.4, angle = 0) + # #scale_x_continuous(expand= expansion(add=10)) + #scale_y_continuous(expand= expansion(add=50)) + scale_color_discrete(guide=F) + theme_bw()-&gt; tsne_plot tsne_plot An overview of tasks in Chinese NLP: See Chinese NLP Exercise 14.1 Use the word_vectors trained in this chapter and visualize the semantic distances of the words included in country below with the t-SNE multi-dimensional scaling techique. (Parameters: perplexity = 2.5) country &lt;- c(&quot;germany&quot;, &quot;berlin&quot;, &quot;france&quot;, &quot;paris&quot;, &quot;china&quot;, &quot;beijing&quot;, &quot;taiwan&quot;,&quot;taipei&quot;, &quot;england&quot;,&quot;london&quot;, &quot;netherlands&quot;,&quot;amsterdam&quot;) "],
["references.html", "Chapter 15 References", " Chapter 15 References "]
]
