[["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Distributional Semantics 12.2 Vector Space Model for Documents 12.3 Vector Space Semantics: Parameters 12.4 Vector Space Model for Words (Self-Study) 12.5 Exercises", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) In this chapter, I would like to talk about the idea of distributional semantics, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized computationally and quantified using the distributional data of the linguistic units in the corpus. Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts. 12.1 Distributional Semantics Distributional approach to semantics was first formulated by John Firth in his famous quotation: You shall know a word by the company it keeps (Firth, 1957, p. 11). In other words, words that occur in the same contexts tend to have similar meanings (Z. Harris, 1954). [D]ifference of meaning correlates with difference of distribution. (Z. S. Harris, 1970, p. 785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves. (De Deyne et al., 2016) In computational linguistics, this idea has been implemented in the modeling of lexical semantics and documents topics. The lexical meanings of words or topics of documents can be computationally represented by the distributional information of their co-occurring words. VSR For Words (Lexcial Semantics) On the one hand, one can extract the distributional information of target words automatically from large corpora, which are referred to as the contextual features of the target words. These co-occurrence frequencies (raw or weigthed) between target words and contextual features can be combined in long vectors, which can be utilized to computationally measure the lexical semantic distance or similarity. VSR For Documents (Document Semantics/Topics) On the other hand, this distributional model can be applied to the semantic representation of documents in corpus as well. One can extract the distributional information of target documents automatically from large corpora, i.e., their contextual features. The co-occurrence frequencies between target documents and contextual features can also be combined in long vectors, which can also be utilized to computationally measure the document similarity/difference. Therefore, this distributional approach to meanings is sometimes referred to as Vector Space Semantics. 12.2 Vector Space Model for Documents Now I would like to demonstrate how we can adopt this vector space model to study the semantics of documents. 12.2.1 Data Processing Flowchart In Chapter 5, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of the corpus documents. Most importantly, a new object class is introduced in Figure 12.1, i.e., the dfm object in quanteda. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterize the documents. The cells in the matrix are the co-occurrence statistics between each document and the feature. Different ways of operationalizing the features and the cell values may lead to different types of dfm. In this section, I would like to show you how to create dfm of a corpus and what are the common ways to define features and cell valus for the analysis of document semantics via vector space representation. Figure 12.1: English Text Analytics Flowchart (v2) 12.2.2 Document-Feature Matrix (dfm) To create a dfm, i.e., Dcument-Feature-Matrix, of your corpus data, there are generally three steps: Create an corpus object of your data; Tokenize the corpus object into a tokens object; Create the dfm object based on the tokens object For English data, quanteda can take care of the word tokenization fairly well, so you can create dfm directly from corpus (See Figure 12.1 above) In the chapter, Chinese Text Processing, we stress that the default tokenization method in quanteda with Chinese data may be limited in several ways. In order to create a dfm that takes into account the appropriateness of the Chinese word segmentation, I would highly recommend you to first create a tokens object using the self-defined jiebaR word segmentation methods, and then feed it to dfm() to create the dfm for your corpus. In this way, the dfm will use the segmented results defined by your word segmenter. In other words, with Chinese data, probably it is not really necessary to have a corpus object; rather, a tokens object of the corpus might be more useful/practical. (In quanteda, most of the functions for corpus can be applied to tokens as well, e.g., kwic(), dfm()) In this tutorial, I will use the same English dataset as we discussed in Chapter 5, the data_corpus_inaugural, preloaded in the package quanteda. For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). ## `corpus` corp_us &lt;- data_corpus_inaugural ## `tokens` corp_us_tokens &lt;- tokens(corp_us) ## `dfm` corp_us_dfm &lt;- dfm(corp_us_tokens) ## check dfm corp_us_dfm Document-feature matrix of: 59 documents, 9,439 features (91.84% sparse) and 4 docvars. features docs fellow-citizens of the senate and house representatives : 1789-Washington 1 71 116 1 48 2 2 1 1793-Washington 0 11 13 0 2 0 0 1 1797-Adams 3 140 163 1 130 0 2 0 1801-Jefferson 2 104 130 0 81 0 0 1 1805-Jefferson 0 101 143 0 93 0 0 0 1809-Madison 1 69 104 0 43 0 0 0 features docs among vicissitudes 1789-Washington 1 1 1793-Washington 0 0 1797-Adams 4 0 1801-Jefferson 1 0 1805-Jefferson 7 0 1809-Madison 0 0 [ reached max_ndoc ... 53 more documents, reached max_nfeat ... 9,429 more features ] ## Check object class class(data_corpus_inaugural) [1] &quot;corpus&quot; &quot;character&quot; class(corp_us) [1] &quot;corpus&quot; &quot;character&quot; class(corp_us_dfm) [1] &quot;dfm&quot; attr(,&quot;package&quot;) [1] &quot;quanteda&quot; 12.2.3 Intuition for DFM What is dfm anyway? A document-feature-matrix is no different from a spread-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s) (i.e., the contextual features in the vector space model). If the contextual feature is a word, then this dfm would be a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the contextual feature is an n-gram, then this dfm would be a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. That is, the contextual feature could be any self-defined linguistic unit. What about the values in the cells of dfm? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the contextual feature (i.e., column) in a particular document (i.e., row). For example, in the corpus data_corpus_inaugural, based on the corp_us_dfm created earlier, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and. By default, the dfm() would create a document-by-word matrix, i.e., generating words as the contextual features. A dfm with words as the contextual features is the simplest way to characterize the documents in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each document as bags of words. That is, how the words are arranged relative to each other is ignored (i.e., the morpho-syntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word dfm should be the most naive characterization of the texts. In many computational tasks, however, it turns out that this simple bag-of-words model is very effective in modeling the semantics of the documents. A document-by-ngram matrix can be more informative because the contextual features take into account (partial &amp; limited) sequential information between words. In quanteda, to create an ngram-based dfm: Create an ngram-based tokens using tokens_ngrams() first; Create an ngram-based dfm based on the ngram-based tokens; ## create ngrams tokens corp_us_ngrams &lt;- tokens_ngrams(corp_us_tokens, n = 2:3) ## create ngram-based dfm corp_us_ngrams_dfm &lt;- dfm(corp_us_ngrams) 12.2.4 Distance/Similarity Metrics The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with the other documents (i.e., the other rows). The idea is that if two documents co-occur with similar sets of contextual features, they are more likely to be similar in their semantics as well. Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically: Figure 12.2: Vector Representation In Math, there are in general two types of metrics to measure the relationship between vectors: distance-based vs. similarity-based metrics. Distance-based Metrics Many distance measures of vectors are based on the following formula and differ in individual parameter settings. \\[\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^y}\\big)^{\\frac{1}{y}}\\] The n in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.) When y is set to 2, it computes the famous Euclidean distance of two vectors, i.e., the direct spatial distance between two points on the n-dimensional space. \\[\\sqrt{\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^2}\\big)}\\] ## Create vectors x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) ## computing pairwise euclidean distance sum(abs(x-y)^2)^(1/2) # XY distance [1] 6 sum(abs(y-z)^2)^(1/2) # YZ distnace [1] 4.472136 sum(abs(x-z)^2)^(1/2) # XZ distnace [1] 8.944272 The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure 12.3) Figure 12.3: Distance-based Metric: Euclidean Distance Similarity-based Metrics In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is Cosine Similarity, which can be computed as follows: \\[cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\] # comuting pairwise cosine similarity sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2))) # xy [1] 0.9778024 sum(y*z)/(sqrt(sum(y^2))*sqrt(sum(z^2))) # yz [1] 0.4961389 sum(x*z)/(sqrt(sum(x^2))*sqrt(sum(z^2))) # xz [1] 0.3032037 The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the greater their cosine similarity, the smaller the arcs, the closer they are. Therefore, it is clear to see that cosine similarity highlights the documents similarities in terms of whether their values on all the dimensions (contextual features) vary in the same directions. However, distance-based metrics would highlight the document similarities in terms of how much their values on all the dimensions differ. Computing pairwise distance/similarity using quanteda In quanteda.textstats library, there are many functions that can help us compute pairwise similarities/distances between vectors using two useful functions: textstat_simil(): similarity-based metrics textstat_dist(): distance-based metrics The expected input argument of these two functions is a dfm: ## Create a simple DFM xyz_dfm &lt;- as.dfm(matrix(c(1,9,1,3,5,1), byrow=T, ncol=2)) xyz_dfm Document-feature matrix of: 3 documents, 2 features (0.00% sparse) and 0 docvars. features docs feat1 feat2 text1 1 9 text2 1 3 text3 5 1 ## Computing cosine similarity textstat_simil(xyz_dfm, method=&quot;cosine&quot;) textstat_simil object; method = &quot;cosine&quot; text1 text2 text3 text1 1.000 0.978 0.303 text2 0.978 1.000 0.496 text3 0.303 0.496 1.000 ## Computing euclidean distance textstat_dist(xyz_dfm, method = &quot;euclidean&quot;) textstat_dist object; method = &quot;euclidean&quot; text1 text2 text3 text1 0 6.00 8.94 text2 6.00 0 4.47 text3 8.94 4.47 0 There are other useful functions in R that can compute the pairwise distance/similarity metrics on a matrix. When using these functions, please pay attention to whether they provide distance or similarity metrics because these two are very different in meanings. For example, amap::Dist() provides cosine-based distance, not similarity. ## Compute cosine distance with amap::Dist() amap::Dist(xyz_dfm, method = &quot;pearson&quot;) text1 text2 text2 0.02219759 text3 0.69679634 0.50386106 ## Compute cosine similarity with amap::Dist() (1- amap::Dist(xyz_dfm, method=&quot;pearson&quot;)) text1 text2 text2 0.9778024 text3 0.3032037 0.4961389 Interim Summary The Euclidean Distance metric is a distance-based metric: the larger the value, the more distant the two vectors. The Cosine Similarity metric is a similarity-based metric: the larger the value, the closer the two vectors. Based on our computations of the metrics for the three vectors, now in terms of the Euclidean Distance, y and z are closer; in terms of Cosine Similarity, x and y are closer. Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the documents/words: The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors) The relative increase/decrease of the values on ecah dimension (i.e., the curvatures of vectors) There are many other distance-based or similarity-based metrics available. For more detail, please see Manning &amp; Schütze (1999) Ch15.2.2. and Jurafsky &amp; Martin (2020) Ch6: Vector Semantics and Embeddings. 12.2.5 Multidimensiona Space Back to our example of corp_us_dfm, it is essentially the same vector representation, but in a multidimensional space (cf. Figure 12.4). The document in each row is represented as a vector of N dimensional space. The size of N depends on the number of contextual features that are included in the analysis of the dfm. Figure 12.4: Example of Document-Feature Matrix 12.2.6 Feature Selection A dfm may not be as informative as we have expected. To better capture the document’s semantics/topics, there are several important factors that need to be more carefully considered with respect to the contextual features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features Granularity In our previous example, we include only words, i.e., unigrams, as our contextual features in the corp_us_dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: from corpus to tokens from tokens to ngram-based tokens from ngram-based tokens to dfm ## Create DFM based on bigrams corp_us_dfm_bigram &lt;- dfm(tokens_ngrams(corp_us_tokens, n =2)) Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: ## Create DFM based on word stems corp_us_dfm_unigram_stem &lt;- dfm_wordstem(corp_us_dfm) We can of course create DFM based on stemmed bigrams: ## Create DFM based on stemmed bigrams corp_us_dfm_bigram_stem &lt;- corp_us_tokens %&gt;% ## tokens tokens_ngrams(n = 2) %&gt;% ## bigram tokens dfm() %&gt;% ## dfm dfm_wordstem() ## stem You need to decide which types of contextual features are more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, these are only heuristics, not rules. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) Informativeness There are words that are not so informative in telling us the similarity and difference between the documents because they almost appear in every document of the corpus, but carray little (referential) semantic contents. These words are usually function words, such as and, the, of. These common words observed in almost all documents are often referred to as stopwords. Therefore, it is not uncommon that analysts sometimes create a list of stopwords to be removed from the dfm. The library quanteda has defined a default English stopword list, i.e., stopwords(\"en\"). ## English stopword stopwords(&quot;en&quot;) %&gt;% head(50) [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; length(stopwords(&quot;en&quot;)) [1] 175 Also, there are tokens that usually carry very limited semantic contents, such as numbers and punctuation. Numbers, symbols and punctuation marks are often treated differently in computational text analytics. When creating the tokens object, we can further specify a few parameters for the function tokens() to remove unimportant non-word tokens: remove_punct = TRUE: remove all characters in the Unicode “Punctuation” [P] class remove_symbols = TRUE: remove all characters in the Unicode “Symbol” [S] class remove_url = TRUE: find and eliminate URLs beginning with http(s) remove_separators = TRUE: remove separators and separator characters (Unicode “Separator” [Z] and “Control” [C] categories) To remove stopwords from the dfm, we can use dfm_remove() on the dfm object. ## Create new DFM ## w/o non-word tokens and stopwords corp_us_dfm_unigram_stop_punct &lt;- corp_us %&gt;% tokens(remove_punct = TRUE, ## remove non-word tokens remove_symbols = TRUE, remove_numbers = TRUE, remove_url = TRUE) %&gt;% dfm() %&gt;% ## Create DFM dfm_remove(stopwords(&quot;en&quot;)) ## Remove stopwords from DFM We can see that the number of features drops significantly after we remove stopwords: ## Changes of Contextual Feature Numbers nfeat(corp_us_dfm) ## default unigram version [1] 9439 nfeat(corp_us_dfm_unigram_stem) ## unigram + stem [1] 5596 nfeat(corp_us_dfm_bigram) ## bigram [1] 64442 nfeat(corp_us_dfm_bigram_stem) ## bigram + stem [1] 58045 nfeat(corp_us_dfm_unigram_stop_punct) ## unigram removing non-words/puncs [1] 9212 Distributional Properties Depending on the granularity of the contextual features you are considering, you may get a considerably large number (e.g., thousands of ngrams) of contextual features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the contextual word occurs only once in the corpus (i.e., hapax legomenon), these words may be highly idiosyncratic, which can be of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurs very frequently, they may be a function word, carrying little semantic content. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate that this feature is too domain-specific. Therefore, sometimes we can control the document frequency of the contextual features (i.e., in how many different texts does the feature occur?) Other Self-defined Weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a document d, the significance of this n may be connected to: the document size of d the total number of w We can therefore utilize association-based metrics as weighted versions of the co-occurrence frequencies (e.g., PMI, LLR etc.) Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. Exercise 12.2 Please get familar with the following functions, provided by quanteda for weighting of the document-feature matrix: dfm_weight(), dfm_tfidf(). In the following demo, we adopt a few simple distrubtional criteria: we create a simple unigram dfm based on the word-forms we remove stopwords, punctuation marks, numbers, and symbols we remove contextual words whose freqency &lt; 10, docfreq &lt; 3, docfreq == ndoc(CORPUS) ## Create trimmed DFM corp_us_dfm_trimmed &lt;- corp_us %&gt;% ## corpus tokens( ## tokens remove_punct = T, remove_numbers = T, remove_symbols = T ) %&gt;% dfm() %&gt;% ## dfm dfm_remove(stopwords(&quot;en&quot;)) %&gt;% ## remove stopwords dfm_trim( min_termfreq = 10, ## frequency max_termfreq = NULL, termfreq_type = &quot;count&quot;, min_docfreq = 3, ## dispersion max_docfreq = ndoc(corp_us) - 1, docfreq_type = &quot;count&quot; ) nfeat(corp_us_dfm_trimmed) [1] 1401 In dfm_trim(), we can specify how cutoff values of min_termfreq/max_termfreq and min_docfreq/max_docfreq are interpreted. \"count\": Raw termfreq/docfreq counts; \"prop\": Normalized termfreq/docfreq (percentages); \"rank\": Inverted ranking of the features in terms of overall termfreq/docfreq; \"quantile\": Quantiles of termfreq/docfreq. 12.2.7 Exploratory Analysis of dfm We can check the top features in the current corpus: topfeatures(corp_us_dfm_trimmed) people government us can must upon great 584 564 505 487 376 371 344 may states world 343 334 319 We can visualize the top features using a word cloud: set.seed(100) textplot_wordcloud( corp_us_dfm_trimmed, max_words = 200, random_order = FALSE, rotation = .25, color = c(&#39;red&#39;, &#39;pink&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;blue&#39;) ) 12.2.8 Document Similarity As shown in 12.4, with the N-dimensional vector representation of each document, we can compute the mathematical distances/similarities between two documents. In Section 12.2.4, we introduced two important metrics: Distance-based metric: Euclidean Distance Similarity-based metric: Cosine Similarity quanteda provides useful functions to compute these metrics (as well as other alternatives): textstat_simil() and textstat_dist() Before computing the document similarity/distance, we usually convert the frequency counts in dfm into more sophisticated metrics using normalization or weighting schemes. There are three common schemes: Normalized Term Frequencies The term frequencies of features in a large document are expected to be larger as well. We can normalize these term frequencies into percentages to reduce the impact of the marginal frequencies (i.e., document size) on the significance of the co-occurrence frequencies. ## Intuition for Normalized Frequencies ## Raw Frequency Counts corp_us_dfm_trimmed[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 1 1 2 2 1 ## Normalized Frequencies dfm_weight(corp_us_dfm_trimmed, scheme=&quot;prop&quot;)[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives 1789-Washington 0.002427184 0.002427184 0.004854369 0.004854369 features docs among 1789-Washington 0.002427184 ## Intuition corp_us_dfm_trimmed[1,1:5]/sum(corp_us_dfm_trimmed[1,]) Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives 1789-Washington 0.002427184 0.002427184 0.004854369 0.004854369 features docs among 1789-Washington 0.002427184 Inverse Document Frequencies For each contextual feature, we can also assess their distinctive power in terms of their dispersion across the entire corpus. In addition to document frequency counts, we often use a more effective metric, Inverse Document Frequency(IDF), to capture the distinctiveness of the features. The IDF of a term (\\(w_i\\)) in a corpus (\\(D\\)) is the log-transformed ratio of the corpus size (\\(|D|\\)) to the term’s document frequency (\\(df_i\\)). \\[ IDF(w_i, D) = log_{10}{\\frac{|D|}{df_i}} \\] ## Intuition for Inverse Document Frequency ## Docfreq of the first ten features docfreq(corp_us_dfm_trimmed)[1:10] fellow-citizens senate house representatives among 19 9 8 14 43 life event greater order received 49 9 29 29 10 ## Inverse Doc docfreq(corp_us_dfm_trimmed, scheme = &quot;inverse&quot;, base = 10)[1:10] fellow-citizens senate house representatives among 0.49209841 0.81660950 0.86776202 0.62472398 0.13738356 life event greater order received 0.08065593 0.81660950 0.30845401 0.30845401 0.77085201 ## Intuition doc_size &lt;- ndoc(corp_us_dfm_trimmed) log(doc_size/docfreq(corp_us_dfm_trimmed)[1:10], 10) fellow-citizens senate house representatives among 0.49209841 0.81660950 0.86776202 0.62472398 0.13738356 life event greater order received 0.08065593 0.81660950 0.30845401 0.30845401 0.77085201 TFIDF A more sophisticated weighting scheme is TF-IDF scheme. We can weight the significance of term frequencies based on the term’s IDF. The TFIDF of a term (\\(w_i\\)) in a document (\\(d_j\\)) is the product of the word’s term frequency in the document (\\(tf_{ij}\\)) and the inverse document frequency of the term (\\(log\\frac{|N|}{df_i}\\)). \\[ TFIDF(w_i, d_j) = tf_{ij} \\times log\\frac{|N|}{df_i} \\] ## Intuition for TFIDF ## Raw Frequency Counts corp_us_dfm_trimmed[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 1 1 2 2 1 ## TF-IDF dfm_tfidf(corp_us_dfm_trimmed)[1,1:5] Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 0.4920984 0.8166095 1.735524 1.249448 0.1373836 ## Intuition TF_ex &lt;- corp_us_dfm_trimmed[1,1:5] IDF_ex &lt;- docfreq(corp_us_dfm_trimmed, scheme= &quot;inverse&quot;)[1:5] TF_ex*IDF_ex Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 4 docvars. features docs fellow-citizens senate house representatives among 1789-Washington 0.4920984 0.8166095 1.735524 1.249448 0.1373836 We weight the dfm with the TF-IDF scheme before the document similarity analysis. ## Distance-based corp_us_euclidean &lt;- corp_us_dfm_trimmed %&gt;% dfm_tfidf %&gt;% textstat_dist(method=&quot;euclidean&quot;) ## Cosine corp_us_cosine &lt;- corp_us_dfm_trimmed %&gt;% dfm_tfidf %&gt;% textstat_simil(method=&quot;cosine&quot;) Distance-based Results Cosine-based Results 12.2.9 Cluster Analysis The pairise distance/similarity matrices are sometimes less comprehensive. We can visulize the document distances in a more comprehensive way by an exploratory technique called hierarchical cluster analysis. ## distance-based corp_us_hist_euclidean &lt;- corp_us_euclidean %&gt;% as.dist %&gt;% ## DFM to dist hclust ## cluster analysis ## Plot dendrogram # plot(corp_us_hist_euclidean, hang = -1, cex = 0.6) require(&quot;ggdendro&quot;) ggdendrogram(corp_us_hist_euclidean, rotate = TRUE, theme_dendro = TRUE) ## similarity corp_us_hist_cosine &lt;- (1 - corp_us_cosine) %&gt;% as.dist %&gt;% hclust ## Plot dendrogram # plot(corp_us_hist_cosine, hang = -1, cex = 0.6) ggdendrogram(corp_us_hist_cosine, rotate = TRUE, theme_dendro = TRUE) Please note that textstat_simil() gives us the similarity matrix. In other words, the numbers in the matrix indicate how similar the documents are. However, for hierarchical cluster analysis, the function hclust() expects a distance-based matrix, namely one indicating how dissimilar the documents are. Therefore, we need to use (1 - corp_us_cosine) in the cosine example before performing the cluster analysis. Cluster anlaysis is a very useful exploratory technique to examine the emerging structure of a large dataset. For more detail introduction to this statistical method, I would recommend Gries (2013) Ch 5.6 and the very nice introductory book, Kaufman &amp; Rousseeuw (1990). 12.3 Vector Space Semantics: Parameters Different studies may however develop different operational definitions in their extraction of the contextual features for vector space model. Contextual Features Types: Bag-of-words: One can include all co-occurring words of the target word/document as contextual features (without considering the linear syntagmatic ordering of words). Structurally-dependent words: One can include co-occurring words of the target word/document within only particular morpho-syntactic frames (or of particular morpho-syntactic categories). Contextual Features Window: When adopting the bag-of-words approach to word meanings, one can determine the size of the context window for the contextual features inclusion (i.e., specify a certain number of tokens on the right and left from the target words). Co-occurrence Metrics The co-occurrence frequencies between target words/documents and contextual features can be statistically weighted to better represent their relationships. Common weights include tf.idf or Pointwise Mutual Information. Marginal Frequencies The co-occurrence frequencies may need to be evaluated according to the marginal frequencies of the contextual features. For example, given a co-occurrence frequency, 50, of a contextual feature with a document, it would be more indicative when the marginal frequency of the contextual feature is 100 (because half occurrences of the contextual feature go with the document). It would be much less indicative when its marginal frequency is 10,000 (because only a tidy proportion of the occurrences of the contextual feature go with the document). Dimensional Reduction When the target words/documents are represented as long vectors, they are often sparse vectors because most of their co-occurrence frequencies would be zero. There are some computational methods, which allow us to automatically extract the semantic fields from the word-based contextual features by reducing the dimensions of the long vectors. For example, it is possible that some of the contextual features (e.g., green, blue, and red) are connected semantically to form a semantic field (e.g., COLOR). A classic example is Latent Semantic Analysis, which is reminiscent of Principal Component Analysis. For more information on vector-space semantics, I would highly recommend the chapter of Vector Semantics and Embeddings, by Dan Jurafsky and James Martin. 12.4 Vector Space Model for Words (Self-Study) So far, we have been talking about applying the vector space model to study the document semantics. Now let’s take a look at how this distributional semantic approach can facilitate a lexical semantic analysis. With a corpus, we can also study the distribution, or contextual features, of words based on their co-occurring words. Now I would like to introduce another object defined in quanteda, i.e., the Feature-Cooccurrence Matrix fcm. 12.4.1 Feature-Coocurrence Matrix (fcm) A Feature-Cooccurrence Matrix is essentially a word cooccurrence matrix. There are two ways to create a fcm: from corpus/tokens to fcm from dfm to fcm 12.4.2 From corpus/tokens to fcm We can create a word-cooccurrence matrix fcm directly from the corpus object. We can further operationalize our contextual features for words: Window-based: Only words co-occurring within a defined window size will be included as contextual features Document-based: All words co-occurring in the same document will be included as contextual features. Example of Window-based fcm: # window-based corp_fcm_win &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;window&quot;, window = 1) # fcm based on window context topfeatures(corp_fcm_win,n = 50) our the in we is to 1978 1766 1359 1217 1062 884 be a all for We people 875 777 749 740 715 679 will their are and it that 661 652 608 555 554 546 been this us States by not 530 527 498 467 444 434 upon as do its world those 433 428 427 426 426 412 must should peace Government It them 397 392 390 386 377 367 great power Constitution any only freedom 361 355 339 325 321 314 may government with they shall nation 302 301 299 297 289 288 I so 283 274 A look at the corp_fcm_win: corp_fcm_win[&quot;our&quot;,] %&gt;% topfeatures(10) national institutions upon common Constitution Union 38 26 22 21 20 19 children Nation fathers within 19 19 18 15 corp_fcm_win[&quot;must&quot;,] %&gt;% topfeatures(10) We America do go continue There citizen keep 46 9 7 5 5 4 4 4 carry realize 4 4 Example of Document-based fcm: corp_fcm_doc &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;document&quot;) # same as the first one topfeatures(corp_fcm_doc,n = 50) our the to in and a 3176109 3022080 2619459 2586044 2310486 1982691 be is we for their that 1923444 1828034 1671039 1449786 1286232 1254497 are it will The not as 1212962 1185232 1140058 1136470 1081138 993573 We by all which has its 979336 923827 908249 861352 857148 835208 people upon or this us been 831527 822480 816930 809940 728218 716072 of I should It must any 681721 678465 672000 668992 641750 641596 have with them so States great 638159 626747 615322 600749 575961 574693 Government power may they only at 570366 556246 544947 525462 522589 518657 from Constitution 515615 491481 corp_fcm_doc[&quot;our&quot;,] %&gt;% topfeatures(10) we our We us must upon should world any power 54260 47064 24405 21543 17170 15701 13697 12791 11170 10692 corp_fcm_doc[&quot;must&quot;,] %&gt;% topfeatures(10) We us upon do any must only America peace when 5147 3914 3258 2126 2015 1996 1995 1956 1846 1506 In the above examples of fcm, you can in fact create the fcm directly with the corpus object (i.e., no need to transform the corpus into tokens). But we choose the tokenize our corpus into tokens first and then create the fcm. The advantage of our current method is that we can manipulate the number as well as the types of tokens we would like to include in the fcm. 12.4.3 From dfm to fcm A fcm can also be created from dfm. The limitation is that we can only create a document-based fcm from dfm. But we can make use of the feature selection discussed in the previous sections to remove irrelevant contextual features before we create the fcm. # convert `dfm` to `fcm` corp_fcm_dfm &lt;- corp_us_dfm_trimmed %&gt;% fcm() topfeatures(corp_fcm_dfm,n = 50) upon us must peace freedom power 179044 151948 127748 116086 106320 104929 government constitution part spirit law people 100582 98820 95195 89563 88701 79380 laws business congress war state shall 78924 77645 76174 74119 73947 73720 today best make within union world 72941 72190 71803 71153 70608 70446 work let progress political come great 68762 68484 67070 65616 65338 65214 always america purpose states god revenue 63302 61248 61052 60565 60324 59702 americans force justice rights institutions countrymen 57926 57685 56809 56506 55619 54818 republic true federal yet foreign others 54714 54148 54024 53330 52400 52114 long action 51463 51055 corp_fcm_dfm[&quot;people&quot;,] %&gt;% topfeatures(10) government upon us states great must 8094 5850 5292 5279 4658 4483 people power shall constitution 4217 4060 3891 3881 corp_fcm_win[&quot;people&quot;,] %&gt;% topfeatures(10) our American The are free themselves great 79 40 27 19 12 8 8 It we Our 8 8 8 corp_fcm_doc[&quot;people&quot;,] %&gt;% topfeatures(10) our we their are The We upon States us It 24668 12532 10826 9777 9064 5785 5759 5217 5202 5182 12.4.4 Which method to choose? In quanteda, we can generate the fcm of a corpus, either directly from the corpus/tokens object or from the dfm object. The feature-cooccurrence matrix measures the co-occurrences of features within a user-defined context. If the input of fcm is a dfm object, the context is set to be documents. In other words, the counts in fcm refers to the number of co-occurrences the two features within the same document. If the input of fmc is a corpus/tokens object, we can specify the context to be a window size. The counts in fcm refers to the number of co-occurrences the two features within the window size. We can conceptualize the structure of the fcm with a simple example: x &lt;- c(&quot;A B C A E F G&quot;, &quot;B C D E F G&quot;, &quot;B D A E F G&quot;) corpus(x) %&gt;% tokens() %&gt;% dfm %&gt;% fcm # fcm based on document context Feature co-occurrence matrix of: 7 by 7 features. features features a b c e f g d a 1 3 2 3 3 3 1 b 0 0 2 3 3 3 2 c 0 0 0 2 2 2 1 e 0 0 0 0 3 3 2 f 0 0 0 0 0 3 2 g 0 0 0 0 0 0 2 d 0 0 0 0 0 0 0 corpus(x) %&gt;% tokens() %&gt;% fcm(context = &quot;window&quot;, window = 2) # fcm based on window context Feature co-occurrence matrix of: 7 by 7 features. features features A B C E F G D A 0 3 2 2 2 0 1 B 0 0 2 0 0 0 2 C 0 0 0 2 0 0 1 E 0 0 0 0 3 3 2 F 0 0 0 0 0 3 1 G 0 0 0 0 0 0 0 D 0 0 0 0 0 0 0 corpus(x) %&gt;% tokens() %&gt;% fcm(context = &quot;document&quot;) # same as the first one Feature co-occurrence matrix of: 7 by 7 features. features features A B C E F G D A 1 3 2 3 3 3 1 B 0 0 2 3 3 3 2 C 0 0 0 2 2 2 1 E 0 0 0 0 3 3 2 F 0 0 0 0 0 3 2 G 0 0 0 0 0 0 2 D 0 0 0 0 0 0 0 12.4.5 Lexical Similarity fcm is an interesting structure because, similar to dfm, we can now examine the pairwise relationships between features. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent features are similar in their co-occurring contexts. - We can perform feature selection using `fcm_select()` - We can identify top important features from the `fcm` - We can create the semantic network of the top features - We can create the dendrogram of the top features # Create window-based `fcm` corp_fcm_win_5 &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;window&quot;, window = 5) # fcm based on window context # Feauture Selection: remove stopwords corp_fcm_win_5_select &lt;- corp_fcm_win_5 %&gt;% fcm_select(pattern = stopwords(), selection = &quot;remove&quot;, case_insensitive = T) # Find top features corp_fcm_win_5_top50 &lt;- names(topfeatures(corp_fcm_win_5_select, 50)) corp_fcm_win_5_top100 &lt;- names(topfeatures(corp_fcm_win_5_select, 100)) # plot network fcm_select(corp_fcm_win_5_select, pattern = corp_fcm_win_5_top50) %&gt;% textplot_network(min_freq = 5) # plot the dendrogram ## compute cosine similarity corp_fcm_win_5_top100_cosine &lt;- corp_fcm_win_5_select[corp_fcm_win_5_top100,] %&gt;% textstat_simil(method=&quot;cosine&quot;) ## create hclust (1-corp_fcm_win_5_top100_cosine) %&gt;% as.dist %&gt;% hclust -&gt; corp_us_fcm_win_5_top100_hclust ## plot dendrogram plot(corp_us_fcm_win_5_top100_hclust, cex = 0.8, main = &quot;Top 50 Features&quot;, xlab=&quot;&quot;, sub=&quot;&quot;) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. 12.5 Exercises Exercise 12.3 In this exercise, please create a dendrogram of the documents included in corp_us according to their similarities in trigram uses. Specific steps are as follows: Please create a dfm, where the contextual features are the trigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only trigrams consisting of \\\\w characters Include only trigrams whose frequencies are larger than 2. Include only trigrams whose document frequencies are larger than 2 (i.e., used in at least two different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trigram-based dfm (after the trimming according to the above distributional cut-off, the total number of trigrams in the dfm is: 7748): Example of the dendrogram based on the trigram-based dfm: Exercise 12.4 Based on the corp_us, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corp_us according to their similarities in their co-occurring words. Specific steps are as follows: Please create a tokens object of corpus by removing punctuations, symbols, and numbers first. Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in quanteda::stopwords() from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (after removing the stopwords, there are 9833 features in the fcm): The dimension of the input matrix for textstats_simil() should be: 50 rows and 9833 columns. Example of the dendrogram of the top 50 features in fcm: Exercise 12.5 In this exercise, please create a dendrogram of the Taiwan Presidential Addresses included in demo_data/TW_President.tar.gz according to their similarities in bigram uses. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus During the word-tokenization, please remove symbols by setting worker(..., symbols=T) Create the dfm of the corpus, where the contextual features are the bigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only bigrams whose frequencies &gt;= 5. Include only bigrams whose document frequencies &gt;= 3 (i.e., used in at least three different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trimmed dfm (Number of features: 337): Example of the dendrogram: Exercise 12.6 Based on the Taiwan Presidential Addresses Corpus included in demo_data/TW_President.tar.gz, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corpus according to their similarities in their co-occurring words. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus During the word-tokenization, please remove symbols by setting worker(..., symbols=T) Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in demo_data/stopwords-ch.txt from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (After trimming, the number of features is: 4908): The dimension of the input matrix for textstats_simil() should be: 50 rows and 4908 columns. Example of the dendrogram: References De Deyne, S., Verheyen, S., &amp; Storms, G. (2016). Structure and organization of the mental lexicon: A network approach derived from syntactic dependency relations and word associations [Book Section]. In A. Mehler, A. Lücking, S. Banisch, P. Blanchard, &amp; B. Job (Eds.), Towards a theoretical framework for analyzing complex linguistic networks (pp. 47–79). Springer. https://doi.org/10.1007/978-3-662-47238-5_3 Firth, J. R. (1957). A synopsis of linguistic theory 1930-1955. In J. R. Firth (Ed.), Studies in linguistic analysis (pp. 1–32). Oxford: Blackwell. Gries, S. T. (2013). 50-something years of work on collocations: What is or should be next…. International Journal of Corpus Linguistics, 18(1), 137–166. Harris, Z. (1954). Distributional structure. Word, 10(2/3), 146–162. Harris, Z. S. (1970). Papers in structural and transformational linguistics [Book]. Reidel. Jurafsky, D., &amp; Martin, J. H. (2020). Speech &amp; language processing 3rd. Available at https://web.stanford.edu/~jurafsky/slp3/ (Accessed on 2020/04/01). Kaufman, L., &amp; Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis. New York: Wiley-Interscience. Manning, C. D., &amp; Schütze, H. (1999). Foundations of statistical natural language processing. MIT press. "]]
