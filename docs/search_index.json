[
["vector-space-representation-ii.html", "Chapter 13 Vector Space Representation II 13.1 Chinese Text Analytics Flowchart 13.2 A Quick View 13.3 Loading the Corpus 13.4 Semgentation 13.5 Corpus Metadata 13.6 Document-Feature Matrix 13.7 Top Features and Wordcloud 13.8 Document Similarity 13.9 Feature Similarity", " Chapter 13 Vector Space Representation II library(tidyverse) library(quanteda) library(tidytext) library(readtext) library(jiebaR) 13.1 Chinese Text Analytics Flowchart Figure 13.1: Chinese Text Analytics Flowchart In Chapter 12, we have demonstrated the potential of a vector representation of documents with the English data. Here, we would like to look at the Chinese data in more detail. In the corpus data processing flowchart, as repeated below (Figure 13.1), we need to deal with the word segmentation with the Chinese data. This prevents us from creating a dfm directly from a corpus object because the default internal word tokenization in quanteda is not optimized for non-English languages. In this chapter, we will be using the dataset demo_data/TaiwanPresidentalSpeech.zip. Please make sure that you have downloaded the dataset from demo_data. 13.2 A Quick View For Chinese data, the major preprocessing steps have been highlighted in Figure 13.1: First load the corpus using readtext() and create corpus tibble/data.frame object (with as_tibble()) Tokenize the text-based corpus data.frame using unnest_tokens() and self-defined word tokenizer from jiebaR::segment(). This will give you a token-based tibble/data.frame version of your corpus. Tokenize the texts column/vector from the text-based DF using jiebaR::segment() and convert the list output into a token object using as.tokens(). This will give you a token version of your corpus. A token object is defined in quanteda, with many similar functions as a corpus object. This is the most important trick with the Chinese data. When you utilize many different libaries in R for your tasks, one thing you need to keep in your mind is that you need to fully understand what kind of objects you are dealing with. That is, you need to keep track of every variable you create in your script in terms of their object type/class. A vector is different from a list; a list is different from a token. Also, some of the object classes are predefined in R (e.g., vector, data.frame) while others are defined in specific libararies (e.g., corpus, token, dfm). As a habit, always check your object class (i.e., class()). 13.3 Loading the Corpus corp_tw &lt;- readtext(file=&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% as_tibble class(corp_tw) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; corp_tw %&gt;% mutate(text = str_sub(text, 1,20)) NB: readtext() creates a readtext or data.frame object. Following the tidy principle, we convert everything into tibble. When I load the corpus, I convert the readtext output directly into a tibble. If you need to process the corpus as a corpus object defined in quanteda, you can use corpus() to convert the readtext into corpus object. For now, we don’t need that. 13.4 Semgentation In order to create a vector representation of the Chinese documents, we need to create the dfm for our corpus. In Chapter 12, we suggest two ways to create the dfm: create the dfm from the corpus object create the dfm from the tokens object The second method is preferred for Chinese texts because we get to use our own word segmenter and self-defined dictionary for word segmentation. Therefore, with the text-based DF, now the next steps include: Initialize the jiebaR word segmenter, where a user dictionary is defined (Always use own dictionary to improve the performance of word segmentation) Subset the text column of corp_tw tokenize the texts and convert the output into a quanteda-compatible object, i.e., tokens # initialize segmenter chi_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = F) corp_tw_tokens &lt;- corp_tw$text %&gt;% # subset `text` column segment(jiebar = chi_seg) %&gt;% # tokenize the texts as.tokens # covert to `tokens` class(corp_tw_tokens) ## [1] &quot;tokens&quot; #str(corp_tw_tokens) corp_tw_tokens[[14]][1:10] ## [1] &quot;為&quot; &quot;年輕人&quot; &quot;打造&quot; &quot;一個&quot; &quot;更好&quot; &quot;的&quot; &quot;國家&quot; &quot;各位&quot; ## [9] &quot;友邦&quot; &quot;的&quot; Please note that when we initilize the segmenter chi_seg, we specify the argument work(…, symbol = F because symbols may not be semantically relevant in our later analysis of vector space representation. But you should be well aware of which tokens have been removed/kept in the preprocessing of your data. 13.5 Corpus Metadata When we subset the texts from the DF corp_tw for word segmentation, all the metadata connected to each text did not go with the texts. So the corp_tw_tokens did not have any metadata information. We can retrieve/add metadata information using quanteda::docvars() for corpus and tokens objects. Now the corp_tw_tokens does not have any metadata: docvars(corp_tw_tokens) So here we extract metadata information from the original filenames of each text stored in the corp_tw, and attach this metadata to corp_tw_tokens. corp_tw_meta &lt;-corp_tw %&gt;% dplyr::select(-text) %&gt;% separate(doc_id, into = c(&quot;YEAR&quot;,&quot;TERM&quot;,&quot;PRESIDENT&quot;),sep = &quot;_&quot;) %&gt;% mutate(PRESIDENT = str_replace(PRESIDENT, &quot;.txt&quot;,&quot;&quot;)) corp_tw_meta docvars(corp_tw_tokens) &lt;- corp_tw_meta docvars(corp_tw_tokens) 13.6 Document-Feature Matrix Now that we have a tokens version of our corpus, we can create dfm using the dfm(). Also, we can take care of the feature selection (cf. Chapter 12.8) using functions like dfm_trim(), dfm_select(). corp_tw_dfm &lt;- corp_tw_tokens %&gt;% dfm(remove = readLines(&quot;demo_data/stopwords-ch.txt&quot;), remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% dfm_trim(min_termfreq = 5, termfreq_type = &quot;count&quot;, min_docfreq = 2, max_docfreq = ndoc(corp_tw_tokens), docfreq_type = &quot;count&quot;) nfeat(corp_tw_dfm) ## [1] 714 class(corp_tw_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; corp_tw_dfm[1:5, 1:10] ## Document-feature matrix of: 5 documents, 10 features (14.0% sparse) and 3 docvars. ## features ## docs 中正 國民大會 憲法 選舉 中華民國 總統 國家 人民 公僕 就職 ## text1 5 1 12 1 5 1 18 17 1 2 ## text2 4 4 3 1 4 1 8 5 1 1 ## text3 5 1 0 1 0 2 3 6 0 1 ## text4 7 3 2 1 1 3 10 4 0 0 ## text5 5 1 1 0 3 2 2 1 1 0 13.7 Top Features and Wordcloud require(wordcloud2) top_features &lt;- corp_tw_dfm %&gt;% topfeatures(100) word_freq &lt;- data.frame(word = names(top_features), freq = top_features) word_freq %&gt;% wordcloud2(size= 0.8,minRotation = -pi/2, maxRotation = -pi/2) 13.8 Document Similarity corp_tw_cosine &lt;- corp_tw_dfm %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_simil(method=&quot;cosine&quot;) corp_tw_hist &lt;- (1-corp_tw_cosine) %&gt;% as.dist %&gt;% hclust hist_labels &lt;- str_c(docvars(corp_tw_dfm,&quot;YEAR&quot;), docvars(corp_tw_dfm,&quot;PRESIDENT&quot;), sep=&quot;_&quot;) plot(corp_tw_hist, hang = -1, cex = 1.2, label = hist_labels) 13.9 Feature Similarity # convert `dfm` to `fcm` corp_tw_fcm &lt;- corp_tw_dfm %&gt;% fcm # select top 30 features corp_tw_topfeatures &lt;- names(topfeatures(corp_tw_fcm, 50)) # plot network fcm_select(x = corp_tw_fcm, pattern = corp_tw_topfeatures) %&gt;% textplot_network(min_freq = 0.5) -&gt;g ggsave(&quot;test.png&quot;, g) Exercise 13.1 Create the network of top 30 bigrams for the corpus corp_tw. The critera for bigrams selection are as follows: include bigrams whose frequency &gt;= 5 and docfreq &gt;= 2 Exercise 13.2 There is an interesting application. When we analyze the document similarity, we create the graph of a dendrogram using hierarchical cluster analysis. In fact, document relations can also be represented by a network as well, as we do with the features in Section 13.9. How could you make use of the function textplot_network() in quanteda to create a network of the presidents? Please create a similar president network as shown below. Hint: Transpose the dfm so that presidents become the features of the matrix. "]
]
