[
["keyword-analysis.html", "Chapter 7 Keyword Analysis 7.1 About Keywords 7.2 Statistics for Keyness 7.3 Implementation", " Chapter 7 Keyword Analysis In this chapter, I would like to talk about the idea of “keyness”. Keywords in corpus linguistics are defined statistically using different measures of keyness. Keyness can be computed for words occurring in a target corpus by comparing their frequencies in that target corpus to that of a reference corpus and quantifying the relative attraction of each word to the target corpus by means of a statistical association metric. This idea of course can be easily extended to key phrases as well. In other words, for keyword analysis, we assume that there is a reference corpus on which the keyness of the words in the target corpus is computed. 7.1 About Keywords Keywords are important for research on languagea and ideology. To discover distinct groups through ideological inclinations expressed in writings, most researchers draw inspiration from Raymond Williams’s idea of keywords, terms presumably carrying sociocultural meanings characteristic of Western capitalist ideologies (Williams 1976). In contrast to William’s intuition-based approach, recent studies have promoted a bottom-up corpus-based method to discover terminology reflecting the ideological undercurrents of the text collections. This data-driven approach to keywords is sympathetic to the notion of statistical keywords popularized by Michael Stubbs (Stubbs 1996, 2003). 7.2 Statistics for Keyness To compute the keyness of a word w, we often need a contingency table as shown in Figure 7.1: Figure 7.1: Frequency distributions of a word and all other words in two corpora Different keyness statistics may have different ways to evaluate the relative importance of the co-occurrences of the word w with the target and the reference corpus (i.e., a and b in Figure 7.1) and statistically determine which connection is stronger. In this chapter, I would like to discuss two common statistics used in keyness analysis. This tutorial is based on Gries (2018), Ch. 5.2.6 the log-likelihood ratio G2 (Dunning 1993); \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] difference coefficient (Leech and Fallon 1992); \\[ difference\\;coefficient = \\frac{a - b}{a + b} \\] relative frequency ratio (Damerau 1993) \\[ rfr = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] 7.3 Implementation In this tutorial we will use two documents as our mini reference and target corpus. These two documents are old Wikipedia entries (provided in Gries (2018)) on Perl and Python respectively. First we initialize necessary packages in R library(tidyverse) library(tidytext) library(readtext) library(quanteda) Then we load the corpus data, which are available as two text files in our demo_data, and transform the corpus into a tidy structure #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) %&gt;% select(textid, text) We convert the text-based corpus into a word-based one for word frequency information corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) #%&gt;% #group_by(textid) %&gt;% #mutate(word_id = row_number()) %&gt;% #ungroup corpus_word %&gt;% head(100) Now we need to get the frequencies of each word in the two documents respectively. corpus_word %&gt;% count(word, textid, sort = T) -&gt; word_freq word_freq As now we are analyzing each word in relation to the two corpora, it would be better for us to have each word type as one independent row, and columns recording their co-occurrence frequencies with the two corpora. How do we get this? With all necessary frequencies, we can compute the three keyness statistics for each word contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) -&gt; keyness_table keyness_table Although now we have the keyness values for words, we still don’t know to which corpus the word is more attracted. What to do next? keyness_table %&gt;% mutate(preference = ifelse(a &gt; a.exp, &quot;perl&quot;,&quot;python&quot;)) %&gt;% select(word, preference, everything())-&gt; keyness_table keyness_table Exercise 7.1 The CSV in demo_data/data-movie-reviews.csv is the IMDB dataset with 50,000 movie reviews and their sentiment tags (source: Kaggle). The CSV has two columns–the first column review includes the raw texts of each movie review; the second column sentiment provides the sentiment tag for the review. Each review is either positive or negative. We can treat the dataset as two separate corpora: negative and positive corpora. Please find the top 10 keywords for each corpus ranked by the G2 statistics. In the data preprocessing, please use the default tokenization in unnest_tokens(..., token = &quot;words&quot;). When computing the keyness, please exclude: words with at least one non-alphanumeric symbols in them (e.g. regex class \\W) words whose frequency is &lt; 10 in each corpus The expected results are provided below for your reference. References "]
]
