[
["chinese-text-processing.html", "Chapter 8 Chinese Text Processing 8.1 Chinese Word Segmenter jiebaR 8.2 Chinese Text Analytics Pipeline 8.3 Loading Text Data 8.4 Case Study 1: Word Frequency and Wordcloud 8.5 Case Study 2: Patterns 8.6 Case Study 3: Lexical Bundles", " Chapter 8 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss English parts-of-speech tagging in Chapter 5, it is easy to do the word tokenization in English as the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. This chapter is devoted to Chinese text processing. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. In later Chapter ??, we will introduce another segmenter for Taiwan Mandarin, i.e., the CKIP Tagger, which comes with more functionalities. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 8.1 Chinese Word Segmenter jiebaR 8.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) ## [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: initilzie a word segmenter object using worker() segment the texts using segment() seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment the document, text, you first initialize a segmenter seg1 using worker() and feed this segmenter to segment(jiebar = seg1)and segment text into words. 8.1.2 Settings There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) Exercise 8.1 In our earlier example, when we created the segmenter seg1, we did not specify any arguments for worker(). Can youo tell what are the default settings for the parameters of worker()? Please try to create work() with different settings and see how the segmented results differ from each other. 8.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. This can be done when you initialize the segmenter using worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 8.1.4 Stopwords When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative. seg3 &lt;- worker(stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣民眾&quot; &quot;黨&quot; ## [25] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; &quot;哲&quot; &quot;7&quot; ## [31] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; ## [37] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; 8.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = &quot;tag&quot; when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; The following table lists the annotations of the POS tagsets used in jiebaR: 8.1.6 Default You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: # show files under `dictpath` dir(show_dictpath()) # Check the default stop_words list # Please change the path to your default dict path scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, what=character(),nlines=50,sep=&#39;\\n&#39;, encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) 8.1.7 Reminder When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and returns a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; 8.2 Chinese Text Analytics Pipeline In Chapter 4, we have talked about the work pipeline for normal English texts processing, as shown below: For Chinese texts, the work flow is pretty much the same. Because the current Chinese word segmenter jiebaR does not return the results in a tidy structure format, the most important trick is that when tokenizing the raw texts using unnest_tokens(), we need to specify our own tokenzier for the argument token = ... in the unnest_tokens(). It is important to note that when we specify a self-defined token function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument byline = TRUE for worker(byline = TRUE). So based on our simple-corpus example above, we first transform the character vector text into a tidy structure corpus data frame. Also, we generate an unique index for each row using row_number(). # a text-based tidy corpus a_small_tidy_corpus &lt;- text %&gt;% corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) a_small_tidy_corpus Second, we initialize the work() for tokenization. # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;) Finally, we use unnest_tokens() to tokenize texts into words. Specifically, we tokenize the texts included in the text column and unnest the tokens in the word column. # tokenization a_small_tidy_corpus_by_word &lt;- a_small_tidy_corpus %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) a_small_tidy_corpus_by_word In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. 8.3 Loading Text Data When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext. The main function in this package, readtext(), which takes a file or a directory name from disk or a URL, and returns a type of data.frame that can be used directly with the corpus() constructor function in quanteda, to create a quanteda corpus object. In other words, the output from readtext can be directly passed on to the processing in the tidy structure framework (i.e., tidytext::unnest_tokens()). The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. The corpus constructor command corpus() works directly on: a vector of character objects, for instance that you have already loaded into the workspace using other tools; a data.frame containing a text column and any other document-level metadata 8.4 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart ??: create a text-based tidy data frame apple_df (i.e., a tibble) intialize a word segmenter using worker() tokenize the corpus data into a word-based tidy data frame using unnest_tokens() # loading the corpus # NB: this may take some time apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% as_tibble() %&gt;% filter(text !=&quot;&quot;) %&gt;% mutate(doc_id = row_number()) apple_df %&gt;% head(10) # tokenization segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T) apple_word &lt;- apple_df %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = segmenter)) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% ungroup apple_word %&gt;% head(100) With a word-based corpus object, we can easily generate a word frequency list as well as a wordcloud to have a quick view of the word distribution in the corpus. library(showtext) # font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots ## If loading showtext throws errors, install: https://www.xquartz.org/ showtext_auto(enable = TRUE) apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) # `wordcloud` version # require(wordcloud) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 100) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) # clear up memory rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 8.5 Case Study 2: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. Important steps are as follows: 8.5.1 Define Own Tokenization Functions Create self-defined tokenization functions for sentence-like units tokenization, as well as word tokenization. But most importantly, these tokenization functions provide POS-tags of all the words included. # word tokenizer chinese_word_tokenizer&lt;- function(text, tagger){ segment(text, tagger) %&gt;% map(function(x) paste(x, names(x), sep=&quot;/&quot;)) } # Chunk tokenizer chinese_chunk_tokenizer &lt;- function(text){ str_split(text, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;) } Initialize worker() # Testing code postagger &lt;-worker(type = &quot;tag&quot;,user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = TRUE) We can try our self-defined functions with one text from the corpus: apple_df$text[1] ## [1] &quot;《蘋果體育》即日起進行虛擬賭盤擂台，每名受邀參賽者進行勝負預測，每周結算在周二公布，累積勝率前3高參賽者可繼續參賽，單周勝率最高者，將加封「蘋果波神」頭銜。註:賭盤賠率如有變動，以台灣運彩為主。\\n資料來源：NBA官網http://www.nba.com\\n\\n金塊(客) 103：92 76人騎士(主) 88：82 快艇活塞(客) 92：75 公牛勇士(客) 108：82 灰熊熱火(客) 103：82 灰狼籃網(客) 90：82 公鹿溜馬(客) 111：100 馬刺國王(客) 112：102 爵士小牛(客) 108：106 拓荒者\\n\\n&quot; apple_df$text[1] %&gt;% chinese_chunk_tokenizer() ## [[1]] ## [1] &quot;&quot; &quot;蘋果體育&quot; ## [3] &quot;即日起進行虛擬賭盤擂台&quot; &quot;每名受邀參賽者進行勝負預測&quot; ## [5] &quot;每周結算在周二公布&quot; &quot;累積勝率前&quot; ## [7] &quot;高參賽者可繼續參賽&quot; &quot;單周勝率最高者&quot; ## [9] &quot;將加封&quot; &quot;蘋果波神&quot; ## [11] &quot;頭銜&quot; &quot;註&quot; ## [13] &quot;賭盤賠率如有變動&quot; &quot;以台灣運彩為主&quot; ## [15] &quot;資料來源&quot; &quot;官網&quot; ## [17] &quot;金塊&quot; &quot;客&quot; ## [19] &quot;人騎士&quot; &quot;主&quot; ## [21] &quot;快艇活塞&quot; &quot;客&quot; ## [23] &quot;公牛勇士&quot; &quot;客&quot; ## [25] &quot;灰熊熱火&quot; &quot;客&quot; ## [27] &quot;灰狼籃網&quot; &quot;客&quot; ## [29] &quot;公鹿溜馬&quot; &quot;客&quot; ## [31] &quot;馬刺國王&quot; &quot;客&quot; ## [33] &quot;爵士小牛&quot; &quot;客&quot; ## [35] &quot;拓荒者&quot; &quot;&quot; apple_df$text[1] %&gt;% chinese_word_tokenizer(postagger) ## [[1]] ## [1] &quot;《/x&quot; &quot;蘋果/n&quot; &quot;體育/vn&quot; &quot;》/x&quot; &quot;即日起/l&quot; &quot;進行/v&quot; ## [7] &quot;虛擬/v&quot; &quot;賭盤/x&quot; &quot;擂台/v&quot; &quot;，/x&quot; &quot;每名/x&quot; &quot;受邀/v&quot; ## [13] &quot;參賽者/n&quot; &quot;進行/v&quot; &quot;勝負/v&quot; &quot;預測/vn&quot; &quot;，/x&quot; &quot;每周/r&quot; ## [19] &quot;結算/v&quot; &quot;在/p&quot; &quot;周二/t&quot; &quot;公布/v&quot; &quot;，/x&quot; &quot;累積/v&quot; ## [25] &quot;勝率/n&quot; &quot;前/f&quot; &quot;3/x&quot; &quot;高/a&quot; &quot;參賽者/n&quot; &quot;可/v&quot; ## [31] &quot;繼續/v&quot; &quot;參賽/n&quot; &quot;，/x&quot; &quot;單周/x&quot; &quot;勝率/n&quot; &quot;最高者/n&quot; ## [37] &quot;，/x&quot; &quot;將/zg&quot; &quot;加封/v&quot; &quot;「/x&quot; &quot;蘋果/n&quot; &quot;波神/x&quot; ## [43] &quot;」/x&quot; &quot;頭銜/n&quot; &quot;。/x&quot; &quot;註/x&quot; &quot;:/x&quot; &quot;賭盤/x&quot; ## [49] &quot;賠率/n&quot; &quot;如有/c&quot; &quot;變動/vn&quot; &quot;，/x&quot; &quot;以/p&quot; &quot;台灣/x&quot; ## [55] &quot;運彩/x&quot; &quot;為主/x&quot; &quot;。/x&quot; &quot;\\n/x&quot; &quot;資料/n&quot; &quot;來源/n&quot; ## [61] &quot;：/x&quot; &quot;NBA/eng&quot; &quot;官網/x&quot; &quot;http/eng&quot; &quot;:/x&quot; &quot;//x&quot; ## [67] &quot;//x&quot; &quot;www/eng&quot; &quot;./x&quot; &quot;nba/eng&quot; &quot;./x&quot; &quot;com/eng&quot; ## [73] &quot;\\n/x&quot; &quot;\\n/x&quot; &quot;金塊/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; ## [79] &quot; /x&quot; &quot;103/m&quot; &quot;：/x&quot; &quot;92/m&quot; &quot; /x&quot; &quot;76/m&quot; ## [85] &quot;人/n&quot; &quot;騎士/n&quot; &quot;(/x&quot; &quot;主/b&quot; &quot;)/x&quot; &quot; /x&quot; ## [91] &quot;88/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; &quot;快艇/n&quot; &quot;活塞/vn&quot; ## [97] &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;92/m&quot; &quot;：/x&quot; ## [103] &quot;75/m&quot; &quot; /x&quot; &quot;公牛/n&quot; &quot;勇士/n&quot; &quot;(/x&quot; &quot;客/n&quot; ## [109] &quot;)/x&quot; &quot; /x&quot; &quot;108/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; ## [115] &quot;灰熊/x&quot; &quot;熱火/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; ## [121] &quot;103/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; &quot;灰狼/n&quot; &quot;籃網/n&quot; ## [127] &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;90/m&quot; &quot;：/x&quot; ## [133] &quot;82/m&quot; &quot; /x&quot; &quot;公鹿/n&quot; &quot;溜/v&quot; &quot;馬/n&quot; &quot;(/x&quot; ## [139] &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;111/m&quot; &quot;：/x&quot; &quot;100/m&quot; ## [145] &quot; /x&quot; &quot;馬刺/nr&quot; &quot;國王/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; ## [151] &quot; /x&quot; &quot;112/m&quot; &quot;：/x&quot; &quot;102/m&quot; &quot; /x&quot; &quot;爵士/n&quot; ## [157] &quot;小牛/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;108/m&quot; ## [163] &quot;：/x&quot; &quot;106/m&quot; &quot; /x&quot; &quot;拓荒者/nr&quot; &quot;\\n/x&quot; &quot;\\n/x&quot; In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a possible sentence-like unit. This can be controversial to many grammarians and syntaticians. However, in practice, this may not be a bad choice as it will become obvious when we extract patterns. For more information related to the unicode ranage for the punctuations in CJK languages, please see this SO discussion thread. 8.5.2 Transform Text-Based to Token-Based Data Frame Now we can apply our self-defined tokenization functions to the text-based corpus data frame apple_df. We first unnest_tokens() the data into a word-based one in order to get the word and pos information. system.time( apple_df %&gt;% unnest_tokens(chunk, text, token = chinese_chunk_tokenizer) %&gt;% filter(nchar(chunk)&gt;1) %&gt;% group_by(doc_id) %&gt;% mutate(chunk_id = row_number()) %&gt;% ungroup %&gt;% unnest_tokens(word, chunk, token = function(x) chinese_word_tokenizer(x, postagger)) %&gt;% group_by(chunk_id) %&gt;% mutate(word_id = row_number()) %&gt;% ungroup -&gt; apple_word_df ) # end sytem.time ## user system elapsed ## 17.296 0.149 17.488 apple_word_df %&gt;% head dim(apple_word_df) ## [1] 2375657 4 And then based on the word-based data frame, we create a chunk-based data frame by concatenating all word/tag in a chunk into a long string. apple_chunk_df &lt;- apple_word_df %&gt;% group_by(doc_id, chunk_id) %&gt;% summarize(chunk = str_c(word, collapse=&quot;\\u3000&quot;)) %&gt;% ungroup apple_chunk_df %&gt;% head(200) dim(apple_chunk_df) ## [1] 550519 3 The chunk-based data frame would be useful for further construction/pattern analysis. 8.5.3 BEI Construction After we tokenize our text-based tidy corpus into a inter-punctuation-unit-based (IPU) tidy corpus, we can make use of the words as well as their parts-of-speech tags to extract the target pattern we are interested: 被 + ... constructions. The data retrieval process is now very straighforward: we only need to go through all the IPU units in the corpus object and see if our target pattern matches any of these IPU units. In the following example, we: define a regular expression \\\\b被_p\\\\b use unnest_tokens() and str_extract_all() to extract target patterns # define regex patterns pattern_bei &lt;- &quot;\\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v&quot; # extract patterns from corp apple_chunk_df %&gt;% unnest_tokens(output = pat_bei, input = chunk, token = function(x) str_extract_all(x, pattern=pattern_bei)) -&gt; result_bei result_bei Please check Chapter (???)(parts-of-speech-tagging) on evaluating the quality of the data retrieved by a regular expression (i.e., precision and recall). To have a more in-depth analysis of BEI construction, we like to automatically extract the verb used in the BEI construction. # Extract BEI + WORD result_bei &lt;- result_bei %&gt;% mutate(VERB = str_replace(pat_bei,&quot;.+\\u3000([^/]+)/v$&quot;,&quot;\\\\1&quot;)) result_bei # Calculate WORD frequency require(wordcloud2) result_bei %&gt;% count(VERB) %&gt;% top_n(200, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.6) Exercise 8.2 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 8.3 Please use the apple_chunk_df as your tidy corpus and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. So please (a) extract all concordance lines with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 8.4 Following Exercise 8.3, please generate a frequency list of the LMs for each spac particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Exercise 8.5 Following Exercise 8.4, for each space particle, please create a word cloud of its co-occuring LMs based on the top 200 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 8.6 From the above provided in Exercise 8.4, do you find any bizarre cases? Can you tell us why? What would be problems? What did we do wrong in the previous processing? 8.6 Case Study 3: Lexical Bundles With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at recurrent four-grams. As we discussed in Chapter 4, a multiword unit can be defined based on at least two metrics: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) As the default tokenization in unnest_tokens() only works with the English data, we start this task by defining our own token function ngram_chi() to extract Chinese n-grams. # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s|\\u3000&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes ONE text (scalar) as an input, and returns a vector of n-grams. Most importantly, this function assumes that in the text string, each word token is delimited by a whitespace. s &lt;- &quot;這 是 一個 測試 的 句子 。&quot; ngram_chi(text = s, n = 2, delimiter = &quot;_&quot;) ## [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; &quot;句子_。&quot; ngram_chi(text = s, n = 4, delimiter = &quot;_&quot;) ## [1] &quot;這_是_一個_測試&quot; &quot;是_一個_測試_的&quot; &quot;一個_測試_的_句子&quot; ## [4] &quot;測試_的_句子_。&quot; ngram_chi(text = s, n = 5, delimiter = &quot; &quot;) ## [1] &quot;這 是 一個 測試 的&quot; &quot;是 一個 測試 的 句子&quot; &quot;一個 測試 的 句子 。&quot; We vectorize the function ngram_chi(). This step is important because in unnest_tokens() the self-defined token function should take a text-based vector as input and return a list of token-based vectors of the same length as the output (cf. Section 8.2). # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) Vectorized functions are a very useful feature of R, but programmers who are used to other languages often have trouble with this concept at first. A vectorized function works not just on a single value, but on a whole vector of values at the same time. In our first defined ngram_chi function, it takes one text vector as an input and process it one at a time. However, we would expect ngram_chi to process a vector of texts (i.e., multiple texts) at the same time and return a list of resulting ngrams vectors at the same time. Therefore, we use Vectorize() as a wrapper to vectorize our function and specifically tell R that the argument text is vectorized, i.e., process each value in the text vector in the same way. Now we can tokenize our corpus into n-grams using our own token function vngram_chi() and the unnest_tokens(). In this case study, we demonstrate the analysis of four-grams in our Apple News corpus. We first remove all POS tags in apple_chunk_df$chunk because n-grams do not need the POS tags We then transform the chunk-based data frame apple_chunk_df into a n-gram-based data frame using unnest_tokens(...) with self-defined token function We remove chunks with no target n-grams extracted system.time( apple_ngram &lt;-apple_chunk_df %&gt;% mutate(chunk = str_replace_all(chunk, &quot;/[^/]+(\\u3000|$)&quot;,&quot;\\\\1&quot;)) %&gt;% # remove pos tags unnest_tokens(ngram, chunk, token = function(x) vngram_chi(text = x, n= 4)) %&gt;% filter(ngram!=&quot;&quot;)) ## user system elapsed ## 81.651 0.153 81.904 apple_ngram %&gt;% head(20) dim(apple_ngram) ## [1] 973985 3 Now that we have the four-grams-based tidy corpus object, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) apple_ngram_dist &lt;- apple_ngram %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 5) Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;被&quot;)) %&gt;% arrange(desc(dispersion)) apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;以&quot;)) %&gt;% arrange(desc(dispersion)) Exercise 8.7 In the above example, if we are only interested in the four-grams with the word 以, how can we revise the regular expression so that we can get rid of tokens like ngrams with 以及, 以上 etc. "]
]
