[
["vector-space-representation-ii.html", "Chapter 11 Vector Space Representation II 11.1 A Quick View 11.2 Loading the Corpus 11.3 Semgentation 11.4 Corpus Metadata 11.5 Document-Feature Matrix 11.6 Wordcloud 11.7 Document Similarity 11.8 Feature Similarity", " Chapter 11 Vector Space Representation II library(tidyverse) library(quanteda) library(readtext) library(jiebaR) Figure 11.1: Corpus Processing Flowchart In Chapter 10, we have demonstrated the potential of a vector representation of documents with the English data. Here, we would like to look at the Chinese data in more detail. In the corpus data processing flowchart, as repeated below (Figure 11.1), we need to deal with the word segmentation with the Chinese data. This prevents us from creating a dfm directly from a corpus object because the default internal word tokenization in quanteda is not optimized for non-English languages. In this chapter, we will be using the dataset TaiwanPresidentalSpeech.zip in our demo_data directory. Please make sure that you have downloaded the dataset from demo_data. 11.1 A Quick View For Chinese data, the major preprocessing steps have been highlighted in Figure 11.1: First read in the corpus using readtext() and create corpus data.frame object Subset the text column of the corpus data.frame for word segmentation Tokenize the texts using segment() in jiebaRand convert the output into a token object using as.token(). A token object is properly defined in quanteda, with many similar functions as a corpus object. This is the most important trick with the Chinese data. When you utilize many different libaries in R for your tasks, one thing you need to keep in your mind is that you need to fully understand what kind of objects you are dealing with. That is, you need to keep track of every variable you create in your script in terms of their object type/class. A vector is different from a list; a list is different from a token. Also, some of the object classes are predefined in R (e.g., vector, data.frame) while others are defined in specific libararies (e.g., corpus, token, dfm). As a habit, always check your object class (i.e., class()). 11.2 Loading the Corpus corp_tw &lt;- readtext(file=&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% as_tibble class(corp_tw) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; corp_tw %&gt;% mutate(text = str_sub(text, 1,20)) NB: readtext() creates a readtext or data.frame object. Following the tidy principle, we convert everything into tibble. 11.3 Semgentation Three important sub-steps in this part: initialize word segmenter, where a user dictionary is defined (Always use own dictionary to improve the performance of word segmentation) subset the text column of corp_tw tokenize the texts and convert the output into a quanteda-compatible object, token # initialize segmenter chi_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user.txt&quot;) corp_tw_tokens &lt;- corp_tw$text %&gt;% segment(jiebar = chi_seg) %&gt;% as.tokens class(corp_tw_tokens) ## [1] &quot;tokens&quot; corp_tw_tokens$text14[1:10] ## [1] &quot;為&quot; &quot;年輕人&quot; &quot;打造&quot; &quot;一個&quot; &quot;更好&quot; &quot;的&quot; &quot;國家&quot; &quot;各位&quot; ## [9] &quot;友邦&quot; &quot;的&quot; 11.4 Corpus Metadata When we subset the texts from corp_tw for word segmentation, all the metadata connected to the texts did not go with the texts. So the corp_tw_tokens did not have any metadata information. All you have is some arbitrary index to each text. docvars(corp_tw_tokens) So here we extract metadata information from the original filenames of each text stored in the corp_tw, and attach this metadata to corp_tw_tokens. corp_tw_meta &lt;-corp_tw %&gt;% dplyr::select(-text) %&gt;% separate(doc_id, into = c(&quot;YEAR&quot;,&quot;TERM&quot;,&quot;PRESIDENT&quot;),sep = &quot;_&quot;) %&gt;% mutate(PRESIDENT = str_replace(PRESIDENT, &quot;.txt&quot;,&quot;&quot;)) corp_tw_meta docvars(corp_tw_tokens) &lt;- corp_tw_meta 11.5 Document-Feature Matrix Now that we have a token version of our corpus, we can create dfm using the dfm() in quanteda. Also, we can take care of the feature selection (cf. 10.4) using functions like dfm_trim(), dfm_select(). corp_tw_dfm &lt;- corp_tw_tokens %&gt;% dfm(reove_punc = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 2, max_docfreq = 10, docfreq_type = &quot;count&quot;) class(corp_tw_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; corp_tw_dfm[1:5, 1:10] ## Document-feature matrix of: 5 documents, 10 features (32.0% sparse). ## 5 x 10 sparse Matrix of class &quot;dfm&quot; ## features ## docs 中正 國民大會 國父 環境 以及 到 以來 知道 自己 此 ## text1 5 1 3 2 2 1 1 2 1 2 ## text2 4 4 1 1 2 2 0 3 0 0 ## text3 5 1 1 0 0 5 0 4 4 0 ## text4 7 3 7 0 0 0 1 1 2 1 ## text5 5 1 1 0 0 0 0 0 0 3 11.6 Wordcloud require(wordcloud2) require(wordcloud) top_features &lt;- corp_tw_dfm %&gt;% topfeatures(100) word_freq &lt;- data.frame(word = names(top_features), freq = top_features) word_freq %&gt;% wordcloud2() 11.7 Document Similarity corp_tw_dist &lt;- corp_tw_dfm %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist corp_tw_hist &lt;- corp_tw_dist %&gt;% as.dist %&gt;% hclust hist_labels &lt;- str_c(docvars(corp_tw_dfm,&quot;YEAR&quot;), docvars(corp_tw_dfm,&quot;PRESIDENT&quot;), sep=&quot;_&quot;) plot(corp_tw_hist, hang = -1, cex = 1.2, label = hist_labels) 11.8 Feature Similarity # convert `dfm` to `fcm` corp_tw_fcm &lt;- corp_tw_dfm %&gt;% fcm # select top 30 features corp_tw_topfeatures &lt;- names(topfeatures(corp_tw_fcm, 50)) # plot network library(showtext) font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) #par(family = &quot;Arial Unicode MS&quot;) fcm_select(corp_tw_fcm, pattern = corp_tw_topfeatures) %&gt;% textplot_network(min_freq = 0.5) -&gt;g ggsave(&quot;test.png&quot;, g) Exercise 11.1 Create the network of top 30 bigrams for the corpus corp_tw. The critera for bigrams selection are as follows: include bigrams whose frequency &gt;= 10 and docfreq &gt;=5 Exercise 11.2 There is an interesting application. When we analyze the document similarity, we create the graph of a dendrogram using hierarchical cluster analysis. In fact, document relations can also be represented by a network as well, as we do with the features in 10.5.3. How could you make use of the function textplot_network() in quanteda to create a network of the presidents? Please create a similar president network as shown below. "]
]
