[
["vector-space-representation.html", "Chapter 13 Vector Space Representation 13.1 Distributional Semantics 13.2 Vector Space Semantics: Parameters 13.3 Vector Space Model for Documents 13.4 Vector Space Model for Words 13.5 Exercises", " Chapter 13 Vector Space Representation library(tidyverse) library(quanteda) In this chapter, I would like to talk about the idea of distributional semantics, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized computationally and quantified using the distributional data of the linguistic units in the corpus. Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts. 13.1 Distributional Semantics Distributional approach to semantics was first formulated by John Firth in his famous quotation: “You shall know a word by the company it keeps” (Firth 1957, 11). In other words, words that occur in the same contexts tend to have similar meanings (Harris 1954). “[D]ifference of meaning correlates with difference of distribution.” (Harris 1970, 785) “The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves.” (De Deyne, Verheyen, and Storms 2016) In computational linguistics, this idea has been implemented in the modeling of lexical semantics and documents topics. The lexical meanings of words or topics of documents can be computationally represented by the distribtional information of their co-occurring words. VSR For Words (Lexcial Semantics) On the one hand, the distributional semantics model extracts the distributional information of target words automatically from large corpora, which are referred to as the contextual features of the target words. These co-occurrence frequencies (raw or weigthed) between target words and contextual features are combined in long vectors, which can be utilized to computationally measure the lexical semantic distance or similarity. VSR For Documents (Document Semantics/Topics) On the other hand, this distributional model can be applied to the semantic representation of documents in corpus as well. One can extract the distributional information of target documents automatically from large corpora, i.e., their contextual features. The co-occurrence frequencies between target documents and contextual features are combined in long vectors, which can also be utilized to computationally measure the document similarity/difference. Therefore, this distributional approach to meanings is sometimes referred to as Vector Space Semantics. 13.2 Vector Space Semantics: Parameters Different studies may however develop different operational definitions in their extraction of the contextual features for vector space model. Contextual Features Types: Bag-of-words: One can include all co-occurring words of the target word/document as contextual features (without considering the linear syntagmatic ordering of words). Structurally-dependent words: One can include co-occurring words of the target word/document within only particular morpho-syntactic frames (or of particular morpho-syntactic categories). Contextual Features Window: When adopting the bag-of-words approach to word meanings, one can determine the size of the context window for the contextual features inclusion (i.e., specify a certain number of tokens on the right and left from the target words). Co-occurrence Metrics The co-occurrence frequencies between target words/documents and contextual features can be statistically weighted to better represent their relationships. Common weights include tf.idf or Pointwise Mutual Information. Marginal Frequencies The co-occurrence frequencies may need to be evaluated according to the marginal frequencies of the contextual features. For example, given a co-occurrence frequency, 50, of a contextual feature with a document, it would be more indicative when the marginal frequency of the contextual feature is 100 (because half occurrences of the contextual feature go with the document). It would be much less indicative when its marginal frequency is 10,000 (because only a tidy proportion of the occurrences of the contextual feature go with the document). Dimensional Reduction When the target words/documents are represented as long vectors, they are often sparse vectors because most of their co-occurrence frequencies would be zero. There are some computational methods, which allow us to automatically extract the semantic fields from the word-based contextual features by reducing the dimensions of the long vectors. For example, it is possible that some of the contextual features (e.g., green, blue, and red) are connected semantically to form a semantic field (e.g., COLOR). A classic example is Latent Semantic Analysis, which is reminiscent of Principal Component Analysis. For more information on vector-space semantics, I would highly recommend the chapter of Vector Semantics and Embeddings, by Dan Jurafsky and James Martin. 13.3 Vector Space Model for Documents Now I would like to demonstrate how we can adopt this vector space model to study the semantics of documents. 13.3.1 Data Processing Flowchart In Chapter 5, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of semantic. Most importantly, a new object class is introduced in Figure 13.1, i.e., the dfm object in quanteda. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterize the documents. The cells in the matrix often refer to the co-occurrence statistics between each document and the feature. Different ways of operationalizing the features and the cell values may lead to different types of dfm. In this section, I would like to show you how we create a dfm of a corpus and what are the common ways to define features and cell valus for the analysis of document semantics via vector space representation. Figure 13.1: English Text Analytics Flowchart (v2) 13.3.2 Document-Feature Matrix (dfm) To create a dfm, i.e., Dcument-Feature-Matrix, quanteda provides two alterantives: create dfm based on an corpus object create dfm based on an tokens object For English data, quanteda can take care of the word tokenization fairly well, so you can create dfm directly from corpus (See Figure 13.1 above) In the chapter, Chinese Text Processing, we stress that the default tokenization method in quanteda with Chinese data may be limited in several ways. In order to create a dfm that takes into account the appropriateness of the Chinese word segmentation, I would highly recommend you to first create a tokens object using the self-defined word segmentation methods, and then feed it to dfm() to create the dfm for your corpus. In this way, the dfm will use the segmented results defined by your word segmenter. In other words, with Chinese data, probably it is not really necessary to have a corpus object; rather, a tokens object of the corpus might be more useful/practical. (In quanteda, most of the functions for corpus can be applied to tokens as well, e.g., kwic(), dfm()) 13.3.3 Corpus In this tutorial, I will use the same English dataset as we discussed in Chapter 5, the data_corpus_inaugural, preloaded in the package quanteda. For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% dfm Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) ## [1] &quot;corpus&quot; &quot;character&quot; class(corp_us) ## [1] &quot;corpus&quot; &quot;character&quot; class(corp_us_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; 13.3.4 Document-Feature Matrix (dfm) What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s) (i.e., the contextual features in the vector space model). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the corpus data_corpus_inaugural, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and A dfm with words as the contextual features is the simplest way to characterize the documents in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each text as bags of words. That is, how the words are arranged relative to each other is ignored (i.e., the morphosyntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word dfm should be a naive characterization of the texts. In many computational tasks, however, it turns out that this simple bag-of-words model is very effective in modeling the semantics of the documents. 13.3.5 Distributional Hypothesis and Distance/Similarity Metrics So in our current context, the idea is that if two documents have similar sets of linguistic units (i.e., contextual features) popping up in them, they are more likely to be similar in their semantics as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with the other documents (i.e., the other rows). Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically: Figure 13.2: Vector Representation In Math, there are in general two types of metrics to measure the relationship between vectors: distance-based vs. similarity-based metrics. 13.3.5.1 Distance-based Metrics Many distance measures of vectors are based on the following formula and differ in individual parameter settings. \\[\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^y}\\big)^{\\frac{1}{y}}\\] The n in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.) When y is set to 2, it computes the famous Euclidean distance of two vectors, i.e., the direct spatial distance between two points on the n-dimensional space. \\[\\sqrt{\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^2}\\big)}\\] x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) # computing pairwise euclidean distance sum(abs(x-y)^2)^(1/2) # XY distance ## [1] 6 sum(abs(y-z)^2)^(1/2) # YZ distnace ## [1] 4.472136 sum(abs(x-z)^2)^(1/2) # XZ distnace ## [1] 8.944272 The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure 13.3) Figure 13.3: Distance-based Metric: Euclidean Distance 13.3.5.2 Similarity-based Metrics In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is Cosine Similarity, which can be computed as follows: \\[cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\] # comuting pairwise cosine similarity sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2))) # xy ## [1] 0.9778024 sum(y*z)/(sqrt(sum(y^2))*sqrt(sum(z^2))) # yz ## [1] 0.4961389 sum(x*z)/(sqrt(sum(x^2))*sqrt(sum(z^2))) # xz ## [1] 0.3032037 The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the greater their cosine similarity, the smaller the arcs, the closer they are. 13.3.5.3 Computing pairwise distance/similarity using quanteda In quanteda, we can compute pairwse similarities/distances between vectors using two useful functions: textstat_simil() textstat_dist() The expected input argument of these two functions is a dfm: # check quantida similarity and distance metrics xyz_dfm &lt;-matrix(c(1,9,1,3,5,1), byrow=T, ncol=2) %&gt;% as.dfm xyz_dfm ## Document-feature matrix of: 3 documents, 2 features (0.0% sparse). ## features ## docs feat1 feat2 ## text1 1 9 ## text2 1 3 ## text3 5 1 # Computing pairwise distance/similarity # using `quanteda` functions textstat_simil(xyz_dfm, method=&quot;cosine&quot;) ## textstat_simil object; method = &quot;cosine&quot; ## text1 text2 text3 ## text1 1.000 0.978 0.303 ## text2 0.978 1.000 0.496 ## text3 0.303 0.496 1.000 textstat_dist(xyz_dfm, method = &quot;euclidean&quot;) ## textstat_dist object; method = &quot;euclidean&quot; ## text1 text2 text3 ## text1 0 6.00 8.94 ## text2 6.00 0 4.47 ## text3 8.94 4.47 0 (1- amap::Dist(xyz_dfm, method=&quot;pearson&quot;)) ## text1 text2 ## text2 0.9778024 ## text3 0.3032037 0.4961389 13.3.5.4 Interim Summary The Euclidean Distance metric is a distance-based metric: the larger the value, the more distant the two vectors. The Cosine Similarity metric is a similatiry-based metric: the larger the value, the closer the two vectors. Based on our computations of the metrics for the three vectors, now in terms of the Euclidean Distance, y and z are closer; in terms of Cosine Similarity, x and y are closer. Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the linguistic units: The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors) The relative increase/decrease of the values on ecah dimension (i.e., the curvatures of vectors) There are many other distance-based or similarity-based metrics available. For more detail, please see Manning and Schütze (1999) Ch15.2.2. and Jurafsky and Martin (2020) Ch6: Vector Semantics and Embeddings. 13.3.6 Multidimensiona Space Back to our example of dfm, it is essentially the same vector representation, but in a multidimensional verson (cf. Figure 13.4). The document in each row is represented as a vector of N dimensional space. The size of N depends on the number of contextual features that are included in the analysis of the dfm. Figure 13.4: Example of Document-Feature Matrix 13.3.7 Vector Semantics Considerations When representing the document semantics with contextual features in a multi-dimensional vector space, two factors would turn out to be very crucial: which contextual features are more representative to be included in the semantic analysis of multidimensional representation? which quantitative metrics should be used to better represent the co-occurring relationship (i.e., association) between the documents and the contextual features? In our current dfm based on bags of words, our concerns would be: which words should be included in the analysis of multidimensional representation? which quantitative metrics should be used to represent the relationship between the documents and the words? 13.3.8 Feature Selection A dfm may not be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered with respect to the contextual features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features 13.3.8.1 Granularity In our previous example, we include only words, i.e., unigrams, as our features in the dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: from corpus to tokens from tokens to ngram-based tokens from ngram-based tokens to dfm corp_us_dfm_bigram &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;) %&gt;% tokens_ngrams(n=2) %&gt;% dfm Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: # unigram dfm + stem corp_us_dfm_unigram_stem &lt;- corp_us %&gt;% dfm(stem = T) # bigram dfm + stem corp_us_dfm_bigram_stem &lt;- corp_us %&gt;% tokens(what=&quot;word&quot;) %&gt;% tokens_ngrams(n = 2) %&gt;% dfm(stem=T) You need to decide which types of contextual features are more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, these are only heuristics, not rules. Exercise 13.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 13.3.8.2 Informativeness There are words that are not so informative in telling us the similarity and difference between the documents because they almost appear in every document of the corpus, but carray little (referential) semantic contents. These words are usually function words, such as and, the, of. These common words observed in almost all documents are often referred to as stopwords. Therefore, it is not uncommon that analysts sometimes create a list of stopwords to be removed from the dfm. The library quanteda has defined a default English stopword list, i.e., stopwords(&quot;en&quot;). stopwords(&quot;en&quot;) %&gt;% head(50) ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; ## [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; ## [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; ## [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; ## [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; ## [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; ## [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; ## [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; ## [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; ## [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; length(stopwords(&quot;en&quot;)) ## [1] 175 Also, there are tokens that usually carry very limited semantic contents, such as numbers and punctuation. Numbers, symbols and punctuations are often treated differently in computational text analytics. When creating the dfm object, we can further specify a few parameters for the function dfm(): remove_punct = TRUE: remove all punctuation tokens remove = vector(): remove all words specified in the character vector here corp_us_dfm_unigram_stop_punct &lt;- corp_us %&gt;% dfm(remove_punct = T, remove = stopwords(&quot;en&quot;)) We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm) # default unigram version ## [1] 9360 nfeat(corp_us_dfm_unigram_stem) # unigram + stem ## [1] 5544 nfeat(corp_us_dfm_bigram) # bigram ## [1] 63591 nfeat(corp_us_dfm_bigram_stem) # bigram + stem ## [1] 57281 nfeat(corp_us_dfm_unigram_stop_punct) # unigram removing stopwords and punks ## [1] 9210 13.3.8.3 Distributional Properties Depending on the granularity of the contextual features you are considering, you may get a considerably large number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the contextual word occurs only once in the corpus (i.e., hapax legomenon), these words may be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurs in all documents, they won’t help much as well. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate that this feature is too domain-specific. Therefore, sometimes we can control the document frequency of the contextual features (i.e., in how many different texts does the feature occur?) Other self-defined weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a document d, the significance of this n may be connected to: the document size of d the total number of w We can therefore utilize association-based metrics as weighted versions of the co-occurrence frequencies (e.g., PMI, LLR etc.) Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. Exercise 13.2 Please get familar with the following functions, provided by quanteda for weighting of the document-feature matrix: dfm_weight(), dfm_tfidf(). In the following demo, we adopt a few simple distrubtional criteria: we use a simple unigram model based on word-forms we remove stopwords, punctuations, numbers, and symbols we remove contextual words whose freqency &lt;= 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% dfm(remove = stopwords(&quot;en&quot;), remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) nfeat(corp_us_dfm_trimmed) ## [1] 1387 13.3.9 Exploratory Analysis of dfm We can check the top features in the current corpus: topfeatures(corp_us_dfm_trimmed) ## people government us can upon must great ## 575 564 478 471 371 366 340 ## may states shall ## 338 333 314 We can visualize the top features using a word cloud: require(RColorBrewer) set.seed(100) textplot_wordcloud(corp_us_dfm_trimmed, max_words = 200, random_order = FALSE, rotation = .25, color = c(&#39;red&#39;, &#39;pink&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;blue&#39;)) 13.3.10 Document Similarity As shown in 13.4, with the N-dimensional vector representation of each document, we can compute the mathematical distances/similarities between two documents. In Section 13.3.5, we introduced two important metrics: Distance-based metric: Euclidean Distance Similarity-based metric: Cosine Similarity quanteda provides useful functions to compute these metrics (as well as other alternatives): textstat_simil() and textstat_dist() When computing the document similarity/distance, we usually normalize the joint frequencies of contextual features (i.e., words) and documents, to reduce the impact of the marginal frequencies on the significance of the co-occurrence frequencies. In our current example, we adopted a simple method of normalization, i.e., converting the raw joint-frequencies into percentages. # Understanding `dfm_weight` prop normalization # Check the first FIVE contextual features # of the first document as.vector(corp_us_dfm_trimmed[1,]/sum(corp_us_dfm_trimmed[1,]))[1:5] ## [1] 0.00243309 0.00243309 0.00486618 0.00486618 0.00243309 as.vector(dfm_weight(corp_us_dfm_trimmed, scheme=&quot;prop&quot;)[1,])[1:5] ## [1] 0.00243309 0.00243309 0.00486618 0.00486618 0.00243309 corp_us_euclidean &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme=&quot;prop&quot;) %&gt;% textstat_dist(method=&quot;euclidean&quot;) corp_us_cosine &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme=&quot;prop&quot;) %&gt;% textstat_simil(method=&quot;cosine&quot;) Distance-based Results Cosine-based Results Based on the distances/similarities, we can further examine how different documents may cluster together in terms of their contextual features (lexical) distributions. Here we apply the hierarchical cluster analysis to examine the sub-groupings of the documents. # distance-based corp_us_hist_euclidean &lt;- corp_us_euclidean %&gt;% as.dist %&gt;% hclust plot(corp_us_hist_euclidean,hang = -1, cex = 0.6) # similarity corp_us_hist_cosine &lt;- (1 - corp_us_cosine) %&gt;% as.dist %&gt;% hclust plot(corp_us_hist_cosine,hang = -1, cex = 0.6) Please note that textstat_simil() gives us the similarity matrix. In other words, the numbers in the matrix indicate how similar the documents are. However, for hierarchical cluster analysis, the function hclust() expects a distance-based matrix, namely one indicating how dissimilar the documents are. Therefore, we need to use (1 - corp_us_cosine) in the cosine example before performing the cluster analysis. Cluster anlaysis is a very useful exploratory technique to examine the emerging structure of a large dataset. For more detail introduction to this statistical method, I would recommend Gries (2013) Ch 5.6 and the very nice introductory book, Kaufman and Rousseeuw (1990). 13.4 Vector Space Model for Words So far, we have been talking about applying the vector space model to study the document semantics. Now let’s take a look at how this distributional semantic approach can facilitate a lexical semantic analysis. With a corpus, we can also study the distribution, or contextual features, of words based on their co-occurring words. Now I would like to introduce another object defined in quanteda, i.e., the Feature-Cooccurrence Matrix fcm. 13.4.1 Feature-Coocurrence Matrix (fcm) A Feature-Cooccurrence Matrix is essentially a word cooccurrence matrix. There are two ways to create a fcm: from corpus/tokens to fcm from dfm to fcm 13.4.2 From corpus/tokens to fcm We can create a word-cooccurrence matrix fcm directly from the corpus object. We can further operationalize our contextual features for words: Window-based: Only words co-occurring within a defined window size will be included as contextual features Document-based: All words co-occurring in the same document will be included as contextual features. Example of Window-based fcm: # window-based corp_fcm_win &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;window&quot;, window = 1) # fcm based on window context topfeatures(corp_fcm_win,n = 50) ## our the in we is to ## 1947 1760 1339 1174 1047 873 ## be a all for We people ## 858 770 739 719 673 668 ## their will are it and that ## 647 638 597 552 550 546 ## been this us States by upon ## 520 508 468 465 440 433 ## not its as world do those ## 427 425 421 418 406 396 ## should Government must peace It them ## 390 386 384 384 375 364 ## great power Constitution any only freedom ## 358 345 333 319 319 314 ## government may with they shall I ## 301 300 297 295 288 279 ## nation so ## 275 270 corp_fcm_win[&quot;our&quot;,] %&gt;% topfeatures(10) ## national institutions upon common Union Nation ## 38 26 22 21 19 19 ## Constitution fathers within children ## 18 18 15 15 corp_fcm_win[&quot;must&quot;,] %&gt;% topfeatures(10) ## We America do go continue There citizen keep ## 44 9 7 5 5 4 4 4 ## carry realize ## 4 4 Example of Document-based fcm corp_fcm_doc &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;document&quot;) # same as the first one topfeatures(corp_fcm_doc,n = 50) ## our the to in and a ## 3129510 3011270 2602989 2563344 2295172 1962138 ## be is we for their that ## 1908996 1805849 1601714 1422858 1278484 1247600 ## are it The will not as ## 1198815 1181485 1127164 1113883 1069303 982782 ## We by all which has its ## 932588 920791 897104 860979 850092 834320 ## people upon or this been us ## 824195 822480 813660 785511 709732 693145 ## of should It I any have ## 679165 670735 665785 664143 637759 631849 ## must with them so States great ## 629025 622985 612097 594896 575039 571501 ## Government power may they only at ## 570366 550081 542214 520077 518446 513932 ## from Constitution ## 512588 487770 corp_fcm_doc[&quot;our&quot;,] %&gt;% topfeatures(10) ## we our We us must upon should world any power ## 51782 46203 23187 20409 16750 15701 13655 12539 11044 10482 corp_fcm_doc[&quot;must&quot;,] %&gt;% topfeatures(10) ## We us upon do any only must peace America freedom ## 4857 3644 3258 2016 1985 1965 1951 1806 1776 1503 In the above examples of fcm, you can in fact create the fcm directly with the corpus object (i.e., no need to transform the corpus into tokens). But we choose the tokenize our corpus into tokens first and then create the fcm. The advantage of our current method is that we can manipulate the number as well as the types of tokens we would like to include in the fcm. 13.4.3 From dfm to fmc A fcm can also be created from dfm. The limitation is that we can only create a document-based fcm from dfm. But we can make use of the feature selection discussed in the previous sections to remove irrelevant contextual features before we create the fcm. # convert `dfm` to `fcm` corp_fcm_dfm &lt;- corp_us_dfm_trimmed %&gt;% fcm() topfeatures(corp_fcm_dfm,n = 50) ## upon us must peace freedom power ## 179031 144149 125040 114473 106245 103742 ## government constitution part spirit law laws ## 100521 98100 95120 89514 88633 78924 ## people business congress state shall war ## 78321 77552 76164 73561 73385 71635 ## best make within union world today ## 71280 70951 70616 70181 68791 66793 ## progress let work political great come ## 65937 65715 65564 65142 64711 62519 ## always states purpose revenue god force ## 62086 60431 60421 59660 57660 57151 ## rights institutions america justice countrymen true ## 56498 55616 55407 55085 54780 54109 ## republic federal foreign others yet action ## 54086 53980 52395 52089 51911 51031 ## long trade ## 51007 50820 corp_fcm_dfm[&quot;people&quot;,] %&gt;% topfeatures(10) ## government upon states us great must ## 8094 5850 5270 5049 4622 4393 ## people power shall constitution ## 4181 4015 3873 3854 corp_fcm_win[&quot;people&quot;,] %&gt;% topfeatures(10) ## our American The are free themselves great ## 78 40 27 19 12 8 8 ## It we Our ## 8 8 8 corp_fcm_doc[&quot;people&quot;,] %&gt;% topfeatures(10) ## our we their are The upon We States It so ## 24332 12060 10762 9665 8992 5759 5553 5209 5158 5044 13.4.4 Which method to choose? In quanteda, we can generate the fcm of a corpus, either directly from the corpus/tokens object or from the dfm object. The feature-cooccurrence matrix measures the co-occurrences of features within a user-defined context. If the input of fcm is a dfm object, the context is set to be documents. In other words, the counts in fcm refers to the number of co-occurrences the two features within the same document. If the input of fmc is a corpus/tokens object, we can specify the context to be a window size. The counts in fcm refers to the number of co-occurrences the two features within the window size. We can conceptualize the structure of the fcm with a simple example: x &lt;- c(&quot;A B C A E F G&quot;, &quot;B C D E F G&quot;, &quot;B D A E F G&quot;) corpus(x) %&gt;% dfm %&gt;% fcm # fcm based on document context ## Feature co-occurrence matrix of: 7 by 7 features. ## features ## features a b c e f g d ## a 1 3 2 3 3 3 1 ## b 0 0 2 3 3 3 2 ## c 0 0 0 2 2 2 1 ## e 0 0 0 0 3 3 2 ## f 0 0 0 0 0 3 2 ## g 0 0 0 0 0 0 2 ## d 0 0 0 0 0 0 0 corpus(x) %&gt;% fcm(context = &quot;window&quot;, window = 2) # fcm based on window context ## Feature co-occurrence matrix of: 7 by 7 features. ## features ## features A B C E F G D ## A 0 3 2 2 2 0 1 ## B 0 0 2 0 0 0 2 ## C 0 0 0 2 0 0 1 ## E 0 0 0 0 3 3 2 ## F 0 0 0 0 0 3 1 ## G 0 0 0 0 0 0 0 ## D 0 0 0 0 0 0 0 corpus(x) %&gt;% fcm(context = &quot;document&quot;) # same as the first one ## Feature co-occurrence matrix of: 7 by 7 features. ## features ## features A B C E F G D ## A 1 3 2 3 3 3 1 ## B 0 0 2 3 3 3 2 ## C 0 0 0 2 2 2 1 ## E 0 0 0 0 3 3 2 ## F 0 0 0 0 0 3 2 ## G 0 0 0 0 0 0 2 ## D 0 0 0 0 0 0 0 13.4.5 Lexical Similarity fcm is an interesting structure because, similar to dfm, we can now examine the pairwise relationships between features. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent features are similar in their co-occurring contexts. # Create window-based `fcm` corp_fcm_win_5 &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;, remove_punct=T, remove_symbol=T, remove_numbers=T) %&gt;% fcm(context = &quot;window&quot;, window = 5) # fcm based on window context # Select top 100 features which are NOT stopwords corp_us_topfeatures &lt;- tolower(names(topfeatures(corp_fcm_win_5, 100))) corp_us_topfeatures &lt;-corp_us_topfeatures[!corp_us_topfeatures %in% stopwords()] # plot network fcm_select(corp_fcm_win_5, pattern = corp_us_topfeatures) %&gt;% textplot_network(min_freq = 5) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. 13.5 Exercises Exercise 13.3 In this exercise, please create a dendrogram of the documents included in corp_us according to their similarities in trigram uses. Specific steps are as follows: Please create a dfm, where the contextual features are the trigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only trigrams consisting of \\\\w characters Include only trigrams whose frequencies are larger than 2. Include only trigrams whose document frequencies are larger than 2 (i.e., used in at least two different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trigram-based dfm (after the trimming according to the above distributional cut-off, the total number of trigrams in the dfm is: 7593): Example of the dendrogram based on the trigram-based dfm: Exercise 13.4 Based on the corp_us, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corp_us according to their similarities in their co-occurring words. Specific steps are as follows: Please create a tokens object of corpus by removing punctuations, symbols, and numbers first. Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in quanteda::stopwords() from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (after removing the stopwords, there are 9740 features in the fcm): The dimension of the input matrix for textstats_simil() should be: 50 rows and 9740 columns. Example of the dendrogram of the top 50 features in fcm: Exercise 13.5 In this exercise, please create a dendrogram of the Taiwan Presidential Addresses included in demo_data/TW_President.tar.gz according to their similarities in bigram uses. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus During the word-tokenization, please remove symbols by setting worker(..., symbols=T) Create the dfm of the corpus, where the contextual features are the bigrams in the documents. Please trim the dfm according to the following distributional criteria: Include only bigrams whose frequencies &gt;= 5. Include only bigrams whose document frequencies &gt;= 3 (i.e., used in at least three different presidential addresses) Please use the cosine-based distance for cluster analysis A Sub-sample of the trimmed dfm (Number of features: 337): Example of the dendrogram: Exercise 13.6 Based on the Taiwan Presidential Addresses Corpus included in demo_data/TW_President.tar.gz, we can study how words are connected to each other. In this exercise, please create a dendrogram of important words in the corpus according to their similarities in their co-occurring words. Specific steps are as follows: Load the corpus data and word-tokenize the texts using jiebaR to create a tokens object of the corpus During the word-tokenization, please remove symbols by setting worker(..., symbols=T) Please create a window-based fcm of the corpus from the tokens object by including words within the window size of 5 as the contextual words Please remove all the stopwords included in demo_data/stopwords-ch.txt from the fcm Please create a dendrogram for the top 50 important words from the resulting fcm using the cosine-based distance metrics. When clustering the top 50 features, use the co-occurrence information from the entire fcm, i.e., clustering these top 50 features according to their co-occurring words within the window size. A Sub-sample of the fcm (After trimming, the number of features is: 4906): The dimension of the input matrix for textstats_simil() should be: 50 rows and 4906 columns. Example of the dendrogram: References "]
]
