[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2019-12-08 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark upon a digital journey to your future career, there are a series of courses provided in the [Department of English, NTNU, Taiwan], offerring necessary skills and knowledge in important disciplines. This course requires as prerequisites basic knoweldge of computational coding. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of : operationalization data retrieval quantifying research questions significance testing the applications of corpus-linguistic methodology in the sub-disciplines of linguistic studies The objective of this course is two-fold. On the one hand, it will introduce the theoretical constructs behind corpus linguistics as well as the theoretical foundations that motivate such an empirical method. On the other hand, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics, Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our course material. There are a few reference books that I would highly recommend: Gries (2018) Baayen (2008) Brezina (2018) Course Website We have a course website. You may need a password to get access to the course materials. If you are an officially enrolled student, please ask the instructor for the access code. Course Demo Data Dropbox Demo Data Directory References "],
["creating-corpus.html", "Chapter 1 Creating Corpus 1.1 HTML Structure 1.2 Web Crawling 1.3 Functional Programming 1.4 Save Corpus", " Chapter 1 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. # Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;, &quot;stringr&quot;, &quot;jiebaR&quot;, &quot;tmcn&quot;, &quot;RCurl&quot;)) library(tidyverse) library(rvest) # Packages needed for further text processing # library(jiebaR) # library(tmcn) #library(RCurl) 1.1 HTML Structure 1.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 1.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 1.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 1.2. Figure 1.2: Tree Structure of A HTML Document 1.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 1.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 1.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT. In particular, we want to extract texts from the Gossiping board. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html session (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 22731 Now gossiping should be on the front page of the Gossiping board. Now we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% str_extract(&quot;[0-9]+&quot;) %&gt;% as.numeric() page.latest ## [1] 38821 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575756964.A.F7C.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575757295.A.249.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575757305.A.5AA.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575757530.A.450.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575757848.A.300.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575758265.A.CDA.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575758861.A.BA8.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575759285.A.9A1.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575759972.A.B55.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575760113.A.920.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575760561.A.4CA.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575760650.A.973.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575760840.A.54F.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575760870.A.402.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575761162.A.562.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575761293.A.EFF.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575761515.A.5DF.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575761594.A.891.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575761893.A.599.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575762098.A.587.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;Guoplus (鍵盤大將軍)&quot; &quot;Gossiping&quot; ## [3] &quot;[爆卦] 肥宅之光輸了&quot; &quot;Sun Dec 8 06:16:02 2019&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;Guoplus&quot; article.title ## [1] &quot;[爆卦] 肥宅之光輸了&quot; article.datetime ## [1] &quot;Sun Dec 8 06:16:02 2019&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) article.content ## [1] &quot;剛剛在阿拉伯打的比賽\\n肥宅世界拳王Ruiz.jr 被AJ復仇成功\\n12回合判定失去重量級王座～\\n\\n--&quot; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 1.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all 1.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) Exercise 1.1 Can you modify the R codes so that the script can automatically scrape more than one index page? "],
["corpus-analysis-a-start.html", "Chapter 2 Corpus Analysis: A Start 2.1 Installing quanteda 2.2 Building a corpus from character vector 2.3 Keyword-in-Context (KWIC) 2.4 KWIC with Regular Expressions 2.5 Tidy Text Format of the Corpus 2.6 Frequency Lists 2.7 Collocations 2.8 Word Cloud", " Chapter 2 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 2.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 2.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. We create a corpus() object with the pre-loaded character vector data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) summary(corp_us) After the corpus is created, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() 2.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or Concordances, is the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. 2.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. 2.5 Tidy Text Format of the Corpus Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson 2017). By keeping the input and output in tidy tables, users can transition fluidly between these packages. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy() objects (see the broom package [Robinson et al cited above]) from popular text mining R packages such as tm (Feinerer, Hornik, and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow where importing, filtering, and processing is done using dplyr and other tidy tools, after which the data is converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) 2.6 Frequency Lists To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(word, text) corp_us_words Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) ## [1] 135562 sum(corp_us_bigrams_freq$n) ## [1] 135504 2.7 Collocations corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% mutate(w1freq = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], w2freq = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% mutate(w12freq_exp = (w1freq*w2freq)/sum(n)) %&gt;% mutate(MI = log2(n/w12freq_exp), t = (n - w12freq_exp)/sqrt(n)) %&gt;% arrange(desc(MI)) corp_us_collocations Exercise 2.1 Create a collocation data frame arranged by other association metrics, such as t-score. Exercise 2.2 Find the top FIVE bigrams ranked according to MI values for each president. 2.8 Word Cloud library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 100, min.freq = 10, scale = c(5,1), color = brewer.pal(8, &quot;Dark2&quot;))) Exercise 2.3 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. require(tidytext) stop_words "],
["tokenization.html", "Chapter 3 Tokenization 3.1 English Tokenization 3.2 Text Analytics Pipeline 3.3 Proper Units for Analysis 3.4 Lexical Bundles (n-grams)", " Chapter 3 Tokenization library(quanteda) library(tidyverse) library(readtext) library(tidytext) Tokenization refers to the process of segmenting a long piece of discourse into smaller linguistic units. These linguistic units, depending on your purposes, may vary in many different ways: paragraphs sentences words syllables/characters letters phonemes In this chapter, we are going to look at this issue in more detail. Specifically, we will discuss the idea of word co-occurrence, which is one of the most fundamental method in corpus linguistics, and relate it to the issue of tokenization. 3.1 English Tokenization To get a clearer idea how tokenization works in unnest_tokens, we first create a simple corpus x in a tidy structure, i.e., a tibble, with one text only. x &lt;- tibble(id = 1, text = &quot;&#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone\\nthough), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very\\n well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;...&quot;) x writeLines(x$text[1]) ## &#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone ## though), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very ## well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;... (Please note that there are two line breaks in the text.) If we use the default setting token = &quot;words&quot; in unnest_tokens, we will get: x %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) Exercise 3.1 Please check the word tokens in the output data frame carefully and list characters that disappear in the word tokens but exist in the original text. Exercise 3.2 For those missing characters, how do you preserve these characters in your output then? 3.2 Text Analytics Pipeline 3.3 Proper Units for Analysis 3.3.1 Sentence Tokenization In text analysitcs, what we often do is the sentence tokenization corp_us &lt;-corpus(data_corpus_inaugural) corp_us_df &lt;- tidy(corp_us) class(corp_us_df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In unnest_token of the tidytext library, you can specify the parameter token to customize tokenzing function. As this library is designed to deal with English texts, there are several built-in options for English text tokenizatios, including words(default), characters, character_shingles, ngrams, skip_ngrams, sentences, lines, paragraphs, regex and ptb (Penn Treebank). corp_us_sent &lt;- corp_us_df %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) corp_us_sent Sometimes it is good to give each sentence of the document an index, e.g., ID, which can help us easily keep track of the relative position of the sentence in the original document. corp_us_sent %&gt;% group_by(Year) %&gt;% mutate(sentID = row_number()) 3.3.2 Words Tokenization Corpus linguistics deal with words all the time. Word tokenization therefore is the most often used method to segment texts. This is not a big concern for languages like English, which usually puts a whitespace between words. corp_us_word &lt;- corp_us_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) corp_us_word Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). 3.4 Lexical Bundles (n-grams) Sometimes it is helpful to identify frequently occurring n-grams, i.e., recurrent multiple word sequences. You can easily create an n-gram frequency list using unnest_tokens(): corp_us_trigram &lt;- corp_us_df %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigram We then can examine which n-grams were most often used by each President: corp_us_trigram %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) Exercise 3.3 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram would be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by different Presidents? So now let’s compute the dispersion of the n-grams in our corp_us_df. Here we define the dispersion of an n-gram as the number of documents where it occurs. corp_us_trigram %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) # # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) Therefore, usually lexical bundles or n-grams are defined based on distrubtional patterns of these multiword units. In particular, cut-off values are often determined to select a list of meaningful lexical bundles. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. "],
["parts-of-speech-tagging.html", "Chapter 4 Parts-of-Speech Tagging 4.1 Parts-of-Speech Tagging 4.2 Metalingusitic Analysis 4.3 Saving POS-tagged Texts", " Chapter 4 Parts-of-Speech Tagging library(tidyverse) library(tidytext) In many textual analysis, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. 4.1 Parts-of-Speech Tagging # install spacyR # devtools::install_github(&quot;quanteda/spacyr&quot;, build_vignettes = FALSE) library(spacyr) #spacy_install() spacy_initialize() txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt,pos = T, tag = T, lemma = T) parsedtxt Two tagsets are included in the output of spacy_parse: pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set. library(quanteda) library(tidytext) corp_us_df &lt;- data_corpus_inaugural %&gt;% corpus %&gt;% tidy corp_us_df corp_us_df$text[1] %&gt;% spacy_parse() %&gt;% unnest_tokens(word, token) One trick here. If the input text character vecotr for spacy_parse() does not specify names() attributes for each text, then by default in the column doc_id of the output, it will use an autoamtic number to refer to each text. If we specify the names() of all our texts, then we can keep the meta information of each text. documents &lt;- corp_us_df$text names(documents)&lt;-str_c(corp_us_df$Year, corp_us_df$FirstName, corp_us_df$President, sep=&quot;_&quot;) corp_us_word_tag &lt;- documents %&gt;% spacy_parse(pos=T) %&gt;% unnest_tokens(word,token) 4.2 Metalingusitic Analysis In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_word_tag and first generate the frequencies of verbs, and number of words for each presidential speech text. corp_us_word_tag_2 &lt;-corp_us_word_tag %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) corp_us_word_tag_2 With the syntactic complexity of each president, we can plot the tendency: corp_us_word_tag_2 %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 4.1 Please add a regression/smooth line to the above plot to indicate the downward trend? 4.3 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time when we process the data, it would be more convenient if we save the tokenized texts with the POS tags in the hard drive. Next time we can import those files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. documents %&gt;% spacy_parse(tag=T) %&gt;% unnest_tokens(word, token, to_lower = F, strip_punct = F) -&gt; corp_us_word_tag_3 spacy_finalize() "],
["keyword-analysis.html", "Chapter 5 Keyword Analysis", " Chapter 5 Keyword Analysis G2 \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Relative Frequency Ratio \\[ RFR = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] Difference Coefficient \\[ DC = \\frac{a-b}{a+b} \\] library(tidyverse) library(tidytext) library(readtext) library(quanteda) #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) corpus corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% count(word, textid) %&gt;% tidyr::spread(textid, n, fill = 0) %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) "],
["chinese-text-processing.html", "Chapter 6 Chinese Text Processing 6.1 Chinese Word Segmenter jiebaR 6.2 Case Study 1: Word Frequency and Wordcloud 6.3 Case Study 2: Patterns", " Chapter 6 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss tokenization in @ref{#tokenization}, it is easy to do the word tokenization in English as the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. In this chapter, we will talk about the most-often used library, jiebaR, for Chinese word segmentation. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 6.1 Chinese Word Segmenter jiebaR First, you haven’t installed the library jiebaR, please install: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) Now let us take a look at a quick example. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment a text, you first initialize a segmenter seg1 using worker() and use this segmenter to segment() texts. There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (default is FALSE) bylines = FALSE: Whether to return each word one line at a time From the above examples, it is clear to see that some of the words are not correctly identified by the current segmenter: 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. seg3 &lt;- worker(stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣民眾&quot; &quot;黨&quot; ## [25] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; &quot;哲&quot; &quot;7&quot; ## [31] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; ## [37] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; So far we did not see the parts-of-speech tag provided by the word segmenter. If you need the tags of the words, you need to specify this need when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; The following table lists the annotations of the POS tagsets used in jiebaR: You can check the dictionaries being used in your current enviroment: dir(show_dictpath()) ## [1] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/jiebaRD/dict&quot; ## [1] &quot;backup.rda&quot; &quot;hmm_model.utf8&quot; &quot;hmm_model.zip&quot; &quot;idf.utf8&quot; ## [5] &quot;idf.zip&quot; &quot;jieba.dict.utf8&quot; &quot;jieba.dict.zip&quot; &quot;model.rda&quot; ## [9] &quot;README.md&quot; &quot;stop_words.utf8&quot; &quot;user.dict.utf8&quot; scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, what=character(),nlines=50,sep=&#39;\\n&#39;, encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) ## [1] &quot;\\&quot;&quot; &quot;.&quot; &quot;。&quot; &quot;,&quot; &quot;、&quot; &quot;！&quot; &quot;？&quot; &quot;：&quot; &quot;；&quot; &quot;`&quot; &quot;﹑&quot; &quot;•&quot; ## [13] &quot;＂&quot; &quot;^&quot; &quot;…&quot; &quot;‘&quot; &quot;’&quot; &quot;“&quot; &quot;”&quot; &quot;〝&quot; &quot;〞&quot; &quot;~&quot; &quot;\\\\&quot; &quot;∕&quot; ## [25] &quot;|&quot; &quot;¦&quot; &quot;‖&quot; &quot;— &quot; &quot;(&quot; &quot;)&quot; &quot;〈&quot; &quot;〉&quot; &quot;﹞&quot; &quot;﹝&quot; &quot;「&quot; &quot;」&quot; ## [37] &quot;‹&quot; &quot;›&quot; &quot;〖&quot; &quot;〗&quot; &quot;】&quot; &quot;【&quot; &quot;»&quot; &quot;«&quot; &quot;』&quot; &quot;『&quot; &quot;〕&quot; &quot;〔&quot; ## [49] &quot;》&quot; &quot;《&quot; rm(seg1, seg2, seg3, seg4) 6.2 Case Study 1: Word Frequency and Wordcloud # loading the corpus # NB: this may take some time apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% as_tibble() %&gt;% filter(text !=&quot;&quot;) apple_df # tokenization segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T) # `user = &quot;USER_DICT.txt&quot;` apple_word &lt;- apple_df %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = segmenter)) apple_word apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) require(wordcloud) library(showtext) # font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 100) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) rm(apple_word, apple_word_freq, segmenter) 6.3 Case Study 2: Patterns # define a function # to concatenate the jieba pos results into: w1_tag1 w2_tag2 w3_tag3 ... sequence my_segmenter &lt;- function(txt, seg){ txt_tag &lt;- segment(txt, seg) str_c(txt_tag, names(txt_tag), collapse=&quot; &quot;, sep=&quot;_&quot;) } tagger &lt;- worker(type=&quot;tag&quot;, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T) my_segmenter(text, tagger) ## [1] &quot;綠黨_n 桃園市_ns 議員_n 王浩宇_x 爆料_n ，_x 指民眾_x 黨_n 不_d 分區_n 被_p 提名_v 人_n 蔡壁如_x 、_x 黃_zg 瀞_x 瑩_zg ，_x 在昨_x （_x 6_x ）_x 日_m 才_d 請辭_v 是_v 為領_x 年終獎金_n 。_x 台灣民眾_x 黨_n 主席_n 、_x 台北_ns 市長_n 柯文哲_n\\r 7_x 日_m 受訪_v 時則_x 說_zg ，_x 都_d 是_v 按_p 流程_n 走_v ，_x 不要_df 把_p 人家_n 想得_x 這麼_r 壞_a 。_x&quot; my_segmenter(txt = &quot;我是在測試一個句子&quot;, seg = tagger) ## [1] &quot;我_r 是_v 在_p 測試_vn 一個_m 句子_n&quot; # IPU tokenization apple_ipu &lt;-apple_df %&gt;% unnest_tokens(IPU, text, token = function(x) str_split(x, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;))%&gt;% #IPU tokenization filter(IPU!=&quot;&quot;) %&gt;% # remove empty IPU mutate(IPU_tag = map_chr(IPU, function(x) my_segmenter(txt=x, seg = tagger))) # tag each IPU map(apple_ipu, class) ## $doc_id ## [1] &quot;character&quot; ## ## $IPU ## [1] &quot;character&quot; ## ## $IPU_tag ## [1] &quot;character&quot; apple_ipu %&gt;% head(2) # Extract BEI + WORD apple_bei &lt;-apple_ipu %&gt;% filter(str_detect(IPU_tag, pattern = &quot;\\\\b被_p\\\\b&quot;)) %&gt;% mutate(PATTERN = str_extract(IPU_tag, pattern = &quot;\\\\b被_p\\\\s([^_]+_[^\\\\s]+\\\\s)*?[^_]+_v&quot;)) %&gt;% filter(PATTERN !=&quot;&quot;) %&gt;% mutate(VERB = str_extract(PATTERN, pattern = &quot;\\\\s[^_]+_v&quot;) %&gt;% str_replace_all(&quot;_v&quot;,&quot;&quot;)) # Calculate WORD frequency require(wordcloud2) apple_bei %&gt;% count(VERB) %&gt;% arrange(desc(n)) %&gt;% wordcloud2(shape=&quot;star&quot;,size = 0.6) # Statistical Assoication For more information related to the unicode ranage for the punctuations in CJK languages, please see this SO discussion thread. "],
["references.html", "Chapter 7 References", " Chapter 7 References "]
]
