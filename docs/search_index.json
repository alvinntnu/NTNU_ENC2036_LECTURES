[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2019-12-05 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark upon a digital journey to your future career, there are a series of courses provided in the [Department of English, NTNU, Taiwan], offerring necessary skills and knowledge in important disciplines. This course requires as prerequisites basic knoweldge of computational coding. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn:  the methodological foundations of Corpus Linguistics  the theoretical bases of Corpus Linguistics  the technical designs and configuration of standard corpora  how to adopt corpus linguistics as a scientific method in terms of :  operationalization  data retrieval  quantifying research questions  significance testing  the applications of corpus-linguistic methodology in the sub-disciplines of linguistic studies The objective of this course is two-fold. On the one hand, it will introduce the theoretical constructs behind corpus linguistics as well as the theoretical foundations that motivate such an empirical method. On the other hand, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics, Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our course material. There are a few reference books that I would highly recommend: Gries (2018) Baayen (2008) Course Website We have a course website. You may need a password to get access to the course materials. If you are an officially enrolled student, please ask the instructor for the access code. Course Demo Data Dropbox Demo Data Directory References "],
["creating-corpus.html", "Chapter 1 Creating Corpus 1.1 HTML Structure 1.2 Web Crawling 1.3 Functional Programming 1.4 Save Corpus", " Chapter 1 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. # Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;, &quot;stringr&quot;, &quot;jiebaR&quot;, &quot;tmcn&quot;, &quot;RCurl&quot;)) library(tidyverse) library(rvest) # Packages needed for further text processing # library(jiebaR) # library(tmcn) #library(RCurl) 1.1 HTML Structure 1.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 1.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 1.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 1.2. Figure 1.2: Tree Structure of A HTML Document 1.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 1.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 1.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT. In particular, we want to extract texts from the Gossiping board. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html session (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 16117 Now gossiping should be on the front page of the Gossiping board. Now we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% str_extract(&quot;[0-9]+&quot;) %&gt;% as.numeric() page.latest ## [1] 39136 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509664.A.D77.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509694.A.FC1.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509702.A.C66.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509792.A.905.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509809.A.5F6.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509871.A.FAA.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509914.A.F7C.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509924.A.351.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509929.A.602.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575509969.A.FC4.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510053.A.19B.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510115.A.B52.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510116.A.1C5.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510136.A.D67.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510170.A.460.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510203.A.36C.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510273.A.80B.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510408.A.642.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1575510422.A.C62.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;FlashMan (央行是現代丁添廖)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;Re: [新聞] 卡神楊蕙如網軍案 柯文哲酸：民進黨都說&quot; ## [4] &quot;Thu Dec 5 09:34:22 2019&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;FlashMan&quot; article.title ## [1] &quot;Re: [新聞] 卡神楊蕙如網軍案 柯文哲酸：民進黨都說&quot; article.datetime ## [1] &quot;Thu Dec 5 09:34:22 2019&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) article.content ## [1] &quot;: (五)蘇處長夫人於2018年12月20日接受媒體訪問時表示，蘇處長遺書「並未言及假新聞造: 成之壓力，而是在完成上級交代之檢討報告後，開會之前一天，表明『不想受到羞辱』之: 遺言，以死明志」，非無所本。:https://buzzorange.com/2018/12/21/about-su-qichen-death/當初蘇處長自殺事件的遺書有諸多疑點\\n\\n\\n外交部選擇在第一時間試圖以遺書的自白內容來卸責\\n\\n但是遺孀又打臉遺書根本沒給外人看過\\n\\n而且暗示外交部說的內容是造假\\n\\n\\n所以到底有沒有人故意造假遺書\\n\\n如果能釐清這一點\\n\\n調查報告應該就更能還原自殺事件的真相\\n\\n--&quot; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 1.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all 1.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) Exercise 1.1 Can you modify the R codes so that the script can automatically scrape more than one index page? "],
["corpus-analysis-a-start.html", "Chapter 2 Corpus Analysis: A Start 2.1 Installing quanteda 2.2 Building a corpus from character vector 2.3 Keyword-in-Context (KWIC) 2.4 KWIC with Regular Expressions 2.5 Tidy Text Format of the Corpus 2.6 Frequency Lists 2.7 Collocations 2.8 Word Cloud", " Chapter 2 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 2.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 2.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. We create a corpus() object with the pre-loaded character vector data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) summary(corp_us) After the corpus is created, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() 2.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or Concordances, is the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. 2.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. 2.5 Tidy Text Format of the Corpus Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson 2017). By keeping the input and output in tidy tables, users can transition fluidly between these packages. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy() objects (see the broom package [Robinson et al cited above]) from popular text mining R packages such as tm (Feinerer, Hornik, and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow where importing, filtering, and processing is done using dplyr and other tidy tools, after which the data is converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) 2.6 Frequency Lists To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(word, text) corp_us_words Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) ## [1] 135562 sum(corp_us_bigrams_freq$n) ## [1] 135504 2.7 Collocations corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% mutate(w1freq = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], w2freq = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% mutate(w12freq_exp = (w1freq*w2freq)/sum(n)) %&gt;% mutate(MI = log2(n/w12freq_exp), t = (n - w12freq_exp)/sqrt(n)) %&gt;% arrange(desc(MI)) corp_us_collocations Exercise 2.1 Create a collocation data frame arranged by other association metrics, such as t-score. Exercise 2.2 Find the top FIVE bigrams ranked according to MI values for each president. 2.8 Word Cloud library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 100, min.freq = 10, scale = c(5,1), color = brewer.pal(8, &quot;Dark2&quot;))) Exercise 2.3 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. require(tidytext) stop_words "],
["text-processing.html", "Chapter 3 Text Processing", " Chapter 3 Text Processing "],
["references.html", "Chapter 4 References", " Chapter 4 References "]
]
