[
["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resourcess", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure 3.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 3.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 3.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 3.2. Figure 3.2: Tree Structure of A HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT Forum. In particular, we will demonstrate how to scape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html_session() (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created html_session and create another session. gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 23204 Now our html sesseion, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest ## [1] 39509 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584129649.A.F92.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584129671.A.C7F.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584129841.A.68B.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584129931.A.C1B.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130003.A.E22.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130086.A.A87.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130096.A.9F9.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130147.A.07E.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130682.A.DB5.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130802.A.EA1.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130854.A.D76.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584130911.A.499.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584131056.A.219.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584131138.A.BB5.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584131227.A.765.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584131446.A.FFB.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584131610.A.87B.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584132045.A.A9A.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584132587.A.A9D.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584132891.A.861.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;genheit (genheit)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;[問卦] 所以現在美國驗武漢肺炎要$$嗎？&quot; ## [4] &quot;Sat Mar 14 04:00:43 2020&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;genheit&quot; article.title ## [1] &quot;[問卦] 所以現在美國驗武漢肺炎要$$嗎？&quot; article.datetime ## [1] &quot;Sat Mar 14 04:00:43 2020&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content ## [1] &quot;阿娜\\n\\n剛才川普總統宣布了新的TESTING的政策\\n，現在在Walmart、target、Walgreens\\n、CVS都可以驗了。\\n\\n但是我沒有聽到到底$$$要怎麼算？\\n\\n要錢嗎？要多少錢？\\n\\n--\\n明教四王，紫白青金；\\n泛藍四黨，國親眾新。\\n\\n\\n--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push ## {xml_nodeset (13)} ## [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [4] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [5] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [6] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [7] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [8] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [9] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [10] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [11] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [12] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [13] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag ## [1] &quot;→&quot; &quot;→&quot; &quot;推&quot; &quot;→&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author ## [1] &quot;ChenWay&quot; &quot;zxc17893&quot; &quot;nedetdo&quot; &quot;mike760616&quot; &quot;evilaffair&quot; ## [6] &quot;CYCUTalker&quot; &quot;hbl420ii&quot; &quot;yesonline&quot; &quot;smallpig02&quot; &quot;sellgd&quot; ## [11] &quot;sellgd&quot; &quot;roccqqck&quot; &quot;yesonline&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content ## [1] &quot;: 不用&quot; ## [2] &quot;: 叫聯準會的柏齊南開印鈔機就好了&quot; ## [3] &quot;: 反正填問卷就可以驗了&quot; ## [4] &quot;: 前幾天說過了，醫療保險公司會支付&quot; ## [5] &quot;: https://i.imgur.com/vNwZvxh.jpg&quot; ## [6] &quot;: 不用 大概一兩週以前檢驗開始免費&quot; ## [7] &quot;: 不用，但連nba才300人都不能全驗&quot; ## [8] &quot;: 保險公司會支付. 但好像沒說免費次數&quot; ## [9] &quot;: 不用，500e鎂進場了&quot; ## [10] &quot;: 國難當頭 還是以支付常見的高額醫療費用?&quot; ## [11] &quot;: 以500億美元來支付 那不是圖利?&quot; ## [12] &quot;: 不用 但治療要錢啊 買手機送耳機的概念&quot; ## [13] &quot;: 確診治療就看你的保單買了那些項目...&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime ## [1] &quot;109.171.130.235 03/14 04:01&quot; &quot;182.234.160.141 03/14 04:02&quot; ## [3] &quot;118.170.90.64 03/14 04:02&quot; &quot;104.39.156.162 03/14 04:02&quot; ## [5] &quot;114.39.219.147 03/14 04:06&quot; &quot;166.177.250.79 03/14 04:11&quot; ## [7] &quot;1.200.44.166 03/14 04:15&quot; &quot;114.35.125.26 03/14 04:15&quot; ## [9] &quot;101.137.201.89 03/14 04:25&quot; &quot;203.222.14.70 03/14 04:30&quot; ## [11] &quot;203.222.14.70 03/14 04:31&quot; &quot;49.216.5.95 03/14 04:31&quot; ## [13] &quot;114.35.125.26 03/14 04:35&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables(): This function takes an article link as the argument and extract the metadata, contents, and pushes of the article. It returns a list of two elements–article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph provides a flow chart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resourcess Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the substainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Teachnical parts for corpus creation (cf. Sinclair’s Appendix) Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Exercise 3.3 Now you should have basic ideas how we can crawl data from the Internet. Sometimes, we not only collect texts but statistics as well. Please try to collect the statistics of COVID-19 outbreak from the Wikipedia 2019–20 coronavirus pandemic. Specifically, write a short script to automatically get the table included in the Wiki page, where the numbers of confirmed cases, deaths, and recoveries for each country are recorded. Your script should output a data frame as follows. Please name the columns of your data frame as follows as well. (Note: The numbers may vary because of the constant updates of the wiki page.) References "]
]
