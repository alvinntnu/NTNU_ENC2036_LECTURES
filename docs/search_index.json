[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data", " Corpus Linguistics Alvin Chen 2020-03-14 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark on a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offerring necessary inter-disciplinary skills and knowledge. This course requires as prerequisite basic knoweldge of computational coding. It is highly recommended for students to have taken ENC2055 or other equivalents before taking this course. Please see the FAQ of the course webiste for more information about the prerequisite. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will lead the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our reference material for the course. However, we will stress the hands-on implementation of the ideas and methods covered in the book. Also, there are a few more reference books listed at the end of the section, which I would highly recommend (e.g., Gries (2018), Baayen (2008), Brezina (2018), McEnery and Hardie (2011)). Course Website We have a course website. You may need a password to access the course materials. If you are an officially enrolled student, please ask the instructor for the passcode. Please read the FAQ of the course website before course registration. Course Demo Data Dropbox Demo Data Directory References "],
["what-is-corpus-linguistics.html", "Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? 1.2 What is corpus? 1.3 What is a corpus linguistic study? 1.4 Additional Information on CL", " Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? There is an unnecessary dichotomy in linguistics “intuiting” linguistic data Inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality Corpus data Highlight the importance of language use in real context Highlight the linguistic tendency in the population (from a sample) Strengths of Corpus Data Data reliability How sure can we be that other people will arrive at the same observations/patterns/conclusions using the same method? Can others replicate the same logical reasoning in intuiting data? Can others make the same “grammatical judgement”? Data validity How well do we understand what real world phenomenon that the data correspond to? Can we know more about language based on one man’s constructed sentences or his grammatical judgement? Can we better generalize our insights from one man’s intuition or the group minds (population vs. sample vs. one-man)? 1.2 What is corpus? This can be tricky: different disciplines, different definitions Literature History Sociology Field Linguistics Linguistic Corpus in corpus linguistics (Stefanowitch 2019) Authentic Representative Large A few well-received definitions “a collection of texts which have been selected and brought together so that language can be studied on the computer” (Wynne 2005) “A corpus is a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research.” (John Sinclair in (Wynne 2005)) “A corpus refers to a machine-readable collection of (spoken or written) texts that were produced in a natural communitive setting, and in which the collection of texts is compiled with the intention (1) to be representative and balanced with respect to a particular linguistic language, variety, register, or genre and (2) to be analyzed linguistically.” (Gries 2018) 1.3 What is a corpus linguistic study? CL characteristics No general agreement as to what it is Not a very homogenous methodological framework (Compared to other sub-disciplines in linguistics) It’s quite new Intertwined with many linguistic fields Interactional linguistics, cognitive linguistics, functional syntax, usage-based grammar etc. Stylometry, computational linguistics, NLP, digital humanities, text mining, sentiment analysis Corpus-based vs. Corpus-driven (cf. Tognini-Bonelli 2001) Corpus-based studies: Typically use corpus data in order to explore a theory or hypothesis, aiming to validate it, refute it or refine it. Take corpus linguistics as a method Corpus-driven studies: Typically reject the characterization of corpus linguistics as a method Claim instead that the corpus itself should be the sole source of our hypotheses about language It is thus claimed that the corpus itself embodies a theory of language (Tognini-Bonelli 2001, 84–85) Stefanowitch (2019) defines Corpus Linguistics as follows: Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus More on Conditional Distribution An exhaustive investigation Text-processing technology Retrieval and coding Regular expressions Common methods KWIC concordances Collocates Frequency lists A systematic investigation The distribution of a linguistic phenomenon under particular conditions (e.g. lexical, syntactic, social, pragmatic etc. contexts) Statistical properties of language Examples When do English speakers use the complementizer that? What are the differences between small and little? When do English speaker choose “He picked up the book” vs. “He picked the book up”? When do English speaker place the adverbial clauses before the matrix clause? Do speakers use different phrases in different genres? Is the word “gay” used differently across different time periods? Do L2 learners use similar collocation patterns as do L1 speakers? Do speakers of different socio-economic classes talk differently? 1.4 Additional Information on CL Important Journals in Corpus Linguistics Corpus Linguistics and Linguistic Theory International Journal of Corpus Linguistics Corpora Applied Linguistics Computational Linguistics Digital scholarship in the Humanities Language Teaching Language Learning Journal of Second Language Writing CALL Language Teaching Research ReCALL System References "],
["r-fundamentals.html", "Chapter 2 R Fundamentals A Quick Note", " Chapter 2 R Fundamentals A Quick Note This course assumes that students have a certain level of background knowledge of R. We will have a quick overview of several fundamental concepts relating to the R language. These topics will be covered in more detail in my other course, ENC2055. Therefore, in the following weeks, we will go over (or review) some of the important chapters in the Lecture Notes of ENC2055, including: Chapter 2: R Fundamentals Chapter 3: Code Format Convention Chapter 4: Subsetting Chapter 6: Data Manipulation Chapter 7: Data Import Chapter 8: String Manipulation Chapter 9: Conditions and Loops Chpater 10: Iterations Please refer Winter (2020) Chapter 1 and Chapter 2 for a comprehensive overview of R fundamentals. References "],
["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resourcess", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure 3.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 3.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 3.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 3.2. Figure 3.2: Tree Structure of A HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT Forum. In particular, we will demonstrate how to scape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html_session() (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created html_session and create another session. gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 10096 Now our html sesseion, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest ## [1] 39510 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584132895.A.377.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133023.A.24B.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133026.A.DF6.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133232.A.F0C.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133314.A.78D.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133346.A.7D1.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133612.A.F74.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584133681.A.52E.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584134247.A.A0E.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584134381.A.5A6.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584134390.A.B68.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584134493.A.CF9.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584134563.A.625.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584135106.A.4C7.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584135131.A.34F.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584135137.A.6C5.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584135467.A.644.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584136171.A.466.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584136191.A.E4F.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1584136196.A.A0C.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;freertos (@@)&quot; &quot;Gossiping&quot; ## [3] &quot;[問卦] 股神川普爸爸？&quot; &quot;Sat Mar 14 04:54:53 2020&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;freertos&quot; article.title ## [1] &quot;[問卦] 股神川普爸爸？&quot; article.datetime ## [1] &quot;Sat Mar 14 04:54:53 2020&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content ## [1] &quot;剛才想跟川普對做\\n被嘎嘎嘎嘎嘎 到天上去\\n賠了三十萬\\n\\n川普爸爸 真股神？\\n\\n有股神川普爸爸的掛嗎？？\\n\\n--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push ## {xml_nodeset (3)} ## [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f3 ... ## [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f3 ... ## [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 hl ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag ## [1] &quot;→&quot; &quot;→&quot; &quot;推&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author ## [1] &quot;tw15&quot; &quot;a0808996&quot; &quot;nikubou5566&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content ## [1] &quot;: 川普投顧==&quot; &quot;: 氣氛大師 解藥都沒出來&quot; ## [3] &quot;: QQ&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime ## [1] &quot;219.71.91.194 03/14 05:16&quot; &quot;123.194.129.167 03/14 05:17&quot; ## [3] &quot;49.216.1.59 03/14 05:52&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables(): This function takes an article link as the argument and extract the metadata, contents, and pushes of the article. It returns a list of two elements–article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph provides a flow chart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resourcess Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the substainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Teachnical parts for corpus creation (cf. Sinclair’s Appendix) Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Exercise 3.3 Now you should have basic ideas how we can crawl data from the Internet. Sometimes, we not only collect texts but statistics as well. Please try to collect the statistics of COVID-19 outbreak from the Wikipedia 2019–20 coronavirus pandemic. Specifically, write a short script to automatically get the table included in the Wiki page, where the numbers of confirmed cases, deaths, and recoveries for each country are recorded. Your script should output a data frame as follows. Please name the columns of your data frame as follows as well. (Note: The numbers may vary because of the constant updates of the wiki page.) References "],
["corpus-analysis-a-start.html", "Chapter 4 Corpus Analysis: A Start 4.1 Installing quanteda 4.2 Building a corpus from character vector 4.3 Keyword-in-Context (KWIC) 4.4 KWIC with Regular Expressions 4.5 Tidy Text Format of the Corpus 4.6 Frequency Lists 4.7 Collocations 4.8 Word Cloud", " Chapter 4 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 4.1 Installing quanteda To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. 4.2 Building a corpus from character vector library(quanteda) library(readtext) library(tidytext) library(dplyr) To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. We create a corpus() object with the pre-loaded character vector data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) summary(corp_us) After the corpus is created, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() 4.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or Concordances, is the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. 4.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. 4.5 Tidy Text Format of the Corpus Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format. Tidy data sets allow manipulation with a standard set of “tidy” tools, including popular packages such as dplyr (Wickham and Francois 2016), tidyr (Wickham 2016), ggplot2 (Wickham 2009), and broom (Robinson 2017). By keeping the input and output in tidy tables, users can transition fluidly between these packages. We’ve found these tidy tools extend naturally to many text analyses and explorations. At the same time, the tidytext package doesn’t expect a user to keep text data in a tidy form at all times during an analysis. The package includes functions to tidy() objects (see the broom package [Robinson et al cited above]) from popular text mining R packages such as tm (Feinerer, Hornik, and Meyer 2008) and quanteda (Benoit and Nulty 2016). This allows, for example, a workflow where importing, filtering, and processing is done using dplyr and other tidy tools, after which the data is converted into a document-term matrix for machine learning applications. The models can then be re-converted into a tidy form for interpretation and visualization with ggplot2. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) 4.6 Frequency Lists To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(word, text) corp_us_words Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) ## [1] 135562 sum(corp_us_bigrams_freq$n) ## [1] 135504 4.7 Collocations corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;)) %&gt;% mutate(w1freq = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], w2freq = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% mutate(w12freq_exp = (w1freq*w2freq)/sum(n)) %&gt;% mutate(MI = log2(n/w12freq_exp), t = (n - w12freq_exp)/sqrt(n)) %&gt;% arrange(desc(MI)) corp_us_collocations Exercise 4.1 Create a collocation data frame arranged by other association metrics, such as t-score. Exercise 4.2 Find the top FIVE bigrams ranked according to MI values for each president. 4.8 Word Cloud library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 100, min.freq = 10, scale = c(5,1), color = brewer.pal(8, &quot;Dark2&quot;))) Exercise 4.3 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. require(tidytext) stop_words "],
["tokenization.html", "Chapter 5 Tokenization 5.1 English Tokenization 5.2 Text Analytics Pipeline 5.3 Proper Units for Analysis 5.4 Lexical Bundles (n-grams)", " Chapter 5 Tokenization library(quanteda) library(tidyverse) library(readtext) library(tidytext) Tokenization refers to the process of segmenting a long piece of discourse into smaller linguistic units. These linguistic units, depending on your purposes, may vary in many different ways: paragraphs sentences words syllables/characters letters phonemes In this chapter, we are going to look at this issue in more detail. Specifically, we will discuss the idea of word co-occurrence, which is one of the most fundamental method in corpus linguistics, and relate it to the issue of tokenization. 5.1 English Tokenization To get a clearer idea how tokenization works in unnest_tokens, we first create a simple corpus x in a tidy structure, i.e., a tibble, with one text only. x &lt;- tibble(id = 1, text = &quot;&#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone\\nthough), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very\\n well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;...&quot;) x writeLines(x$text[1]) ## &#39;When I&#39;M a Duchess,&#39; she said to herself, (not in a very hopeful tone ## though), &#39;I won&#39;t have any pepper in my kitchen AT ALL. Soup does very ## well without--Maybe it&#39;s always pepper that makes people hot-tempered,&#39;... (Please note that there are two line breaks in the text.) If we use the default setting token = &quot;words&quot; in unnest_tokens, we will get: x %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) Exercise 5.1 Please check the word tokens in the output data frame carefully and list characters that disappear in the word tokens but exist in the original text. Exercise 5.2 For those missing characters, how do you preserve these characters in your output then? 5.2 Text Analytics Pipeline 5.3 Proper Units for Analysis 5.3.1 Sentence Tokenization In text analysitcs, what we often do is the sentence tokenization corp_us &lt;-corpus(data_corpus_inaugural) corp_us_df &lt;- tidy(corp_us) class(corp_us_df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In unnest_token of the tidytext library, you can specify the parameter token to customize tokenzing function. As this library is designed to deal with English texts, there are several built-in options for English text tokenizatios, including words(default), characters, character_shingles, ngrams, skip_ngrams, sentences, lines, paragraphs, regex and ptb (Penn Treebank). corp_us_sent &lt;- corp_us_df %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) corp_us_sent Sometimes it is good to give each sentence of the document an index, e.g., ID, which can help us easily keep track of the relative position of the sentence in the original document. corp_us_sent %&gt;% group_by(Year) %&gt;% mutate(sentID = row_number()) 5.3.2 Words Tokenization Corpus linguistics deal with words all the time. Word tokenization therefore is the most often used method to segment texts. This is not a big concern for languages like English, which usually puts a whitespace between words. corp_us_word &lt;- corp_us_df %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) corp_us_word Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). 5.4 Lexical Bundles (n-grams) Sometimes it is helpful to identify frequently occurring n-grams, i.e., recurrent multiple word sequences. You can easily create an n-gram frequency list using unnest_tokens(): corp_us_trigram &lt;- corp_us_df %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigram We then can examine which n-grams were most often used by each President: corp_us_trigram %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) Exercise 5.3 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram would be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by different Presidents? So now let’s compute the dispersion of the n-grams in our corp_us_df. Here we define the dispersion of an n-gram as the number of documents where it occurs. corp_us_trigram %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) # # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) Therefore, usually lexical bundles or n-grams are defined based on distrubtional patterns of these multiword units. In particular, cut-off values are often determined to select a list of meaningful lexical bundles. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. "],
["parts-of-speech-tagging.html", "Chapter 6 Parts-of-Speech Tagging 6.1 Parts-of-Speech Tagging 6.2 Metalingusitic Analysis 6.3 Saving POS-tagged Texts", " Chapter 6 Parts-of-Speech Tagging library(tidyverse) library(tidytext) In many textual analysis, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. 6.1 Parts-of-Speech Tagging # install spacyR # devtools::install_github(&quot;quanteda/spacyr&quot;, build_vignettes = FALSE) library(spacyr) #spacy_install(version=&#39;2.2.3&#39;) # # Please install the latest version Spacy!! spacy_initialize() txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt,pos = T, tag = T, lemma = T) parsedtxt Two tagsets are included in the output of spacy_parse: pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set. library(quanteda) library(tidytext) corp_us_df &lt;- data_corpus_inaugural %&gt;% corpus %&gt;% tidy corp_us_df corp_us_df$text[1] %&gt;% spacy_parse() %&gt;% unnest_tokens(word, token) One trick here. If the input text character vecotr for spacy_parse() does not specify names() attributes for each text, then by default in the column doc_id of the output, it will use an autoamtic number to refer to each text. If we specify the names() of all our texts, then we can keep the meta information of each text. documents &lt;- corp_us_df$text names(documents)&lt;-str_c(corp_us_df$Year, corp_us_df$FirstName, corp_us_df$President, sep=&quot;_&quot;) corp_us_word_tag &lt;- documents %&gt;% spacy_parse(pos=T) %&gt;% unnest_tokens(word,token) 6.2 Metalingusitic Analysis In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_word_tag and first generate the frequencies of verbs, and number of words for each presidential speech text. corp_us_word_tag_2 &lt;-corp_us_word_tag %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) corp_us_word_tag_2 With the syntactic complexity of each president, we can plot the tendency: corp_us_word_tag_2 %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 6.1 Please add a regression/smooth line to the above plot to indicate the downward trend? 6.3 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time when we process the data, it would be more convenient if we save the tokenized texts with the POS tags in the hard drive. Next time we can import those files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. documents %&gt;% spacy_parse(tag=T) %&gt;% unnest_tokens(word, token, to_lower = F, strip_punct = F) -&gt; corp_us_word_tag_3 spacy_finalize() "],
["keyword-analysis.html", "Chapter 7 Keyword Analysis", " Chapter 7 Keyword Analysis G2 \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] Relative Frequency Ratio \\[ RFR = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] Difference Coefficient \\[ DC = \\frac{a-b}{a+b} \\] library(tidyverse) library(tidytext) library(readtext) library(quanteda) #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) corpus corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) corpus_word %&gt;% count(word, textid) %&gt;% tidyr::spread(textid, n, fill = 0) %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) "],
["constructions-and-idioms.html", "Chapter 8 Constructions and Idioms 8.1 Chinese Four-character Idioms 8.2 Dictionary Entries 8.3 Case Study: X來Y去 8.4 Exercises", " Chapter 8 Constructions and Idioms library(tidyverse) 8.1 Chinese Four-character Idioms Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. This chapter will provide a exploratory analysis of four-character idioms in Chinese. 8.2 Dictionary Entries In our demo_data directory, there is a file dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first import the idioms in the file. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;) head(all_idioms) ## [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; tail(all_idioms) ## [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; length(all_idioms) ## [1] 56536 In order to make use of the tidy structure in R, we convert the data into a tibble: idiom &lt;- tibble(string = all_idioms) 8.3 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來X去: idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we may need to extract the X and Y in the schema: idiom_laiqu &lt;-idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% mutate(pattern = str_replace(string, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(pattern, into = c(&quot;w1&quot;, &quot;w2&quot;), sep = &quot;_&quot;) idiom_laiqu One empirical question is how many of these idioms are of the pattern X=Y (e.g., 想來想去, 直來直去) and how many are of X!=Y (e.g., 說來道去, 朝來暮去): idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() 8.4 Exercises Exercise 8.1 Please use idiom and extract the idioms with the schema of 一X一Y. Exercise 8.2 Also with the idiom as our data source, now if we are interested in all idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Also, provide the distribution of the two types. Exercise 8.3 Following Exercise 8.2, for each type of the idioms, please provide their respective proportions of X=Y vs. X!=Y. Exercise 8.4 Folloing Exercise 8.3, please identify the character that is duplicated in the idioms. One follow-up analysis would be to look at the distribution of these pivotal characters. Can you reproduce a graph as shown below as closely as possible? "],
["chinese-text-processing.html", "Chapter 9 Chinese Text Processing 9.1 Chinese Word Segmenter jiebaR 9.2 Chinese Text Analytics Pipeline 9.3 Case Study 1: Word Frequency and Wordcloud 9.4 Case Study 2: Patterns 9.5 Case Study 3: Lexical Bundles", " Chapter 9 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss tokenization in Chapter 5, it is easy to do the word tokenization in English as the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. This chapter is devoted to Chinese text processing. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 9.1 Chinese Word Segmenter jiebaR 9.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) ## [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: initilzie a word segmenter object using worker() segment the texts using segment() seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment the document, text, you first initialize a segmenter seg1 using worker() and feed this segmenter to segment(jiebar = seg1)and segment text into words. 9.1.2 Settings There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not 9.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. This can be done when you initialize the segmenter using worker(). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 9.1.4 Stopwords When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative. seg3 &lt;- worker(stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣民眾&quot; &quot;黨&quot; ## [25] &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; &quot;哲&quot; &quot;7&quot; ## [31] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; ## [37] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; 9.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = &quot;tag&quot; when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; The following table lists the annotations of the POS tagsets used in jiebaR: 9.1.6 Default You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: dir(show_dictpath()) scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, what=character(),nlines=50,sep=&#39;\\n&#39;, encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) 9.1.7 Reminder When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and return a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; 9.2 Chinese Text Analytics Pipeline In Chapter 5, we have talked about the work pipeline for normal English texts processing, as shown below: For Chinese texts, the work flow is pretty much the same. The most important trick is in the step of tokenization, i.e., unnest_tokens(): we need to specify our own tokenzier for the argument token = ... in the unnest_tokens(). It is important to note that when we specify a self-defined token function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument byline = TRUE for worker(byline = TRUE). So based on our simple-corpus example above, we can create # a text-based tidy corpus a_small_tidy_corpus &lt;- text %&gt;% corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) a_small_tidy_corpus # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;) # tokenization a_small_tidy_corpus_by_word &lt;- a_small_tidy_corpus %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) a_small_tidy_corpus_by_word In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. 9.3 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart ??: create a text-based tidy corpus object apple_df (i.e., a tibble) intialize a word segmenter using worker() tokenize the corpus into a word-based tidy corpus object using unnest_tokens() # loading the corpus # NB: this may take some time apple_df &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% as_tibble() %&gt;% filter(text !=&quot;&quot;) %&gt;% mutate(doc_id = row_number()) apple_df # tokenization segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T) apple_word &lt;- apple_df %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = segmenter)) apple_word With a word-based corpus object, we can easily generate a word frequency list as well as a wordcloud to have a quick view of the word distribution in the corpus. library(showtext) # font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots ## If loading showtext throws errors, install: https://www.xquartz.org/ showtext_auto(enable = TRUE) apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) # `wordcloud` version # require(wordcloud) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 100) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 9.4 Case Study 2: Patterns In this case study, we are looking at a more complex example. In linguistics analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. Important steps are as follows: Create a self-defined tokenization function, which takes a text and returns the same text but with the POS-tags of all the words included. # define a function # to concatenate the jieba pos results into: w1_tag1 w2_tag2 w3_tag3 ... sequence chi_pos_tagger &lt;- function(txt, tagger){ txt_tag &lt;- segment(txt, tagger) str_c(txt_tag, names(txt_tag), collapse=&quot; &quot;, sep=&quot;_&quot;) } We defined the function chi_pos_tagger(), which takes a text in and returns the text out with the POS tags of the words appended at the end of each word. mytagger &lt;- worker(type=&quot;tag&quot;, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = FALSE) chi_pos_tagger(txt = text, tagger = mytagger) ## [1] &quot;綠黨_n 桃園市_x 議員_n 王浩宇_x 爆料_n ，_x 指民眾_x 黨_n 不_d 分區_n 被_p 提名_v 人_n 蔡壁如_x 、_x 黃_zg 瀞_x 瑩_zg ，_x 在昨_x （_x 6_x ）_x 日_m 才_d 請辭_v 是_v 為領_x 年終獎金_n 。_x 台灣_x 民眾_x 黨_n 主席_n 、_x 台北_x 市長_x 柯文_nz 哲_n 7_x 日_m 受訪_v 時則_x 說_zg ，_x 都_d 是_v 按_p 流程_n 走_v ，_x 不要_df 把_p 人家_n 想得_x 這麼_x 壞_a 。_x&quot; chi_pos_tagger(txt = &quot;我是在測試一個句子&quot;, tagger = mytagger) ## [1] &quot;我_r 是_v 在_p 測試_vn 一個_x 句子_n&quot; Tokenize the text-based tidy corpus into a sentence-based one, and POS-tag each sentence and put the tagged version of the sentences in a new column # IPU tokenization apple_ipu &lt;-apple_df %&gt;% unnest_tokens(IPU, text, token = function(x) str_split(x, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;))%&gt;% #IPU tokenization filter(IPU!=&quot;&quot;) %&gt;% # remove empty IPU mutate(IPU_tag = map_chr(IPU, function(x) chi_pos_tagger(txt=x, tagger = mytagger))) # tag each IPU apple_ipu In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a possible sentence-like unit. This can be controversial to many grammarians and syntaticians. However, in practice, this may not be a bad choice as it will become obvious when we extract patterns. For more information related to the unicode ranage for the punctuations in CJK languages, please see this SO discussion thread. After we tokenize our text-based tidy corpus into a inter-punctuation-unit-based (IPU) tidy corpus, we can make use of the words as well as their parts-of-speech tags to extract the target pattern we are interested: 被 + ... constructions. The data retrieval process is now very straighforward: we only need to go through all the IPU units in the corpus object and see if our target pattern matches any of these IPU units. In the following example, we: subset IPUs with the target pattern \\\\b被_p\\\\b using str_detect() extract the strings that match the target pattern \\\\b被_p\\\\s([^_]+_[^\\\\s]+\\\\s)*?[^_]+_v using str_extract() and add these strings to a new column using mutate() extract the verb in the BEI construction using str_extract() and add these verbs to a new column using mutate # Extract BEI + WORD apple_bei &lt;-apple_ipu %&gt;% filter(str_detect(IPU_tag, pattern = &quot;\\\\b被_p\\\\b&quot;)) %&gt;% mutate(PATTERN = str_extract(IPU_tag, pattern = &quot;\\\\b被_p\\\\s([^_]+_[^\\\\s]+\\\\s)*?[^_]+_v&quot;)) %&gt;% filter(PATTERN !=&quot;&quot;) %&gt;% mutate(VERB = str_extract(PATTERN, pattern = &quot;[^_\\\\s]+_v&quot;) %&gt;% str_replace_all(&quot;_v&quot;,&quot;&quot;)) apple_bei # Calculate WORD frequency require(wordcloud2) apple_bei %&gt;% count(VERB) %&gt;% top_n(300, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.6) # Statistical Assoication Exercise 9.1 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 9.2 Please use the apple_ipu as your tidy corpus and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. So please (a) extract all concordance lines with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 9.3 Following Exercise 9.2, please generate a frequency list of the LMs for each spac particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Exercise 9.4 Following Exercise 9.3, for each space particle, please create a word cloud of its co-occuring LMs based on the top 200 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 9.5 From the above provided in Exercise 9.3, the graph of 內 shows a few LMs that are counter intuitive to our native knowledge: for example, 出車, 的, 做, 到. Can you tell us why? What would be problems? What did we do wrong in the previous processing? 9.5 Case Study 3: Lexical Bundles With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at recurrent four-grams. As we discussed in Chapter 5, a multiword unit can be defined based on at least two metrics: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) As the default tokenization in unnest_tokens() only works with the English data, we start this task by defining our own token function ngram_chi() to extract Chinese n-grams. # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes ONE text (scalar) as an input, and returns a vector of n-grams. Most importantly, this function assumes that in the text string, each word token is delimited by a whitespace. s &lt;- &quot;這 是 一個 測試 的 句子 。&quot; ngram_chi(text = s, n = 2, delimiter = &quot;_&quot;) ## [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; &quot;句子_。&quot; ngram_chi(text = s, n = 4, delimiter = &quot;_&quot;) ## [1] &quot;這_是_一個_測試&quot; &quot;是_一個_測試_的&quot; &quot;一個_測試_的_句子&quot; ## [4] &quot;測試_的_句子_。&quot; ngram_chi(text = s, n = 5, delimiter = &quot; &quot;) ## [1] &quot;這 是 一個 測試 的&quot; &quot;是 一個 測試 的 句子&quot; &quot;一個 測試 的 句子 。&quot; We vectorize the function ngram_chi(). This step is important because in unnest_tokens() the self-defined token function should take a text-based vector as input and return a list of token-based vectors of the same length as the output (cf. Section 9.2). # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) #system.time(parallel::pvec(x&lt;-apple_ipu$IPU_tag,function(x) vngram_chi(x, n=4))) # 47s Vectorized functions are a very useful feature of R, but programmers who are used to other languages often have trouble with this concept at first. A vectorized function works not just on a single value, but on a whole vector of values at the same time. In our first defined ngram_chi function, it takes one text vector as an input and process it one at a time. However, we would expect ngram_chi to process a vector of texts (i.e., multiple texts) at the same time and return a list of resulting ngrams vectors at the same time. Therefore, we use Vectorize() as a wrapper to vectorize our function and specifically tell R that the argument text is vectorized, i.e., process each value in the text vector in the same way. Now we can tokenize our corpus into n-grams using our own token function vngram_chi() and the unnest_tokens(). In this case study, we demonstrate the analysis of four-grams in our Apple News corpus. Because we need calculate not only the frequencies of the n-grams but also their dispersions, we begin by first creating a sentence ID for the IPUs of each article. Then we remove all the POS tags because n-grams extraction do not need the POS tag information. Finally, these cleaned versions of IPU, stored in the new column IPU_word, are used as input for unnest_tokens() and we specify our own token function vngram_chi() to tokenize IPU_word into four-grams. apple_ngram &lt;-apple_ipu %&gt;% group_by(doc_id) %&gt;% mutate(sentID = row_number()) %&gt;% ungroup %&gt;% mutate(IPU_word = str_replace_all(IPU_tag, &quot;_[^ ]+&quot;,&quot;&quot;)) %&gt;% unnest_tokens(ngram, IPU_word, token = function(x) vngram_chi(text = x, n= 4)) %&gt;% filter(ngram!=&quot;&quot;) apple_ngram Now that we have the four-grams-based tidy corpus object, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. apple_ngram_dist &lt;- apple_ngram %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion)) Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) "],
["ckiptagger.html", "Chapter 10 CKIP Tagger 10.1 Installation 10.2 Download the Model Files 10.3 R-Python Communication 10.4 Word Segmentation in R 10.5 R Environment Setting 10.6 Loading Python Modules 10.7 Segmenting Texts 10.8 Define Own Dictionary 10.9 Beyond Word Boundaries 10.10 Tidy Up the Results", " Chapter 10 CKIP Tagger The current state-of-art Chinese segmenter for Taiwan Mandarin available is probably the CKIP tagger, created by the Chinese Knowledge and Information Processing (CKIP) group at the Academia Sinica. The ckiptagger is released as a python module. In this chpater, I will demonstrate how to use the module for Chinese word segmentation but in an R environment, i.e., how to integrate Python modules in R coherently to perform complex tasks. 10.1 Installation Because ckiptagger is built in python, we need to have python installed in our working environment. Please install the following applications on your own before you start: Anaconda + Python 3.7+ ckiptagger module in Python (Please install the module using the Anaconda Navigator) (Please consult the github of the ckiptagger for more details on installation.) For some reasons, the module ckiptagger cannot be found in the base channel. In Anaconda Navigator, please add specifically the following channel to the environment so that your Anaconda can find ckiptagger module: https://conda.anaconda.org/roccqqck 10.2 Download the Model Files All NLP applications have their models behind their fancy performances. To use the tagger provided in ckiptagger, we need to download their trained model files. Please go to the github of CKIP tagger to download the model files. (The file is very big. It takes a while.) After you download the zip file, unzip the data/ directory to your working directory. 10.3 R-Python Communication In order to call Python functions in R/Rstudio, we need to install an R library in your R. The R-Python communication is made possible through the R library reticulate. Please make sure that you have this library installed in your R. install.packages(&quot;reticulate&quot;) 10.4 Word Segmentation in R Before we proceed, please check if you have everything ready: Anaconda + Python 3.7+ Python module: ckiptagger R library: reticulate CKIP model files under your working directory ./data If yes, then we are ready to go. 10.5 R Environment Setting We first load the library reticulate and specify in R which Python we will be using in the current R(It is highly likely that there is more than one Python version installed in your system). Please change the path_to_python to your own path, which includes the Anaconda Python you just installed. library(reticulate) # spacy_initialize() would set the default python to miniconda/spacy_condaenviron # install ckiptagger and tensorflow-1.13.1 in spacy_condaenviron #path_to_python &lt;- &quot;/Users/Alvin/opt/anaconda3/envs/r-reticulate/bin/python&quot; #use_python(path_to_python, required = T) 10.6 Loading Python Modules ## If you like to run ckiptagger in a new environment: #conda_create(&quot;r-reticulate&quot;, conda = &quot;/Users/Alvin/opt/anaconda3/bin/conda&quot;) #use_condaenv(&quot;r-reticulate&quot;)#, conda = &quot;/Users/Alvin/opt/anaconda3/bin/conda&quot;) ## Import ckiptagger module ckip &lt;- import(module = &quot;ckiptagger&quot;) ## Intialize models ws &lt;- ckip$WS(&quot;./data&quot;) 10.7 Segmenting Texts ## Raw text corpus (sentences) sents &lt;- c(&quot;傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。&quot;, &quot;美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。&quot;, &quot;土地公有政策?？還是土地婆有政策。.&quot;, &quot;… 你確定嗎… 不要再騙了……&quot;, &quot;最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.&quot;, &quot;科長說:1,坪數對人數為1:3。2,可以再增加。&quot;) words &lt;- ws(sents) words ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; &quot;，&quot; &quot;卻&quot; &quot;突然&quot; ## [9] &quot;爆出&quot; &quot;自己&quot; &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來&quot; &quot;體育台&quot; ## [17] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; ## [25] &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公&quot; &quot;有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地&quot; &quot;婆&quot; ## [9] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; The word segmenter ws() returns a list object, each element of which is a word-based vector of the original sentence. 10.8 Define Own Dictionary The performance of Chinese word segmenter depends highly on the dictionary. Texts in different disciplinces may have verry different vocabulary. To prioritize a set of words in a dictionary, we can further ensure the accuracy of the word segmentation. To create a dictionary for ckiptagger, we need to create a list with names = “the new words” and elements = “the weights”. Then we use the python function ckip$construct_dictionary() to create the dictionary Python object, which is the input argument for word segmenter ws(..., recommend_dictionary = ...). # Define new words in own dictionary new_words &lt;- c(&quot;土地婆&quot;,&quot;土地公有政策&quot;,&quot;緯來體育台&quot;) # Transform the `vector` into `list` for Python new_words_py &lt;- as.list(rep(1,length(new_words))) # cf. `list(rep, 1 , length(new_words))` names(new_words_py) &lt;- new_words # To create a dictionary for `construct_dictionary()` # We need a list, with names as the words and list elements as the weights in the dictionary # Create Python `dictionary` object, required by `ckiptagger.wc()` dictionary&lt;-ckip$construct_dictionary(new_words_py) # Segment texts using dictionary words_1 &lt;- ws(sents, recommend_dictionary = dictionary) words_1 ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; ## [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; ## [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; ## [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; ## [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公有政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; ## [6] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; Exercise 10.1 We usually have a list of new words saved in a text file. Can you write a R function, which loads the words in the demo_data/dict-sample.txt into a list named new_words, which can easily serve as the input for ckip$construct_dictionary() to create the python dictionary object? (Note: All weights are default to 1) new_words&lt;-loadDictionary(input = &quot;demo_data/dict-sample.txt&quot;) dictionary&lt;-ckip$construct_dictionary(new_words) # Segment texts using dictionary words_2 &lt;- ws(sents, recommend_dictionary = dictionary) words_2 ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; ## [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; ## [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; ## [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; ## [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公有政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; ## [6] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; 10.9 Beyond Word Boundaries In addition to primitive word segmentation, the ckiptagger provides also the parts-of-speech tags for words and named entity recognitions for the texts. The ckiptagger follows the pipeline below for text processing. Load the models To perform these additional tasks, we need to load the necessary models (pretrained and provided by the CKIP) first as well. They should all have been included in the model directory you unzipped earlier (cf. ./data). # loading other necessary models system.time((pos &lt;- ckip$POS(&quot;./data&quot;))) # 詞性 495s ## user system elapsed ## 4.959 1.353 5.420 system.time((ner &lt;- ckip$NER(&quot;./data&quot;))) # 實體辨識 426s ## user system elapsed ## 5.004 1.809 6.084 POS tagging and NER # Parts-of-speech Tagging pos_words &lt;- pos(words_1) pos_words ## [[1]] ## [1] &quot;Nb&quot; &quot;Nd&quot; &quot;D&quot; &quot;VC&quot; ## [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; ## [9] &quot;VJ&quot; &quot;Nh&quot; &quot;Neu&quot; &quot;Nf&quot; ## [13] &quot;Ng&quot; &quot;P&quot; &quot;Nc&quot; &quot;VC&quot; ## [17] &quot;COMMACATEGORY&quot; &quot;Nh&quot; &quot;D&quot; &quot;VK&quot; ## [21] &quot;Nh&quot; &quot;Ncd&quot; &quot;VJ&quot; &quot;Nc&quot; ## [25] &quot;PERIODCATEGORY&quot; ## ## [[2]] ## [1] &quot;Nc&quot; &quot;Nc&quot; &quot;P&quot; &quot;Nd&quot; ## [5] &quot;Na&quot; &quot;Nb&quot; &quot;D&quot; &quot;VC&quot; ## [9] &quot;DE&quot; &quot;Na&quot; &quot;Nb&quot; &quot;VC&quot; ## [13] &quot;VC&quot; &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;VE&quot; ## [17] &quot;Nh&quot; &quot;D&quot; &quot;D&quot; &quot;Dfa&quot; ## [21] &quot;VH&quot; &quot;VC&quot; &quot;Nc&quot; &quot;VC&quot; ## [25] &quot;COMMACATEGORY&quot; &quot;VG&quot; &quot;Nes&quot; &quot;Nc&quot; ## [29] &quot;D&quot; &quot;Neu&quot; &quot;Nf&quot; &quot;DE&quot; ## [33] &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; ## [37] &quot;PERIODCATEGORY&quot; ## ## [[3]] ## [1] &quot;VH&quot; &quot;QUESTIONCATEGORY&quot; &quot;QUESTIONCATEGORY&quot; &quot;Caa&quot; ## [5] &quot;Nb&quot; &quot;V_2&quot; &quot;Na&quot; &quot;PERIODCATEGORY&quot; ## [9] &quot;PERIODCATEGORY&quot; ## ## [[4]] ## [1] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;Nh&quot; &quot;VK&quot; &quot;T&quot; ## [6] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;D&quot; &quot;D&quot; &quot;VC&quot; ## [11] &quot;Di&quot; &quot;ETCCATEGORY&quot; &quot;ETCCATEGORY&quot; ## ## [[5]] ## [1] &quot;VH&quot; &quot;VJ&quot; &quot;Neu&quot; &quot;Nf&quot; ## [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;Caa&quot; &quot;Neu&quot; ## [9] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; ## [13] &quot;D&quot; &quot;VH&quot; &quot;T&quot; &quot;PERIODCATEGORY&quot; ## [17] &quot;Nep&quot; &quot;SHI&quot; &quot;Na&quot; &quot;DE&quot; ## [21] &quot;Na&quot; &quot;PERIODCATEGORY&quot; ## ## [[6]] ## [1] &quot;Na&quot; &quot;VE&quot; &quot;Neu&quot; &quot;Na&quot; ## [5] &quot;P&quot; &quot;Na&quot; &quot;VG&quot; &quot;Neu&quot; ## [9] &quot;PERIODCATEGORY&quot; &quot;Neu&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; ## [13] &quot;D&quot; &quot;VHC&quot; &quot;PERIODCATEGORY&quot; # Named Entity Recognition ner &lt;- ner(words_1, pos_words) ner ## [[1]] ## {(18, 22, &#39;DATE&#39;, &#39;20年前&#39;), (23, 28, &#39;ORG&#39;, &#39;緯來體育台&#39;), (0, 3, &#39;PERSON&#39;, &#39;傅達仁&#39;)} ## ## [[2]] ## {(42, 45, &#39;ORG&#39;, &#39;參議院&#39;), (7, 9, &#39;DATE&#39;, &#39;今天&#39;), (21, 24, &#39;PERSON&#39;, &#39;趙小蘭&#39;), (56, 58, &#39;ORDINAL&#39;, &#39;第一&#39;), (0, 2, &#39;GPE&#39;, &#39;美國&#39;), (17, 21, &#39;ORG&#39;, &#39;勞工部長&#39;), (2, 5, &#39;ORG&#39;, &#39;參議院&#39;), (60, 62, &#39;NORP&#39;, &#39;華裔&#39;), (11, 13, &#39;PERSON&#39;, &#39;布什&#39;)} ## ## [[3]] ## {(10, 13, &#39;PERSON&#39;, &#39;土地婆&#39;)} ## ## [[4]] ## set() ## ## [[5]] ## {(14, 18, &#39;CARDINAL&#39;, &#39;5.9萬&#39;), (4, 10, &#39;CARDINAL&#39;, &#39;59,000&#39;)} ## ## [[6]] ## {(14, 15, &#39;CARDINAL&#39;, &#39;3&#39;), (4, 6, &#39;CARDINAL&#39;, &#39;1,&#39;), (12, 13, &#39;CARDINAL&#39;, &#39;1&#39;), (16, 17, &#39;CARDINAL&#39;, &#39;2&#39;)} 10.10 Tidy Up the Results sent_corp &lt;- data.frame(id = mapply(rep, c(1:length(sents)), sapply(words_1, length)) %&gt;% unlist, words = do.call(c, words_1), pos = do.call(c, pos_words)) sent_corp Exercise 10.2 How to tidy up the results of ner so that we can include the recognized named entities in the same data frame sent_corp? "],
["structured-corpus.html", "Chapter 11 Structured Corpus 11.1 NCCU Spoken Mandarin 11.2 Connecting SPID to Metadata 11.3 More Socialinguistic Analyses", " Chapter 11 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for lingustic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. 11.1 NCCU Spoken Mandarin CHILDES format 11.1.1 Loading the Corpus NCCU &lt;- readtext(&quot;demo_data/NCCU_SPOKEN.tar.gz&quot;) %&gt;% as_tibble 11.1.2 Line Segmentation NCCU_lines &lt;- NCCU %&gt;% unnest_tokens(line, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) 11.1.3 Metadata vs. Transcript NCCU_lines_meta &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^@&quot;)) NCCU_lines_data &lt;- NCCU_lines %&gt;% filter(str_detect(line, &quot;^[^@]&quot;)) %&gt;% group_by(doc_id) %&gt;% mutate(lineID = row_number()) %&gt;% ungroup %&gt;% separate(line, into = c(&quot;SPID&quot;,&quot;line&quot;), sep=&quot;\\t&quot;) %&gt;% mutate(line2 = line %&gt;% str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) NCCU_lines_data 11.1.4 Word Tokenization NCCU_words &lt;- NCCU_lines_data %&gt;% unnest_tokens(word, line2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words 11.1.5 Word frequencies and Wordcloud NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% select(word, freq) %&gt;% #mutate(freq = log(freq)) %&gt;% wordcloud2::wordcloud2(minSize = 0.5, size=1, shape=&quot;diamonds&quot;) 11.1.6 Concordances # extracting particular patterns NCCU_lines_data %&gt;% filter(str_detect(line2, &quot;覺得&quot;)) 11.1.7 N-grams (Lexical Bundles) ########################## # Chinse ngrams functin # ########################## # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) NCCU_ngrams &lt;- NCCU_lines_data %&gt;% select(-line, -SPID) %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 5, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq 11.2 Connecting SPID to Metadata NCCU_lines_meta NCCU_lines_data # Self-defined function fill_spid &lt;- function(vec){ vec_filled &lt;-vec for(i in 1:length(vec_filled)){ if(vec_filled[i]==&quot;&quot;){ vec_filled[i]&lt;-vec_filled[i-1] }else{ i &lt;- i+1 } #endif }#endfor return(vec_filled) }#endfunc # Please check M005.cha NCCU_lines_data %&gt;% group_by(doc_id) %&gt;% filter(lineID == 1 &amp; SPID==&quot;&quot;) # Remove the typo case NCCU_lines_data_filled &lt;- NCCU_lines_data %&gt;% filter(!(doc_id ==&quot;M005.cha&quot; &amp; lineID==1)) %&gt;% group_by(doc_id) %&gt;% mutate(SPID = str_replace_all(SPID, &quot;[*:]&quot;,&quot;&quot;)) %&gt;% mutate(SPID_FILLED = fill_spid(SPID)) %&gt;% mutate(DOC_SPID = str_c(doc_id, SPID_FILLED, sep=&quot;_&quot;)) %&gt;% ungroup %&gt;% select(doc_id, lineID, line2, DOC_SPID) NCCU_lines_data_filled Based on the metadata of each file hedaer, we can extract demographic information related to each speaker, including their ID, age, gender, etc. NCCU_meta &lt;- NCCU_lines_meta %&gt;% filter(str_detect(line, &quot;^@(id)&quot;)) %&gt;% separate(line, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 11.3 More Socialinguistic Analyses 11.3.1 Check Ngram Distribution By Age Groups NCCU_ngram_with_meta &lt;- NCCU_lines_data_filled %&gt;% unnest_tokens(ngram, line2, token = function(x) vngram_chi(text = x, n = 3, delimiter = &quot;_&quot;)) %&gt;% filter(ngram!=&quot;&quot;) %&gt;% filter(!(str_detect(ngram,&quot;[&lt;a-z:]&quot;))) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_ngram_by_age &lt;- NCCU_ngram_with_meta %&gt;% count(ngram,AGE_GROUP, DOC_SPID) %&gt;% group_by(ngram, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) Below20 Word Cloud Order ggplot barplots by factor frequencies require(wordcloud2) NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(ngram, freq) %&gt;% wordcloud2 NCCU_ngram_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(ngram,freq) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_age_ordered &lt;- NCCU_ngram_by_age %&gt;% group_by(AGE_GROUP) %&gt;% top_n(20, freq) %&gt;% ungroup() %&gt;% arrange(AGE_GROUP, freq) %&gt;% mutate(order=row_number()) # transparency indicates dispersion NCCU_ngram_by_age_ordered %&gt;% ggplot(aes(order, freq, fill = AGE_GROUP, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ AGE_GROUP, scales = &quot;free&quot;) + labs(y = &quot;Ngram Frequency&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_age_ordered$order, labels=NCCU_ngram_by_age_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) 11.3.2 Check Word Distribution of different genders NCCU_ngram_by_gender &lt;- NCCU_ngram_with_meta %&gt;% count(ngram, GENDER, DOC_SPID) %&gt;% group_by(ngram, GENDER) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% filter(dispersion &gt; 1) %&gt;% ungroup NCCU_ngram_by_gender NCCU_ngram_by_gender %&gt;% #filter(dispersion &gt; 10) %&gt;% count(GENDER) %&gt;% ggplot(aes(x=GENDER, y = n, fill=GENDER))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;male&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 NCCU_ngram_by_gender %&gt;% filter(GENDER == &quot;female&quot;) %&gt;% select(ngram,dispersion) %&gt;% wordcloud2 #$$$$$$$$$$$$$ NCCU_ngram_by_gender_ordered &lt;- NCCU_ngram_by_gender %&gt;% group_by(GENDER) %&gt;% top_n(20, dispersion) %&gt;% ungroup() %&gt;% arrange(GENDER,dispersion) %&gt;% mutate(order=row_number()) NCCU_ngram_by_gender_ordered %&gt;% ggplot(aes(order, dispersion, fill = GENDER, alpha = dispersion)) + geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) + facet_wrap( ~ GENDER, scales = &quot;free&quot;) + labs(y = &quot;Ngram Dispersion&quot;, x = NULL) + coord_flip() + scale_x_continuous(breaks=NCCU_ngram_by_gender_ordered$order, labels=NCCU_ngram_by_gender_ordered$ngram, expand = c(0,0))+ theme(text = element_text(family=&quot;Arial Unicode MS&quot;)) "],
["xml.html", "Chapter 12 XML 12.1 BNC Spoken 2014 12.2 Process the Whole Directory of BNC2014 Sample 12.3 Metadata 12.4 BNC2014 for Socialinguistic Variation", " Chapter 12 XML library(tidyverse) library(readtext) library(rvest) library(tidytext) library(quanteda) This chapter shows you how to process the recently released BNC 2014, which is by far the largest representative collection of spoken English collected in UK. For the purpose of our in-class tutorials, I have included a small sample of the BNC2014 in our demo_data. However, the whole dataset is now available via the official website: British National Corpus 2014. Please sign up for the complete access to the corpus if you need this corpus for your own research. 12.1 BNC Spoken 2014 XML is similar to HTML. Before you process the data, you need to understand the structure of the XML tags in the files. Other than that, the steps are pretty much similar to what we have done before. First, we read the XML using read_html(): # read one file at a time corp_bnc&lt;-read_html(&quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Now it is intuitive that our next step is to extract all utterances (with the tag of &lt;u&gt;...&lt;/u&gt;) in the XML file. So you may want to do the following: corp_bnc %&gt;% html_nodes(xpath = &quot;//u&quot;) %&gt;% html_text %&gt;% head ## [1] &quot;\\r\\nanhourlaterhopeshestaysdownratherlate&quot; ## [2] &quot;\\r\\nwellshehadthosetwohoursearlier&quot; ## [3] &quot;\\r\\nyeahIknowbutthat&#39;swhywe&#39;reanhourlateisn&#39;tit?mmI&#39;mtirednow&quot; ## [4] &quot;\\r\\n&quot; ## [5] &quot;\\r\\ndidyoutext--ANONnameM&quot; ## [6] &quot;\\r\\nyeahyeahhewrotebacknobotherlad&quot; See the problem? Using the above method, you lose the word boundary information from the corpus. What if you do the following? corp_bnc %&gt;% html_nodes(xpath = &quot;//w&quot;) %&gt;% html_text %&gt;% head(20) ## [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; ## [8] &quot;rather&quot; &quot;late&quot; &quot;well&quot; &quot;she&quot; &quot;had&quot; &quot;those&quot; &quot;two&quot; ## [15] &quot;hours&quot; &quot;earlier&quot; &quot;yeah&quot; &quot;I&quot; &quot;know&quot; &quot;but&quot; At the first sight, probably it seems that we have solved the problem but we don’t. There are even more problems created: Our second method does not extract non-word tokens within each utterance (e.g., &lt;pause .../&gt;, &lt;vocal .../&gt;) Our second method loses the utterance information (i.e., we don’t know which utterance each word belongs to) Exercise 12.1 Please come up with a way to extract both words and non-word tokens from each utterance. Ideally, the resulting data frame would consist of rows being the utterances, and columns recording the attributes of each autterances. Most importantly, the data frame should record not only the tokens of the utterance but at the same time the token-level attributes of each word/non-word token as well, e.g., the parts-of-speech, duration of pause etc. # xml_to_df function read_xml_bnc2014 &lt;- function(xmlfile){ # read one file at a time corp_bnc&lt;-read_html(xmlfile) # word/pause token nodes node_w &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;(//w)|(//pause)&quot;) # u nodes node_u &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;//u&quot;) node_u_id &lt;- node_u %&gt;% html_attr(name = &quot;n&quot;) node_u_who &lt;- node_u %&gt;% html_attr(name =&quot;who&quot;) node_u_trans &lt;- node_u %&gt;% html_attr(name=&quot;trans&quot;) node_u_whoConfidence &lt;- node_u %&gt;% html_attr(name=&quot;whoConfidence&quot;) # Define function # to extract word-based data frame from each u nodeset extractWords &lt;- function(u_node){ cur_w_nodeset &lt;- u_node %&gt;% html_children() corp_df &lt;- tibble( word = cur_w_nodeset %&gt;% html_text, lemma = cur_w_nodeset %&gt;% html_attr(name=&quot;lemma&quot;, default=&quot;@&quot;), pos = cur_w_nodeset %&gt;% html_attr(name=&quot;pos&quot;, default=&quot;@&quot;), usas = cur_w_nodeset %&gt;% html_attr(name = &quot;usas&quot;, default=&quot;@&quot;), dur = cur_w_nodeset %&gt;% html_attr(name = &quot;dur&quot;, default=&quot;@&quot;)) return(corp_df %&gt;% mutate(word = ifelse(word==&quot;&quot;,&quot;&lt;PAUSE&gt;&quot;,word))) }# endfunc extractWords(node_u[[1]]) -&gt;x x x$word[4]==&quot;&quot; # create tidy word-based df for current xml file corp_df &lt;- tibble( node_u_id, node_u_who, node_u_trans, node_w = map(node_u, extractWords) ) %&gt;% unnest_tokens(word, node_w, token = function(x) map(x, function(y) str_c(y$word, y$lemma, y$pos, y$usas, y$dur, sep=&quot;_&quot;))) %&gt;% mutate(xmlid = basename(xmlfile)) %&gt;% separate(word, into = c(&quot;word&quot;,&quot;lemma&quot;,&quot;pos&quot;,&quot;usas&quot;,&quot;dur&quot;), sep=&quot;_&quot;) corp_df } # endfunc 12.2 Process the Whole Directory of BNC2014 Sample 12.2.1 Define Function In Section 12.1, if you have figured how to extract utterances as well as token-based information from the xml file, you can easily wrap the whole procedure as one function. With this function, we can perform the same procedure to all the xml files of the BNC2014. For example, let’s assume that we have defined a function: read_xml_bnc2014 &lt;- function(xml){ ... } This function takes one xml file as an argument and return a data frame, consisting of utterances and other relevant information from the xml. read_xml_bnc2014(xmlfile = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Exercise 12.2 Now your job is to write this function, read_xml_BNC2014(). 12.2.2 Process the all files in the Directory Now we utilize the self-defined function, read_xml_BNC2014(), and process all xml files in the demo_data/corp-bnc-spoken2014-sample/. Also, we combine the data.frame returned from each xml into a bigger one, i.e., corp_bnc_df: s.t &lt;- Sys.time() bnc_flist &lt;- dir(&quot;demo_data/corp-bnc-spoken2014-sample/&quot;,full.names = T) corp_bnc_df &lt;- map(bnc_flist, function(x) read_xml_bnc2014(x)) %&gt;% do.call(rbind, .) Sys.time()-s.t ## Time difference of 1.003302 mins It takes about one and half minute to process the sample directory. You may store this corp_bnc_df data frame output for later use so that you don’t have to process the XML files every time you work on BNC2014. write_csv(corp_bnc_df, &quot;demo_data/corp_bnc_df.csv&quot;,col_names = T) 12.3 Metadata The best thing about BNC2014 is its rich demographic information relating to the settings and speakers of the conversations collected. The whole corpus comes with two metadata sets: bnc2014spoken-textdata.tsv: metadata for each text transcript bnc2014spoken-speakerdata.tsv: metadata for each speaker ID These two metadata sets allow us to get more information about each transcript as well as the speakers in those transcripts. 12.3.1 Text Metadata bnc_text_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-textdata.tsv&quot;, col_names = FALSE) bnc_text_meta_names &lt;-read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-text.txt&quot;, skip =2, col_names = F) names(bnc_text_meta) &lt;- c(&quot;textid&quot;, bnc_text_meta_names$X2) bnc_text_meta 12.3.2 Speaker Metadata bnc_sp_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-speakerdata.tsv&quot;, col_names = F) bnc_sp_meta_names &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-speaker.txt&quot;, skip = 3, col_names = F) names(bnc_sp_meta) &lt;- c(&quot;spid&quot;, bnc_sp_meta_names$X2) bnc_sp_meta 12.4 BNC2014 for Socialinguistic Variation BNC2014 was born for the study of socialinguistic variation. Here we show you some naitve examples, but you should get the ideas. 12.4.1 Word Frequency vs. Gender Now we are ready to explore the gender differences in language. 12.4.1.1 Preprocessing To begin with, there are some utterances with no words at all. We probably like to remove these tokens. #corp_bnc_df &lt;- read_csv(&quot;demo_data/corp_bnc_df.csv&quot;) corp_bnc_df &lt;- corp_bnc_df %&gt;% filter(!is.na(utterance)) corp_bnc_df 12.4.1.2 Target Structures Let’s assume that we like to know which verbs are most frequently used by men and women. corp_bnc_verb_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame corp_bnc_verb_gender ## Problems ### Use own tokenization function ### Default tokenization increase the number of tokens quite a bit word_by_gender &lt;- corp_bnc_verb_gender %&gt;% unnest_tokens(word, utterance, to_lower = F,token = function(x) strsplit(x, split = &quot;\\\\s&quot;)) %&gt;% # tokenize utterance into words filter(str_detect(word, &quot;[^_]+_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # include VERB only mutate(word = str_replace(word, &quot;_(JJ)|(JJR)|(JJT)&quot;,&quot;&quot;)) %&gt;% # remove pos tags #filter(!str_detect(word, &quot;^&#39;&quot;)) %&gt;% # remove contraction #filter(!word %in% stopwords(&quot;en&quot;)) %&gt;% count(gender, word) %&gt;% group_by(gender) %&gt;% top_n(200,n) %&gt;% ungroup Female wordcloud require(wordcloud2) word_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) Male wordcloud word_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) 12.4.2 Degree ADV + ADJ corp_bnc_pattern_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;[^_]+_RG [^_]+_JJ&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame pattern_by_gender &lt;- corp_bnc_pattern_gender %&gt;% unnest_tokens(pattern, utterance, to_lower = F, token = function(x) str_extract_all(x, &quot;[^_ ]+_RG [^_ ]+_JJ &quot;)) %&gt;% mutate(pattern = pattern %&gt;% str_trim %&gt;% str_replace_all(&quot;_[^_ ]+&quot;,&quot;&quot;)) %&gt;% # remove pos tags separate(pattern, into = c(&quot;ADV&quot;,&quot;ADJ&quot;), sep = &quot;\\\\s&quot;) %&gt;% count(gender, ADJ) %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup corp_bnc_pattern_gender %&gt;% #select(n, utterance) %&gt;% filter(n == 235) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(minSize = 0.5, size = 3) 12.4.3 Trigrams trigram_by_gender &lt;- corp_bnc_df %&gt;% mutate(utterance = utterance %&gt;% str_replace_all(&quot;_[^ ]+&quot;,&quot;&quot;)) %&gt;% unnest_tokens(trigram, utterance, token = &quot;ngrams&quot;, n =3, ngram_delim = &quot;_&quot;) %&gt;% filter(!is.na(trigram)) %&gt;% left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) %&gt;% count(gender, trigram) %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup Exercise 12.3 Remove stopwords from the frequency list of words (unigrams). Exercise 12.4 Include dispersion metrics in the n-gram freuency list. trigram_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(trigram, n) %&gt;% wordcloud2(minSize = 0.5, size = 3, rotateRatio = 0.3) trigram_by_gender %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(trigram, n) %&gt;% wordcloud2(minSize = 0.5, size = 1.5, rotateRatio = 0.8) "],
["vector-space-representation.html", "Chapter 13 Vector Space Representation 13.1 Data Processing Flowchart 13.2 Document-Feature Matrix (dfm) 13.3 Defining Feature in dfm 13.4 Feature Selection 13.5 Applying DFM", " Chapter 13 Vector Space Representation library(tidyverse) library(quanteda) 13.1 Data Processing Flowchart 13.2 Document-Feature Matrix (dfm) Two ways to create Dcument-Feature-Matrix: create dfm based on an corpus object create dfm based on an token object For English data, quanteda can take care of the word tokenization fairly well so you can create dfm directly from corpus. However, for Chinese data, it is suggested to create your own corpus token object first, and then feed it to dfm() to create dfm for your corpus. In this section, we demonstrate the document-feature-matrix using the English data we discussed in Chapter 5, the data_corpus_inaugural preloaded in the library quanteda. corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% dfm For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) ## [1] &quot;corpus&quot; 13.3 Defining Feature in dfm What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the following example, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and corp_us_dfm[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] Distributional properties like co-occurrences are very important information in corpus linguistics. Most of the studies in corpus linguistics adopt an implicit distributional hypothesis, which can be illustrated by a few famous quotes: You shall know a word by the comany it keeps. (Firth, 1957, p.11) [D]ifference of meaning correlates with difference of distribution. (Harris, 1970, p.785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves (De Deyne et al. 2016) So in our current context, the idea is that if two documents have similar sets of linguistic units popping up in them, they are more likely to be similar in meaning as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with other documents (i.e., other rows). This is essentially a vector computation (cf. Figure 13.1): the document in each row is represented as a vector of N dimensional space. The size of N depends on the number of linguistic units that are included in the analysis. Figure 13.1: Example of Document-Feature Matrix 13.4 Feature Selection A dfm may be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered when creating a dfm: The granularity of the linguistic unit Stopwords The distributional cut-offs of the linguistic unit 13.4.1 Determining Linguistic Granularity In our previous example, we include only words, i.e., unigrams, as our features in the dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: corp_us_dfm_ngram &lt;- corp_us %&gt;% dfm(ngrams = 2) corp_us_dfm_ngram[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: corp_us_dfm_stem &lt;- corp_us %&gt;% dfm(stem = T) corp_us_dfm_stem[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (38.0% sparse) and 4 docvars. ## features ## docs fellow-citizen of the senat and hous repres : among ## 1789-Washington 1 71 116 1 48 2 2 1 1 ## 1793-Washington 0 11 13 0 2 0 0 1 0 ## 1797-Adams 3 140 163 1 130 3 3 0 4 ## 1801-Jefferson 2 104 130 0 81 0 1 1 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 7 ## 1809-Madison 1 69 104 0 43 0 1 0 0 ## features ## docs vicissitud ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 1 ## [ reached max_ndoc ... 4 more documents ] You need to decide which type of linguistic units is more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, there is no rule for how to do this. Exercise 13.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 13.4.2 Stopwords There are words that are not so informative in telling us the similarity and difference between the documents because they almost occur in every document of the corpus, but carray little (refential) semantic contents. These words are usually the function words, such as and, the, of. Also, there are tokens that usually carry limited semantic contents, such as numbers and punctuation. Therefore, it is not uncommon that analysts sometimes create a list of words to be removed from the dfm. These words are referred to as stopwords. The library quanteda has determined a default English stopword list, i.e., stopwords(&quot;en&quot;). When creating the dfm object, we can further specify a few parameters: remove_punct: remove all punctuation tokens remove: remove all words specified in the character vector here corp_us_dfm_stp &lt;- corp_us %&gt;% dfm(remove_punct = T, remove = stopwords(&quot;en&quot;)) corp_us_dfm_stp[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (60.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among ## 1789-Washington 1 1 2 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 ## 1801-Jefferson 2 0 0 0 1 ## 1805-Jefferson 0 0 0 0 7 ## 1809-Madison 1 0 0 0 0 ## features ## docs vicissitudes incident life event filled ## 1789-Washington 1 1 1 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 0 0 2 0 0 ## 1801-Jefferson 0 0 1 0 0 ## 1805-Jefferson 0 0 2 0 0 ## 1809-Madison 0 0 1 0 1 ## [ reached max_ndoc ... 4 more documents ] We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm) ## [1] 9399 nfeat(corp_us_dfm_ngram) ## [1] 9399 nfeat(corp_us_dfm_stem) ## [1] 5584 nfeat(corp_us_dfm_stp) ## [1] 9248 13.4.3 Distributional Cut-offs for Features Depending on the granularity of the linguistic units you consider, you may get a considerable number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the word occurs only once in the corpus (i.e., hapax legomenon), these words can be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurrs in all documents, they won’t help much as well. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate something else. Therefore, sometimes we can control the document frequency of the features (i.e., in how many different texts does the feature occur?) Other self-defined weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a text d, the significance of this n may be connected to: the document size of d the total number of w Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. In the following demo, we adopt a few simple distrubtional criteria: we remove stopwords and punctuations we remove words whose freqency &lt; 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% dfm(remove = stopwords(&quot;en&quot;), remove_punct = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) corp_us_dfm_trimmed[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (52.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among life event ## 1789-Washington 1 1 2 2 1 1 2 ## 1793-Washington 0 0 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 2 0 ## 1801-Jefferson 2 0 0 0 1 1 0 ## 1805-Jefferson 0 0 0 0 7 2 0 ## 1809-Madison 1 0 0 0 0 1 0 ## features ## docs greater order received ## 1789-Washington 1 2 1 ## 1793-Washington 0 0 0 ## 1797-Adams 0 4 0 ## 1801-Jefferson 1 1 0 ## 1805-Jefferson 0 3 0 ## 1809-Madison 0 0 0 ## [ reached max_ndoc ... 4 more documents ] 13.5 Applying DFM 13.5.1 Wordcloud With a dfm of a corpus, we can quickly explore the nature of this corpus by examining the top features of this corpus: topfeatures(corp_us_dfm_trimmed) ## people government us can upon must great ## 574 564 478 471 371 366 340 ## may states shall ## 338 333 314 Or we can visualize the distrubtion of these top features using the wordcloud: set.seed(123) corp_us_dfm_trimmed %&gt;% textplot_wordcloud(min_count = 60, rotation = .35) 13.5.2 Document Similarity As shown in 13.1, with the N-dimensional vector representation of each document, we can easily compute the mathematical similarities between two documents. Based on the similarities, we can further examine how different documents may cluster together in terms of their lexical similarities. corp_us_dist &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist() corp_us_hist &lt;- corp_us_dist %&gt;% as.dist %&gt;% hclust plot(corp_us_hist,hang = -1, cex = 0.7) 13.5.3 Feature Similarity What if we transpose a document-feature matrix? A transposed dfm would be a feature-document matrix. This is an interesting structure because we then can do the same tricks with all the features in the corpus. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent words are similar. # convert `dfm` to `fcm` corp_us_fcm &lt;- corp_us_dfm_trimmed %&gt;% fcm # select top 30 features corp_us_topfeatures &lt;- names(topfeatures(corp_us_fcm, 30)) # plot network fcm_select(corp_us_fcm, pattern = corp_us_topfeatures) %&gt;% textplot_network(min_freq = 0.5) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. Exercise 13.2 Please create a network of the top 30 bigrams based on the corpus corp_us. The criteria for bigrams selection are as follows: Include bigrams that consist of alphanumeric characters only (no punctuations) Include bigrams whose frequency &gt;= 10 and docfreq &gt;= 5 but &lt;= half number of the corpus "],
["vector-space-representation-ii.html", "Chapter 14 Vector Space Representation II 14.1 A Quick View 14.2 Loading the Corpus 14.3 Semgentation 14.4 Corpus Metadata 14.5 Document-Feature Matrix 14.6 Wordcloud 14.7 Document Similarity 14.8 Feature Similarity", " Chapter 14 Vector Space Representation II library(tidyverse) library(quanteda) library(readtext) library(jiebaR) Figure 14.1: Corpus Processing Flowchart In Chapter 13, we have demonstrated the potential of a vector representation of documents with the English data. Here, we would like to look at the Chinese data in more detail. In the corpus data processing flowchart, as repeated below (Figure 14.1), we need to deal with the word segmentation with the Chinese data. This prevents us from creating a dfm directly from a corpus object because the default internal word tokenization in quanteda is not optimized for non-English languages. In this chapter, we will be using the dataset TaiwanPresidentalSpeech.zip in our demo_data directory. Please make sure that you have downloaded the dataset from demo_data. 14.1 A Quick View For Chinese data, the major preprocessing steps have been highlighted in Figure 14.1: First read in the corpus using readtext() and create corpus data.frame object Subset the text column of the corpus data.frame for word segmentation Tokenize the texts using segment() in jiebaRand convert the output into a token object using as.token(). A token object is properly defined in quanteda, with many similar functions as a corpus object. This is the most important trick with the Chinese data. When you utilize many different libaries in R for your tasks, one thing you need to keep in your mind is that you need to fully understand what kind of objects you are dealing with. That is, you need to keep track of every variable you create in your script in terms of their object type/class. A vector is different from a list; a list is different from a token. Also, some of the object classes are predefined in R (e.g., vector, data.frame) while others are defined in specific libararies (e.g., corpus, token, dfm). As a habit, always check your object class (i.e., class()). 14.2 Loading the Corpus corp_tw &lt;- readtext(file=&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% as_tibble class(corp_tw) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; corp_tw %&gt;% mutate(text = str_sub(text, 1,20)) NB: readtext() creates a readtext or data.frame object. Following the tidy principle, we convert everything into tibble. 14.3 Semgentation Three important sub-steps in this part: initialize word segmenter, where a user dictionary is defined (Always use own dictionary to improve the performance of word segmentation) subset the text column of corp_tw tokenize the texts and convert the output into a quanteda-compatible object, token # initialize segmenter chi_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user.txt&quot;) corp_tw_tokens &lt;- corp_tw$text %&gt;% segment(jiebar = chi_seg) %&gt;% as.tokens class(corp_tw_tokens) ## [1] &quot;tokens&quot; corp_tw_tokens$text14[1:10] ## NULL 14.4 Corpus Metadata When we subset the texts from corp_tw for word segmentation, all the metadata connected to the texts did not go with the texts. So the corp_tw_tokens did not have any metadata information. All you have is some arbitrary index to each text. docvars(corp_tw_tokens) So here we extract metadata information from the original filenames of each text stored in the corp_tw, and attach this metadata to corp_tw_tokens. corp_tw_meta &lt;-corp_tw %&gt;% dplyr::select(-text) %&gt;% separate(doc_id, into = c(&quot;YEAR&quot;,&quot;TERM&quot;,&quot;PRESIDENT&quot;),sep = &quot;_&quot;) %&gt;% mutate(PRESIDENT = str_replace(PRESIDENT, &quot;.txt&quot;,&quot;&quot;)) corp_tw_meta docvars(corp_tw_tokens) &lt;- corp_tw_meta 14.5 Document-Feature Matrix Now that we have a token version of our corpus, we can create dfm using the dfm() in quanteda. Also, we can take care of the feature selection (cf. 13.4) using functions like dfm_trim(), dfm_select(). corp_tw_dfm &lt;- corp_tw_tokens %&gt;% dfm(reove_punc = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 2, max_docfreq = 10, docfreq_type = &quot;count&quot;) class(corp_tw_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; corp_tw_dfm[1:5, 1:10] ## Document-feature matrix of: 5 documents, 10 features (32.0% sparse) and 3 docvars. ## features ## docs 中正 國民大會 國父 環境 以及 到 以來 知道 自己 此 ## text1 5 1 3 2 2 1 1 2 1 2 ## text2 4 4 1 1 2 2 0 3 0 0 ## text3 5 1 1 0 0 5 0 4 4 0 ## text4 7 3 7 0 0 0 1 1 2 1 ## text5 5 1 1 0 0 0 0 0 0 3 14.6 Wordcloud require(wordcloud2) require(wordcloud) top_features &lt;- corp_tw_dfm %&gt;% topfeatures(100) word_freq &lt;- data.frame(word = names(top_features), freq = top_features) word_freq %&gt;% wordcloud2() 14.7 Document Similarity corp_tw_dist &lt;- corp_tw_dfm %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_dist corp_tw_hist &lt;- corp_tw_dist %&gt;% as.dist %&gt;% hclust hist_labels &lt;- str_c(docvars(corp_tw_dfm,&quot;YEAR&quot;), docvars(corp_tw_dfm,&quot;PRESIDENT&quot;), sep=&quot;_&quot;) plot(corp_tw_hist, hang = -1, cex = 1.2, label = hist_labels) 14.8 Feature Similarity # convert `dfm` to `fcm` corp_tw_fcm &lt;- corp_tw_dfm %&gt;% fcm # select top 30 features corp_tw_topfeatures &lt;- names(topfeatures(corp_tw_fcm, 50)) # plot network library(showtext) font_add(&quot;Arial Unicode MS&quot;, &quot;Arial Unicode.ttf&quot;) ## Automatically use showtext to render plots showtext_auto(enable = TRUE) #par(family = &quot;Arial Unicode MS&quot;) fcm_select(corp_tw_fcm, pattern = corp_tw_topfeatures) %&gt;% textplot_network(min_freq = 0.5) -&gt;g ggsave(&quot;test.png&quot;, g) Exercise 14.1 Create the network of top 30 bigrams for the corpus corp_tw. The critera for bigrams selection are as follows: include bigrams whose frequency &gt;= 10 and docfreq &gt;=5 Exercise 14.2 There is an interesting application. When we analyze the document similarity, we create the graph of a dendrogram using hierarchical cluster analysis. In fact, document relations can also be represented by a network as well, as we do with the features in 13.5.3. How could you make use of the function textplot_network() in quanteda to create a network of the presidents? Please create a similar president network as shown below. "],
["references.html", "Chapter 15 References", " Chapter 15 References "]
]
