[["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resources 3.6 Final Remarks", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections (cf. Structured Corpus and XML), chances are that sometimes you still need to collect your own data for a particular research question. But please note that when you are creating your own corpus for specific research questions, always pay attention to the three important criteria: representativeness, authenticity, and size. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. If you are new to tidyverse R, please check its official webpage for learning resources. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure The HyperText Markup Language, or HTML is the standard markup language for documents designed to be displayed in a web browser. 3.1.1 HTML Syntax To illustrate the structure of the HTML, please download the sample html file from: demo_data/data-sample-html.html and first open it with your browser. &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? This is how to get back to the course page: &lt;a href=&quot;https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/&quot;, target=&quot;_blank&quot;&gt;ENC2036&lt;/a&gt;. &lt;/p&gt; &lt;h1&gt; Contents of the Page &lt;/h1&gt; &lt;p&gt; Anything you can say about the page.....&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; An HTML document includes several important elements (cf. Figure 3.1): DTD: document type definition which informs the browser about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= \"index.html\"&gt; Homepage &lt;/a&gt;). They are expressed as name = \"value\" pairs. Figure 3.1: Syntax of An HTML Tag Element An HTML document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the webpage textual contents would go into the &lt;body&gt; part. Most of the web-related codes and metadata (e.g., javascripts, CSS) are often included in the &lt;head&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in Figure 3.2. Figure 3.2: Tree Structure of An HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags and elements. However, in order to scrape the textual data from the Internet, you need to know at least from which parts of HTML elements you need your textual data from on the web pages. Usually, before you scrape the data from the webpage, bear the following questions in mind: From which HTML elements/tags would you like to extract the data for corpus construction? Do you need the textual content of the HTML element? Do you need a specific attribute of the HTML element? 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The idea is that CSS specifies the formats/styles of the HTML elements. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } You probably would wonder how to link a set of CSS style definitions to an HTML document. There are in general three ways: inline, internal and external. You can learn more about this in W3School.com. Here I will show you an example of the internal method. Below is a CSS style definition for &lt;h1&gt;. h1 { color: red; margin-bottom: 2em; } We can embed this within a &lt;style&gt;...&lt;/style&gt; element. Then you put the entire &lt;style&gt; element under &lt;head&gt; of the HTML file you would like to style. &lt;style&gt; h1 { color: red; margin-bottom: 1.5em; } &lt;/style&gt; After you include the &lt;style&gt; in the HTML file, refresh the web page to see if the CSS style works. 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In the following demonstration, the text data scraped from the PTT forum is presented as it is without adjustment. However, please note that the language on PTT may strike some readers as profane, vulgar or even offensive. library(tidyverse) library(rvest) In this tutorial, let’s assume that we like to scrape texts from PTT Forum. In particular, we will demonstrate how to scrape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create an session() (like we open a browser linking to the page) gossiping.session &lt;- session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created session() and create another session. gossiping &lt;- session_submit( x = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html Status: 200 Type: text/html; charset=utf-8 Size: 25604 Now our html sesseion, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest [1] 39392 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% session_jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748375.A.5E5.html&quot; [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748382.A.8B8.html&quot; [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748390.A.008.html&quot; [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748414.A.D7C.html&quot; [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748419.A.49E.html&quot; [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748419.A.380.html&quot; [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748450.A.F0A.html&quot; [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748455.A.C0C.html&quot; [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748553.A.EAE.html&quot; [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748574.A.00A.html&quot; [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748580.A.9E4.html&quot; [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748584.A.403.html&quot; [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748586.A.B11.html&quot; [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748588.A.3D1.html&quot; [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748590.A.406.html&quot; [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748598.A.280.html&quot; [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748599.A.E2C.html&quot; [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748616.A.DAA.html&quot; [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748618.A.FCE.html&quot; [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748689.A.676.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% session_jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. Because we are interested in the metadata and the contents of each article, now the question is: where are they in the HTML? We need to go back to the source page of the article HTML again: After a closer inspection of the article HTML, we know that: The metadata of the article are included in &lt;span&gt; tag elements, belonging to the class class=\"article-meta-value\" The contents of the article are included in the &lt;div&gt; element, whose ID is ID=\"main-content\" Now we are ready to extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() article.header [1] &quot;taiwannext (2016綠營重新執政 )&quot; [2] &quot;Gossiping&quot; [3] &quot;[問卦]台灣對中國貿易順差多 對中國統戰有利嗎&quot; [4] &quot;Fri Feb 25 08:19:33 2022&quot; The metadata of each PTT article in fact includes four pieces of information: author, board name, title, post time. The above code retrieves directly the values of these metadata. We can retrieve the tags of these metadata values as well: temp.html %&gt;% html_nodes(&quot;span.article-meta-tag&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() [1] &quot;作者&quot; &quot;看板&quot; &quot;標題&quot; &quot;時間&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author [1] &quot;taiwannext&quot; article.title [1] &quot;[問卦]台灣對中國貿易順差多 對中國統戰有利嗎&quot; article.datetime [1] &quot;Fri Feb 25 08:19:33 2022&quot; Now we extract the main contents of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content [1] &quot;八卦有些人常酸說台灣反中國但是對中國貿易順差創新高，\\n\\n台灣賺敵人錢，拿中國賺來的錢發展軍事外交，看起來對台灣是有利吧？\\n\\n不過台灣對中國貿易順差多，\\n\\n 對中國統戰有利嗎？\\n\\n一直讓台灣賺錢買高品質武器\\n\\n這樣對中國解放軍打台灣好像不利吧？\\n\\n還是這其實是失敗的統戰？\\n\\n--不然台灣這5年反中，中國這幾年反而買更多台灣產品的原因？&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt;. These children &lt;div&gt; or &lt;span class=“f2”&gt; of the &lt;div id = “main-content”&gt; include the push comments (推文) of the article, which are not the main content of the article. Now we combine all information related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push {xml_nodeset (10)} [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [4] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [5] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... [6] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... [7] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [8] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... [9] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... [10] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag [1] &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;→&quot; &quot;推&quot; &quot;噓&quot; &quot;→&quot; &quot;→&quot; &quot;噓&quot; &quot;→&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author [1] &quot;uku&quot; &quot;deepdish&quot; &quot;tontonplus&quot; &quot;javis3021&quot; &quot;CD133&quot; [6] &quot;deepdish&quot; &quot;CD133&quot; &quot;CD133&quot; &quot;iso2288&quot; &quot;iso2288&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content [1] &quot;: 就傻&quot; [2] &quot;: 你看抖音要付錢嗎？zzzzzzzzz&quot; [3] &quot;: 人家也買你晶片做武器啊，笑死&quot; [4] &quot;: 你可以問看看塔綠斑看法&quot; [5] &quot;: 台灣這一點錢讓利都ok&quot; [6] &quot;: 不知道免費的最貴齁&quot; [7] &quot;: 塔綠班格局就這麼大，認為台灣賺好大&quot; [8] &quot;: 也就塔綠班説得出是中國需要台灣……&quot; [9] &quot;: 中國光對美國順差去年就3553億鎂了&quot; [10] &quot;: 台灣去年對中國順差1716億鎂&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime [1] &quot;180.176.146.235 02/25 08:20&quot; &quot;220.134.89.190 02/25 08:21&quot; [3] &quot;175.182.21.227 02/25 08:21&quot; &quot;39.10.11.60 02/25 08:21&quot; [5] &quot;163.15.166.183 02/25 08:22&quot; &quot;220.134.89.190 02/25 08:23&quot; [7] &quot;163.15.166.183 02/25 08:23&quot; &quot;163.15.166.183 02/25 08:24&quot; [9] &quot;1.169.176.171 02/25 08:26&quot; &quot;1.169.176.171 02/25 08:28&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to collect text data in large amounts: For each index page, we need to extract all the article hyperlinks of the page. For each article hyperlink, we need to extract the article content, metadata, and the push comments. So, it would be great if we can wrap these two routines into two functions. 3.3.1 extract_art_links() extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. It returns a vector of article links. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% session_jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } For example, we can extract all the article links from the most recent index page: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Get all article links from the most recent index page cur_art_links &lt;-extract_art_links(cur_index_page, gossiping) cur_art_links [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748375.A.5E5.html&quot; [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748382.A.8B8.html&quot; [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748390.A.008.html&quot; [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748414.A.D7C.html&quot; [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748419.A.49E.html&quot; [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748419.A.380.html&quot; [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748450.A.F0A.html&quot; [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748455.A.C0C.html&quot; [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748553.A.EAE.html&quot; [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748574.A.00A.html&quot; [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748580.A.9E4.html&quot; [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748584.A.403.html&quot; [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748586.A.B11.html&quot; [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748588.A.3D1.html&quot; [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748590.A.406.html&quot; [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748598.A.280.html&quot; [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748599.A.E2C.html&quot; [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748616.A.DAA.html&quot; [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748618.A.FCE.html&quot; [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1645748689.A.676.html&quot; 3.3.2 extract_article_push_tables() extract_article_push_tables(): This function takes an article link link as the argument and extracts the metadata, textual contents, and pushes of the article. It returns a list of two elements—article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% session_jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge push table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc For example, we can get the article and push tables from the first article link: extract_article_push_tables(cur_art_links[1]) $article.table # A tibble: 1 × 5 datetime title author content url &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Fri Feb 25 08:19:33 2022 [問卦]台灣對… taiwan… &quot;八卦有些人常酸說… https://www.… $push.table # A tibble: 10 × 5 tag author content datetime url &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 推 uku : 就傻 180.176.14… https://ww… 2 → deepdish : 你看抖音要付錢嗎？zzzzzzzzz 220.134.89… https://ww… 3 推 tontonplus : 人家也買你晶片做武器啊，笑死 175.182.21… https://ww… 4 → javis3021 : 你可以問看看塔綠斑看法 39.10.11.6… https://ww… 5 推 CD133 : 台灣這一點錢讓利都ok 163.15.166… https://ww… 6 噓 deepdish : 不知道免費的最貴齁 220.134.89… https://ww… 7 → CD133 : 塔綠班格局就這麼大，認為台灣賺好大 163.15.166… https://ww… 8 → CD133 : 也就塔綠班説得出是中國需要台灣…… 163.15.166… https://ww… 9 噓 iso2288 : 中國光對美國順差去年就3553億鎂了 1.169.176.… https://ww… 10 → iso2288 : 台灣去年對中國順差1716億鎂 1.169.176.… https://ww… 3.3.3 Streamline the Codes Now we can simplify our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scrape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # number of articles on this index page length(ptt_data) [1] 20 # Check the first contents of 1st hyperlink ptt_data[[1]]$article.table ptt_data[[1]]$push.table Finally, the last thing we can do is to combine all article tables from each index page into one; and all push tables into one for later analysis. # Merge all article.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows # Merge all push.tables into one push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph summarizes our work flowchart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scraped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resources Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the sustainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata representation of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Technical parts for corpus creation (cf. Sinclair’s Appendix) 3.6 Final Remarks Please pay attention to the ethical aspects involved in the process of web crawling (esp. with personal private matters). If the website has their own API built specifically for one to gather data, use it instead of scraping. Always read the terms and conditions provided by the website regarding data gathering. Always be gentle with the data scraping (e.g., off-peak hours, spacing out the requests) Value the data you gather and treat the data with respect. Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Your script may look something like: # I define my own `scrapePTT()` function: # ptt_url: specify the board to scrape texts from # num_index_page: specify the number of index pages to be scraped # return: list(article, push) PTT_data &lt;-scrapePTT(ptt_url = &quot;https://www.ptt.cc/bbs/Gossiping&quot;, num_index_page = 3) PTT_data$article %&gt;% head PTT_data$push %&gt;% head # corpus size PTT_data$article$content %&gt;% nchar %&gt;% sum [1] 16955 Exercise 3.3 Please choose a website (other than PTT) you are interested in and demonstrate how you can use R to retrieve textual data from the site. The final scraped text collection could be from only one static web page. This purpose of this exercise is to show that you know how to parse the HTML structure of the web page and retrieve the data you need from the website. References Munzert, S., Rubba, C., Meißner, P., &amp; Nyhuis, D. (2014). Automated data collection with R: A practical guide to web scraping and text mining. John Wiley &amp; Sons. "]]
