[
["creating-corpus.html", "Chapter 2 Creating Corpus 2.1 HTML Structure 2.2 Web Crawling 2.3 Functional Programming 2.4 Save Corpus", " Chapter 2 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. # Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;, &quot;stringr&quot;, &quot;jiebaR&quot;, &quot;tmcn&quot;, &quot;RCurl&quot;)) library(tidyverse) library(rvest) # Packages needed for further text processing # library(jiebaR) # library(tmcn) #library(RCurl) 2.1 HTML Structure 2.1.1 HTML Syntax &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; A html document includes several important elements (cf. 2.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 2.1: Syntax of A HTML Tag Element A html document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in 2.2. Figure 2.2: Tree Structure of A HTML Document 2.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 2.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 2.1.4 HTML + CSS Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 2.2 Web Crawling In this tutorial, let’s assume that we like to scape texts from PTT. In particular, we want to extract texts from the Gossiping board. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html session (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 11234 Now gossiping should be on the front page of the Gossiping board. Now we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% str_extract(&quot;[0-9]+&quot;) %&gt;% as.numeric() page.latest ## [1] 39191 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295761.A.436.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295832.A.E89.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295845.A.02E.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295884.A.7AD.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295924.A.0A5.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295951.A.588.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295985.A.534.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576295994.A.DCC.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296067.A.DFE.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296143.A.CB2.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296165.A.3EB.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296219.A.322.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296219.A.084.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296219.A.924.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296341.A.305.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296448.A.B85.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296518.A.3D9.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296542.A.CD0.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296602.A.323.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1576296778.A.7E3.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. We are ready to extract article information. We first extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% html_text() article.header ## [1] &quot;electronicyi (電子益)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;Re: [新聞] 「學姊」疑遭市府顧問性騷案 北市府調查&quot; ## [4] &quot;Sat Dec 14 11:55:58 2019&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;electronicyi&quot; article.title ## [1] &quot;Re: [新聞] 「學姊」疑遭市府顧問性騷案 北市府調查&quot; article.datetime ## [1] &quot;Sat Dec 14 11:55:58 2019&quot; Now we extract the main content of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) article.content ## [1] &quot;請問一下\\n\\n為什麼同樣的一件事情自由時報的陳璟民記者所報導的\\n\\n貼上八卦的兩篇會不太一樣呢？\\n\\n首先是這一篇: 1.媒體來源:: 自由時報:: 2.記者署名: 記者陳璟民／台北報導:: 3.完整新聞標題:: 學姊疑遭性騷案 北市府調查結果不公開:: 4.完整新聞內文:: 台灣民眾黨不分區立委提名人黃瀞瑩，在台北市政府副發言人任內，疑遭時任市府顧問劉: 嘉仁性騷擾案，市府經過月餘調查，昨由副市長黃珊珊發布新聞訊息表示，市府資訊局性: 騷擾委員會已完成調查報告，交由人事單位依法辦理，因為案件涉及隱私，除當事人及主: 管機關，不宜公開調查結果。:: 記者撥打黃瀞瑩、劉嘉仁手機，都跳進語音信箱，無法得知他們的回應。這起性騷擾疑雲: 發生在八、九月間，據傳幕後原因是市長、民眾黨主席柯文哲與核心幕僚蔡壁如，希望黃: 瀞瑩點頭代表民眾黨參選區域立委，由劉嘉仁設法說服黃瀞瑩，劉以討論行程為由，找黃: 到顧問室談，還把門帶上，另外，劉還頻繁用通訊軟體LINE傳訊息給黃，讓黃覺得很煩、: 不堪其擾，事情經府內人士向上反映，柯才有所處理。:: 根據月前市議會詢答、媒體訪問時柯文哲的說法，「當時聽一聽，直覺應該是工作上有摩: 擦，職務上隔離就好了」，並未落實處理性騷案的標準作業程序，送請性別工作平等委員: 會調查，事情曝光後引發非議。:: 十一月初風波爆開，劉嘉仁當月四日「辭職明志」，後來被發現回任北市立聯合醫院當總: 院長黃勝堅特助；黃瀞瑩續任副發言人，直到排進台灣民眾黨不分區立委名單，並經中央: 選舉委員會審定資格通過後，在十二月初辭職。:: 5.完整新聞連結 (或短網址):: ※ 當新聞連結過長時，需提供短網址方便網友點擊::https://news.ltn.com.tw/news/Taipei/paper/1338830:: 6.備註:: 準備要船過水無痕 的意思？:然後\\n\\n我想起不久前才有另一位板友DuDu貼出類似的\\n\\n也一樣是自由時報的陳璟民報導\\n\\n但是內容不太一樣呢:: 1.媒體來源:: 自由時報: 2.記者署名: 記者陳璟民／台北報導: 3.完整新聞標題:: 「學姊」疑遭市府顧問性騷案 北市府調查完成但不公開: 4.完整新聞內文:: 台北市政府歷經月餘調查前市府副發言人黃瀞瑩疑遭前市府顧問劉嘉仁性騷擾案，今由副: 市長黃珊珊發布新聞訊息表示，資訊局性騷擾委員會已完成調查報告，交由人事單位依法: 辦理，因為案件涉及隱私，除當事人及主管機關，不宜公開調查结果。:: 記者午後撥打黃瀞瑩、劉嘉仁手機，都跳進語音信箱，還無法得知他們的回應。:: 這起性騷擾疑雲發生在8、9月間，11月初爆開，劉嘉仁很快地「辭職明志」，聲明決無不: 法、踰矩，後來被發現回任台北市立聯合醫院當總院長黃勝堅特助；黃瀞瑩續任副發言人: ，直到排進台灣民眾黨不分區立委名單，並經中央選舉委員會審定資格通過後，在12月初: 辭職。:: 根據月前市議會詢答、媒體訪問時，市長柯文哲的說法，「當時聽一聽，直覺應該是工作: 上有摩擦，職務上隔離就好了」，並未落實處理性騷案的標準作業程序，送請性別工作平: 等委員會調查，事情曝光後引發非議。:: 市府今稱調查已完成，但引市府資訊局工作場所性騷擾防治措施、申訴及懲戒要點，指「: 參與性騷擾事件之處理、調查及決議人員，對於知悉之申訴事件內容應予保密」，又引勞: 動部依性別工作平等法訂定的工作場所性騷擾防治措施申訴及懲戒辦法訂定準則，指「雇: 主處理性騷擾之申訴，應以不公開方式為之」，不公開調查結果。: 5.完整新聞連結 (或短網址)::https://news.ltn.com.tw/news/politics/breakingnews/3008090: 6.備註:: ※ 一個人一天只能張貼一則新聞，被刪或自刪也算額度內，超貼者水桶，請注意:前陣子的那篇，在新聞的最後很明顯寫出了為什麼不公開調查結果的原因以及有所根據的相關法條了怎麼後來新的這篇\\n\\n只有簡單寫著\\n\\n「因為案件涉及隱私，除當事人及主管機關，不宜公開調查結果。」\\n\\n而已呢？\\n\\n\\n\\n\\n\\n\\n沒關係，我在這裡再上色一次市府今稱調查已完成，但引市府資訊局工作場所性騷擾防治措施、申訴及懲戒要點，指「參與性騷擾事件之處理、調查及決議人員，對於知悉之申訴事件內容應予保密」，又引勞動部依性別工作平等法訂定的工作場所性騷擾防治措施申訴及懲戒辦法訂定準則，指「雇主處理性騷擾之申訴，應以不公開方式為之」，不公開調查結果。這是市府資訊局跟勞動部的法條喔\\n\\n結果還有人在那邊要求一切公開透明\\n\\n請問根據勞動部條例不公開會被罵不公開透明那公開了是不是就會被說是違反勞動部法條呢？\\n\\n我很好奇\\n\\n嘻嘻\\n\\n--\\n\\nSent from my ROG Phone II\\n\\n--&quot; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 2.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- sapply(article.push, function(x) html_nodes(x, &quot;span.push-content&quot;) %&gt;% html_text(trim=TRUE) %&gt;% paste(collapse=&quot;&quot;) %&gt;% str_sub) # push contents (new, edited by Alvin.deal with div.push has more than 1 span.push-content) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # Merge all article.tables into one, all push.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all 2.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) Exercise 2.1 Can you modify the R codes so that the script can automatically scrape more than one index page? "]
]
