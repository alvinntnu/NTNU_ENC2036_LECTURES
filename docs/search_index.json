[
["index.html", "Corpus Linguistics Preface Course Objective Textbook Course Website Course Demo Data Necessary Packages", " Corpus Linguistics Alvin Chen 2020-04-05 Preface Welcome to ENC2036. This is a grad-level course, which is devoted to Corpus Linguistics. Have you decided to embark on a digital journey to your future career, there are a series of courses provided in the Department of English, NTNU, Taiwan, offerring necessary inter-disciplinary skills and knowledge. This course requires as prerequisite basic knoweldge of computational coding. It is highly recommended for students to have taken ENC2055 or other equivalents before taking this course. Please see the FAQ of the course webiste for more information about the prerequisite. Course Objective This course aims to introduce theories and practices of Corpus Linguistics as a scientific discipline of its own. Corpus Linguistics has now been considered an interdisciplinary subject, requiring knowledge of linguistic theories, quantitative statistics and data processing. Therefore, this course aims to provide the necessary foundation as well as computational skills for students who are interested in conducting corpus-based linguistic research or language-related research. Students are expected to learn: the methodological foundations of Corpus Linguistics the theoretical bases of Corpus Linguistics the technical designs and configuration of standard corpora how to adopt corpus linguistics as a scientific method in terms of: corpus creation operationalization data retrieval quantifying research questions significance testing the common applications of corpus-linguistic methodology: concordances frequency lists collocations keywords lexical bundles word clouds vector-space representation of words and texts This course is extremely hands-on and will lead the students through classic examples of these corpus-based applications via in-class tutorial sessions and take-home assignments. The main objective of this course is to provide students enough computational skills to perform similar corpus-based analyses on their own data or research questions. Also, it will provide specific hands-on tutorials to equip students with the necessary skills of text and statistical processing. This course will be a prerequisite for more advanced courses such as Computational Linguistics and Quantitative Corpus Linguistics. Textbook We will be using Stefanowitch (2019) as our reference material for the course. However, we will stress the hands-on implementation of the ideas and methods covered in the book. Also, there are a few more reference books listed at the end of the section, which I would highly recommend (e.g., Gries (2018), Baayen (2008), Brezina (2018), McEnery and Hardie (2011)). Course Website We have a course website. You may need a password to access the course materials. If you are an officially enrolled student, please ask the instructor for the passcode. Please read the FAQ of the course website before course registration. Course Demo Data Dropbox Demo Data Directory Necessary Packages In this course, we will need the following R packages for tutorials and exercises. library(DiagrammeR) library(dplyr) library(ggplot2) library(ggrepel) library(gutenbergr) library(htmlwidgets) library(igraph) library(jiebaR) library(kableExtra) library(purrr) library(quanteda) library(RColorBrewer) library(readtext) library(reticulate) library(Rtsne) library(rvest) library(showtext) library(spacyr) library(stringr) library(text2vec) library(textdata) library(tidyr) library(tidytext) library(tidyverse) library(visNetwork) library(wordcloud) library(wordcloud2) References "],
["what-is-corpus-linguistics.html", "Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? 1.2 What is corpus? 1.3 What is a corpus linguistic study? 1.4 Additional Information on CL", " Chapter 1 What is Corpus Linguistics? 1.1 Why do we need corpus data? There is an unnecessary dichotomy in linguistics “intuiting” linguistic data Inventing sentences exemplifying the phenomenon under investigation and then judging their grammaticality Corpus data Highlight the importance of language use in real context Highlight the linguistic tendency in the population (from a sample) Strengths of Corpus Data Data reliability How sure can we be that other people will arrive at the same observations/patterns/conclusions using the same method? Can others replicate the same logical reasoning in intuiting data? Can others make the same “grammatical judgement”? Data validity How well do we understand what real world phenomenon that the data correspond to? Can we know more about language based on one man’s constructed sentences or his grammatical judgement? Can we better generalize our insights from one man’s intuition or the group minds (population vs. sample vs. one-man)? 1.2 What is corpus? This can be tricky: different disciplines, different definitions Literature History Sociology Field Linguistics Linguistic Corpus in corpus linguistics (Stefanowitch 2019) Authentic Representative Large A few well-received definitions “a collection of texts which have been selected and brought together so that language can be studied on the computer” (Wynne 2005) “A corpus is a collection of pieces of language text in electronic form, selected according to external criteria to represent, as far as possible, a language or language variety as a source of data for linguistic research.” (John Sinclair in (Wynne 2005)) “A corpus refers to a machine-readable collection of (spoken or written) texts that were produced in a natural communitive setting, and in which the collection of texts is compiled with the intention (1) to be representative and balanced with respect to a particular linguistic language, variety, register, or genre and (2) to be analyzed linguistically.” (Gries 2018) 1.3 What is a corpus linguistic study? CL characteristics No general agreement as to what it is Not a very homogenous methodological framework (Compared to other sub-disciplines in linguistics) It’s quite new Intertwined with many linguistic fields Interactional linguistics, cognitive linguistics, functional syntax, usage-based grammar etc. Stylometry, computational linguistics, NLP, digital humanities, text mining, sentiment analysis Corpus-based vs. Corpus-driven (cf. Tognini-Bonelli 2001) Corpus-based studies: Typically use corpus data in order to explore a theory or hypothesis, aiming to validate it, refute it or refine it. Take corpus linguistics as a method Corpus-driven studies: Typically reject the characterization of corpus linguistics as a method Claim instead that the corpus itself should be the sole source of our hypotheses about language It is thus claimed that the corpus itself embodies a theory of language (Tognini-Bonelli 2001, 84–85) Stefanowitch (2019) defines Corpus Linguistics as follows: Corpus linguistics is the investigation of linguistic research questions that have been framed in terms of the conditional distribution of linguistic phenomena in a linguistic corpus More on Conditional Distribution An exhaustive investigation Text-processing technology Retrieval and coding Regular expressions Common methods KWIC concordances Collocates Frequency lists A systematic investigation The distribution of a linguistic phenomenon under particular conditions (e.g. lexical, syntactic, social, pragmatic etc. contexts) Statistical properties of language Examples When do English speakers use the complementizer that? What are the differences between small and little? When do English speaker choose “He picked up the book” vs. “He picked the book up”? When do English speaker place the adverbial clauses before the matrix clause? Do speakers use different phrases in different genres? Is the word “gay” used differently across different time periods? Do L2 learners use similar collocation patterns as do L1 speakers? Do speakers of different socio-economic classes talk differently? 1.4 Additional Information on CL Important Journals in Corpus Linguistics Corpus Linguistics and Linguistic Theory International Journal of Corpus Linguistics Corpora Applied Linguistics Computational Linguistics Digital scholarship in the Humanities Language Teaching Language Learning Journal of Second Language Writing CALL Language Teaching Research ReCALL System References "],
["r-fundamentals.html", "Chapter 2 R Fundamentals A Quick Note", " Chapter 2 R Fundamentals A Quick Note This course assumes that students have a certain level of background knowledge of R. We will have a quick overview of several fundamental concepts relating to the R language. These topics will be covered in more detail in my other course, ENC2055. Therefore, in the following weeks, we will go over (or review) some of the important chapters in the Lecture Notes of ENC2055, including: Chapter 2: R Fundamentals Chapter 3: Code Format Convention Chapter 4: Subsetting Chapter 6: Data Manipulation Chapter 7: Data Import Chapter 8: String Manipulation Chapter 9: Conditions and Loops Chpater 10: Iterations Please refer Winter (2020) Chapter 1 and Chapter 2 for a comprehensive overview of R fundamentals. References "],
["creating-corpus.html", "Chapter 3 Creating Corpus 3.1 HTML Structure 3.2 Web Crawling 3.3 Functional Programming 3.4 Save Corpus 3.5 Additional Resources", " Chapter 3 Creating Corpus Linguistic data are important to us linguists. Data usually tell us something we don’t know, or something we are not sure of. In this chapter, I would like to show you a quick way to extract lingusitic data from web pages, which is by now undoubtedly the largest source of textual data available. While there are many existing text data collections, chances are that sometimes you still need to collect your own data for a particular research question. Following the spirit of tidy , we will mainly do our tasks with the libraries of tidyverse and rvests. ## Uncomment the following line for installation # install.packages(c(&quot;tidyverse&quot;, &quot;rvest&quot;)) library(tidyverse) library(rvest) 3.1 HTML Structure 3.1.1 HTML Syntax To illustrate the structure of the HTML, please download the sample html file from: demo_data/data-sample-html.html and first open the with your browser. &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;My First HTML &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; Introduction &lt;/h1&gt; &lt;p&gt; Have you ever read the source code of a html page? &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; An HTML document includes several important elements (cf. 3.1): DTD: document type definition which informs the browswer about the version of the HTML standard that the document adheres to (e.g., &lt;!DOCTYPE HTML&gt;) tag: named braces that enclose the content and define its structural function (e.g., title, body, p) element: the combination of start tag, content, and end tag (e.g, &lt;title&gt;My First HTML&lt;/title&gt;) attribute: specific properties of the tag, which are often placed in the start end of the element (e.g., &lt;a href= &quot;index.html&quot;&gt; Homepage &lt;/a&gt;). They are expressed as name = &quot;value&quot; pairs. Figure 3.1: Syntax of An HTML Tag Element An HTML document starts with the root element &lt;html&gt;, which splits into two branches, &lt;head&gt; and &lt;body&gt;. Most of the main webpage contents would go into the &lt;body&gt; part. All elements need to be strictly nested within each other in a well-formed and valid HTML file, as shown in Figure 3.2. Figure 3.2: Tree Structure of An HTML Document 3.1.2 Tags and Attributes HTML has plenty of legal tags and attributes. On W3CSchools, there is a complete list of HTML tags and attributes for your reference. Common tags may include: Anchor tag &lt;a&gt; Metadata tag &lt;meta&gt; External tag &lt;link&gt; Emphasizing tags &lt;b&gt;, &lt;i&gt;, &lt;strong&gt; Paragraph tags &lt;p&gt; Heading tags &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt; Listing content tags &lt;ul&gt;, &lt;ol&gt; Block tags &lt;div&gt;, &lt;span&gt; Form-related tag &lt;form&gt;, &lt;input&gt; Foreign script tag &lt;script&gt; Table tag &lt;table&gt;, &lt;th&gt;, &lt;tr&gt;, &lt;td&gt; You probably don’t have to know all the detail about the HTML tags. However, in order to scrape the data from the Internet, you need to know at least from which types of HTML elements you need your textual data from on the web pages. 3.1.3 CSS Cascading Style Sheet (CSS) is a language for describing the layout of HTML and other markup documents (e.g., XML). HTML + CSS is by now the standard way to create and design web pages. The idea is that CSS specifies the formats/styles of the HTML elements. The following is an example of the CSS: div.warnings { color: pink; font-family: &quot;Arial&quot; font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } div.warnings { color: pink; font-family: \"Arial\" font-size: 120% } h1 { padding-top: 20px padding-bottom: 20px } 3.1.4 HTML + CSS ( + JavaScript) Try it: HTML: the language for building web pages CSS: the language for styling web pages JavaScript: the language for programming web pages 3.2 Web Crawling In the following demonstration, the text data scraped from the PTT forum is presented as it is without adjustment. However, please note that the language on PTT may strike some readers as profane, vulgar or even offensive. In this tutorial, let’s assume that we like to scape texts from PTT Forum. In particular, we will demonstrate how to scape texts from the Gossiping board of PTT. ptt.url &lt;- &quot;https://www.ptt.cc/bbs/Gossiping&quot; If you use your browser to view PTT Gossiping page, you would see that you need to go through the age verification before you can enter the content page. So, our first job is to pass through this age verification. First, we create a html_session() (like we open a browswer linking to the page) gossiping.session &lt;- html_session(ptt.url) Second, we extract the age verification form from the current page (form is also a defined HTML element) gossiping.form &lt;- gossiping.session %&gt;% html_node(&quot;form&quot;) %&gt;% html_form Then we automatically submit an yes to the age verification form in the earlier created html_session and create another session. gossiping &lt;- submit_form( session = gossiping.session, form = gossiping.form, submit = &quot;yes&quot; ) gossiping ## &lt;session&gt; https://www.ptt.cc/bbs/Gossiping/index.html ## Status: 200 ## Type: text/html; charset=utf-8 ## Size: 11623 Now our html sesseion, i.e., gossiping, should be on the front page of the Gossiping board. Most browsers come with the functionality to inspect the page source (i.e., HTML). This is very useful for web crawling. Before we scrape data from the webpage, we often need to inspect the structure of the web page first. Most importantly, we need to know (a) which HTML elements, or (b) which particular attributes/values of the HTML elements we are interested in . Next we need to find the most recent index page of the board # Decide the number of index pages ---- page.latest &lt;- gossiping %&gt;% html_nodes(&quot;a&quot;) %&gt;% # extract all &lt;a&gt; elements html_attr(&quot;href&quot;) %&gt;% # extract the attributes `href` str_subset(&quot;index[0-9]{2,}\\\\.html&quot;) %&gt;% # find the `href` with the index number str_extract(&quot;[0-9]+&quot;) %&gt;% # extract the number as.numeric() page.latest ## [1] 38807 On the most recent index page, we need to extract the hyperlinks to the articles # Retreive links ----- link &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) links.article &lt;- gossiping %&gt;% jump_to(link) %&gt;% # move session to the most recent page html_nodes(&quot;a&quot;) %&gt;% # extract article &lt;a&gt; html_attr(&quot;href&quot;) %&gt;% # extract article &lt;a&gt; `href` attributes str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% # extract links str_c(&quot;https://www.ptt.cc&quot;,.) links.article ## [1] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586040841.A.6E0.html&quot; ## [2] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586040857.A.1A6.html&quot; ## [3] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586041368.A.D4E.html&quot; ## [4] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586041510.A.43E.html&quot; ## [5] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586041989.A.653.html&quot; ## [6] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042187.A.957.html&quot; ## [7] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042512.A.DA7.html&quot; ## [8] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042594.A.E9D.html&quot; ## [9] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042627.A.085.html&quot; ## [10] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042757.A.5EB.html&quot; ## [11] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042868.A.75A.html&quot; ## [12] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042877.A.2FF.html&quot; ## [13] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586042961.A.6B4.html&quot; ## [14] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043042.A.DB2.html&quot; ## [15] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043339.A.BB2.html&quot; ## [16] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043629.A.0E2.html&quot; ## [17] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043676.A.B5D.html&quot; ## [18] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043733.A.B98.html&quot; ## [19] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043781.A.E9B.html&quot; ## [20] &quot;https://www.ptt.cc/bbs/Gossiping/M.1586043818.A.F54.html&quot; Next step is to scrape texts from each article hyperlink. Let’s consider one link first. article.url &lt;- links.article[1] temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the article Now the temp.html is like a browser on the article page. Because we are interested in the metadata and the contents of each article, now the question is: where are they in the HTML? We need to go back to the source page of the article HTML again: After a closer inspection of the article HTML, we know that: The metadata of the article are included in &lt;span&gt; tag elements, belonging to the class class=&quot;article-meta-value&quot; The contents of the article are included in the &lt;div&gt; element, whose ID is ID=&quot;main-content&quot; Now we are ready to extract the metadata of the article. # Extract article metadata article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # get &lt;span&gt; of a particular class html_text() article.header ## [1] &quot;jody893011 (一切有為法，如夢幻泡影)&quot; ## [2] &quot;Gossiping&quot; ## [3] &quot;Re: [問卦] 到底是什麼邏輯會害怕偽陰性我真的不懂&quot; ## [4] &quot;Sun Apr 5 06:53:57 2020&quot; From the article.header, we are able to extract the author, title, and time stamp of the article. article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp article.author ## [1] &quot;jody893011&quot; article.title ## [1] &quot;Re: [問卦] 到底是什麼邏輯會害怕偽陰性我真的不懂&quot; article.datetime ## [1] &quot;Sun Apr 5 06:53:57 2020&quot; Now we extract the main contents of the article article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% # extract texts str_c(collapse = &quot;&quot;) # combine all lines into one article.content ## [1] &quot;偽陽性的問題 有的人說起來有多難 大陸那邊大概只用三秒鐘就解決了\\n\\n全部關進方倉醫院統一隔離\\n\\n寧可錯殺一百 也不能放過一個\\n\\n病情不是重症的\\n\\n就是弄個地方給你可以吃吃喝喝睡睡打兩袋點滴的成本+把你封起來的成本\\n\\n你敢跑出來 就是就地射殺\\n\\n一直用負壓隔離病房譬喻根本就是搞不清楚狀況\\n\\n為了避免大規模的群聚\\n\\n就是要用這種手段介入與隔離 董?\\n\\n目前我們只是在等疫情爆炸而已 我的感覺是這樣\\n\\n另外不要再說我是小粉紅 刻意搗蛋的9.2\\n\\n上站3995次 發文超過一千篇 除了廢死跟防疫這類破事\\n\\n我應該是忠貞817吧?: 還有一個很嚴重的問題叫偽陽性: 說偽陰性會趴趴走 我倒不覺得這是個理由: 政府怎麼可能管不了: 驗出陰性 不代表你不能逼人家乖乖隔離14天: 這應該是規定上改一下 然後衛教積極一點就可以解決的: 但是偽陽性會直接耗損台灣所剩不多的負壓病房: 偽陽性是指沒有得病 卻被驗成有病: 約有1%的人會是偽陽性: 舉例來說 你驗了1000人 裡面真病人假設有10個: 那剩下沒被感染的人有990個 因為偽陽性率1% 就驗出9~10個假病人: 可是你要把真假病人都丟去負壓病房 瞬間就少了19間: 等於有9間負壓病房是浪費掉的: 對於已經大爆炸 確診率超高的國家 偽陽性不是個問題: 因為你檢驗下去 會抓出很多很多真病人 那假病人只是零頭: 舉例來說 像紐約這種爆炸區 將近3~4成被檢驗者都有病: 假設驗1000人 裡面有400個真病人 剩下的600人裡面 就會有6個假病人: 那真假病人比400:6 假病人浪費的醫療資源 少到可以忽略: 因此紐約寧可多驗 有假病人也沒差: 台灣現在的狀況是 檢驗的陽性比例: 只有1%上下 不像紐約義大利最高都超過30%: 顯然社區傳染沒有很嚴重: 代表我們驗出的偽陽性人數 比重會非常的高: 如果現在開始擴大檢驗 假陽性人數就會更多: 我們有限的負壓病房就會不夠用: 我想這才是一直不敢馬上擴大檢驗的主因: 我覺得政府如果真的想抓出很多黑數 又怕負壓超載: 應該是讓輕症或無症者到別處隔離 學韓國那樣 不要浪費負壓病房: 但現在政府似乎不太想這樣 不確定為什麼: ※ 引述《raugeon (上輩子的情人)》之銘言：: : 我有看支持不驗的說法: : 簡單說就是支持指揮中心的說法: : 1.若有病: : 驗陽性-&gt;確診: : 偽陰性-&gt;到處跑-&gt;傳染更多人: : 不驗-&gt;等病發後去就醫一樣會確診: : 這個邏輯不覺得很有問題嗎？: : 第一，為什麼假定不驗的人就不會到處跑？: : 第二，為什麼假定偽陰性的人就會到處跑？: : 第三，為什麼假定病發後一定會去就醫？: : 現在的台灣狀況中，: : 明明就一再打臉這種說法了: : 一點一點的講--&quot; XPath (or XML Path Language) is a query language which is useful for addressing and extracting particular elements from XML/HTML documents. XPath allows you to exploit more features of the hierarchical tree that an HTML file represents in locating the relevant HTML elements. For more information, please see Munzert et al. (2014), Chapter 4. In the above example, the XPath identifies the nodes under &lt;div id = “main-content”&gt;, but excludes sister nodes that are &lt;div&gt; or &lt;span class=“f2”&gt; Now we combine all infomation related to the article into a data frame article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) article.table Next we extract the push comments at the end of the article article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) article.push ## {xml_nodeset (20)} ## [1] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [2] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [3] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [4] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [5] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [6] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [7] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [8] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;hl push-tag&quot;&gt;推 &lt;/span&gt;&lt;span class=&quot;f3 h ... ## [9] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [10] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [11] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [12] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [13] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [14] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [15] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [16] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [17] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [18] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;噓 &lt;/span&gt;&lt;span class=&quot;f ... ## [19] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... ## [20] &lt;div class=&quot;push&quot;&gt;\\n&lt;span class=&quot;f1 hl push-tag&quot;&gt;→ &lt;/span&gt;&lt;span class=&quot;f ... We then extract relevant information from each push nodes article.push. push types push authors push contents # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) push.table.tag ## [1] &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;推&quot; &quot;→&quot; &quot;噓&quot; &quot;噓&quot; &quot;推&quot; &quot;噓&quot; &quot;噓&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; &quot;→&quot; ## [16] &quot;噓&quot; &quot;→&quot; &quot;噓&quot; &quot;→&quot; &quot;→&quot; # push authors push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author push.table.author ## [1] &quot;traman&quot; &quot;hika1234&quot; &quot;A80211ab&quot; &quot;somanyee&quot; &quot;somanyee&quot; ## [6] &quot;mynumber55&quot; &quot;windyyw&quot; &quot;aletheia&quot; &quot;uini&quot; &quot;v21038616&quot; ## [11] &quot;v21038616&quot; &quot;v21038616&quot; &quot;v21038616&quot; &quot;v21038616&quot; &quot;v21038616&quot; ## [16] &quot;novastar&quot; &quot;jody893011&quot; &quot;v21038616&quot; &quot;v21038616&quot; &quot;v21038616&quot; # push contents push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) push.table.content ## [1] &quot;: 說你五毛還需要什麼證據 大家快來噓&quot; ## [2] &quot;: 你質疑政府時候你就是小粉紅&quot; ## [3] &quot;: 合理 說你五毛真的不用理由&quot; ## [4] &quot;: 寧可錯殺 不可放過 剛好適合防疫。歐美文化是不願誤殺&quot; ## [5] &quot;: ，所以這次很慘&quot; ## [6] &quot;: 喔，投票就了不起喔&quot; ## [7] &quot;: 跟黨不同調就是五毛啦&quot; ## [8] &quot;: 還敢提中國啊，要抹你韓粉了&quot; ## [9] &quot;: 所以你信中國零確診了，怎麼不買張機票飛過去&quot; ## [10] &quot;: 民主國家你想以極權國家當理想模板喔&quot; ## [11] &quot;: 你一個禮拜前說乾咳欸&quot; ## [12] &quot;: 要不要把你關進方倉醫院，等到疫情結束才放出來&quot; ## [13] &quot;: 不要跟我說你只想關兩週，你在方倉會跟病人有接觸&quot; ## [14] &quot;: 所以要關到全部都沒事才能放出來&quot; ## [15] &quot;: 阿關到沒病變有病算你雖&quot; ## [16] &quot;: 跟kmt87像 覺得你是間諜就關起來 不配合就殺掉&quot; ## [17] &quot;: 我說驗到陽性統一收治 你跟我說乾咳？&quot; ## [18] &quot;: 中國方倉有症狀就關起來啊&quot; ## [19] &quot;: 你以為你在吃自助餐喔，喜歡極權國家的哪個就給你挑，&quot; ## [20] &quot;: 哪個不要就不要&quot; # push time push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp push.table.datetime ## [1] &quot;04/05 06:55&quot; &quot;04/05 07:05&quot; &quot;04/05 07:06&quot; &quot;04/05 07:07&quot; &quot;04/05 07:07&quot; ## [6] &quot;04/05 07:07&quot; &quot;04/05 07:10&quot; &quot;04/05 07:11&quot; &quot;04/05 07:15&quot; &quot;04/05 07:17&quot; ## [11] &quot;04/05 07:18&quot; &quot;04/05 07:18&quot; &quot;04/05 07:19&quot; &quot;04/05 07:19&quot; &quot;04/05 07:19&quot; ## [16] &quot;04/05 07:36&quot; &quot;04/05 07:39&quot; &quot;04/05 07:46&quot; &quot;04/05 07:46&quot; &quot;04/05 07:46&quot; Finally, we combine all into one Push data frame. push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url) push.table 3.3 Functional Programming It should now be clear that there are several routines that we need to do again and again if we want to scape the PTT texts in a large amount: For each index page, we need to extract all the article hyperlinks on the page For each article hyperlink, we need to extract the article content, metadata, and the push comments So, it would be great if we can wrap these two routines into two functions: extract_art_links(): This function takes an HTML session session and an index page of the PTT Gossiping index_page as the arguments and extract all article links from the index page. It returns a vector of article links. extract_art_links &lt;- function(index_page, session){ links.article &lt;- session %&gt;% jump_to(index_page) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) %&gt;% str_subset(&quot;[A-z]\\\\.[0-9]+\\\\.[A-z]\\\\.[A-z0-9]+\\\\.html&quot;) %&gt;% str_c(&quot;https://www.ptt.cc&quot;,.) return(links.article) } extract_article_push_tables(): This function takes an article link link as the argument and extract the metadata, contents, and pushes of the article. It returns a list of two elements—article and push data frames. extract_article_push_tables &lt;- function(link){ article.url &lt;- link temp.html &lt;- gossiping %&gt;% jump_to(article.url) # link to the www # article header article.header &lt;- temp.html %&gt;% html_nodes(&quot;span.article-meta-value&quot;) %&gt;% # meta info regarding the article html_text() # article meta article.author &lt;- article.header[1] %&gt;% str_extract(&quot;^[A-z0-9_]+&quot;) # athuor article.title &lt;- article.header[3] # title article.datetime &lt;- article.header[4] # time stamp # article content article.content &lt;- temp.html %&gt;% html_nodes( # article body xpath = &#39;//div[@id=&quot;main-content&quot;]/node()[not(self::div|self::span[@class=&quot;f2&quot;])]&#39; ) %&gt;% html_text(trim = TRUE) %&gt;% str_c(collapse = &quot;&quot;) # Merge article table article.table &lt;- tibble( datetime = article.datetime, title = article.title, author = article.author, content = article.content, url = article.url ) # push nodes article.push &lt;- temp.html %&gt;% html_nodes(xpath = &quot;//div[@class = &#39;push&#39;]&quot;) # extracting pushes # NOTE: If CSS is used, div.push does a lazy match (extracting div.push.... also) # push tags push.table.tag &lt;- article.push %&gt;% html_nodes(&quot;span.push-tag&quot;) %&gt;% html_text(trim = TRUE) # push types (like or dislike) # push author id push.table.author &lt;- article.push %&gt;% html_nodes(&quot;span.push-userid&quot;) %&gt;% html_text(trim = TRUE) # author # push content push.table.content &lt;- article.push %&gt;% html_nodes(&quot;span.push-content&quot;) %&gt;% html_text(trim = TRUE) # push datetime push.table.datetime &lt;- article.push %&gt;% html_nodes(&quot;span.push-ipdatetime&quot;) %&gt;% html_text(trim = TRUE) # push time stamp # merge pusg table push.table &lt;- tibble( tag = push.table.tag, author = push.table.author, content = push.table.content, datetime = push.table.datetime, url = article.url ) # return return(list(article.table = article.table, push.table = push.table)) }#endfunc Now we can simply our codes quite a bit: # Get index page cur_index_page &lt;- str_c(ptt.url, &quot;/index&quot;, page.latest, &quot;.html&quot;) # Scape all article.tables and push.tables from each article hyperlink cur_index_page %&gt;% extract_art_links(session = gossiping) %&gt;% map(extract_article_push_tables) -&gt; ptt_data # number of articles on this index page length(ptt_data) ## [1] 20 # Check the first contents of 1st hyperlink ptt_data[[1]]$article.table ptt_data[[1]]$push.table Finally, the last thing we can do is to combine all article tables into one; and all push tables into one for later analysis. # Merge all article.tables into one article.table.all &lt;- ptt_data %&gt;% map(function(x) x$article.table) %&gt;% bind_rows # Merge all push.tables into one push.table.all &lt;- ptt_data %&gt;% map(function(x) x$push.table) %&gt;% bind_rows article.table.all push.table.all There is still one problem with the Push data frame. Right now it is still not very clear how we can match the pushes to the articles from which they were extracted. The only shared index is the url. It would be better if all the articles in the data frame have their own unique indices and in the Push data frame each push comment corresponds to a particular article index. The following graph summarizes our work flowchart for PTT Gossipping Scraping: 3.4 Save Corpus You can easily save your scaped texts in a CSV format. # Save ------ write_csv(article.table, path = &quot;PTT_GOSSIPPING_ARTICLE.csv&quot;) write_csv(push.table, path = &quot;PTT_GOSSIPPING_PUSH.csv&quot;) 3.5 Additional Resources Collecting texts and digitizing them into machine-readable files is only the initial step for corpus construction. There are many other things that need to be considered to ensure the effectiveness and the substainability of the corpus data. In particular, I would like to point you to a very useful resource, Developing Linguistic Corpora: A Guide to Good Practice, compiled by Martin Wynne. Other important issues in corpus creation include: Adding linguistic annotations to the corpus data (cf. Leech’s Chapter 2) Metadata representation of the documents (cf. Burnard’s Chapter 4) Spoken corpora (cf. Thompson’s Chapter 5) Technical parts for corpus creation (cf. Sinclair’s Appendix) Exercise 3.1 Can you modify the R codes so that the script can automatically scrape more than one index page? Exercise 3.2 Please utilize the code from Exercise 3.1 and collect all texts on PTT/Gossipings from 3 index pages. Please have the articles saved in PTT_GOSSIPING_ARTICLE.csv and the pushes saved in PTT_GOSSIPING_PUSH.csv under your working directory. Also, at the end of your code, please also output in the Console the corpus size, including both the articles and the pushes. Please provide the total number of characters of all your PTT text data collected (Note: You DO NOT have to do the word segmentation yet. Please use the characters as the base unit for corpus size.) Hint: nchar() Your script may look something like: # I define my own `scrapePTT()` function: # ptt_url: specify the board to scrape texts from # num_index_page: specify the number of index pages to be scraped # return: list(artcle, push) PTT_data &lt;-scrapePTT(ptt_url = &quot;https://www.ptt.cc/bbs/Gossiping&quot;, num_index_page = 3) PTT_data$article %&gt;% head PTT_data$push %&gt;% head # corpus size PTT_data$article$content %&gt;% nchar %&gt;% sum ## [1] 33061 Exercise 3.3 Now you should have basic ideas how we can crawl data from the Internet. Sometimes, we not only collect texts but statistics as well. Please try to collect the statistics of COVID-19 outbreak from the Wikipedia 2019–20 coronavirus pandemic. Specifically, write a short script to automatically get the table included in the Wiki page, where the numbers of confirmed cases, deaths, and recoveries for each country are recorded. Your script should output a data frame as follows. Please name the columns of your data frame as follows as well. (Note: The numbers may vary because of the constant updates of the wiki page.) References "],
["corpus-analysis-a-start.html", "Chapter 4 Corpus Analysis: A Start 4.1 Installing quanteda 4.2 Building a corpus from character vector 4.3 Keyword-in-Context (KWIC) 4.4 KWIC with Regular Expressions 4.5 Tidy Text Format of the Corpus 4.6 Frequency Lists 4.7 Word Cloud 4.8 Collocations 4.9 Constructions", " Chapter 4 Corpus Analysis: A Start In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data. 4.1 Installing quanteda There are many packages that are made for computational text analytics in R. You may consult the CRAN Task View: Natural Language Processing for a lot more alternatives. To start with, this tutorial will use a powerful package, quanteda, for managing and analyzing textual data in R. You may refer to the official documentation of the package for more detail. quanteda is not included in the default R installation. Please install the package if you haven’t done so. install.packages(&quot;quanteda&quot;) install.packages(&quot;readtext&quot;) Also, as noted on the quanteda documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers. If you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN. If you are using macOS, you should install the macOS tools. If you run into any installation errors, please go to the official documentation page for additional assistance. library(quanteda) library(readtext) library(tidytext) library(dplyr) packageVersion(&quot;quanteda&quot;) ## [1] &#39;2.0.1&#39; 4.2 Building a corpus from character vector To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the quanteda package, data_corpus_inaugural. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present. data_corpus_inaugural ## Corpus consisting of 58 documents and 4 docvars. ## 1789-Washington : ## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot; ## ## 1793-Washington : ## &quot;Fellow citizens, I am again called upon by the voice of my c...&quot; ## ## 1797-Adams : ## &quot;When it was first perceived, in early times, that no middle ...&quot; ## ## 1801-Jefferson : ## &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot; ## ## 1805-Jefferson : ## &quot;Proceeding, fellow citizens, to that qualification which the...&quot; ## ## 1809-Madison : ## &quot;Unwilling to depart from examples of the most revered author...&quot; ## ## [ reached max_ndoc ... 52 more documents ] class(data_corpus_inaugural) ## [1] &quot;corpus&quot; &quot;character&quot; We create a corpus() object with the pre-loaded corpus in quanteda– data_corpus_inaugural: corp_us &lt;- corpus(data_corpus_inaugural) # save the `corpus` to a short obj name summary(corp_us) After the corpus is loaded, we can use summary() to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents. require(ggplot2) corp_us %&gt;% summary %&gt;% ggplot(aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() + theme_bw() Exercise 4.1 Could you reproduce the above line plot and add information of President to the plot as labels of the dots? Hints: Please check ggplot2::geom_text() or more advanced one, ggrepel::geom_text_repel() 4.3 Keyword-in-Context (KWIC) Keyword-in-Context (KWIC), or concordances, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examining how it is being used in a wider context. We can use kwic() to perform a search for a word and retrieve its concordances from the corpus: kwic(corp_us, &quot;terror&quot;) kwic() returns a data frame, which can be easily output to a CSV file for later use. Please note that kwic(), when taking a corpus object as the argument, will automatically tokenize the corpus data and do the keyword-in-context search on a word basis. In other words, the pattern you look for cannot be a linguistic pattern across several words. We will talk about how to extract constructions later. Also, for languages without explicit word boundaries (e.g., Chinese), this may be a problem with quanteda. We will talk more about this in the later chapter on Chinese Texts Analytics. 4.4 KWIC with Regular Expressions For more complex searches, we can use regular expressions as well in kwic(). For example, if you want to include terror and all its other related word forms, such as terrorist, terrorism, terrors, you can do a regular expression search. kwic(corp_us, &quot;terror.*&quot;, valuetype = &quot;regex&quot;) By default, the kwic() is word-based. If you like to look up a multiword combination, use phrase(): kwic(corp_us, phrase(&quot;our country&quot;)) It should be noted that the output of kwic includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line. Exercise 4.2 Please create a bar plot, showing the number of uses of the word country in each president’s address. Please include different variants of the word, e.g., countries, Countries, Country, in your kwic() search. 4.5 Tidy Text Format of the Corpus So far our corpus is a corpus object defined in quanteda. In most of the R standard packages, people normally follow the using tidy data principles to make handling data easier and more effective. As described by Hadley Wickham (Wickham and Grolemund 2017), tidy data has a specific structure: Each variable is a column Each observation is a row Each type of observational unit is a table With text data like a corpus, we can also define the tidy text format as being a data.frame with one-token-per-row. A token is a meaningful unit of text, such as a word that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. In computational text analytics, the token (i.e., each row in the data frame) is most often a single word, but can also be an n-gram, sentence, or paragraph. The tidytext package in R is made for the handling of the tidy text format of the corpus data. Tidy datasets allow manipulation with a standard set of tidy tools, including popular packages such as dplyr, tidyr, and ggplot2. Figure 4.1: Computational Text Processing Flowchart The tidytext package includes functions to tidy() objects from quanteda. library(tidytext) corp_us_tidy &lt;- tidy(corp_us) # convert `corpus` to `data.frame` class(corp_us_tidy) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.6 Frequency Lists 4.6.1 Word (Unigram) To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages. The tidytext provides a powerful function, unnest_tokens() to tokenize a data frame with larger linguistic units (e.g., texts) into one with smaller units (e.g., words). That is, the unnest_tokens() convert a text-based data frame (each row is a text document) into a token-based data frame(each row is a token splitted from the text). corp_us_words &lt;- corp_us_tidy %&gt;% unnest_tokens(output = word, input = text, token = &quot;words&quot;) # tokenize the `text` column into `word` corp_us_words The unnest_tokens() is optimized for English tokenization of other linguistic units, such as words, ngrams, sentences, lines, and paragraphs (check ?unnest_tokens()). To handle Chinese data, however, we need to define own ways of tokenization unnest_tokens(…, token = …). We will discuss the principles for Chinese text processing in a later chapter. Please note that by default, token = “words” would normalize the texts to lower-casing letters. Also, all the non-word tokens are automatically removed. If you would like to preserve the casing differences and the punctuations, you can include the following arguments in unnest_tokens(…, token = “words”,strip_punct = F, strip_numeric = F). Now we can count the word frequencies: corp_us_words_freq &lt;- corp_us_words %&gt;% count(word, sort=TRUE) corp_us_words_freq 4.6.2 Bigrams Frequency lists can be generated for bigrams or any other multiword combinations as well: corp_us_bigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(output = bigram, input = text, token = &quot;ngrams&quot;, n = 2) corp_us_bigrams To create bigram frequency list: corp_us_bigrams_freq &lt;- corp_us_bigrams %&gt;% count(bigram, sort=TRUE) corp_us_bigrams_freq sum(corp_us_words_freq$n) # size of unigrams ## [1] 135562 sum(corp_us_bigrams_freq$n) # size of bigrams ## [1] 135504 Exercise 4.3 The function unnest_tokens() does a lot of work behind the scene. Please take a closer look at the outputs of unnest_tokens() and examine how it takes care of the case normalization and punctuations within the sentence. Will these treatments affect the frequency lists we get in any important way? Please elaborate. 4.6.3 Ngrams (Lexical Bundles) corp_us_trigrams &lt;- corp_us_tidy %&gt;% unnest_tokens(trigrams, text, token = &quot;ngrams&quot;, n = 3) corp_us_trigrams We then can examine which n-grams were most often used by each President: corp_us_trigrams %&gt;% count(President, trigrams) %&gt;% group_by(President) %&gt;% top_n(3, n) %&gt;% arrange(President, desc(n)) Exercise 4.4 Please subset the top 3 trigrams of President Don. Trump, Bill Clinton, John Adams, from corp_us_trigram. 4.6.4 Frequency and Dispersion When looking at frequency lists, there is another distributional metric we need to consider: dispersion. An n-gram can be meaningful if its frequency is high. However, this high frequency may come in different meanings. What if the n-gram only occurs in ONE particular document, i.e., used only by a particular President? Or alternatively, what if the n-gram appears in many different documents, i.e., used by many different Presidents? The degrees of n-gram dispersion has a lot to do with the significance of its frequency. So now let’s compute the dispersion of the n-grams in our corp_us_trigrams. Here we define the dispersion of an n-gram as the number of Presidents who have used the n-gram at least once in his address(es). # method 1 corp_us_trigrams %&gt;% count(trigrams, President) %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = sum(n), DISPERSION = n()) %&gt;% filter(DISPERSION &gt;= 5) %&gt;% arrange(desc(DISPERSION)) # method2 corp_us_trigrams %&gt;% group_by(trigrams) %&gt;% summarize(FREQ = n(), DISPERSION = n_distinct(President)) %&gt;% filter(DISPERSION &gt;= 5) %&gt;% arrange(desc(DISPERSION)) # Arrange according to frequency # corp_us_trigram %&gt;% # count(trigrams, President) %&gt;% # group_by(trigrams) %&gt;% # summarize(freq = sum(n), dispersion = n()) %&gt;% # arrange(desc(freq)) In particular, cut-off values are often determined to select a list of meaningful n-grams. These cut-off values include: the frequency of the n-grams, as well as the dispersion of the n-grams. A subset of n-grams that are defined and selected based on these distributional criteria (i.e., frequency and dispersion) are often referred to as Lexical bundles. Exercise 4.5 Please create a list of four-grams lexical bundles that have been used in at least FIVE different presidential addressess. Arrange the resulting data frame according to the frequency of the four-grams. 4.7 Word Cloud With frequency data, we can visualize important words in the corpus with a Word Cloud. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts. library(wordcloud) set.seed(123) with(corp_us_words_freq, wordcloud(word, n, max.words = 400, min.freq = 20, scale = c(2,0.5), color = brewer.pal(8, &quot;Dark2&quot;), vfont=c(&quot;serif&quot;,&quot;plain&quot;))) Exercise 4.6 Word cloud would be more informative if we first remove functional words. In tidytext, there is a preloaded data frame, stop_words, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. (Criteria: Frequency &gt;= 20; Max Number of Words Plotted = 400) Hint: Check dplyr::anti_join() require(tidytext) stop_words Exercise 4.7 Get yourself familiar with another R package for creating word clouds, wordcloud2, and re-create a word cloud as requested in Exercise 4.6 but in a fancier format, i.e., a star-shaped one. (Criteria: Frequency &gt;= 20; Max Number of Words Plotted = 400) 4.8 Collocations With unigram and bigram frequencies of the corpus, we can further examine the collocations within the corpus. Collocation refers to a frequent phenomenon where two words tend to co-occur very often in use. This co-occurrence is defined statistically by their lexical associations. 4.8.1 Cooccurrence Table and Observed Frequencies Cooccurrence frequency data for a word pair, w1 and w2, are often organized in a contingency table extracted from a corpus, as shown in Figure 4.2. The cell counts of this contingency table are called the observed frequencies O11, O12, O21, and O22. Figure 4.2: Cooccurrence Freqeucny Table The sum of all four observed frequencies (called the sample size N) is equal to the total number of bigrams extracted from the corpus. R1 and R2 are the row totals of the observed contingency table, while C1 and C2 are the corresponding column totals. The row and column totals are also called marginal frequencies, being written in the margins of the table, and O11 is called the joint frequency. 4.8.2 Expected Frequencies Equations for all association measures are given in terms of the observed frequencies, marginal frequencies, and the expected frequencies E11, …, E22. Expected frequencies refer to the expected number of co-occurrences under the null hypothesis that W1 and W2 are statistically independent. The expected frequencies can easily be computed from the marginal frequencies as shown in Figure 4.3. Figure 4.3: Computing Expected Frequencies Maybe it would be easier for us to illustrate this with a simple example: Figure 4.4: Computing Expected Frequencies How do we compute the expected frequencies of the four cells? Figure 4.5: Computing Expected Frequencies example &lt;- matrix(c(90, 10, 110, 290), byrow=T, nrow=2) Exercise 4.8 Please compute the expected frequencies for the above matrix example in R. 4.8.3 Association Measures The idea of lexical assoication is to measure how much the observed frequencies deviate from the expected. Some of the metrics (e.g., t-statistic, MI) consider only the joint frequency deviation (i.e., O11), while others (e.g., G2, a.k.a Log Likelihood Ratio) consider the deviations of ALL cells. Here I would like to show you how we can compute the most common two asssociation metrics for all the bigrams found in the corpus: t-test statistic and Mutual Information (MI). \\(t = \\frac{O_{11}-E_{11}}{\\sqrt{E_{11}}}\\) \\(MI = log_2\\frac{O_{11}}{E_{11}}\\) \\(G^2 = 2 \\sum_{ij}{O_{ij}log\\frac{O_{ij}}{E_{ij}}}\\) corp_us_bigrams_freq %&gt;% head(10) corp_us_collocations &lt;- corp_us_bigrams_freq %&gt;% filter(n &gt; 5) %&gt;% # set bigram frequency cut-off rename(O11 = n) %&gt;% tidyr::separate(bigram, c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;\\\\s&quot;) %&gt;% # split bigrams into two columns mutate(R1 = corp_us_words_freq$n[match(w1, corp_us_words_freq$word)], C1 = corp_us_words_freq$n[match(w2, corp_us_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), t = (O11 - E11)/sqrt(E11)) %&gt;% # compute associations arrange(desc(MI)) # sorting corp_us_collocations Please note that in the above example, we compute the lexical associations for bigrams whose frequency &gt; 5. This is necessary in collocation studies because bigrams of very low frequency would not be informative even though its association can be very strong. However, the cut-off value can be arbitrary, depending on the corpus size or researchers’ considerations. How to compute lexical assoications is a non-trivial issue. There are many more ways to compute the association strengths between two words. Please refer to Stefan Evert’s site for a very comprehensive review of lexical assoication meaasures. Exercise 4.9 Sort the collocation data frame corp_us_collocations according to the t-score and compare the results sorted by MI scores. Please describe what you find. Exercise 4.10 Based on the formula provided above, please create a new column for corp_us_collocations, which gives the Log-Likelihood Ratios of all the bigrams. When you do the above exercise, you may run into a couple of problems: Some of the bigrams have NaN values in their LLR. This may be due to the issue of NAs produced by integer overflow. Please solve this. After solving the above overflow issue, you may still have a few bigrams with NaN in their LLR, which may be due to the computation of the log value. In Math, how do we define log(1/0) and log(0/1)? Do you know when you would get an undefined value NaN in the computation of log()? To solve the problems, please assign the value 0 if the log returns NaN values. Exercise 4.11 Find the top FIVE bigrams ranked according to MI values for each president. The result would be a data frame as shown below. Create a plot as shown below to visualize your results. 4.9 Constructions We are often interested in the use of linguistic patterns, which are beyond the lexical boundaries. My experience is that usually it is better to work with the corpus on a sentential level. We can use the same tokenization function, unnest_tokens() to convert our text-based corpus data frame, corpus_us_tidy, into a sentence-based tidy structure: corp_us_sents &lt;- corp_us_tidy %&gt;% unnest_tokens(output = sentence, input = text, token = &quot;sentences&quot;) # tokenize the `text` column into `sentence` corp_us_sents With each sentence, we can investigate particular constructions in more detail. Let’s assume that we are interested in the use of Perfect aspect in English by different presidents. We can try to extract Perfect constructions (including Present/Past Perfect) from each sentence using the regular expression. Here we make a simple naive assumption: Perfect constructions include all have/has/had + VERB-en/ed tokens from the sentences. require(stringr) # Perfect corp_us_sents %&gt;% unnest_tokens(perfect, sentence, token = function(x) str_extract_all(x, &quot;ha[d|ve|s] \\\\w+(en|ed)&quot;)) -&gt; result_perfect result_perfect In the above example, we specify the token= argument in unnest_tokens(…, token = …) with a self-defined function. The idea of tokenization in unnest_tokens() is that the token argument should be a function which takes a text-based vector as input (i.e, each element of the input vector may be a document text) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (see Figure below). In our demonstration, we define a tokenization function, which takes sentence as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions in individual sentences in sentence. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Figure 4.6: Intuition for token= in unnest_tokens() And of course we can do an exploratory analysis of the frequencies of Perfect constructions by different presidents: require(tidyr) # table result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) # graph result_perfect %&gt;% group_by(President) %&gt;% summarize(TOKEN_FREQ = n(), TYPE_FREQ = n_distinct(perfect)) %&gt;% pivot_longer(c(&quot;TOKEN_FREQ&quot;, &quot;TYPE_FREQ&quot;), names_to = &quot;STATISTIC&quot;, values_to = &quot;NUMBER&quot;) %&gt;% ggplot(aes(President, NUMBER, fill = STATISTIC)) + geom_bar(stat = &quot;identity&quot;,position = position_dodge()) + theme(axis.text.x = element_text(angle=90)) There are quite a few things we need to take care of more thoroughly: The auxilliary HAVE and the past participle do not necessarily have to stand next to each other for Perfect constructions. We now lose track of one important information: from which sentence of the Presidental addressess did we collect each Perfect constructional token? Any ideas how to solve all these issues? Exercise 4.12 Please create a better regular expression to retrieve more tokens of English Perfect constructions, where the auxilliary and participle may not stand together. Exercise 4.13 Re-generate a result_perfect data frame, where you can keep track of: From the N-th sentence of the address did the Perfect come? (e.g., SENT_ID column below) From which president’s address did the Perfect come? (e.g., INDEX column below) You may have a data frame as shown below. Exercise 4.14 Re-create the above bar plot in a way that the type and token frequencies are computed based on each address and the x axis should be arranged accordingly (i.e., by the years and presidents). Your resulting graph should look similar to the one below. References "],
["parts-of-speech-tagging.html", "Chapter 5 Parts-of-Speech Tagging 5.1 Installing the Package 5.2 Quick Overview 5.3 Working Pipeline 5.4 Parsing Your Texts 5.5 Metalingusitic Analysis 5.6 Construction Analysis 5.7 Issues on Pattern Retrieval 5.8 Saving POS-tagged Texts 5.9 Finalize spaCy", " Chapter 5 Parts-of-Speech Tagging library(tidyverse) library(tidytext) library(quanteda) In many textual analyses, word classes can give us additional information about the text we analyze. These word classes typically are referred to as parts-of-speech tags of the words. In this chapter, we will show you how to POS tag a raw-text corpus to get the syntactic categories of words, and what to do with those POS tags. In particular, I will introduce a powerful package spacyr, which is an R wrapper to the spaCy— “industrial strength natural language processing” Python library from https://spacy.io. In addition to POS tagging, the package provides other linguistically relevant annotations for more in-depth analysis of the English texts. Again, the spaCy is optimized for many languages but Chinese. We will talk about Chinese text processing in a later chapter. 5.1 Installing the Package Please consult the spacyr github for more instructions on installing the package. There are at least four steps: Install miniconda (or any other conda version for Python) Install the spacyr R package install.packages(&quot;spacyr&quot;) Because spacyr is an R wrapper to a Python pacakge, you need to have Python installed in your OS system as well. The easiest way to install Python spaCy and R spacyr is through the function spacyr::spacy_install(). This function by default creates a new conda environment called spacy_condaenv, as long as some version of conda is installed on the user’s the system. (spacyr uses Python 3.6.x and spaCy 2.2.3+) library(spacyr) spacy_install(version=&#39;2.2.3&#39;) The spacy_install() will create a stand-alone conda environment including a python executable separate from your system Python (or anaconda python), install the latest version of spaCy (and its required packages), and download the English language model. If you don’t have any conda version installed on your system, you can install miniconda from [https://conda.io/miniconda.html]https://conda.io/miniconda.html. (Choose the 64-bit version.) Also, the spacy_install() will automatically install the miniconda (if there’s no conda installed on the system) for MAC users. Windows users may need to consult the spacyr github for more important instructions on installation. For Windows, you need to run R as an administrator to make installation work properly. To do so, right click the RStudio icon (or R desktop icon) and select “Run as administrator” when launching R. Initializing spaCy in R library(spacyr) spacy_initialize() ## Found &#39;spacy_condaenv&#39;. spacyr will use this environment ## successfully initialized (spaCy Version: 2.2.3, language model: en_core_web_sm) ## (python options: type = &quot;condaenv&quot;, value = &quot;spacy_condaenv&quot;) 5.2 Quick Overview The spacyr provides a useful function, spacy_parse(), which allows us to parse an English text in a very convenient way. txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt, pos = T, tag = T, lemma = T, entity = T, dependency = T) parsedtxt The output parsedtext is a data frame, which includes annotations of the original texts at multiple granularities. All texts have been tokenized into words with each word, sentence, and text given an unique ID (i.e., doc_id, sentence_id, token_id) Lemmatization is also done (i.e., lemma) POS Tags can also be found (i.e., pos and tag) pos: this column uses the Universal tagset for parts-of-speech, a general POS scheme that would suffice most needs, and provides equivalencies across languages tag: this column provides a more detailed tagset, defined in each spaCy language model. For English, this is the OntoNotes 5 version of the Penn Treebank tag set (cf. Penn Treebank Tagset) Depending on the argument setting for spacy_parse(), you can get more annotations, such as named entities (entity) and dependency relations (del_rel). 5.3 Working Pipeline In Chapter 4, we provide a primitive working pipeline for text analytics. Here we like to revise the workflow to satisfy different goals in computational text analytics (See Figure 5.1). After we secure a collection of raw texts as our corpus, if we do not need additional parts-of-speech information, we follow the workflow on the right. If we need additional annotations from spacyr, we follow the workflow on the left. Figure 5.1: English Text Analytics Flowchart 5.4 Parsing Your Texts Now let’s use this spacy_parse() to analyze the presidential addresses we’ve seen in Chapter 4: the data_corpus_inaugural from quanteda. To illustrate the annotation more clearly, let’s parse the first text in data_corpus_inaugural: library(quanteda) library(tidytext) doc1 &lt;- spacy_parse(data_corpus_inaugural[1]) doc1 We can parse the whole corpus collection as well: we first apply the spacy_parse to each text in data_corpus_inaugural using map() and then rbind() individual resulting data frames into one using do.call(). system.time( corp_us_words &lt;- data_corpus_inaugural %&gt;% map(spacy_parse, tag = T) %&gt;% do.call(rbind, .)) ## user system elapsed ## 13.705 1.164 14.870 The function system.time() is a useful function which gives you the CPU times that the expression in the parathesis used. In other words, you can put any R expression in the parenthesis of system.time() as its argument and measure the time required for the expression. This is sometimes necessary because some of the data processing can be very time consuming. And we would like to know HOW time-consuming it is in case that we may need to run the prodecure again. corp_us_words Before we move on, we need to clean up the doc_id column of corp_us_words. By default, spacy_parse() uses arbitrary ID’s for each text, e.g., text1, text2. We like the text ID’s to be more revealing, i.e., with the names and years of the presidents. corp_us_words &lt;-corp_us_words %&gt;% mutate(doc_id = str_replace(row.names(.), &quot;\\\\.\\\\d+$&quot;,&quot;&quot;)) Exercise 5.1 In corpus linguistics analysis, we often need to examine constructions on a sentential level. It would be great if we can transform the word-based data frame into a sentence-based one for more efficient later analysis. Also, on the sentential level, it would be great if we can preserve the information of the lexical POS tags. How can you transform the corp_us_words into one as provided below? (You may name the sentence-based data frame as corp_us_sents.) 5.5 Metalingusitic Analysis Now spacy_parse() has enriched our corpus data with more linguistic annotations. We can now utilize the additional POS tags for more analysis. In many applied linguistics studies, people sometimes look at the syntactic complexity of the language across a particular factor. For example, people may look at the syntacitc complexity development of L2 learners of varying proficiency levels, or of L1 speakers in different acquisition stages, or of writers in different genres (e.g., academic vs. nonacademic). To operationalize the construct sytactmic complexity, we use a simple metric, Fichtner's C, which is defined as: \\[ Fichtner&#39;s\\;C = \\frac{Number\\;of\\;Verbs}{Number\\;of\\;Sentences} \\times \\frac{Number\\;of\\;Words}{Number\\;of\\;Sentences} \\] Now we can take the corp_us_words and first generate the frequencies of verbs, and number of words for each presidential speech text. syn_com &lt;- corp_us_words %&gt;% group_by(doc_id) %&gt;% summarize(verb_num = sum(pos==&quot;VERB&quot;), sent_num = max(sentence_id), word_num = n()) %&gt;% mutate(F_C = (verb_num/sent_num)*(word_num/sent_num)) %&gt;% ungroup syn_com With the syntactic complexity of each president, we can plot the tendency: syn_com %&gt;% ggplot(aes(x = doc_id, y = F_C, fill = doc_id)) + geom_col() + theme(axis.text.x = element_text(angle=90)) + labs(title = &quot;Syntactic Complexty&quot;, x = &quot;Presidents&quot;, y = &quot;Fichtner&#39;s C&quot;) + guides(fill = F) It’s interesting to see a decreasing trend in syntactic complexity! Exercise 5.2 Please add a regression/smooth line to the above plot to indicate the downward trend? 5.6 Construction Analysis Now with parts-of-speech tags, we are able to look at more linguistic patterns or constructions in detail. These POS tags allow us to extract more precisely the target patterns we are interested in. In this section, we will use the output from Exercise 5.1. We assume that now we have a sentence-based corpus data frame, corp_us_sents. Here I like to provide a case study on English Preposition Phrases. ## ###################################### ## If you haven&#39;t finished the exercise, ## the dataset is also available in ## `demo_data/corp_us_sents.RDS ## ###################################### ## Uncomment this line if you dont have `corp_us_sents` # corp_us_sents &lt;- readRDS(&quot;demo_data/corp_us_sents.RDS&quot;) corp_us_sents We can utilize the regular expressions to extract PREP + NOUN combinations from the corpus data. # define regex patterns pattern_pat1 &lt;- &quot;[^/ ]+/ADP [^/]+/NOUN&quot; # extract patterns from corp corp_us_sents %&gt;% unnest_tokens(output = pat_pp, input = sentence_tag, token = function(x) str_extract_all(x, pattern=pattern_pat1)) -&gt; result_pat1 result_pat1 In the above example, we specify the token= argument in unnest_tokens(..., token = ...) with a self-defined function. The idea of tokenization in unnest_tokens() is that the token argument should be a function which takes a text-based vector as input (i.e, each element of the input vector may be a document text) and returns a list, each element of which is a token-based version (i.e., vector) of the original input vector element (cf. Figure 5.2). Figure 5.2: Intuition for token= in unnest_tokens() In our demonstration, we define a tokenization function, which takes sentence_tag as the input and returns a list, each element of which consists a vector of tokens matching the regular expressions in individual sentences in sentence_tag. (Note: The function object is not assigned to an object name, thus never being created in the R working session.) Exercise 5.3 Create a new column, pat_clean, with all annotations removed in the data frame result_pat1. With these constructional tokens of English PP’s, we can then do further analysis. We first identify the PREP and NOUN for each constructional token. We then clean up the data by removing POS annotations. # extract the prep and head result_pat1 %&gt;% tidyr::separate(col=&quot;pat_pp&quot;, into=c(&quot;PREP&quot;,&quot;NOUN&quot;), sep=&quot;\\\\s+&quot; ) %&gt;% mutate(PREP = str_replace_all(PREP, &quot;/[^ ]+&quot;,&quot;&quot;), NOUN = str_replace_all(NOUN, &quot;/[^ ]+&quot;,&quot;&quot;)) -&gt; result_pat1a result_pat1a Now we are ready to explore the text data. We can look at how each preposition is being used by different presidents: # President Top 2 prep result_pat1a %&gt;% count(doc_id, PREP) %&gt;% arrange(doc_id, desc(n)) We can examine the most frequent NOUN that co-occurrs with each PREP: # Most freq NOUN for each PREP result_pat1a %&gt;% count(PREP, NOUN) %&gt;% group_by(PREP) %&gt;% top_n(1,n) %&gt;% arrange(desc(n)) We can also look at a more complex usage pattern: how each president uses the PREP of in terms of their co-occurring NOUNs? # NOUNS for `of` uses across different presidents result_pat1a %&gt;% filter(PREP == &quot;of&quot;) %&gt;% count(doc_id, PREP, NOUN) %&gt;% tidyr::pivot_wider( id_cols = c(&quot;doc_id&quot;), names_from = &quot;NOUN&quot;, values_from = &quot;n&quot;, values_fill = list(n=0)) Exercise 5.4 In our earlier demonstration, we made a naive assumption: Preposition Phrases include only those cases where PREP and NOUN are adjacent to each other. But there are many more tokens where words do come between the PREP and the NOUN (e.g., with greater anxieties, by your order). Please revise the regular expression to improve the retrieval of the English Preposition Phrases from the corpus data corp_us_sents. Specifically, we can define an English PP as a sequence of words, which start with a preposition, and end at the first word after the preposition that is tagged as NOUN, PROPN, or PRON. Exercise 5.5 Based on the output from Exercise 5.4, please identify the PREP and NOUN for each constructional token and save information in two new columns. 5.7 Issues on Pattern Retrieval Any automatic pattern retrieval comes with a price: there are always errors returned by the system. I would like to discuss this issue based on the second text, 1793-Washington. First let’s take a look at the Preposition Phrases extracted by my regular expression used in Exercise 5.4 and 5.5: ## ###################################### ## If you haven&#39;t finished the exercise, ## the dataset is also available in ## `demo_data/result_pat2a.RDS ## ###################################### ## uncomment this line if you dont have `result_pat2a` # result_pat2a &lt;- readRDS(&quot;demo_data/result_pat2a.RDS&quot;) result_pat2a %&gt;% filter(doc_id == &quot;1793-Washington&quot;) My regular expression has identified 20 PP’s from the text. However, if we go through the text carefully and do the PP annotation manually, we may have different results. Figure 5.3: Manual Annotation of English PP’s in 1793-Washington There are two types of errors: False Positives: Patterns identified by the system but in fact they are not true patterns. False Negatives: True patterns in the data but are not successfully identified by the system. As shown in Figure 5.3, manual annotations have identified 21 PP’s from the text while the regular expression identified 20 tokens. A comparison of the two results shows that: In the regex result, the following returned tokens (rows highlighted in red) are False Positives—the regular expression identified them as PP but in fact they were NOT PP according to manual annotations. doc_id sentence_id PREP NOUN pat_pp row_id 1793-Washington 1 by voice by/adp the/det voice/noun 1 1793-Washington 1 of country of/adp my/det country/noun 2 1793-Washington 1 of chief of/adp its/det chief/propn 3 1793-Washington 2 for it for/adp it/pron 4 1793-Washington 2 of honor of/adp this/det distinguished/adj honor/noun 5 1793-Washington 2 of confidence of/adp the/det confidence/noun 6 1793-Washington 2 in me in/adp me/pron 7 1793-Washington 2 by people by/adp the/det people/noun 8 1793-Washington 2 of united of/adp united/propn 9 1793-Washington 3 to execution to/adp the/det execution/noun 10 1793-Washington 3 of act of/adp any/det official/adj act/noun 11 1793-Washington 3 of president of/adp the/det president/propn 12 1793-Washington 3 of office of/adp office/noun 13 1793-Washington 4 in presence in/adp your/det presence/noun 14 1793-Washington 4 during administration during/adp my/det administration/noun 15 1793-Washington 4 of government of/adp the/det government/propn 16 1793-Washington 4 in instance in/adp any/det instance/noun 17 1793-Washington 5 to upbraidings to/adp the/det upbraidings/noun 18 1793-Washington 5 of who of/adp all/det who/pron 19 1793-Washington 5 of ceremony of/adp the/det present/adj solemn/adj ceremony/noun 20 In the above manual annotation (Figure 5.3), phrases highlighted in red are NOT successfully identified by the current regex query, i.e., False Negatives. We can summarize the pattern retrieval results as: Most importantly, we can describe the quality of the pattern retrieval with two important measures. \\(Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Positives}\\) \\(Precision = \\frac{True\\;Positives}{True\\;Positives + False\\;Negatives}\\) In our case: \\(Precision = \\frac{18}{18+2} = 90%\\) \\(Precision = \\frac{18}{18 + 3} = 85.71%\\) It is always very difficult to reach 100% precision or 100% recall for automatic retrieval of the target patterns. Researchers often need to make a compromise. The following are some heuristics based on my experiences: For small datasets, probably manual annotations give the best result. For moderate-sized dataset, semi-automatic annotations may help. Do the automatic annotations first and follow up with manual checkups. For large datasets, automatic annotations are preferred in order to examine the general tendency. However, it is always good to have a random sample of the data to check the query performance. The more semantics-related the annotations, the more likely one would adopt a manual approach to annotation (e.g., conceptual metaphors, sense distinctions, dialogue acts). Common annotations of corpus data may prefer an automatic approach, such as Chinese word segmentation, POS tagging, named entity recognition, chunking, noun-phrase extractions, or dependency relations(?). 5.8 Saving POS-tagged Texts We may very often get back to our corpus texts again and again when we explore the data. In order NOT to re-tag the texts every time we analyze the data, it would be more convenient if we save the tokenized texts with the POS tags in external files. Next time we can directly load these files without going trough the POS-tagging again. However, when saving the POS-tagged results to an external file, it is highly recommended to keep all the tokens of the original texts. That is, leave all the word tokens as well as the non-word tokens intact. A few suggestions: If you are dealing with a small corpus, I would probably suggest you to save the resulting data frame from spacy_parse() as a csv for later use. If you are dealing with a big corpus, I would probably suggest you to save the parsed output of each text file in an independent csv for later use. write_csv(corp_us_words, &quot;corp-inaugural-word.csv&quot;) 5.9 Finalize spaCy While running spaCy on Python through R, a Python process is always running in the background and Rsession will take up a lot of memory (typically over 1.5GB). spacy_finalize() terminates the Python process and frees up the memory it was using. spacy_finalize() Exercise 5.6 In this exercise, please use the corpus data provided in quanteda.textmodels::data_corpus_moviereviews. This dataset is provided as a corpus object in the package quanteda.textmodels (please install the package on your own). The data_corpus_moviereviews includes 2,000 movie reviews. Please use the spacyr to parse the texts and provide the top 20 adjectives for positive and negative reviews respectively. Adjectives are naively defined as any words whose pos tags start with “J”. When computing the word frequencies, please use the lemmas instead of the word forms. Please provide the top 20 words that are content words for positive and negative reviews ranked by a weighted score, which is computed using the formula provided below. Content words are naively defined as any words whose pos tags satrt with N, V, or J. \\[Word\\;Frequency \\times log(\\frac{Numbe\\; of \\; Documents}{Word\\;Diserpsion}) \\] For example, if the lemma action occurs 691 times in the negative reviews collection. These occurrences are scattered in 337 different documents. There are 1000 negative texts in the current corpus. Then the wegithed score for action is: \\[691 \\times log(\\frac{1000}{337}) = 751.58 \\] summary(quanteda.textmodels::data_corpus_moviereviews) ans1 ans2 In our earlier chapters, we have discussed the issues of word frequencies and their significance in relation to the dispersion of the words in the entire corpus. In terms of identifying important words from a text collection, our assumption is that: if a word is scattered in almost every documnt in the corpus collection, it is probably less informative. For example, words like a, the would probably be observed in all documents included in the corpus. Therefore, the high frequencies of these widely-dispersed words may not be as important compared to the high frequencies of those which occur in only a subset of the corpus collection. The word frequency is sometimes referred to as term frequency (tf) in information retrieval; the dispersion of the word is referred to as document frequency (df). In information retrieval, people often use a weighting scheme for word frequencies in order to extract informative words from the text collection. The scheme is as follows: \\[tf \\times log(\\frac{N}{df}) \\] N refers to the total number of documents in the corpus. The \\(log\\frac{N}{df}\\) is referred to as inversed document frequency (idf). This tf.idf weighting scheme is popular in many practical applications. "],
["keyword-analysis.html", "Chapter 6 Keyword Analysis 6.1 About Keywords 6.2 Statistics for Keyness 6.3 Implementation 6.4 Tidy Data 6.5 Computing Keynesss", " Chapter 6 Keyword Analysis In this chapter, I would like to talk about the idea of “keyness”. Keywords in corpus linguistics are defined statistically using different measures of keyness. Keyness can be computed for words occurring in a target corpus by comparing their frequencies in that target corpus to that of a reference corpus and quantifying the relative attraction of each word to the target corpus by means of a statistical association metric. This idea of course can be easily extended to key phrases as well. In other words, for keyword analysis, we assume that there is a reference corpus on which the keyness of the words in the target corpus is computed. 6.1 About Keywords Keywords are important for research on languagea and ideology. To discover distinct groups through ideological inclinations expressed in writings, most researchers draw inspiration from Raymond Williams’s idea of keywords, terms presumably carrying sociocultural meanings characteristic of Western capitalist ideologies (Williams 1976). In contrast to William’s intuition-based approach, recent studies have promoted a bottom-up corpus-based method to discover terminology reflecting the ideological undercurrents of the text collections. This data-driven approach to keywords is sympathetic to the notion of statistical keywords popularized by Michael Stubbs (Stubbs 1996, 2003). For a comprehensive discussion on the statistical nature of keyword analysis, please see Gabrielatos (2018). 6.2 Statistics for Keyness To compute the keyness of a word w, we often need a contingency table as shown in Figure 6.1: Figure 6.1: Frequency distributions of a word and all other words in two corpora Different keyness statistics may have different ways to evaluate the relative importance of the co-occurrences of the word w with the target and the reference corpus (i.e., a and b in Figure 6.1) and statistically determine which connection is stronger. In this chapter, I would like to discuss two common statistics used in keyness analysis. This tutorial is based on Gries (2018), Ch. 5.2.6 the log-likelihood ratio G2 (Dunning 1993); \\[ G^2 = 2 \\sum_{i=1}^4 obs \\times ln \\frac{obs}{exp} \\] difference coefficient (Leech and Fallon 1992); \\[ difference\\;coefficient = \\frac{a - b}{a + b} \\] relative frequency ratio (Damerau 1993) \\[ rfr = \\frac{(a\\div b)}{(a+c)\\div(b+d)} \\] 6.3 Implementation In this tutorial we will use two documents as our mini reference and target corpus. These two documents are old Wikipedia entries (provided in Gries (2018)) on Perl and Python respectively. First we initialize necessary packages in R library(tidyverse) library(tidytext) library(readtext) library(quanteda) Then we load the corpus data, which are available as two text files in our demo_data, and transform the corpus into a tidy structure #flist&lt;- dir(&quot;demo_data&quot;,pattern = &quot;corp_\\\\w+.txt&quot;, full.names = T) flist &lt;- c(&quot;demo_data/corp_perl.txt&quot;, &quot;demo_data/corp_python.txt&quot;) corpus &lt;- readtext(flist) %&gt;% corpus %&gt;% tidy %&gt;% mutate(textid = basename(flist)) %&gt;% select(textid, text) We convert the text-based corpus into a word-based one for word frequency information corpus_word &lt;- corpus %&gt;% unnest_tokens(word, text, token=&quot;words&quot;) #%&gt;% #group_by(textid) %&gt;% #mutate(word_id = row_number()) %&gt;% #ungroup corpus_word %&gt;% head(100) Now we need to get the frequencies of each word in the two documents respectively. corpus_word %&gt;% count(word, textid, sort = T) -&gt; word_freq word_freq As now we are analyzing each word in relation to the two corpora, it would be better for us to have each word type as one independent row, and columns recording their co-occurrence frequencies with the two corpora. How do we get this? 6.4 Tidy Data Now I would like to talk about the idea of tidy dataset before we move on. Wickham and Grolemund (2017) suggests that a tidy dataset needs to satisfy the following three interrelated rules: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In our current word_freq, there should be no doubt that our observation is a word. Therefore, a tidy dataset needs to have every independent word (type) as an independent row in the table. However, each row in word_freq in fact represents the combination of (word, textid) because the column n contains the values for those variables. In addition, the same word appears twice in the dataset in the rows (e.g., the, a) This is the real life: we often encounter datasets that are NOT tidy at all. Instead of expecting others to do the tidying for you (which is very unlikely), we might as well learn how to deal with messy dataset. Wickham and Grolemund (2017) suggests two common strategies data scientists often apply: One variable might be spread across multiple columns (from long to wide) One observation might be scattered across multiple rows (from wide to long) 6.4.1 An Long-to-Wide Example Here I would like to illustrate the idea of tidy dataset and also ways of tidying with a simple dataset provided in Wickham and Grolemund (2017), Chapter 12. people &lt;- tribble( ~name, ~profile, ~values, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) people The above dataset people is not tidy because the column profile contains more than one variable. An observation (e.g., Phillip Woods) is scattered across several rows. To tidy up people, we need strategy I: One variable might be spread across multiple columns The function tidyr::pivot_wider() is made for this. There are two important parameters in pivot_wider(): names_from = ...: The column to take variable names from. Here it’s profile. values_from = ...: The column to take values from. Here it’s values. Figure 6.2: From Long to Wide: pivot_wider() require(tidyr) people %&gt;% pivot_wider(names_from = profile, values_from = values) 6.4.2 A Wide-to-Long Example preg &lt;- tribble( ~pregnant, ~male, ~female, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) preg The above dataset preg can be tidied up as follows: we can have a column sex we can have a column pregnant we can have a column count (representing the number of observations for the combinations of sex and pregnant) In other words, we need strategy II: One observation might be scattered across multiple rows (from wide to long) (Each level combination of pregnant and sex can be one observation.) And the function tidyr::pivot_longer() is made for this. There are three important parameters: cols = ...: The set of columns whose names are values, not variables. Here they are male and female. names_to = ...: The name of the variable to move the column names to. Here it is sex. values_to = ...: The name of the variable to move the values to. Here it is count. Figure 6.3: From Wide to Long: pivot_longer() preg %&gt;% pivot_longer(cols=c(&quot;male&quot;,&quot;female&quot;), names_to = &quot;sex&quot;, values_to = &quot;count&quot;) 6.4.3 Word Frequency Transformation Now back to our word_freq: word_freq %&gt;% head(10) Now it is probably clearer to you what we should do to tidy up word_freq. It is obvious that some words (observations) are scattered across several rows and the column textid can be pivoted into two variables: “perl” vs. “python”. In order words, we need strategy I: to make this table wider. One variable might be spread across multiple columns Exercise 6.1 Could you make word_freq wider as shown below and save the wide version in contingency_table? contingency_table Before we compute the keyness, we restructure the table by: including words consisting of alphabets only; renaming the columns to match the cell labels in Figure 6.1 above; creating necessary frequencies (columns) for keyness computation contingency_table %&gt;% filter(word %&gt;% str_detect(&quot;^[a-zA-Z]+&quot;)) %&gt;% rename(a = corp_perl.txt, b = corp_python.txt) %&gt;% mutate(c = sum(a)-a, d = sum(b)-b) -&gt; contingency_table contingency_table 6.5 Computing Keynesss With all necessary frequencies, we can compute the three keyness statistics for each word contingency_table %&gt;% mutate(a.exp = ((a+b)*(a+c))/(a+b+c+d), b.exp = ((a+b)*(b+d))/(a+b+c+d), c.exp = ((c+d)*(a+c))/(a+b+c+d), d.exp = ((c+d)*(b+d))/(a+b+c+d), G2 = 2*(a*log(a/a.exp) + b*log(b/b.exp) + c*log(c/c.exp) + d*log(d/d.exp)), DC= (a-b)/(a+b), RFR = (a/b)/((a+c)/(b+d))) %&gt;% arrange(desc(G2)) -&gt; keyness_table keyness_table Although now we have the keyness values for words, we still don’t know to which corpus the word is more attracted. What to do next? keyness_table %&gt;% mutate(preference = ifelse(a &gt; a.exp, &quot;perl&quot;,&quot;python&quot;)) %&gt;% select(word, preference, everything())-&gt; keyness_table keyness_table Exercise 6.2 The CSV in demo_data/data-movie-reviews.csv is the IMDB dataset with 50,000 movie reviews and their sentiment tags (source: Kaggle). The CSV has two columns–the first column review includes the raw texts of each movie review; the second column sentiment provides the sentiment tag for the review. Each review is either positive or negative. We can treat the dataset as two separate corpora: negative and positive corpora. Please find the top 10 keywords for each corpus ranked by the G2 statistics. In the data preprocessing, please use the default tokenization in unnest_tokens(..., token = &quot;words&quot;). When computing the keyness, please exclude: words with at least one non-alphanumeric symbols in them (e.g. regex class \\W) words whose frequency is &lt; 10 in each corpus The expected results are provided below for your reference. References "],
["chinese-text-processing.html", "Chapter 7 Chinese Text Processing 7.1 Chinese Word Segmenter jiebaR 7.2 Chinese Text Analytics Pipeline 7.3 Comparing Tokenization Methods 7.4 Data 7.5 Loading Text Data 7.6 quanteda::tokens() vs. jiebaR::segment() 7.7 Case Study 1: Word Frequency and Wordcloud 7.8 Case Study 2: Patterns 7.9 Case Study 3: Lexical Bundles 7.10 Afterwords", " Chapter 7 Chinese Text Processing In this chapter, we will discuss one of the most important issues in Chinese language/text processing, i.e., word segmentation. When we discuss English parts-of-speech tagging in Chapter 5, it is easy to do the word tokenization in English because the word boundaries in English are more clearly delimited by whitespaces. Chinese, however, does not have whitespaces between characters, which leads to a serious problem for word tokenization. This chapter is devoted to Chinese text processing. We will look at the issues of word tokenization and talk about the most-often used library, jiebaR, for Chinese word segmentation. Also, we will include several case studies on Chinese text processing. In later Chapter 9, we will introduce another segmenter for Taiwan Mandarin, i.e., the CKIP Tagger, which comes with more functionalities. library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 7.1 Chinese Word Segmenter jiebaR 7.1.1 Start First, if you haven’t installed the library jiebaR, you may need to install it manually: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) This is the version used for this tutorial. packageVersion(&quot;jiebaR&quot;) ## [1] &#39;0.11&#39; Now let us take a look at a quick example. Let us assume that in our corpus, we have collected only one text document, with only a short paragraph. text &lt;- &quot;綠黨桃園市議員王浩宇爆料，指民眾黨不分區被提名人蔡壁如、黃瀞瑩，在昨（6）日才請辭是為領年終獎金。台灣民眾黨主席、台北市長柯文哲7日受訪時則說，都是按流程走，不要把人家想得這麼壞。&quot; There are two important steps in Chinese word segmentation: Initilzie a word segmenter object using worker() Segment the texts using segment() seg1 &lt;- worker() segment(text, jiebar = seg1) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; To segment the document, text, you first initialize a segmenter seg1 using worker() and feed this segmenter to segment(jiebar = seg1)and segment text into words. 7.1.2 Settings There are many different parameters you can specify when you initialize the segmenter worker(). You may get more detail via the documentation ?worker. Some of the important arguments include: user = ...: This argument is to specify the path to a user-defined dictionary stop_word = ...: This argument is to specify the path to a stopword list symbol = FALSE: Whether to return symbols (the default is FALSE) bylines = FALSE: Whether to return a list or not (crucial if you are using tidytext::unnest_tokens()) Exercise 7.1 In our earlier example, when we created the segmenter seg1, we did not specify any arguments for worker(). Can you tell what are the default settings for the parameters of worker()? Please try to create work() with different settings and see how the segmented results differ from each other. 7.1.3 User-defined dictionary From the above example, it is clear to see that some of the words are not correctly identified by the current segmenter: for example, 民眾黨, 不分區, 黃瀞瑩, 柯文哲. It is always recommended to include a user-defined dictionary when doing the word segmentation because different corpora may have their own unique vocabulary. This can be done when you initialize the segmenter using worker(..., user = ...). seg2 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;) #segment(text, seg1) segment(text, seg2) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; ## [19] &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [25] &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; ## [31] &quot;時則&quot; &quot;說&quot; &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; ## [37] &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; ## [43] &quot;壞&quot; The format of the user-defined dictionary is a text file, with one word per line. Also, the default encoding of the dictionary is UTF-8. Please note that in Windows, the default encoding of a txt file created by Notepad may not be UTF-8. Creating a user-defined dictionary may take a lot of time. You may consult 搜狗詞庫, which includes many domain-specific dictionaries created by others. However, it should be noted that the format of the dictionaries is .scel. You may need to convert the .scel to .txt before you use it in jiebaR. To do the coversion automatically, please consult the library cidian. Also, you need to do the traditional-simplified Chinese conversion as well. For this, you may consult the library ropencc in R. 7.1.4 Stopwords When you initialize the segmenter, you can also specify a stopword list, i.e., words you do not need to include in the later analyses. For example, in text mining, functional words are usually less informative. seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg3) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; ## [7] &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; ## [13] &quot;黃瀞瑩&quot; &quot;在昨&quot; &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; ## [19] &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; ## [25] &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;按&quot; ## [31] &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; ## [37] &quot;這麼&quot; &quot;壞&quot; 7.1.5 POS Tagging So far we haven’t seen the parts-of-speech tags provided by the word segmenter. If you need the POS tags of the words, you need to specify the argument type = &quot;tag&quot; when you initialize the worker(). seg4 &lt;- worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) segment(text, seg4) ## n ns n x n n x ## &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指&quot; &quot;民眾黨&quot; ## x p v n x x x ## &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡壁如&quot; &quot;黃瀞瑩&quot; &quot;在昨&quot; ## x d v x n x x ## &quot;6&quot; &quot;才&quot; &quot;請辭&quot; &quot;為領&quot; &quot;年終獎金&quot; &quot;台灣&quot; &quot;民眾黨&quot; ## n ns n x x v x ## &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;受訪&quot; &quot;時則&quot; ## zg p n v df p n ## &quot;說&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; &quot;把&quot; &quot;人家&quot; ## x r a ## &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; Every POS tagger has its own predefined tagset. The following table lists the annotations of the POS tagsets used in jiebaR: 7.1.6 Default You can check the dictionaries and the stopword list being used by jiebaR in your current enviroment: # show files under `dictpath` dir(show_dictpath()) # Check the default stop_words list # Please change the path to your default dict path # scan(file=&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, # what=character(),nlines=50,sep=&#39;\\n&#39;, # encoding=&#39;utf-8&#39;,fileEncoding=&#39;utf-8&#39;) readLines(&quot;/Library/Frameworks/R.framework/Versions/3.6/Resources/library/jiebaRD/dict/stop_words.utf8&quot;, n = 50) 7.1.7 Reminder When we use segment() as a tokenization method in the unnest_tokens(), it is very important to specify bylines = TRUE in worker(). This setting would make sure that segment() takes a text-based vector as input and returns a list of word-based vectors of the same length as output. NB: When bylines = FALSE, segment() returns a vector. seg_byline_1 &lt;- worker(bylines = T) seg_byline_0 &lt;- worker(bylines = F) (text_tag_1 &lt;- segment(text, seg_byline_1)) ## [[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; (text_tag_0 &lt;- segment(text, seg_byline_0)) ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;指民眾&quot; ## [7] &quot;黨&quot; &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; &quot;在昨&quot; &quot;6&quot; ## [19] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為領&quot; &quot;年終獎金&quot; ## [25] &quot;台灣民眾&quot; &quot;黨&quot; &quot;主席&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文&quot; ## [31] &quot;哲&quot; &quot;7&quot; &quot;日&quot; &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; ## [37] &quot;都&quot; &quot;是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;不要&quot; ## [43] &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; class(text_tag_1) ## [1] &quot;list&quot; class(text_tag_0) ## [1] &quot;character&quot; 7.2 Chinese Text Analytics Pipeline In Chapter 4, we have talked about the work pipeline for normal English texts processing, as shown below: Figure 7.1: English Text Analytics Flowchart For Chinese texts, the work flow is pretty much the same. In the following Chinese Text Analytics Flowchar (Figure 7.2), I have highlighted the steps that are crucial to Chinese processing. Because the current Chinese word segmenter jiebaR does not return the results in a tidy structure format, the most important trick is that when tokenizing the raw texts using unnest_tokens(), we need to specify our own tokenzier for the argument token = ... in the unnest_tokens(). Figure 7.2: Chinese Text Analytics Flowchart It is important to note that when we specify a self-defined unnest_tokens(…,token=…) function, this function should take a character vector (i.e., a text-based vector) and return a list of character vectors (i.e., word-based vectors) of the same length. In other words, when initializing the Chinese word segmenter, we need to specify the argument worker(…, byline = TRUE). So based on our simple corpus example above, we first transform the character vector text into a corpus object—text_corpus. With this, we can apply quanteda::summary() and quanteda::kwic() with the corpus object. text_corpus &lt;- text %&gt;% corpus summary(text_corpus) kwic(text_corpus, pattern = &quot;柯&quot;) kwic(text_corpus, pattern = &quot;柯文哲&quot;) Exercise 7.2 Do you know why there are no tokens of concordance lines from kwic(text_corpus, pattern = &quot;柯文哲&quot;)? We can also transform the corpus object into a text-based TIBBLE using tidy(). Also, we generate an unique index for each row using row_number(). # a text-based tidy corpus text_corpus_tidy &lt;-text_corpus %&gt;% tidy %&gt;% mutate(textID = row_number()) text_corpus_tidy For word segmentation, we initialize the jiebaR Chinese segmenter using work(). # initialize segmenter my_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) Finally, we use unnest_tokens() to tokenize the text-based TIBBLE text_corpus_tidy into word-based TIBBLE text_corpus_tidy_word. That is, texts included in the text column are tokenized into words, which are unnested into rows of the word column in the new TIBBLE. # tokenization text_corpus_tidy_word &lt;- text_corpus_tidy %&gt;% unnest_tokens(word, text, token = function(x) segment(x, jiebar = my_seg)) text_corpus_tidy_word 7.3 Comparing Tokenization Methods quanteda also provides its own default word tokenization for Chinese texts. However, its default tokenization method does not allow us to add our own dictionary to the segmentation process, which renders the results less reliable. We can compare the the two results. we can use quanteda::tokens() to see how quanteda tokenizes Chinese texts. The function returns a tokens object. # create TOKENS object using quanteda default text_corpus %&gt;% tokens -&gt; text_tokens_qtd we can also use our own tokenization function segment() and convert the list to a tokens object using as.tokens(). (This of course will give us the same tokenization result as we get in the earlier unnest_tokens() because we are using the same segmenter my_seg.) # create TOKENS object manually text_corpus %&gt;% segment(jiebar = my_seg) %&gt;% as.tokens -&gt; text_tokens_jb Now let’s compare the two resulting tokens objects: These are the tokens based on self-defined segmenter: # compare our tokenization with quanteda tokenization text_tokens_jb[[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王浩宇&quot; &quot;爆料&quot; &quot;，&quot; ## [7] &quot;指&quot; &quot;民眾黨&quot; &quot;不分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; ## [13] &quot;蔡壁如&quot; &quot;、&quot; &quot;黃瀞瑩&quot; &quot;，&quot; &quot;在昨&quot; &quot;（&quot; ## [19] &quot;6&quot; &quot;）&quot; &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; ## [25] &quot;為領&quot; &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾黨&quot; &quot;主席&quot; ## [31] &quot;、&quot; &quot;台北&quot; &quot;市長&quot; &quot;柯文哲&quot; &quot;7&quot; &quot;日&quot; ## [37] &quot;受訪&quot; &quot;時則&quot; &quot;說&quot; &quot;，&quot; &quot;都&quot; &quot;是&quot; ## [43] &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; &quot;不要&quot; &quot;把&quot; ## [49] &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; &quot;。&quot; These are the tokens based on default quanteda tokenizer: text_tokens_qtd[[1]] ## [1] &quot;綠黨&quot; &quot;桃園市&quot; &quot;議員&quot; &quot;王&quot; &quot;浩&quot; &quot;宇&quot; ## [7] &quot;爆&quot; &quot;料&quot; &quot;，&quot; &quot;指&quot; &quot;民眾&quot; &quot;黨&quot; ## [13] &quot;不&quot; &quot;分區&quot; &quot;被&quot; &quot;提名&quot; &quot;人&quot; &quot;蔡&quot; ## [19] &quot;壁&quot; &quot;如&quot; &quot;、&quot; &quot;黃&quot; &quot;瀞&quot; &quot;瑩&quot; ## [25] &quot;，&quot; &quot;在&quot; &quot;昨&quot; &quot;（&quot; &quot;6&quot; &quot;）&quot; ## [31] &quot;日&quot; &quot;才&quot; &quot;請辭&quot; &quot;是&quot; &quot;為&quot; &quot;領&quot; ## [37] &quot;年終獎金&quot; &quot;。&quot; &quot;台灣&quot; &quot;民眾&quot; &quot;黨主席&quot; &quot;、&quot; ## [43] &quot;台北市&quot; &quot;長&quot; &quot;柯&quot; &quot;文&quot; &quot;哲&quot; &quot;7&quot; ## [49] &quot;日&quot; &quot;受&quot; &quot;訪&quot; &quot;時&quot; &quot;則&quot; &quot;說&quot; ## [55] &quot;，&quot; &quot;都是&quot; &quot;按&quot; &quot;流程&quot; &quot;走&quot; &quot;，&quot; ## [61] &quot;不要&quot; &quot;把&quot; &quot;人家&quot; &quot;想得&quot; &quot;這麼&quot; &quot;壞&quot; ## [67] &quot;。&quot; Therefore, for linguistic analysis, I would suggest to define own Chinese word segmenter, which is tailored to specific tasks/corpora. 7.4 Data In the following sections, we look at a few more case studies of Chinese text processing using the news articles collected from Apple News as our example corpus. The dataset is available in our course dropbox drive: demo_data/applenews10000.tar.gz. (This dataset was collected by Meng-Chen Wu when he was working on his MA thesis project with me years ago. The demo data here was a random sample of the original Apple News Corpus.) 7.5 Loading Text Data When we need to load text data from external files (e.g., txt, tar.gz files), there is a simple and powerful R package for loading texts: readtext. The main function in this package, readtext(), which takes a file or a directory name from disk or a URL, and returns a type of data.frame that can be used directly with the corpus() constructor function in quanteda, to create a quanteda corpus object. In other words, the output from readtext can be directly passed on to the processing in the tidy structure framework (i.e., tidytext::unnest_tokens()). The function readtext() works on: text (.txt) files; comma-separated-value (.csv) files; XML formatted data; data from the Facebook API, in JSON format; data from the Twitter API, in JSON format; and generic JSON data. The corpus constructor command corpus() works directly on: a vector of character objects, for instance that you have already loaded into the workspace using other tools; a data.frame containing a text column and any other document-level metadata the output of readtext::readtext() # loading the corpus # NB: this may take some time apple_corpus &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% corpus summary(apple_corpus, 10) 7.6 quanteda::tokens() vs. jiebaR::segment() In Chapter 4, we’ve seen that after we create a corpus object, we can apply kwic() to get the concordance lines of a particular word. At that time, we emphasized that this worked because quanteda underlyingly tokenized the texts behind the scene. We can do the same the with Chinese texts as well: kwic(apple_corpus, &quot;勝率&quot;) In Section 7.3, I have made it clear that quanteda does have its own tokenization method (i.e., tokens()) for Chinese texts. It uses the tokenizer, stringi::stri_split_boundaries, which utilizes a library called ICU (International Components for Unicode) and the library uses dictionaries for segmentation of texts in Chinese. The biggest problem is that we cannot add our own dictionary when using the default tokenization tokens() (at least I don’t know how). In other words, when we apply kwic() to apple_corpus, quanteda tokenizes the Chinese texts using its default tokenizer and perform the keyword-in-context search. Like we did in Section 7.3, we can compare the word segmentation results between quanteda defaults and jiebaR (with own dictionary) with our current news corpus. First we tokenize all texts in apple_corpus using jiebaR::segment() and the segmenter initilized with user-defined dictionary. Second, we convert the returned list from segment() into a tokens object using as.tokens(). Third, we use quanteda default tokens() to convert the corpus object into tokens object. # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) # Tokenization using jiebaR apple_corpus %&gt;% segment(jiebar = segmenter) %&gt;% as.tokens -&gt; apple_tokens # Tokenization using qunateda::tokens() apple_corpus %&gt;% tokens -&gt; apple_tokens_qtd Now we can compare the two versions of word segmentation. Let’s take a look at the first document: apple_tokens[[1]] %&gt;% length ## [1] 168 apple_tokens_qtd[[1]] %&gt;% length ## [1] 148 apple_tokens[[1]] %&gt;% as.character ## [1] &quot;《&quot; &quot;蘋果&quot; &quot;體育&quot; &quot;》&quot; &quot;即日起&quot; &quot;進行&quot; &quot;虛擬&quot; &quot;賭盤&quot; ## [9] &quot;擂台&quot; &quot;，&quot; &quot;每名&quot; &quot;受邀&quot; &quot;參賽者&quot; &quot;進行&quot; &quot;勝負&quot; &quot;預測&quot; ## [17] &quot;，&quot; &quot;每周&quot; &quot;結算&quot; &quot;在&quot; &quot;周二&quot; &quot;公布&quot; &quot;，&quot; &quot;累積&quot; ## [25] &quot;勝率&quot; &quot;前&quot; &quot;3&quot; &quot;高&quot; &quot;參賽者&quot; &quot;可&quot; &quot;繼續&quot; &quot;參賽&quot; ## [33] &quot;，&quot; &quot;單周&quot; &quot;勝率&quot; &quot;最高者&quot; &quot;，&quot; &quot;將&quot; &quot;加封&quot; &quot;「&quot; ## [41] &quot;蘋果&quot; &quot;波神&quot; &quot;」&quot; &quot;頭銜&quot; &quot;。&quot; &quot;註&quot; &quot;:&quot; &quot;賭盤&quot; ## [49] &quot;賠率&quot; &quot;如有&quot; &quot;變動&quot; &quot;，&quot; &quot;以&quot; &quot;台灣&quot; &quot;運彩&quot; &quot;為主&quot; ## [57] &quot;。&quot; &quot;\\n&quot; &quot;資料&quot; &quot;來源&quot; &quot;：&quot; &quot;NBA&quot; &quot;官網&quot; &quot;http&quot; ## [65] &quot;:&quot; &quot;/&quot; &quot;/&quot; &quot;www&quot; &quot;.&quot; &quot;nba&quot; &quot;.&quot; &quot;com&quot; ## [73] &quot;\\n&quot; &quot;\\n&quot; &quot;金塊&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;103&quot; ## [81] &quot;：&quot; &quot;92&quot; &quot; &quot; &quot;76&quot; &quot;人&quot; &quot;騎士&quot; &quot;(&quot; &quot;主&quot; ## [89] &quot;)&quot; &quot; &quot; &quot;88&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;快艇&quot; &quot;活塞&quot; ## [97] &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;92&quot; &quot;：&quot; &quot;75&quot; &quot; &quot; ## [105] &quot;公牛&quot; &quot;勇士&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;108&quot; &quot;：&quot; ## [113] &quot;82&quot; &quot; &quot; &quot;灰熊&quot; &quot;熱火&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; ## [121] &quot;103&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;灰狼&quot; &quot;籃網&quot; &quot;(&quot; &quot;客&quot; ## [129] &quot;)&quot; &quot; &quot; &quot;90&quot; &quot;：&quot; &quot;82&quot; &quot; &quot; &quot;公鹿&quot; &quot;溜&quot; ## [137] &quot;馬&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;111&quot; &quot;：&quot; &quot;100&quot; ## [145] &quot; &quot; &quot;馬刺&quot; &quot;國王&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; &quot; &quot; &quot;112&quot; ## [153] &quot;：&quot; &quot;102&quot; &quot; &quot; &quot;爵士&quot; &quot;小牛&quot; &quot;(&quot; &quot;客&quot; &quot;)&quot; ## [161] &quot; &quot; &quot;108&quot; &quot;：&quot; &quot;106&quot; &quot; &quot; &quot;拓荒者&quot; &quot;\\n&quot; &quot;\\n&quot; apple_tokens_qtd[[1]] %&gt;% as.character ## [1] &quot;《&quot; &quot;蘋果&quot; &quot;體育&quot; ## [4] &quot;》&quot; &quot;即日起&quot; &quot;進行&quot; ## [7] &quot;虛擬&quot; &quot;賭&quot; &quot;盤&quot; ## [10] &quot;擂台&quot; &quot;，&quot; &quot;每名&quot; ## [13] &quot;受邀&quot; &quot;參賽者&quot; &quot;進行&quot; ## [16] &quot;勝負&quot; &quot;預測&quot; &quot;，&quot; ## [19] &quot;每周&quot; &quot;結算&quot; &quot;在&quot; ## [22] &quot;周二&quot; &quot;公布&quot; &quot;，&quot; ## [25] &quot;累積&quot; &quot;勝率&quot; &quot;前&quot; ## [28] &quot;3&quot; &quot;高&quot; &quot;參賽者&quot; ## [31] &quot;可&quot; &quot;繼續&quot; &quot;參賽&quot; ## [34] &quot;，&quot; &quot;單&quot; &quot;周&quot; ## [37] &quot;勝率&quot; &quot;最高&quot; &quot;者&quot; ## [40] &quot;，&quot; &quot;將&quot; &quot;加封&quot; ## [43] &quot;「&quot; &quot;蘋果&quot; &quot;波&quot; ## [46] &quot;神&quot; &quot;」&quot; &quot;頭銜&quot; ## [49] &quot;。&quot; &quot;註&quot; &quot;:&quot; ## [52] &quot;賭&quot; &quot;盤&quot; &quot;賠&quot; ## [55] &quot;率&quot; &quot;如有&quot; &quot;變動&quot; ## [58] &quot;，&quot; &quot;以&quot; &quot;台灣&quot; ## [61] &quot;運&quot; &quot;彩&quot; &quot;為主&quot; ## [64] &quot;。&quot; &quot;資料&quot; &quot;來源&quot; ## [67] &quot;：&quot; &quot;NBA&quot; &quot;官&quot; ## [70] &quot;網&quot; &quot;http://www.nba.com&quot; &quot;金塊&quot; ## [73] &quot;(&quot; &quot;客&quot; &quot;)&quot; ## [76] &quot;103&quot; &quot;：&quot; &quot;92&quot; ## [79] &quot;76&quot; &quot;人&quot; &quot;騎士&quot; ## [82] &quot;(&quot; &quot;主&quot; &quot;)&quot; ## [85] &quot;88&quot; &quot;：&quot; &quot;82&quot; ## [88] &quot;快艇&quot; &quot;活塞&quot; &quot;(&quot; ## [91] &quot;客&quot; &quot;)&quot; &quot;92&quot; ## [94] &quot;：&quot; &quot;75&quot; &quot;公牛&quot; ## [97] &quot;勇士&quot; &quot;(&quot; &quot;客&quot; ## [100] &quot;)&quot; &quot;108&quot; &quot;：&quot; ## [103] &quot;82&quot; &quot;灰&quot; &quot;熊&quot; ## [106] &quot;熱火&quot; &quot;(&quot; &quot;客&quot; ## [109] &quot;)&quot; &quot;103&quot; &quot;：&quot; ## [112] &quot;82&quot; &quot;灰&quot; &quot;狼&quot; ## [115] &quot;籃網&quot; &quot;(&quot; &quot;客&quot; ## [118] &quot;)&quot; &quot;90&quot; &quot;：&quot; ## [121] &quot;82&quot; &quot;公鹿&quot; &quot;溜&quot; ## [124] &quot;馬&quot; &quot;(&quot; &quot;客&quot; ## [127] &quot;)&quot; &quot;111&quot; &quot;：&quot; ## [130] &quot;100&quot; &quot;馬&quot; &quot;刺&quot; ## [133] &quot;國王&quot; &quot;(&quot; &quot;客&quot; ## [136] &quot;)&quot; &quot;112&quot; &quot;：&quot; ## [139] &quot;102&quot; &quot;爵士&quot; &quot;小牛&quot; ## [142] &quot;(&quot; &quot;客&quot; &quot;)&quot; ## [145] &quot;108&quot; &quot;：&quot; &quot;106&quot; ## [148] &quot;拓荒者&quot; kwic(apple_tokens, &quot;勝率&quot;) kwic(apple_tokens_qtd, &quot;勝率&quot;) Therefore, to work with the Chinese texts, if you need to utilize more advanced text-analytic functions provided by quanteda, please perform the word tokenization on the texts using your own word segmenter first and convert the object into a tokens, which can then be properly passed on to other functions in quanteda (e.g., dfm). (In other words, for Chinese text analytics, probably corpus object is less practical; rather, creating a tokens object of your corpus might be more useful.) In the later demonstrations, we will use our own defined segmenter for word segmentation/tokenization. 7.7 Case Study 1: Word Frequency and Wordcloud We follow the same steps as illstrated in the above flowchart 7.2 and deal with the Chinese texts using the tidy structure framework: Load the corpus data using readtext() and convert it into an corpus object Create a text-based tidy structure DF apple_corpus_tidy (i.e., a tibble) Intialize a word segmenter using worker() Tokenize the text-based data frame into a word-based tidy data frame using unnest_tokens() # loading corpus apple_df &lt;- apple_corpus %&gt;% tidy %&gt;% filter(text !=&quot;&quot;) %&gt;% #remove empty documents mutate(doc_id = row_number()) # create doccument index # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = T, symbol = T) # Tokenization: Word-based DF apple_word &lt;- apple_df %&gt;% unnest_tokens(output = word, input= text, token = function(x) segment(x, jiebar = segmenter)) %&gt;% group_by(doc_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word index within each document ungroup apple_word %&gt;% head(100) These tokenization results should be the same as our earlier apple_tokens: apple_word %&gt;% filter(doc_id == 1) %&gt;% mutate(word_quanteda_tokens = apple_tokens[[1]]) Creating unique indices for your data is very important. In corpus linguistic analysis, we often need to trace back to the original context where the word, phrase or sentence comes from. With all these unique indices, we can easily keep track of the sources of all tokenized linguistic units. Also, if the metadata of the source documents are available, these unique indices would allow us to connect the tokenized linguistic units to the metadata information (e.g., genres, registers, author profiles) With a word-based tidy DF, we can easily generate a word frequency list as well as a wordcloud to have a quick overview of the word distribution in the corpus. apple_word_freq &lt;- apple_word %&gt;% anti_join(tibble(word = readLines(&quot;demo_data/stopwords-ch.txt&quot;))) %&gt;% filter(word %&gt;% str_detect(pattern = &quot;\\\\D+&quot;)) %&gt;% count(word) %&gt;% arrange(desc(n)) # `wordcloud` version # require(wordcloud) # font_family &lt;- par(&quot;family&quot;) # the previous font family # par(family = &quot;wqy-microhei&quot;) # change to a nice Chinese font # with(apple_word_freq, wordcloud(word, n, # max.words = 100, # min.freq = 10, # scale = c(4,0.5), # color = brewer.pal(8, &quot;Dark2&quot;)), family = &quot;wqy-microhei&quot;) # par(family = font_family) # switch the font back library(wordcloud2) apple_word_freq %&gt;% filter(n &gt; 400) %&gt;% filter(nchar(word) &gt;=2) %&gt;% wordcloud2(shape = &quot;star&quot;, size = 0.3) # clear up memory rm(apple_word, apple_word_freq, segmenter, seg_byline_0, seg_byline_1) 7.8 Case Study 2: Patterns In this case study, we are looking at a more complex example. In corpus linguistic analysis, we often need to extract a particular pattern from the texts. In order to retrieve the target patterns at a high accuracy rate, we often need to make use of the additional annotations provided by the corpus. The most often-used information is the parts-of-speech tags of words. So here we demonstrate how to add POS tags information to our current tidy corpus design. 7.8.1 Define Own Tokenization Functions We define two tokenization functions: chinese_chunk_tokenizer(): This function tokenizes a document text into a series of inter-punctuation units. We refer to these units as sentence-like chunks. chinese_word_tokenizer(): This function tokenizes a text into a vector of “word/tag” tokens. # Chunk tokenizer chinese_chunk_tokenizer &lt;- function(text){ str_split(text, pattern = &quot;[^\\u4E00-\\u9FFF]+&quot;) } # word tokenizer chinese_word_tokenizer&lt;- function(text, tagger){ segment(text, tagger) %&gt;% map(function(x) paste(x, names(x), sep=&quot;/&quot;)) } Initialize worker() When initilizing the word segmenter worker(), remember to specify the argument type = &quot;tag&quot; to get POS tags. Also, we specify own dictionary (user = ...) and keep symbols (symbol=T) when doing the word tokenization. # Testing code postagger &lt;-worker(type = &quot;tag&quot;, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = TRUE) We can try our self-defined functions with one text from the corpus: apple_df$text[1] ## [1] &quot;《蘋果體育》即日起進行虛擬賭盤擂台，每名受邀參賽者進行勝負預測，每周結算在周二公布，累積勝率前3高參賽者可繼續參賽，單周勝率最高者，將加封「蘋果波神」頭銜。註:賭盤賠率如有變動，以台灣運彩為主。\\n資料來源：NBA官網http://www.nba.com\\n\\n金塊(客) 103：92 76人騎士(主) 88：82 快艇活塞(客) 92：75 公牛勇士(客) 108：82 灰熊熱火(客) 103：82 灰狼籃網(客) 90：82 公鹿溜馬(客) 111：100 馬刺國王(客) 112：102 爵士小牛(客) 108：106 拓荒者\\n\\n&quot; apple_df$text[1] %&gt;% chinese_chunk_tokenizer() ## [[1]] ## [1] &quot;&quot; &quot;蘋果體育&quot; ## [3] &quot;即日起進行虛擬賭盤擂台&quot; &quot;每名受邀參賽者進行勝負預測&quot; ## [5] &quot;每周結算在周二公布&quot; &quot;累積勝率前&quot; ## [7] &quot;高參賽者可繼續參賽&quot; &quot;單周勝率最高者&quot; ## [9] &quot;將加封&quot; &quot;蘋果波神&quot; ## [11] &quot;頭銜&quot; &quot;註&quot; ## [13] &quot;賭盤賠率如有變動&quot; &quot;以台灣運彩為主&quot; ## [15] &quot;資料來源&quot; &quot;官網&quot; ## [17] &quot;金塊&quot; &quot;客&quot; ## [19] &quot;人騎士&quot; &quot;主&quot; ## [21] &quot;快艇活塞&quot; &quot;客&quot; ## [23] &quot;公牛勇士&quot; &quot;客&quot; ## [25] &quot;灰熊熱火&quot; &quot;客&quot; ## [27] &quot;灰狼籃網&quot; &quot;客&quot; ## [29] &quot;公鹿溜馬&quot; &quot;客&quot; ## [31] &quot;馬刺國王&quot; &quot;客&quot; ## [33] &quot;爵士小牛&quot; &quot;客&quot; ## [35] &quot;拓荒者&quot; &quot;&quot; apple_df$text[1] %&gt;% chinese_word_tokenizer(postagger) ## [[1]] ## [1] &quot;《/x&quot; &quot;蘋果/n&quot; &quot;體育/vn&quot; &quot;》/x&quot; &quot;即日起/l&quot; &quot;進行/v&quot; ## [7] &quot;虛擬/v&quot; &quot;賭盤/x&quot; &quot;擂台/v&quot; &quot;，/x&quot; &quot;每名/x&quot; &quot;受邀/v&quot; ## [13] &quot;參賽者/n&quot; &quot;進行/v&quot; &quot;勝負/v&quot; &quot;預測/vn&quot; &quot;，/x&quot; &quot;每周/r&quot; ## [19] &quot;結算/v&quot; &quot;在/p&quot; &quot;周二/t&quot; &quot;公布/v&quot; &quot;，/x&quot; &quot;累積/v&quot; ## [25] &quot;勝率/n&quot; &quot;前/f&quot; &quot;3/x&quot; &quot;高/a&quot; &quot;參賽者/n&quot; &quot;可/v&quot; ## [31] &quot;繼續/v&quot; &quot;參賽/n&quot; &quot;，/x&quot; &quot;單周/x&quot; &quot;勝率/n&quot; &quot;最高者/n&quot; ## [37] &quot;，/x&quot; &quot;將/zg&quot; &quot;加封/v&quot; &quot;「/x&quot; &quot;蘋果/n&quot; &quot;波神/x&quot; ## [43] &quot;」/x&quot; &quot;頭銜/n&quot; &quot;。/x&quot; &quot;註/x&quot; &quot;:/x&quot; &quot;賭盤/x&quot; ## [49] &quot;賠率/n&quot; &quot;如有/c&quot; &quot;變動/vn&quot; &quot;，/x&quot; &quot;以/p&quot; &quot;台灣/x&quot; ## [55] &quot;運彩/x&quot; &quot;為主/x&quot; &quot;。/x&quot; &quot;\\n/x&quot; &quot;資料/n&quot; &quot;來源/n&quot; ## [61] &quot;：/x&quot; &quot;NBA/eng&quot; &quot;官網/x&quot; &quot;http/eng&quot; &quot;:/x&quot; &quot;//x&quot; ## [67] &quot;//x&quot; &quot;www/eng&quot; &quot;./x&quot; &quot;nba/eng&quot; &quot;./x&quot; &quot;com/eng&quot; ## [73] &quot;\\n/x&quot; &quot;\\n/x&quot; &quot;金塊/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; ## [79] &quot; /x&quot; &quot;103/m&quot; &quot;：/x&quot; &quot;92/m&quot; &quot; /x&quot; &quot;76/m&quot; ## [85] &quot;人/n&quot; &quot;騎士/n&quot; &quot;(/x&quot; &quot;主/b&quot; &quot;)/x&quot; &quot; /x&quot; ## [91] &quot;88/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; &quot;快艇/n&quot; &quot;活塞/vn&quot; ## [97] &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;92/m&quot; &quot;：/x&quot; ## [103] &quot;75/m&quot; &quot; /x&quot; &quot;公牛/n&quot; &quot;勇士/n&quot; &quot;(/x&quot; &quot;客/n&quot; ## [109] &quot;)/x&quot; &quot; /x&quot; &quot;108/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; ## [115] &quot;灰熊/x&quot; &quot;熱火/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; ## [121] &quot;103/m&quot; &quot;：/x&quot; &quot;82/m&quot; &quot; /x&quot; &quot;灰狼/n&quot; &quot;籃網/n&quot; ## [127] &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;90/m&quot; &quot;：/x&quot; ## [133] &quot;82/m&quot; &quot; /x&quot; &quot;公鹿/n&quot; &quot;溜/v&quot; &quot;馬/n&quot; &quot;(/x&quot; ## [139] &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;111/m&quot; &quot;：/x&quot; &quot;100/m&quot; ## [145] &quot; /x&quot; &quot;馬刺/nr&quot; &quot;國王/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; ## [151] &quot; /x&quot; &quot;112/m&quot; &quot;：/x&quot; &quot;102/m&quot; &quot; /x&quot; &quot;爵士/n&quot; ## [157] &quot;小牛/n&quot; &quot;(/x&quot; &quot;客/n&quot; &quot;)/x&quot; &quot; /x&quot; &quot;108/m&quot; ## [163] &quot;：/x&quot; &quot;106/m&quot; &quot; /x&quot; &quot;拓荒者/nr&quot; &quot;\\n/x&quot; &quot;\\n/x&quot; In the above example, we adopt a very naive approach by treating any linguistic unit in-between the punctuation marks as a possible sentence-like unit. This can be controversial to many grammarians and syntaticians. However, in practice, this may not be a bad choice as it will become obvious when we extract patterns. For more information related to the unicode range for the punctuations in CJK languages, please see this SO discussion thread. 7.8.2 Transform Text-Based to Token-Based Data Frame Now we can apply our self-defined tokenization functions to the text-based DF apple_df. We first unnest_tokens() the text-based DF into a chunk-based DF using the tokenizer chinese_chunk_tokenizer(). Then we transform the chunk-based DF into a word-based DF using chinese_word_tokenizer(). system.time( apple_df %&gt;% unnest_tokens(output = chunk, input = text, token = chinese_chunk_tokenizer) %&gt;% filter(nchar(chunk)&gt;1) %&gt;% # remove one-char chunk group_by(doc_id) %&gt;% mutate(chunk_id = row_number()) %&gt;% # create chunk_id ungroup %&gt;% unnest_tokens(output = word, input = chunk, token = function(x) chinese_word_tokenizer(x, postagger)) %&gt;% group_by(chunk_id) %&gt;% mutate(word_id = row_number()) %&gt;% # create word_id ungroup -&gt; apple_word_df ) # end sytem.time ## user system elapsed ## 13.604 0.263 13.867 apple_word_df %&gt;% head dim(apple_word_df) ## [1] 2375674 4 The word-based data frame now has parts-of-speech tags for every word in the corpus. Based on the word_id, chunk_id, and doc_id, we can easily keep track of their source documents as well. Now based on the word-based data frame, we create a chunk-based data frame again by concatenating all word/tag in a chunk into a long string. In our earlier chunk tokenization, we only split texts into chunks without performing the word segmentation and POS tagging yet. The word boundary and POS information is only available when we perform the word tokenization using chinese_word_tokenizer. Therefore, to get a chunk with both words and POS tags, we can concatenate “word/tag” tokens into a long string on a chunk basis. system.time( apple_chunk_df &lt;- apple_word_df %&gt;% group_by(doc_id, chunk_id) %&gt;% summarize(chunk = str_c(word, collapse=&quot;\\u3000&quot;)) %&gt;% ungroup ) ## user system elapsed ## 3.010 0.121 3.132 apple_chunk_df %&gt;% head(200) dim(apple_chunk_df) ## [1] 550504 3 This step may seem a bit redundant at the first sight. Why do we need to combine “word/tag” tokens into a longer string AGAIN?? It is indeed NOT necessary but the chunk-based data frame would be more useful for further construction/pattern analysis. Syntactic patterns/constructions often span word boundaries but stay within a sentence frame. Exercise 7.3 In the above demonstration, we perform two rounds of tokenizations in the text preprocessing: we first tokenize the texts into chunks using unnest_tokens(..., token = chinese_chunk_tokenizer); we then tokenize the chunks into words using unnest_tokens(..., token = chinese_word_tokenizer). However, this may be unnecessary. It is also possible to get the POS-tagged version of the entire text with words and tags available without chunking. This would probably be more efficient. How would you create one additional column like text_tag directly from the text-based DF apple_df, as shown below? (Please note that it is still a text-based DF and therefore probably you can use mutate). ## user system elapsed ## 6.093 0.096 6.190 7.8.3 BEI Construction This section will show you how we can make use of the chunk-based corpus data frame with POS tags. I would like to illustrate its usefulness with a case study: 被 + ... Construction. After we tokenize the text-based tidy corpus into a inter-punctuation-unit-based (IPU), i.e., chunk-based data frame, we can make use of the words as well as their parts-of-speech tags to extract the target pattern we are interested: 被 + ... Construction. The data retrieval process is now very straighforward: we only need to go through all the chunks in the corpus object and see if our target pattern matches any of these chunks. The assumption is that: the BEI-Construction will NOT span different chunks. In the following example, we: define a regular expression \\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v for BEI-Construction, i.e., 被 + VERB use unnest_tokens() and str_extract_all() to extract target patterns # define regex patterns pattern_bei &lt;- &quot;\\\\b被/p\\\\s([^/]+/[^\\\\s]+\\\\s)*?[^/]+/v&quot; # extract patterns from corp apple_chunk_df %&gt;% unnest_tokens(output = pat_bei, input = chunk, token = function(x) str_extract_all(x, pattern=pattern_bei)) -&gt; result_bei result_bei Please check Chapter 5 Parts of Speech Tagging on evaluating the quality of the data retrieved by a regular expression (i.e., precision and recall). To have a more in-depth analysis of BEI construction, we like to automatically extract the verb used in the BEI construction. # Extract BEI + WORD result_bei &lt;- result_bei %&gt;% mutate(VERB = str_replace(pat_bei,&quot;.+\\u3000([^/]+)/v$&quot;,&quot;\\\\1&quot;)) result_bei # Calculate WORD frequency require(wordcloud2) result_bei %&gt;% count(VERB) %&gt;% mutate(n = log(n)) %&gt;% top_n(100, n) %&gt;% wordcloud2(shape=&quot;diamond&quot;,size = 0.3) Exercise 7.4 When you take a closer look at the resulting word cloud above, you would see the copular verb 是 showing up in the graph, which is counter to our native speaker intuition. How do you check the instances of these 是 tokens? After you examine these cases, what do you think may be the source of the problem? Exercise 7.5 To more properly evaluate the quality of the pattern queries, it would be great if we still have the original chunk texts available in the resulting data frame result_bei. How do we keep this information? That is, please have one column in result_bei, which shows the original chunk texts from which the construction token is extracted. Exercise 7.6 Please use the apple_chunk_df as your tidy corpus and extract Chinese particle constructions of ... 外/內/中. Usually a space particle construction like these consists of a landmark NP (LM) and the space particle (SP). For example, in 任期內, 任期 is the landmark NP and 內 is the space particle. In this exercise, we will naively assume that the word directly preceding the space particle is our landmark NP head noun. So please (a) extract all concordance lines with these space particles and (b) at the same time identify their respective SP and LM, as shown below. Exercise 7.7 Following Exercise 7.6, please generate a frequency list of the LMs for each spac particle. Show us the top 10 LMs of each space particle and arrange the frequencies of the LMs in a descending order, as shown below. Exercise 7.8 Following Exercise 7.7, for each space particle, please create a word cloud of its co-occuring LMs based on the top 100 LMs of each particle. PS: The word frequencies in the word clouds shown below are on a log scale. Exercise 7.9 Based on the word clouds provided in Exercise 7.7, do you find any bizarre cases? Can you tell us why? What would be the problems? Or what did we do wrong in the text preprocessing that may lead to these cases? Please discuss these issues in relation to the steps in our data processing, i.e., word segmentation, POS tagging, and pattern retrievals. 7.9 Case Study 3: Lexical Bundles 7.9.1 N-grams Extraction With word boundaries, we can also analyze the recurrent multiword units in Chinese news. Here let’s take a look at recurrent four-grams. As we discussed in Chapter 4, a multiword unit can be defined based on at least two metrics: the frequency of the whole multiword unit (i.e., frequency) the number of texts where the multiword unit is observed (i.e., dispersion) As the default n-gram tokenization in unnest_tokens() only works with the English data, we start this task by defining our own token function ngram_chi() to extract Chinese n-grams. # Generate ngram sequences from `text` # By default, `text` is assumed to have whitespaces as delimiters between tokens ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s|\\u3000&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc This ngram_chi() takes ONE text (scalar) as an input, and returns a vector of n-grams. Most importantly, this function assumes that in the text string, each word token is delimited by a whitespace (i.e., a word-segmented text!!) s &lt;- &quot;這 是 一個 測試 的 句子&quot; ngram_chi(text = s, n = 2, delimiter = &quot;_&quot;) ## [1] &quot;這_是&quot; &quot;是_一個&quot; &quot;一個_測試&quot; &quot;測試_的&quot; &quot;的_句子&quot; ngram_chi(text = s, n = 4, delimiter = &quot;_&quot;) ## [1] &quot;這_是_一個_測試&quot; &quot;是_一個_測試_的&quot; &quot;一個_測試_的_句子&quot; ngram_chi(text = s, n = 5, delimiter = &quot; &quot;) ## [1] &quot;這 是 一個 測試 的&quot; &quot;是 一個 測試 的 句子&quot; ngram_chi(text = s, n = 7, delimiter = &quot;_&quot;) # empty string ## [1] &quot;&quot; We vectorize the function ngram_chi(). This step is important because in unnest_tokens() the self-defined token function should take a text-based vector as input and return a list of token-based vectors of the same length as the output (cf. Section 7.2). # Wrapper to Vectorize the function vngram_chi &lt;- Vectorize(ngram_chi, vectorize.args = &quot;text&quot;) Vectorized functions are a very useful feature of R, but programmers who are used to other languages often have trouble with this concept at first. A vectorized function works not just on a single value, but on a whole vector of values at the same time. In our first defined ngram_chi function, it takes one text vector as an input and processes it one at a time. However, we would expect ngram_chi to process a vector of texts (i.e., multiple texts) at the same time and return a list of resulting ngrams vectors at the same time. Therefore, we use Vectorize() as a wrapper to vectorize our function and specifically tell R that the argument text is vectorized, i.e., process each value in the text argument in the same way. Now we can tokenize our corpus into n-grams using our own token function vngram_chi() and the unnest_tokens(). In this case study, we demonstrate the analysis of four-grams in our Apple News corpus. We first remove all POS tags in apple_chunk_df$chunk because n-grams do not need the POS tags We then transform the chunk-based data frame apple_chunk_df into a n-gram-based data frame using unnest_tokens(...) with self-defined token function We remove chunks with no target n-grams extracted (Chunks with less than four words will have NO four-grams extracted.) system.time( apple_ngram &lt;-apple_chunk_df %&gt;% mutate(chunk = str_replace_all(chunk, &quot;/[^/]+(\\u3000|$)&quot;,&quot;\\\\1&quot;)) %&gt;% # remove pos tags unnest_tokens(ngram, chunk, token = function(x) vngram_chi(text = x, n= 4)) %&gt;% filter(ngram!=&quot;&quot;)) ## user system elapsed ## 52.755 0.185 52.944 apple_ngram %&gt;% head(20) dim(apple_ngram) ## [1] 974022 3 Exercise 7.10 Because n-grams extraction often requires no POS tags, it is not necessary (or redundant) to perform the POS tagging first and then remove the tags again indeed. For this task, we can split the raw text corpus into chunks and then do the word segmentation as well as n-grams extraction at the same time. It is also possible to create a self-defined function like chinese_ngram_tokenizer, which takes a simpler word segmenter, and directly get the apple_ngram from apple_df Please define a function chinese_ngram_tokenizer to make the following tokenization codes work so that we can generate ngrams directly from apple_df. The following codes should produce the same result as the above apple_ngram. # Simpler word segmenter wordsegmenter &lt;-worker(user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = T, bylines = TRUE) # N-grams Extraction system.time( apple_df %&gt;% unnest_tokens(output = chunk, input = text, token = chinese_chunk_tokenizer) %&gt;% filter(nchar(chunk)&gt;1) %&gt;% # remove one-char chunk group_by(doc_id) %&gt;% mutate(chunk_id = row_number()) %&gt;% # create chunk_id ungroup %&gt;% unnest_tokens(output = ngram, input = chunk, token = function(x) chinese_ngram_tokenizer(x,wordsegmenter, n = 4)) %&gt;% filter(ngram!=&quot;&quot;) -&gt; apple_ngram_2 ) # end sytem.time ## user system elapsed ## 26.071 0.422 26.497 nrow(apple_ngram_2) ## [1] 974022 nrow(apple_ngram) ## [1] 974022 apple_ngram_2 %&gt;% head(20) 7.9.2 Frequency and Dispersion Now that we have the four-grams-based DF, we can compute their token frequencies and document frequencies in the corpus using the normal data manipulation tricks. We set cut-offs for four-grams: dispersion &gt;= 5 (i.e., four-grams that occur in at least five different documents) apple_ngram_dist &lt;- apple_ngram %&gt;% group_by(ngram) %&gt;% summarize(freq = n(), dispersion = n_distinct(doc_id)) %&gt;% filter(dispersion &gt;= 5) Please take a look at the four-grams, both arranged by frequency and dispersion: # arrange by dispersion apple_ngram_dist %&gt;% arrange(desc(dispersion)) %&gt;% head(10) # arrange by freq apple_ngram_dist %&gt;% arrange(desc(freq)) %&gt;% head(10) We can also look at four-grams with particular lexical words: apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;被&quot;)) %&gt;% arrange(desc(dispersion)) apple_ngram_dist %&gt;% filter(str_detect(ngram, &quot;以&quot;)) %&gt;% arrange(desc(dispersion)) Exercise 7.11 In the above example, if we are only interested in the four-grams with the word 以, how can we revise the regular expression so that we can get rid of tokens like ngrams with 以及, 以上 etc. 7.10 Afterwords Tokenizations are complex in Chinese text processing. Many factors may need to be taken into account when determining the right tokenization method. While word segmentation is almost a necessary step in Chinese computational text analytics, several important questions may also be relevant to the data processing methods: Do you need the parts-of-speech tags of words in your research? What is the base linguistic unit you would like to work with? Texts? Chunks? Sentences? N-grams? Words? Do you need non-word tokens such as symbols, punctuations, or numbers in your analysis? Your answers to the above questions should help you determine the most effective structure of the tokenization methods for your data. "],
["constructions-and-idioms.html", "Chapter 8 Constructions and Idioms 8.1 Collostruction 8.2 Corpus 8.3 Word Segmentation 8.4 Extract Constructions 8.5 Distributional Information Needed for CA 8.6 Chinese Four-character Idioms 8.7 Dictionary Entries 8.8 Case Study: X來Y去 8.9 Exercises", " Chapter 8 Constructions and Idioms library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) 8.1 Collostruction In this chapter, I would like to talk about the relationship between a construction and words. Words may co-occur to form collocation patterns. When words co-occur with a particular morphosyntactic pattern, they would form collostruction patterns. Here I would like to introduce a widely-applied method for research on the meanings of constructional schemas—Collostructional Aanalysis (Stefanowitsch and Gries 2003). This is the major framework in corpus linguistics for the study of the relationship between words and constructions. The idea behind collostructional analysis is simple: the meaning of a morphosyntactic construction can be determined very often by its co-occurring words. In particular, words that are strongly associated (i.e., co-occurring) with the construction are referred to as collexemes of the construction. Collostruction Analysis is an umbrella term, which covers several sub-analyses for constructional semantics: collexeme analysis co-varying collexeme analysis distinctive collexeme analysis This chapter will focus on the first one, collexeme analysis, whose principles can be extended to the other analyses. Also, I will demonstrate how we can conduct a collexeme analysis by using the R script written by Stefan Gries (Collostructional Analysis). 8.2 Corpus I will use the Apple News Corpus from Chapter 7 as our corpus. And in this demonstration, I would like to look at a particular morphosyntactic frame in Chinese, X + 起來. Our goal is simple: in order to find out the semantics of this constructional schema, it would be very informative if we can find out which words tend to strongly occupy this X slot of the constructional schema. So our first step is to load the corpus into R. apple_corpus &lt;- readtext(&quot;demo_data/applenews10000.tar.gz&quot;) %&gt;% corpus 8.3 Word Segmentation Because Apple News Corpus is a raw-text corpus, we first word-segment the corpus. # Initialize the segmenter segmenter &lt;- worker(user=&quot;demo_data/dict-ch-user.txt&quot;, bylines = F, symbol = T) word_seg_text &lt;- function(x, tagger){ x %&gt;% segment(jiebar = tagger) %&gt;% str_c(collapse=&quot; &quot;) } apple_df &lt;- apple_corpus %&gt;% tidy %&gt;% filter(text !=&quot;&quot;) %&gt;% #remove empty documents mutate(doc_id = row_number()) apple_df &lt;- apple_df %&gt;% # create doccument index mutate(text_tag = map_chr(text, word_seg_text, segmenter)) 8.4 Extract Constructions With the words information, we can now extract our target patterns from the corpus using regular expressions. # extract pattern pattern_qilai = &quot;[^\\\\s]+\\\\s起來\\\\b&quot; apple_df %&gt;% select(-text) %&gt;% unnest_tokens(output = construction, input = text_tag, token = function(x) str_extract_all(x, pattern=pattern_qilai)) -&gt; apple_qilai apple_qilai 8.5 Distributional Information Needed for CA To perform the collostructional analysis, which is essentially a statistical analysis of the association between the words and the constructions, we need to collect necessary distributional information. Also, to use Stefan Gries’ R script of Collostructional Analysis, we need the following information: Joint Frequencies of Words and Constructions Frequencies of Words in Corpus Corpus Size (total number of words in corpus) Construction Size (total number of constructions in corpus) 8.5.1 Word Frequency List # word freq apple_df %&gt;% select(-text) %&gt;% unnest_tokens(word, text_tag, token = function(x) str_split(x, &quot;\\\\s+|\\u3000&quot;)) %&gt;% filter(nzchar(word)) %&gt;% count(word, sort = T) -&gt; apple_word apple_word 8.5.2 Construction Frequencies apple_qilai %&gt;% count(construction, sort=T) %&gt;% tidyr::separate(col=&quot;construction&quot;, into = c(&quot;w1&quot;,&quot;construction&quot;), sep=&quot;\\\\s&quot;) %&gt;% mutate(w1_freq = apple_word$n[match(w1,apple_word$word)]) -&gt; apple_qilai_table # prepare for coll analysis apple_qilai_table %&gt;% select(w1, w1_freq, n) %&gt;% write_tsv(&quot;qilai.tsv&quot;) In the later Stefan Gries’ R script, we need to have our input as a tab-delimited file. 8.5.3 Other Information We prepare necessary distributional information for the later collostructional analysis. # corpus information sink(&quot;qilai_info.txt&quot;) cat(&quot;Corpus Size: &quot;, sum(apple_word$n), &quot;\\n&quot;) ## Corpus Size: 3209617 cat(&quot;Construction Size: &quot;, sum(apple_qilai_table$n), &quot;\\n&quot;) ## Construction Size: 546 sink() 8.5.4 Creat Output File This is to create an empty output txt file to keep the results from the Collostructional Analysis script. # output file file.create(&quot;qilai_results.txt&quot;) ## [1] TRUE 8.5.5 Run coll.analysis.r Finally we are now really to perform the collostructional analysis using Stefan Gries’ coll.analysis.r. source(&quot;http://www.stgries.info/teaching/groningen/coll.analysis.r&quot;) This is an R script with interactive instructions. When you run the analysis, you will be prompted with guide questions, to which you would need to fill out necessary information/answers. Specifically, data to be entered include: analysis to perform: 1 name of construction: QILAI corpus size: 3209617 freq of constructions: 546 index of association strength: 1 (=fisher-exact) sorting: 4 (=collostruction strength) decimals: 2 text file with the raw data: &lt;qilai.tsv&gt; output file: &lt;qilai_results.txt&gt; The output of coll.analysis.r is as shown below: 8.6 Chinese Four-character Idioms Many studies have shown that Chinese makes use of large proportion of four-character idioms in the discourse. This chapter will provide a exploratory analysis of four-character idioms in Chinese. 8.7 Dictionary Entries In our demo_data directory, there is a file dict-ch-idiom.txt, which includes a list of four-character idioms in Chinese. These idioms are collected from 搜狗輸入法詞庫 and the original file formats (.scel) have been combined, removed of duplicate cases, and converted to a more machine-readable format, i.e., .txt. Let’s first import the idioms in the file. all_idioms &lt;- readLines(con = &quot;demo_data/dict-ch-idiom.txt&quot;) head(all_idioms) ## [1] &quot;阿保之功&quot; &quot;阿保之勞&quot; &quot;阿鼻地獄&quot; &quot;阿鼻叫喚&quot; &quot;阿斗太子&quot; &quot;阿芙蓉膏&quot; tail(all_idioms) ## [1] &quot;罪無可逭&quot; &quot;罪人不帑&quot; &quot;作纛旗兒&quot; &quot;坐纛旂兒&quot; &quot;作姦犯科&quot; &quot;作育英才&quot; length(all_idioms) ## [1] 56536 In order to make use of the tidy structure in R, we convert the data into a tibble: idiom &lt;- tibble(string = all_idioms) 8.8 Case Study: X來Y去 We can create a regular expression pattern to extract all idioms with the format of X來X去: idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) To analyze the meaning of this constructional schema, we may need to extract the X and Y in the schema: idiom_laiqu &lt;-idiom %&gt;% filter(str_detect(string, &quot;.來.去&quot;)) %&gt;% mutate(pattern = str_replace(string, &quot;(.)來(.)去&quot;, &quot;\\\\1_\\\\2&quot;)) %&gt;% separate(pattern, into = c(&quot;w1&quot;, &quot;w2&quot;), sep = &quot;_&quot;) idiom_laiqu One empirical question is how many of these idioms are of the pattern X=Y (e.g., 想來想去, 直來直去) and how many are of X!=Y (e.g., 說來道去, 朝來暮去): idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) idiom_laiqu %&gt;% mutate(structure = ifelse(w1==w2, &quot;XX&quot;,&quot;XY&quot;)) %&gt;% count(structure) %&gt;% ggplot(aes(structure, n, fill = structure)) + geom_col() 8.9 Exercises Exercise 8.1 Please use idiom and extract the idioms with the schema of 一X一Y. Exercise 8.2 Also with the idiom as our data source, now if we are interested in all idioms that have duplicated characters in them, with schemas like either _A_A or A_A_, where A is a fixed character. How can we extract all idioms of these two types from idiom? Also, provide the distribution of the two types. Exercise 8.3 Following Exercise 8.2, for each type of the idioms, please provide their respective proportions of X=Y vs. X!=Y. Exercise 8.4 Folloing Exercise 8.3, please identify the character that is duplicated in the idioms. One follow-up analysis would be to look at the distribution of these pivotal characters. Can you reproduce a graph as shown below as closely as possible? References "],
["ckiptagger.html", "Chapter 9 CKIP Tagger 9.1 Installation 9.2 Download the Model Files 9.3 R-Python Communication 9.4 Word Segmentation in R 9.5 R Environment Setting 9.6 Loading Python Modules 9.7 Segmenting Texts 9.8 Define Own Dictionary 9.9 Beyond Word Boundaries 9.10 Tidy Up the Results", " Chapter 9 CKIP Tagger The current state-of-art Chinese segmenter for Taiwan Mandarin available is probably the CKIP tagger, created by the Chinese Knowledge and Information Processing (CKIP) group at the Academia Sinica. The ckiptagger is released as a python module. In this chpater, I will demonstrate how to use the module for Chinese word segmentation but in an R environment, i.e., how to integrate Python modules in R coherently to perform complex tasks. 9.1 Installation Because ckiptagger is built in python, we need to have python installed in our working environment. Please install the following applications on your own before you start: Anaconda + Python 3.7+ ckiptagger module in Python (Please install the module using the Anaconda Navigator) (Please consult the github of the ckiptagger for more details on installation.) For some reasons, the module ckiptagger cannot be found in the base channel. In Anaconda Navigator, please add specifically the following channel to the environment so that your Anaconda can find ckiptagger module: https://conda.anaconda.org/roccqqck 9.2 Download the Model Files All NLP applications have their models behind their fancy performances. To use the tagger provided in ckiptagger, we need to download their trained model files. Please go to the github of CKIP tagger to download the model files. (The file is very big. It takes a while.) After you download the zip file, unzip the data/ directory to your working directory. 9.3 R-Python Communication In order to call Python functions in R/Rstudio, we need to install an R library in your R. The R-Python communication is made possible through the R library reticulate. Please make sure that you have this library installed in your R. install.packages(&quot;reticulate&quot;) 9.4 Word Segmentation in R Before we proceed, please check if you have everything ready: Anaconda + Python 3.7+ Python module: ckiptagger R library: reticulate CKIP model files under your working directory ./data If yes, then we are ready to go. 9.5 R Environment Setting We first load the library reticulate and specify in R which Python we will be using in the current R(It is highly likely that there is more than one Python version installed in your system). Please change the path_to_python to your own path, which includes the Anaconda Python you just installed. library(reticulate) # spacy_initialize() would set the default python to miniconda/spacy_condaenviron # install ckiptagger and tensorflow-1.13.1 in spacy_condaenviron # path_to_python &lt;- &quot;/Users/Alvin/opt/anaconda3/envs/r-reticulate/bin/python&quot; # use_python(path_to_python, required = T) 9.6 Loading Python Modules ## If you like to run ckiptagger in a new environment: #conda_create(&quot;r-reticulate&quot;, conda = &quot;/Users/Alvin/opt/anaconda3/bin/conda&quot;) use_condaenv(&quot;r-reticulate&quot;)#, conda = &quot;/Users/Alvin/opt/anaconda3/bin/conda&quot;) ## Import ckiptagger module ckip &lt;- reticulate::import(module = &quot;ckiptagger&quot;) ## Intialize models ws &lt;- ckip$WS(&quot;./data&quot;) 9.7 Segmenting Texts ## Raw text corpus (sentences) sents &lt;- c(&quot;傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。&quot;, &quot;美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。&quot;, &quot;土地公有政策?？還是土地婆有政策。.&quot;, &quot;… 你確定嗎… 不要再騙了……&quot;, &quot;最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.&quot;, &quot;科長說:1,坪數對人數為1:3。2,可以再增加。&quot;) words &lt;- ws(sents) words ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; &quot;，&quot; &quot;卻&quot; &quot;突然&quot; ## [9] &quot;爆出&quot; &quot;自己&quot; &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來&quot; &quot;體育台&quot; ## [17] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; ## [25] &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公&quot; &quot;有&quot; &quot;政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地&quot; &quot;婆&quot; ## [9] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; The word segmenter ws() returns a list object, each element of which is a word-based vector of the original sentence. 9.8 Define Own Dictionary The performance of Chinese word segmenter depends highly on the dictionary. Texts in different disciplinces may have verry different vocabulary. To prioritize a set of words in a dictionary, we can further ensure the accuracy of the word segmentation. To create a dictionary for ckiptagger, we need to create a list with names = “the new words” and elements = “the weights”. Then we use the python function ckip$construct_dictionary() to create the dictionary Python object, which is the input argument for word segmenter ws(..., recommend_dictionary = ...). # Define new words in own dictionary new_words &lt;- c(&quot;土地婆&quot;,&quot;土地公有政策&quot;,&quot;緯來體育台&quot;) # Transform the `vector` into `list` for Python new_words_py &lt;- as.list(rep(1,length(new_words))) # cf. `list(rep, 1 , length(new_words))` names(new_words_py) &lt;- new_words # To create a dictionary for `construct_dictionary()` # We need a list, with names as the words and list elements as the weights in the dictionary # Create Python `dictionary` object, required by `ckiptagger.wc()` dictionary&lt;-ckip$construct_dictionary(new_words_py) # Segment texts using dictionary words_1 &lt;- ws(sents, recommend_dictionary = dictionary) words_1 ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; ## [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; ## [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; ## [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; ## [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公有政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; ## [6] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; Exercise 9.1 We usually have a list of new words saved in a text file. Can you write a R function, which loads the words in the demo_data/dict-sample.txt into a list named new_words, which can easily serve as the input for ckip$construct_dictionary() to create the python dictionary object? (Note: All weights are default to 1) new_words&lt;-loadDictionary(input = &quot;demo_data/dict-sample.txt&quot;) dictionary&lt;-ckip$construct_dictionary(new_words) # Segment texts using dictionary words_2 &lt;- ws(sents, recommend_dictionary = dictionary) words_2 ## [[1]] ## [1] &quot;傅達仁&quot; &quot;今&quot; &quot;將&quot; &quot;執行&quot; &quot;安樂死&quot; ## [6] &quot;，&quot; &quot;卻&quot; &quot;突然&quot; &quot;爆出&quot; &quot;自己&quot; ## [11] &quot;20&quot; &quot;年&quot; &quot;前&quot; &quot;遭&quot; &quot;緯來體育台&quot; ## [16] &quot;封殺&quot; &quot;，&quot; &quot;他&quot; &quot;不&quot; &quot;懂&quot; ## [21] &quot;自己&quot; &quot;哪裡&quot; &quot;得罪到&quot; &quot;電視台&quot; &quot;。&quot; ## ## [[2]] ## [1] &quot;美國&quot; &quot;參議院&quot; &quot;針對&quot; &quot;今天&quot; &quot;總統&quot; &quot;布什&quot; ## [7] &quot;所&quot; &quot;提名&quot; &quot;的&quot; &quot;勞工部長&quot; &quot;趙小蘭&quot; &quot;展開&quot; ## [13] &quot;認可&quot; &quot;聽證會&quot; &quot;，&quot; &quot;預料&quot; &quot;她&quot; &quot;將&quot; ## [19] &quot;會&quot; &quot;很&quot; &quot;順利&quot; &quot;通過&quot; &quot;參議院&quot; &quot;支持&quot; ## [25] &quot;，&quot; &quot;成為&quot; &quot;該&quot; &quot;國&quot; &quot;有史以來&quot; &quot;第一&quot; ## [31] &quot;位&quot; &quot;的&quot; &quot;華裔&quot; &quot;女性&quot; &quot;內閣&quot; &quot;成員&quot; ## [37] &quot;。&quot; ## ## [[3]] ## [1] &quot;土地公有政策&quot; &quot;?&quot; &quot;？&quot; &quot;還是&quot; &quot;土地婆&quot; ## [6] &quot;有&quot; &quot;政策&quot; &quot;。&quot; &quot;.&quot; ## ## [[4]] ## [1] &quot;…&quot; &quot; &quot; &quot;你&quot; &quot;確定&quot; &quot;嗎&quot; &quot;…&quot; &quot; &quot; &quot;不要&quot; &quot;再&quot; &quot;騙&quot; ## [11] &quot;了&quot; &quot;…&quot; &quot;…&quot; ## ## [[5]] ## [1] &quot;最多&quot; &quot;容納&quot; &quot;59,000&quot; &quot;個&quot; &quot;人&quot; &quot;,&quot; &quot;或&quot; &quot;5.9萬&quot; ## [9] &quot;人&quot; &quot;,&quot; &quot;再&quot; &quot;多&quot; &quot;就&quot; &quot;不行&quot; &quot;了&quot; &quot;.&quot; ## [17] &quot;這&quot; &quot;是&quot; &quot;環評&quot; &quot;的&quot; &quot;結論&quot; &quot;.&quot; ## ## [[6]] ## [1] &quot;科長&quot; &quot;說&quot; &quot;:1,&quot; &quot;坪數&quot; &quot;對&quot; &quot;人數&quot; &quot;為&quot; &quot;1:3&quot; &quot;。&quot; &quot;2&quot; ## [11] &quot;,&quot; &quot;可以&quot; &quot;再&quot; &quot;增加&quot; &quot;。&quot; 9.9 Beyond Word Boundaries In addition to primitive word segmentation, the ckiptagger provides also the parts-of-speech tags for words and named entity recognitions for the texts. The ckiptagger follows the pipeline below for text processing. Load the models To perform these additional tasks, we need to load the necessary models (pretrained and provided by the CKIP) first as well. They should all have been included in the model directory you unzipped earlier (cf. ./data). # loading other necessary models system.time((pos &lt;- ckip$POS(&quot;./data&quot;))) # 詞性 495s ## user system elapsed ## 5.020 2.306 6.700 system.time((ner &lt;- ckip$NER(&quot;./data&quot;))) # 實體辨識 426s ## user system elapsed ## 5.062 1.671 6.372 POS tagging and NER # Parts-of-speech Tagging pos_words &lt;- pos(words_1) pos_words ## [[1]] ## [1] &quot;Nb&quot; &quot;Nd&quot; &quot;D&quot; &quot;VC&quot; ## [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; ## [9] &quot;VJ&quot; &quot;Nh&quot; &quot;Neu&quot; &quot;Nf&quot; ## [13] &quot;Ng&quot; &quot;P&quot; &quot;Nc&quot; &quot;VC&quot; ## [17] &quot;COMMACATEGORY&quot; &quot;Nh&quot; &quot;D&quot; &quot;VK&quot; ## [21] &quot;Nh&quot; &quot;Ncd&quot; &quot;VJ&quot; &quot;Nc&quot; ## [25] &quot;PERIODCATEGORY&quot; ## ## [[2]] ## [1] &quot;Nc&quot; &quot;Nc&quot; &quot;P&quot; &quot;Nd&quot; ## [5] &quot;Na&quot; &quot;Nb&quot; &quot;D&quot; &quot;VC&quot; ## [9] &quot;DE&quot; &quot;Na&quot; &quot;Nb&quot; &quot;VC&quot; ## [13] &quot;VC&quot; &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;VE&quot; ## [17] &quot;Nh&quot; &quot;D&quot; &quot;D&quot; &quot;Dfa&quot; ## [21] &quot;VH&quot; &quot;VC&quot; &quot;Nc&quot; &quot;VC&quot; ## [25] &quot;COMMACATEGORY&quot; &quot;VG&quot; &quot;Nes&quot; &quot;Nc&quot; ## [29] &quot;D&quot; &quot;Neu&quot; &quot;Nf&quot; &quot;DE&quot; ## [33] &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; &quot;Na&quot; ## [37] &quot;PERIODCATEGORY&quot; ## ## [[3]] ## [1] &quot;VH&quot; &quot;QUESTIONCATEGORY&quot; &quot;QUESTIONCATEGORY&quot; &quot;Caa&quot; ## [5] &quot;Nb&quot; &quot;V_2&quot; &quot;Na&quot; &quot;PERIODCATEGORY&quot; ## [9] &quot;PERIODCATEGORY&quot; ## ## [[4]] ## [1] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;Nh&quot; &quot;VK&quot; &quot;T&quot; ## [6] &quot;ETCCATEGORY&quot; &quot;WHITESPACE&quot; &quot;D&quot; &quot;D&quot; &quot;VC&quot; ## [11] &quot;Di&quot; &quot;ETCCATEGORY&quot; &quot;ETCCATEGORY&quot; ## ## [[5]] ## [1] &quot;VH&quot; &quot;VJ&quot; &quot;Neu&quot; &quot;Nf&quot; ## [5] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;Caa&quot; &quot;Neu&quot; ## [9] &quot;Na&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; &quot;D&quot; ## [13] &quot;D&quot; &quot;VH&quot; &quot;T&quot; &quot;PERIODCATEGORY&quot; ## [17] &quot;Nep&quot; &quot;SHI&quot; &quot;Na&quot; &quot;DE&quot; ## [21] &quot;Na&quot; &quot;PERIODCATEGORY&quot; ## ## [[6]] ## [1] &quot;Na&quot; &quot;VE&quot; &quot;Neu&quot; &quot;Na&quot; ## [5] &quot;P&quot; &quot;Na&quot; &quot;VG&quot; &quot;Neu&quot; ## [9] &quot;PERIODCATEGORY&quot; &quot;Neu&quot; &quot;COMMACATEGORY&quot; &quot;D&quot; ## [13] &quot;D&quot; &quot;VHC&quot; &quot;PERIODCATEGORY&quot; # Named Entity Recognition ner &lt;- ner(words_1, pos_words) ner ## [[1]] ## {(23, 28, &#39;ORG&#39;, &#39;緯來體育台&#39;), (18, 22, &#39;DATE&#39;, &#39;20年前&#39;), (0, 3, &#39;PERSON&#39;, &#39;傅達仁&#39;)} ## ## [[2]] ## {(17, 21, &#39;ORG&#39;, &#39;勞工部長&#39;), (7, 9, &#39;DATE&#39;, &#39;今天&#39;), (2, 5, &#39;ORG&#39;, &#39;參議院&#39;), (0, 2, &#39;GPE&#39;, &#39;美國&#39;), (56, 58, &#39;ORDINAL&#39;, &#39;第一&#39;), (60, 62, &#39;NORP&#39;, &#39;華裔&#39;), (11, 13, &#39;PERSON&#39;, &#39;布什&#39;), (21, 24, &#39;PERSON&#39;, &#39;趙小蘭&#39;), (42, 45, &#39;ORG&#39;, &#39;參議院&#39;)} ## ## [[3]] ## {(10, 13, &#39;PERSON&#39;, &#39;土地婆&#39;)} ## ## [[4]] ## set() ## ## [[5]] ## {(4, 10, &#39;CARDINAL&#39;, &#39;59,000&#39;), (14, 18, &#39;CARDINAL&#39;, &#39;5.9萬&#39;)} ## ## [[6]] ## {(16, 17, &#39;CARDINAL&#39;, &#39;2&#39;), (12, 13, &#39;CARDINAL&#39;, &#39;1&#39;), (4, 6, &#39;CARDINAL&#39;, &#39;1,&#39;), (14, 15, &#39;CARDINAL&#39;, &#39;3&#39;)} 9.10 Tidy Up the Results sent_corp &lt;- data.frame(id = mapply(rep, c(1:length(sents)), sapply(words_1, length)) %&gt;% unlist, words = do.call(c, words_1), pos = do.call(c, pos_words)) sent_corp Exercise 9.2 How to tidy up the results of ner so that we can include the recognized named entities in the same data frame sent_corp? "],
["structured-corpus.html", "Chapter 10 Structured Corpus 10.1 NCCU Spoken Mandarin 10.2 CHILDES Format 10.3 Loading the Corpus 10.4 From Text-based to Turn-based DF 10.5 Metadata vs. Utterances 10.6 Word-based DF and Frequency List 10.7 Concordances 10.8 Collocations (Bigrams) 10.9 N-grams (Lexical Bundles) 10.10 Connecting SPID to Metadata 10.11 Corpus Headers 10.12 Sociolinguistic Analyses", " Chapter 10 Structured Corpus library(tidyverse) library(readtext) library(tidytext) library(quanteda) There are a lot of pre-collected corpora available for linguistic studies. This chapter will demonstrate how you can load existing corpora in R and perform basic corpus analysis with these data. 10.1 NCCU Spoken Mandarin In this demonstration, I will use the dataset of Taiwan Mandarin Corpus for illustration. This dataset, collected by Prof. Kawai Chui at National Cheng-Chi University, includes spontaneous face-to-face conversations of Taiwan Mandarin. The data transcription conventions can be found in the NCCU Corpus Official Website. Generally, the corpus transcripts follow the conventions of CHILDES format. In computational text analytics, the first step is always to analyze the structure of the textual data. 10.2 CHILDES Format The following is an excerpt from the file demo_data/data-nccu-M001.cha from the NCCU Corpus of Taiwan Mandarin. The conventions of CHILDES transcription include: The lines with header information begin with @ The lines with utterances begin with @ The indented lines refer to the utterances of the continuing speaker turn Words are separated by spaces The meanings of transcription symbols used in the corpus can be found in the documention of the corpus. 10.3 Loading the Corpus The corpus data is available in our demo_data/corp-NCCU-SPOKEN.tar.gz, which is a zipped archived file, i.e., one zipped tar file including all the corpus documents. We can use the readtext::readtext() to easily load the data. In this step, we treat all the *.cha files as if they are normal text files (i.e. .txt) and load the entire corpus into a data frame with two columns: doc_id and text (The warning messages only warn you that by default readtext() takes only .txt files). NCCU &lt;- readtext(&quot;demo_data/corp-NCCU-SPOKEN.tar.gz&quot;) %&gt;% as_tibble 10.4 From Text-based to Turn-based DF Before we do the turn-tokenization, we first concatenate all same-turn utterances (but with no speaker ID at the initial of the line) with their initial utterance of the speaker turn, and then we use unnest_tokens() to transform the text-based DF into a turn-based DF. NCCU_turns &lt;- NCCU %&gt;% mutate(text = str_replace_all(text,&quot;\\n\\t&quot;,&quot; &quot;)) %&gt;% # deal with same-speaker-turn utterances unnest_tokens(turn, text, token = function(x) str_split(x, pattern = &quot;\\n&quot;)) NCCU_turns 10.5 Metadata vs. Utterances Lines starting with @ are the headers of the transcript while lines starting with * are the utterances of the conversation. We split our NCCU_turns into: NCCU_turns_meta: a DF with all header lines NCCU_turns_utterance: a DF with all utterance lines NCCU_turns_meta &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^@&quot;)) NCCU_turns_utterance &lt;- NCCU_turns %&gt;% filter(str_detect(turn, &quot;^\\\\*&quot;)) %&gt;% group_by(doc_id) %&gt;% mutate(turn_id = row_number()) %&gt;% ungroup %&gt;% tidyr::separate(col=&quot;turn&quot;, into = c(&quot;SPID&quot;, &quot;turn&quot;), sep = &quot;:\\t&quot;) %&gt;% mutate(turn2 = turn %&gt;% str_replace_all(&quot;\\\\([(\\\\.)0-9]+?\\\\)&quot;,&quot; &lt;PAUSE&gt; &quot;) %&gt;% # &lt;PAUSE&gt; str_replace_all(&quot;\\\\&amp;\\\\=[a-z]+&quot;,&quot; &lt;EXTRALING&gt; &quot;) %&gt;% # &lt;EXTRALING&gt; str_replace_all(&quot;[\\u2308\\u2309\\u230a\\u230b]&quot;,&quot; &quot;) %&gt;% # overlapping talk tags str_replace_all(&quot;@[a-z:]+&quot;,&quot; &quot;) %&gt;% # code switching tags str_replace_all(&quot;\\\\s+&quot;,&quot; &quot;) %&gt;% # additional whitespaces str_trim()) NCCU_turns_utterance 10.6 Word-based DF and Frequency List As all the words have been separated by spaces, we can easily transform the turn-based DF into a word-based DF using unnest_tokens(). The key is that we specify our own tokenization function token =.... NCCU_words &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(word, turn2, token = function(x) str_split(x, &quot;\\\\s+&quot;)) %&gt;% filter(word!=&quot;&quot;) NCCU_words With word frequencies, we can generate a word cloud to have a quick overview of the word distributions in NCCU corpus. NCCU_words_freq &lt;-NCCU_words %&gt;% count(word, doc_id) %&gt;% group_by(word) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) # wordcloud require(wordcloud2) NCCU_words_freq %&gt;% filter(str_detect(word, &quot;^[^&lt;a-z]&quot;)) %&gt;% select(word, freq) %&gt;% #mutate(freq = log(freq)) %&gt;% wordcloud2::wordcloud2(minSize = 0.5, size=1, shape=&quot;diamonds&quot;) 10.7 Concordances If we need to identify turns with a particular linguistic unit, we can make use of the data wrangling tricks to easily extract speaker turns with the target pattern. # extracting particular patterns NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;覺得&quot;)) NCCU_turns_utterance %&gt;% filter(str_detect(turn2, &quot;這樣子&quot;)) Exercise 10.1 If we are interested in the use of the verb 覺得. After we extract all the speaker turns with the verb 覺得, we may need to know the subjects that often go with the verb. Please identify the word before the verb for each concordance token as one independent column of the resulting data frame (see below). Please note that one speaker turn may have more than one use of 覺得. Please create a barplot as shown below to summarize the distribution of the top 10 frequent words that directly precedes 覺得. Among the top 10 words, you would see “的 覺得” combinations, which are counter-intuitive. Please examine these tokens and explain why. 10.8 Collocations (Bigrams) Now we extend our analysis beyond single words. Please recall the ngram_chi() function we have defined and used several times in previous chapters. # functions from ch Chinese Text Processing ngram_chi &lt;- function(text, n = 2, delimiter = &quot;_&quot;){ word_vec = strsplit(text, &quot;\\\\s|\\u3000&quot;) %&gt;% unlist if(length(word_vec)&gt;=n){ map2_chr(.x= 1:(length(word_vec)-n+1), .y = n:length(word_vec), .f= function(x,y) str_c(word_vec[x:y], collapse=delimiter)) }else{ return(&quot;&quot;) }#endif }#endfunc We use the self-defined tokenization function together with unnest_tokens() to transform the turn-based DF into a bigram-based DF. NCCU_bigrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(bigrams, turn2, token = function(x) map(x, ngram_chi, n = 2)) %&gt;% filter(bigrams!=&quot;&quot;) NCCU_bigrams To determine the bigrams that are significant, we compute their relevant distributional statistics, including: frequencies dispersion collocation strength (lexical associations) NCCU_bigrams_freq &lt;- NCCU_bigrams %&gt;% count(bigrams, doc_id) %&gt;% group_by(bigrams) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(freq), desc(dispersion)) NCCU_bigrams_freq In the above example, we compute the dispersion based on the number of documents where the bigram occurs. Please note that the dispersion can be defined on the basis of the speakers as well, i.e., the number of speakers who use the bigram at least once in the corpus. You may think about how we can get dispersion statistics like this. To compute the lexical associations, we need to: remove bigrams with para-linguistic tags exclude bigrams of low dispersion get necessary observed frequencies (e.g., w1 and w2 frequencies) get expected frequencies (for more advanced lexical association metrics) NCCU_bigrams_freq %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% # exclude bigrams with para tags filter(dispersion &gt;= 5) %&gt;% # set bigram dispersion cut-off rename(O11 = freq) %&gt;% tidyr::separate(col=&quot;bigrams&quot;, c(&quot;w1&quot;, &quot;w2&quot;), sep=&quot;_&quot;) %&gt;% # split bigrams into two columns mutate(R1 = NCCU_words_freq$freq[match(w1, NCCU_words_freq$word)], C1 = NCCU_words_freq$freq[match(w2, NCCU_words_freq$word)]) %&gt;% # retrieve w1 w2 unigram freq mutate(E11 = (R1*C1)/sum(O11)) %&gt;% # compute expected freq of bigrams mutate(MI = log2(O11/E11), # compute associations t = (O11 - E11)/sqrt(E11)) -&gt; NCCU_collocations NCCU_collocations %&gt;% arrange(desc(dispersion), desc(MI)) # sorting by MI NCCU_collocations %&gt;% arrange(desc(dispersion), desc(t)) # sorting by t 10.9 N-grams (Lexical Bundles) We can also extend our analysis to n-grams of larger sizes, i.e., the lexical bundles. NCCU_ngrams &lt;- NCCU_turns_utterance %&gt;% select(-turn) %&gt;% unnest_tokens(ngram, turn2, token = function(x) map(x, ngram_chi, n = 4, delimiter = &quot;_&quot;)) %&gt;% filter(ngram != &quot;&quot;) # remove empty tokens (due to the short lines) NCCU_ngrams %&gt;% count(ngram, doc_id) %&gt;% group_by(ngram) %&gt;% summarize(freq = sum(n), dispersion = n()) %&gt;% arrange(desc(dispersion), desc(freq)) %&gt;% ungroup %&gt;% filter(!str_detect(ngram,&quot;&lt;&quot;)) -&gt; NCCU_ngrams_freq NCCU_ngrams_freq %&gt;% filter(dispersion &gt;= 5) 10.10 Connecting SPID to Metadata So far the previous analyses have not used any information of the headers. In other words, the connection between the utterances and their corresponding speakers’ profiles are not transparent in our current corpus analysis. However, for socio-linguists, the headers of the transcripts can be very informative. NCCU_turns_utterance NCCU_turns_meta Here I would like to demonstrate how to extract speaker-related information from the headers and link these speaker profiles to our corpus data (i.e., utterances). 10.11 Corpus Headers Based on the metadata of each file header, we can extract demographic information related to each speaker, including their ID, age, gender, etc. In the headers of each transcript, the demographic profiles of each speaker are provided in the lines starting with @id:\\t; and each piece of information is separated by a pipe sign | in the line. All speakers’ profiles in the corpus follow the same structure. NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) NCCU_meta &lt;- NCCU_turns_meta %&gt;% filter(str_detect(turn, &quot;^@(id)&quot;)) %&gt;% separate(col=&quot;turn&quot;, into=str_c(&quot;V&quot;,1:11, sep=&quot;&quot;), sep = &quot;\\\\|&quot;) %&gt;% select(doc_id, V2, V3, V4, V5, V7, V10) %&gt;% mutate(DOC_SPID = str_c(doc_id, V3, sep=&quot;_&quot;)) %&gt;% rename(AGE = V4, GENDER = V5, GROUP = V7, RELATION = V10, LANG = V2) %&gt;% select(-V3) NCCU_meta 10.12 Sociolinguistic Analyses Now with NCCU_meta and NCCU_turns_utterance, we can easily connect each utterance to a particular speaker (via SPID in NCCU_turns_utterance and DOC_SPID in NCCU_meta) and therefore study the linguistic variation across speakers of varying sub-groups/communities. The steps are as follows: We first extract the patterns we are interested in from NCCU_turns_utterance; We then connect the concordance tokens to their corresponding SPID profiles in NCCU_meta; We analyze how the patterns vary according to speakers of different profiles. NCCU_turns_utterance 10.12.1 Check Bigrams Distribution By Age Groups For example, we can look at bigrams used by speakers of varying age groups. The analysis requires the following steps: we retrieve target bigrams from NCCU_bigrams we generate DOC_SPID for all bigrams tokens extracted we map the DOC_SPID to NCCU_meta to get the speaker profiles of each bigram token we recode the speaker’s age into a three-level factor for more comprehensive analysis (i.e., AGE_GROUP) for each age group, we compute the bigram frequencies and dispersion NCCU_bigrams_with_meta &lt;- NCCU_bigrams %&gt;% filter(!str_detect(bigrams, &quot;&lt;&quot;)) %&gt;% mutate(DOC_SPID = str_c(doc_id, str_replace_all(SPID,&quot;\\\\*&quot;,&quot;&quot;), sep=&quot;_&quot;)) %&gt;% left_join(NCCU_meta, by = c(&quot;DOC_SPID&quot; = &quot;DOC_SPID&quot;)) %&gt;% mutate(AGE=AGE %&gt;% str_replace_all(&quot;;&quot;,&quot;&quot;) %&gt;% as.numeric) %&gt;% mutate(AGE_GROUP = cut(AGE, breaks = c(0,20,40, 60), label = c(&quot;Below_20&quot;,&quot;20-40&quot;,&quot;40-60&quot;))) NCCU_bigrams_by_age &lt;- NCCU_bigrams_with_meta %&gt;% count(bigrams,AGE_GROUP, DOC_SPID) %&gt;% group_by(bigrams, AGE_GROUP) %&gt;% summarize(freq= sum(n), dispersion = n()) %&gt;% filter(dispersion &gt;= 5) %&gt;% ungroup NCCU_bigrams_by_age 10.12.2 Numbers of Bigrams above Cut-off by Age NCCU_bigrams_by_age %&gt;% count(AGE_GROUP) %&gt;% ggplot(aes(x=AGE_GROUP, y = n, fill=AGE_GROUP))+ geom_bar(stat=&quot;identity&quot;) + labs(y = &quot;Number of N-grams of Dispersion &gt; 1 (Speaker)&quot;) 10.12.3 Bigram Word clouds by Age require(wordcloud2) NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;Below_20&quot;) %&gt;% select(bigrams, freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2 NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;20-40&quot;) %&gt;% select(bigrams, freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2 NCCU_bigrams_by_age %&gt;% filter(AGE_GROUP == &quot;40-60&quot;) %&gt;% select(bigrams,freq) %&gt;% top_n(100, freq) %&gt;% wordcloud2 Exercise 10.2 Please create a barplot, showing the top 20 bigrams ranked according to the bigram frequencies for each age group. Also, in the bar graph please include the information of dispersion for each bigram, using the transparency of the bars. The more transparent, the less dispersed (See below). Exercise 10.3 Please create a barplot, showing the top 20 bigrams ranked according to the bigram frequencies for each male and female speakers. Also, in the graph please include the information of dispersion for each bigram, using the transparency of the bars. The more transparent, the less dispersed (See below). "],
["xml.html", "Chapter 11 XML 11.1 BNC Spoken 2014 11.2 Process the Whole Directory of BNC2014 Sample 11.3 Metadata 11.4 BNC2014 for Socialinguistic Variation 11.5 Word Frequency vs. Gender 11.6 Degree ADV + ADJ", " Chapter 11 XML library(tidyverse) library(readtext) library(rvest) library(tidytext) library(quanteda) This chapter shows you how to process the recently released BNC 2014, which is by far the largest representative collection of spoken English collected in UK. For the purpose of our in-class tutorials, I have included a small sample of the BNC2014 in our demo_data. However, the whole dataset is now available via the official website: British National Corpus 2014. Please sign up for the complete access to the corpus if you need this corpus for your own research. 11.1 BNC Spoken 2014 XML is similar to HTML. Before you process the data, you need to understand the structure of the XML tags in the files. Other than that, the steps are pretty much similar to what we have done before. First, we read the XML using read_html(): # read one file at a time corp_bnc&lt;-read_html(&quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Now it is intuitive that our next step is to extract all utterances (with the tag of &lt;u&gt;...&lt;/u&gt;) in the XML file. So you may want to do the following: corp_bnc %&gt;% html_nodes(xpath = &quot;//u&quot;) %&gt;% html_text %&gt;% head ## [1] &quot;\\r\\nanhourlaterhopeshestaysdownratherlate&quot; ## [2] &quot;\\r\\nwellshehadthosetwohoursearlier&quot; ## [3] &quot;\\r\\nyeahIknowbutthat&#39;swhywe&#39;reanhourlateisn&#39;tit?mmI&#39;mtirednow&quot; ## [4] &quot;\\r\\n&quot; ## [5] &quot;\\r\\ndidyoutext--ANONnameM&quot; ## [6] &quot;\\r\\nyeahyeahhewrotebacknobotherlad&quot; See the problem? Using the above method, you lose the word boundary information from the corpus. What if you do the following? corp_bnc %&gt;% html_nodes(xpath = &quot;//w&quot;) %&gt;% html_text %&gt;% head(20) ## [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; ## [8] &quot;rather&quot; &quot;late&quot; &quot;well&quot; &quot;she&quot; &quot;had&quot; &quot;those&quot; &quot;two&quot; ## [15] &quot;hours&quot; &quot;earlier&quot; &quot;yeah&quot; &quot;I&quot; &quot;know&quot; &quot;but&quot; At the first sight, probably it seems that we have solved the problem but we don’t. There are even more problems created: Our second method does not extract non-word tokens within each utterance (e.g., &lt;pause .../&gt;, &lt;vocal .../&gt;) Our second method loses the utterance information (i.e., we don’t know which utterance each word belongs to) So we cannot extract &lt;u&gt; elements all at once; nor can we extract all &lt;w&gt; elements all at once. Probably we need to process each &lt;u&gt; node one at a time. First, let’s get all the &lt;u&gt; nodes. node_u &lt;- corp_bnc %&gt;% html_nodes(xpath=&quot;//u&quot;) node_u[[1]] ## {html_node} ## &lt;u n=&quot;1&quot; who=&quot;S0024&quot; trans=&quot;nonoverlap&quot; whoconfidence=&quot;high&quot;&gt; ## [1] &lt;w pos=&quot;AT1&quot; lemma=&quot;a&quot; class=&quot;ART&quot; usas=&quot;Z5&quot;&gt;an&lt;/w&gt; ## [2] &lt;w pos=&quot;NNT1&quot; lemma=&quot;hour&quot; class=&quot;SUBST&quot; usas=&quot;T1:3&quot;&gt;hour&lt;/w&gt; ## [3] &lt;w pos=&quot;RRR&quot; lemma=&quot;later&quot; class=&quot;ADV&quot; usas=&quot;T4&quot;&gt;later&lt;/w&gt; ## [4] &lt;pause dur=&quot;short&quot;&gt;&lt;/pause&gt; ## [5] &lt;w pos=&quot;VV0&quot; lemma=&quot;hope&quot; class=&quot;VERB&quot; usas=&quot;X2:6&quot;&gt;hope&lt;/w&gt; ## [6] &lt;w pos=&quot;PPHS1&quot; lemma=&quot;she&quot; class=&quot;PRON&quot; usas=&quot;Z8&quot;&gt;she&lt;/w&gt; ## [7] &lt;w pos=&quot;VVZ&quot; lemma=&quot;stay&quot; class=&quot;VERB&quot; usas=&quot;M8&quot;&gt;stays&lt;/w&gt; ## [8] &lt;w pos=&quot;RP&quot; lemma=&quot;down&quot; class=&quot;ADV&quot; usas=&quot;Z5&quot;&gt;down&lt;/w&gt; ## [9] &lt;pause dur=&quot;short&quot;&gt;&lt;/pause&gt; ## [10] &lt;w pos=&quot;RG&quot; lemma=&quot;rather&quot; class=&quot;ADV&quot; usas=&quot;A13:5&quot;&gt;rather&lt;/w&gt; ## [11] &lt;w pos=&quot;JJ&quot; lemma=&quot;late&quot; class=&quot;ADJ&quot; usas=&quot;T4&quot;&gt;late&lt;/w&gt; Take the first node in the XML document for example, each utterance node includes words as well as non-word tokens (i.e., paralinguistic annotations &lt;pause ...&gt;&lt;/pause&gt;). We can retrieve: words in an utterance lemma forms of all words in the utterance pos tags of all words in the utterance (BNC2014 uses UCREL CLAWS6 Tagset) paralinguistic tags in the utterance node_u[[1]] %&gt;% html_children %&gt;% html_text ## [1] &quot;an&quot; &quot;hour&quot; &quot;later&quot; &quot;&quot; &quot;hope&quot; &quot;she&quot; &quot;stays&quot; &quot;down&quot; ## [9] &quot;&quot; &quot;rather&quot; &quot;late&quot; node_u[[1]] %&gt;% html_children %&gt;% html_attr(&quot;pos&quot;) ## [1] &quot;AT1&quot; &quot;NNT1&quot; &quot;RRR&quot; NA &quot;VV0&quot; &quot;PPHS1&quot; &quot;VVZ&quot; &quot;RP&quot; NA ## [10] &quot;RG&quot; &quot;JJ&quot; node_u[[1]] %&gt;% html_children %&gt;% html_attr(&quot;lemma&quot;) ## [1] &quot;a&quot; &quot;hour&quot; &quot;later&quot; NA &quot;hope&quot; &quot;she&quot; &quot;stay&quot; &quot;down&quot; ## [9] NA &quot;rather&quot; &quot;late&quot; Exercise 11.1 Please come up with a way to extract both word and non-word tokens from each utterance. Ideally, the resulting data frame should consist of rows being the utterances, and columns including the attributes of each autterances. Most importantly, the data frame should record not only the strings of the utterance but at the same time for the word tokens, it should preserve the token-level annotation of word part-of-speech tags (see the utterance column in the table below). A sample utterance-based data frame is provided below. 11.2 Process the Whole Directory of BNC2014 Sample 11.2.1 Define Function In Section 11.1, if you have figured how to extract utterances as well as token-based information from the xml file, you can easily wrap the whole procedure as one function. With this function, we can perform the same procedure to all the xml files of the BNC2014. For example, let’s assume that we have defined a function: read_xml_bnc2014 &lt;- function(xml){ ... } This function takes one xml file as an argument and return a data frame, consisting of utterances and other relevant token-level information from the xml. read_xml_bnc2014(xmlfile = &quot;demo_data/corp-bnc-spoken2014-sample/S2A5-tgd.xml&quot;) Exercise 11.2 Now your job is to write this function, read_xml_BNC2014(xml = &quot;&quot;). 11.2.2 Process the all files in the Directory Now we utilize the self-defined function, read_xml_BNC2014(), and process all xml files in the demo_data/corp-bnc-spoken2014-sample/. Also, we combine the individual data.frame returned from each xml into a bigger one, i.e., corp_bnc_df: s.t &lt;- Sys.time() bnc_flist &lt;- dir(&quot;demo_data/corp-bnc-spoken2014-sample/&quot;,full.names = T) corp_bnc_df &lt;- map(bnc_flist, function(x) read_xml_bnc2014(x)) %&gt;% # map `read_xml_bnc2014()` to each xml in the dir do.call(rbind, .) # rbind all individual DFs Sys.time()-s.t ## Time difference of 1.116706 mins It takes about one and half minute to process the sample directory. You may store this corp_bnc_df data frame output for later use so that you don’t have to process the XML files every time you work with BNC2014. write_csv(corp_bnc_df, &quot;demo_data/corp_bnc_df.csv&quot;,col_names = T) 11.3 Metadata The best thing about BNC2014 is its rich demographic information relating to the settings and speakers of the conversations collected. The whole corpus comes with two metadata sets: bnc2014spoken-textdata.tsv: metadata for each text transcript bnc2014spoken-speakerdata.tsv: metadata for each speaker ID These two metadata sets allow us to get more information about each transcript as well as the speakers in those transcripts. 11.3.1 Text Metadata There are two files that are relevant to the text metadata: bnc2014spoken-textdata.tsv: This file includes the header/metadata information of each text file metadata-fields-text.txt: This file includes the column names/meanings of the previous text metadata tsv, i.e., bnc2014spoken-textdata.tsv. bnc_text_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-textdata.tsv&quot;, col_names = FALSE) bnc_text_meta bnc_text_meta_names &lt;-read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-text.txt&quot;, skip =2, col_names = F) bnc_text_meta_names names(bnc_text_meta) &lt;- c(&quot;textid&quot;, bnc_text_meta_names$X2) bnc_text_meta 11.3.2 Speaker Metadata There are two files that are relevant to the speaker metadata: bnc2014spoken-speakerdata.tsv: This file includes the demographic information of each speaker metadata-fields-speaker.txt: This file includes the column names/meanings of the previous speaker metadata tsv, i.e., bnc2014spoken-speakerdata.tsv. bnc_sp_meta &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/bnc2014spoken-speakerdata.tsv&quot;, col_names = F) bnc_sp_meta bnc_sp_meta_names &lt;- read_tsv(&quot;demo_data/corp-bnc-spoken2014-metadata/metadata-fields-speaker.txt&quot;, skip = 3, col_names = F) bnc_sp_meta_names names(bnc_sp_meta) &lt;- c(&quot;spid&quot;, bnc_sp_meta_names$X2) bnc_sp_meta 11.4 BNC2014 for Socialinguistic Variation Now with both the text-level and speker-level metadata, bnc_text_meta and bnc_sp_meta, we can easily connect the utterances to speaker and text profiles using their unique ID’s. BNC2014 was born for the study of socio-linguistic variation. Here I would like to show you some naitve examples, but you should get the ideas and the potentials of BNC2014. 11.5 Word Frequency vs. Gender Now we are ready to explore the gender differences in language. 11.5.1 Preprocessing To begin with, there are some utterances with no words at all. We probably like to remove these tokens. #corp_bnc_df &lt;- read_csv(&quot;demo_data/corp_bnc_df.csv&quot;) corp_bnc_df &lt;- corp_bnc_df %&gt;% filter(!is.na(utterance)) corp_bnc_df 11.5.2 Target Structures Let’s assume that we like to know which adjectives are most frequently used by men and women. corp_bnc_verb_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # extract utterances with at least one adj left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame corp_bnc_verb_gender 11.5.3 Analysis After we extract utterances with our target structures, we tokenize the utterances and create frequency lists of target structures, i.e., the adjectives. ## Problems ### Use own tokenization function ### Default tokenization increase the number of tokens quite a bit word_by_gender &lt;- corp_bnc_verb_gender %&gt;% unnest_tokens(word, utterance, to_lower = F, token = function(x) strsplit(x, split = &quot;\\\\s&quot;)) %&gt;% # tokenize utterance into words filter(str_detect(word, &quot;[^_]+_(JJ)|(JJR)|(JJT)&quot;)) %&gt;% # include adj only mutate(word = str_replace(word, &quot;_(JJ)|(JJR)|(JJT)&quot;,&quot;&quot;)) %&gt;% # remove pos tags count(gender, word) word_by_gender_top200 &lt;- word_by_gender %&gt;% group_by(gender) %&gt;% top_n(200,n) %&gt;% ungroup word_by_gender_top200 Female wordcloud require(wordcloud2) word_by_gender_top200 %&gt;% filter(gender==&quot;F&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Male wordcloud word_by_gender_top200 %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(word, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 11.3 Which adjectives are more often used by male and female speakers? This should be a statistical problem. We can in fact extend our keyword analysis (cf. Chapter 6) to this question. Please use the statistics of keyword analysis to find out the top 20 adjectives that are strongly attracted to female and male speakers according to G2 statistics. Please include in the analysis words whose frequencies &gt;= 20 in the entire corpus. Also, please note the problem of the NaN values out of the log(). 11.6 Degree ADV + ADJ In this section I would like to show you an example where we can extend our lexical analysis to a particular syntactic pattern. Specifically, I like to look at the adjectives that are emphasized in conversations (e.g., too bad, very good, quite cheap) and examine how these emphatic adjectives may differ in speakers of different genders. Here we define our patterns, utilizing the POS tags and the regular expressions: [^_]+_RG [^_]+_JJ corp_bnc_pattern_gender &lt;- corp_bnc_df %&gt;% filter(str_detect(utterance, &quot;[^_]+_RG [^_]+_JJ&quot;)) %&gt;% # extract utterances with at least one verb left_join(bnc_sp_meta, by = c(&quot;who&quot;=&quot;spid&quot;)) # attach SP metadata to the data frame pattern_by_gender &lt;- corp_bnc_pattern_gender %&gt;% unnest_tokens(pattern, utterance, to_lower = F, token = function(x) str_extract_all(x, &quot;[^_ ]+_RG [^_ ]+_JJ &quot;)) %&gt;% mutate(pattern = pattern %&gt;% str_trim %&gt;% str_replace_all(&quot;_[^_ ]+&quot;,&quot;&quot;)) %&gt;% # remove pos tags separate(pattern, into = c(&quot;ADV&quot;,&quot;ADJ&quot;), sep = &quot;\\\\s&quot;) %&gt;% count(gender, ADJ) # print top 100 pattern_by_gender %&gt;% group_by(gender) %&gt;% top_n(100,n) %&gt;% ungroup # wordcloud pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) pattern_by_gender %&gt;% filter(gender==&quot;M&quot;) %&gt;% select(ADJ, n) %&gt;% wordcloud2(size = 2, minRotation = -pi/2, maxRotation = -pi/2) Exercise 11.4 In the previous task, we have got the frequency list of adjectives by gender, i.e., pattern_by_gender. Please create a wide version of the frequency list, where each row is a word type and the columns include the frequencies of the word in male and female speakers, as well as the dispersion of the word in male and female speakers. A sample has been provided below. Dispersion is defined as the number of speakers who use the adjective at least once. Exercise 11.5 Following Exercise 11.4, now it should be clear to you that which adjectives are more likely to be emphasized by male and female speakers should be a statistical question. Please use the statistics G2 from keyword analysis to find out the top 10 emphasized adjectives that are strongly attracted to female and male speakers according to G2 statistics. Please include in the analysis adjectives whose dispersion &gt;= 2 in the respective corpus, i.e., adjectives that have been used by at least TWO different male or female speakers. Also, please note the problem of the NaN values out of the log(). Exercise 11.6 Please analyze the verbs that co-occur with the first-person pronoun I in BNC2014 in terms of speakers of different genders. Please create a frequency list of verbs that follow the first person pronoun I in demo_data/corp-bnc-spoken2014-sample. Verbs are defined as any words whose POS tag starts with VV. Also, please create the word clouds of the top 100 verbs for male and female speakers. Exercise 11.7 Please analyze the recurrent trigrams used by male and female speakers by showing the top 20 four-grams used by males and females respectively ranked according to their dispersions. Dispersion of four-grams is defined as the number of texts where the four-gram is observed. "],
["vector-space-representation.html", "Chapter 12 Vector Space Representation 12.1 Data Processing Flowchart 12.2 Document-Feature Matrix (dfm) 12.3 Corpus 12.4 Document-Feature Matrix (dfm) 12.5 Distributional Hypothesis and Distance/Similarity Metrics 12.6 Multidimensiona Space 12.7 Vector Semantics Considerations 12.8 Feature Selection 12.9 Exploratory Analysis of dfm 12.10 Document Similarity 12.11 Feature-Coocurrence Matrix (fcm)", " Chapter 12 Vector Space Representation library(tidyverse) library(quanteda) In this chapter, I would like to talk about the idea of distributional semantics, which features the hypothesis that the meaning of a linguistic unit is closely connected to its co-occurring contexts (co-texts). I will show you how this idea can be operationalized and quantified using the distributional data of the linguistic units in the corpus. Because English and Chinese text processing requires slightly different procedures, this chapter will first focus on English texts. 12.1 Data Processing Flowchart In Chapter 5, I have provided a data processing flowchart for the English texts. Here I would like to add to the flowchart several follow-up steps with respect to the vector-based representation of semantic. Most importantly, a new object class is introduced in Figure 12.1, i.e., the dfm object in quanteda. It stands for Document-Feature-Matrix. It’s a two-dimensional co-occurrence table, with the rows being the documents in the corpus, and columns being the features used to characterizing the documents. The cells in the matrix often refer to the co-occurrence statistics between each document and the feature. Different ways of operationalizing the features and the cell values may lead to different types of dfm. In this chapter, I would like to show you how we create a dfm of a corpus and what are the common ways to define features and cell valus for the analysis of semantics via vector space representation. Figure 12.1: English Text Analytics Flowchart (v2) 12.2 Document-Feature Matrix (dfm) To create a dfm, i.e., Dcument-Feature-Matrix, quanteda provides two alterantives: create dfm based on an corpus object create dfm based on an tokens object For English data, quanteda can take care of the word tokenization fairly well so you can create dfm directly from corpus (See Figure 12.1) In Chapter (???)(chinese-text-processing), we stress that the default tokenization method in quanteda with Chinese data may be limited in several ways. In order to create a dfm that takes into account the appropriateness of the Chinese word segmentation, I would highly recommend you to first create atokens object using the self-defined word segmentation methods, and then feed it to dfm() to create the dfm for your corpus. In this way, the dfm will use the segmented results defined by your word segmenter. In other words, with Chinese data, probably it is not really necessary to have a corpus object; rather, a token object of the corpus might be more useful/practical. (In quanteda, most of the functions for corpus can be applied to tokens as well, e.g., kwic(), dfm()) 12.3 Corpus In this chapter, I will use the same English dataset we discussed in Chapter 5, the data_corpus_inaugural preloaded in the package quanteda. For English data, the process is simple: we first load the corpus and create a dfm object of the corpus using dfm(). corp_us &lt;- data_corpus_inaugural corp_us_dfm &lt;- corp_us %&gt;% dfm Please note that the default data_corpus_inaugural preloaded with quanteda is a corpus object already. class(data_corpus_inaugural) ## [1] &quot;corpus&quot; &quot;character&quot; class(corp_us) ## [1] &quot;corpus&quot; &quot;character&quot; class(corp_us_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; 12.4 Document-Feature Matrix (dfm) What is dfm anyway? A document-feature-matrix is no different from a spead-sheet like table. In a dfm, each row refers to a document in the corpus, and the column refers to a linguistic unit that occurs in the document(s). If the linguistic unit you are interested in is a word, then this is a document-word-matrix, with the columns referring to all the words observed in the corpus, i.e., the vocabulary of the corpus. If the linguistic unit you are interested in is an n-gram, then this is a document-ngram-matrix, with the columns referring to all the n-grams observed in the corpus. What about the cells? What values are usually stored in the matrix? The most intuitive values in the cells are the co-occurrence frequencies, i.e., the number of occurrences of the linguistic unit (i.e., column) in a particular document (i.e., row). For example, in the following example, we can see that in the first document, i.e., 1789-Washington, there are 2 occurrences of representatives, 48 occurrences of and corp_us_dfm[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (43.0% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_ndoc ... 4 more documents ] A dfm with words as the features is the simplest way to characterize the texts in the corpus, namely, to analyze the semantics of the documents by looking at the words occurring in the documents. This document-by-word matrix treats each text as bags of words. In other words, how the words are arranged relative to each other is ignored (i.e., the morphosyntactic relationships between words in texts are greatly ignored). Therefore, this document-by-word dfm should be a naive characterization of the texts. In many computational tasks, however, it turns out that this simple bag-of-words model is very effective in modeling the semantics of the documents. 12.5 Distributional Hypothesis and Distance/Similarity Metrics The effectiveness of the dfm in semantic analysis lies in one important hypoethesis shared in the corpus linguistic community: distributional hypothesis. Distributional properties like co-occurrences are very important information in corpus linguistics. Most of the studies in corpus linguistics adopt an implicit distributional hypothesis, which can be illustrated by a few famous quotes: You shall know a word by the comany it keeps. (Firth, 1957, p.11) [D]ifference of meaning correlates with difference of distribution. (Harris, 1970, p.785) The meaning of a [construction] in the network is represented by how it is linked to other words and how these are interlinked themselves (De Deyne et al. 2016) So in our current context, the idea is that if two documents have similar sets of linguistic units popping up in them, they are more likely to be similar in their semantics as well. The advantage of creating a document-feature-matrix is that now each document is not only a series of character strings, but also a list of numeric values (i.e., a row of co-occurring frequencies), which can be compared mathematically with other documents (i.e., other rows). Take a two-dimensional space for instance. If we have vectors on this space, we can compute their distance/similarity mathematically: Figure 12.2: Vector Representation In Math, there are in general two types of metrics to measure the relationship between vectors: distance-based vs. similarity-based metrics. 12.5.1 Distance-based Metrics Many distance measures of vectors are based on the following formula and differ in individual parameter settings. \\[\\big( \\sum_{i = 1}^{n}{|x_i - y_i|^y}\\big)^{\\frac{1}{y}}\\] The n in the above formula refers to the number of dimensions of the vectors. (In other words, all the concepts we discuss here can be easily extended to vectors in multidimensional spaces.) When y is set to 2, it computes the famous Euclidean distance of two vectors, i.e., the direct spatial distance between two points on the n-dimensional space. x &lt;- c(1,9) y &lt;- c(1,3) z &lt;- c(5,1) sum(abs(x-y)^2)^(1/2) # XY distance ## [1] 6 sum(abs(y-z)^2)^(1/2) # YZ distnace ## [1] 4.472136 sum(abs(x-z)^2)^(1/2) # YZ distnace ## [1] 8.944272 The geometrical meanings of the Euclidean distance are easy to conceptualize (c.f., the dashed lines in Figure 12.3) Figure 12.3: Distance-based Metric: Euclidean Distance 12.5.2 Similarity-based Metrics In addition to distance-based metrics, the other type is similarity-based metric, which often utilizes the idea of correlations. The most commonly used one is Cosine Similarity, which can be computed as follows: \\[cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\] sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2))) # xy ## [1] 0.9778024 sum(y*z)/(sqrt(sum(y^2))*sqrt(sum(z^2))) # yz ## [1] 0.4961389 sum(x*z)/(sqrt(sum(x^2))*sqrt(sum(z^2))) # yz ## [1] 0.3032037 The geometric meanings of cosines of two vectors are connected to the arcs between the vectors: the great their cosine similarity, the smaller the arcs, the closer they are. 12.5.3 Interim Summary The Euclidean Distance metric is a distance-based metric: the larger the value, the more distant the two vectors. The Cosine Similarity metric is a similatiry-based metric: the larger the value, the closer the two vectors. Based on our computations of the metrics for the three vectors, now in terms of the Euclidean Distance, y and z are closer; in terms of Cosine Similarity, x and y are closer. Therefore, it should now be clear that the analyst needs to decide which metric to use, or more importantly, which metric is more relevant. The key is which of the following is more important in the semantic representation of the linguistic units: The absolute value differences that the vectors have on each dimension (i.e., the lengths of the vectors) The relative increase/decrease of the values on ecah dimension (i.e., the curvatures of vectors) There are many other distance-based or similarity-based metrics available. For more detail, please see Manning and Schütze (1999) Ch15.2.2. and Jurafsky and Martin (2020) Ch6: Vector Semantics and Embeddings. 12.6 Multidimensiona Space Back to our example of dfm, it is essentially the same vector representation, but in a multidimensional verson (cf. Figure 12.4). The document in each row is represented as a vector of N dimensional space. The size of N depends on the number of linguistic units that are included in the analysisdfm`. Figure 12.4: Example of Document-Feature Matrix 12.7 Vector Semantics Considerations When representing semantics of linguistic units in vector space, two factors would turn out to be very crucial: which features should be included in the analysis of multidimensional representation? which quantitative metrics should be used to represent the relationship between the linguistic unit and the features? In our current dfm based on bags of words, our concerns would be: which words should be included in the analysis of multidimensional representation? which quantitative metrics should be used to represent the relationship between the texts and the words? 12.8 Feature Selection A dfm may not be as informative as we have expected. To better capture the documental semantic similarity, there are several important factors that need to be more carefully considered with respect to the features of the dfm: The granularity of the features The informativeness of the features The distributional properties of the features 12.8.1 Granularity In our previous example, we include only words, i.e., unigrams, as our features in the dfm. We can in fact include linguistic units at multiple granularities: words (skipped) n-grams lemmas/stems For example, if you want to include bigrams, not unigrams, as features in the dfm, you can do the following: - from `corpus` -&gt; `tokens` - from `tokens` -&gt; `ngram-based tokens` - from `ngram-based tokens` -&gt; `dfm` corp_us_dfm_ngram &lt;- corp_us %&gt;% tokens(what = &quot;word&quot;) %&gt;% tokens_ngrams(n=2) %&gt;% dfm corp_us_dfm_ngram[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (71.0% sparse) and 4 docvars. ## features ## docs fellow-citizens_of of_the the_senate senate_and and_of ## 1789-Washington 1 20 1 1 2 ## 1793-Washington 0 4 0 0 1 ## 1797-Adams 0 29 0 0 2 ## 1801-Jefferson 0 28 0 0 3 ## 1805-Jefferson 0 17 0 0 1 ## 1809-Madison 0 20 0 0 2 ## features ## docs the_house house_of of_representatives representatives_: ## 1789-Washington 2 2 2 1 ## 1793-Washington 0 0 0 0 ## 1797-Adams 0 0 0 0 ## 1801-Jefferson 0 0 0 0 ## 1805-Jefferson 0 0 0 0 ## 1809-Madison 0 0 0 0 ## features ## docs :_among ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 0 ## [ reached max_ndoc ... 4 more documents ] Or for English data, if you want to ignore the stem variations between words (i.e., house and houses may not be differ so much), you can do it this way: corp_us_dfm_stem &lt;- corp_us %&gt;% dfm(stem = T) corp_us_dfm_stem[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (38.0% sparse) and 4 docvars. ## features ## docs fellow-citizen of the senat and hous repres : among ## 1789-Washington 1 71 116 1 48 2 2 1 1 ## 1793-Washington 0 11 13 0 2 0 0 1 0 ## 1797-Adams 3 140 163 1 130 3 3 0 4 ## 1801-Jefferson 2 104 130 0 81 0 1 1 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 7 ## 1809-Madison 1 69 104 0 43 0 1 0 0 ## features ## docs vicissitud ## 1789-Washington 1 ## 1793-Washington 0 ## 1797-Adams 0 ## 1801-Jefferson 0 ## 1805-Jefferson 0 ## 1809-Madison 1 ## [ reached max_ndoc ... 4 more documents ] You need to decide which type of linguistic units is more relevant to your research question. In many text mining applications, people often make use of both unigrams and n-grams. However, these are only heuristics, not rules. Exercise 12.1 Based on the dataset corp_us, can you create a dfm, where the features are trigrams but all the words in the trigrams are word stems not the original surface word forms? (see below) 12.8.2 Informativeness There are words that are not so informative in telling us the similarity and difference between the documents because they almost appear in every document of the corpus, but carray little (referential) semantic contents. These words are usually the function words, such as and, the, of. The common words in almost all documents are often referred to as stopwords. Therefore, it is not uncommon that analysts sometimes create a list of stopwords to be removed from the dfm. The library quanteda has defined a default English stopword list, i.e., stopwords(&quot;en&quot;). stopwords(&quot;en&quot;) ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; ## [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; ## [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; ## [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; ## [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; ## [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; ## [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; ## [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; ## [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; ## [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; ## [51] &quot;does&quot; &quot;did&quot; &quot;doing&quot; &quot;would&quot; &quot;should&quot; ## [56] &quot;could&quot; &quot;ought&quot; &quot;i&#39;m&quot; &quot;you&#39;re&quot; &quot;he&#39;s&quot; ## [61] &quot;she&#39;s&quot; &quot;it&#39;s&quot; &quot;we&#39;re&quot; &quot;they&#39;re&quot; &quot;i&#39;ve&quot; ## [66] &quot;you&#39;ve&quot; &quot;we&#39;ve&quot; &quot;they&#39;ve&quot; &quot;i&#39;d&quot; &quot;you&#39;d&quot; ## [71] &quot;he&#39;d&quot; &quot;she&#39;d&quot; &quot;we&#39;d&quot; &quot;they&#39;d&quot; &quot;i&#39;ll&quot; ## [76] &quot;you&#39;ll&quot; &quot;he&#39;ll&quot; &quot;she&#39;ll&quot; &quot;we&#39;ll&quot; &quot;they&#39;ll&quot; ## [81] &quot;isn&#39;t&quot; &quot;aren&#39;t&quot; &quot;wasn&#39;t&quot; &quot;weren&#39;t&quot; &quot;hasn&#39;t&quot; ## [86] &quot;haven&#39;t&quot; &quot;hadn&#39;t&quot; &quot;doesn&#39;t&quot; &quot;don&#39;t&quot; &quot;didn&#39;t&quot; ## [91] &quot;won&#39;t&quot; &quot;wouldn&#39;t&quot; &quot;shan&#39;t&quot; &quot;shouldn&#39;t&quot; &quot;can&#39;t&quot; ## [96] &quot;cannot&quot; &quot;couldn&#39;t&quot; &quot;mustn&#39;t&quot; &quot;let&#39;s&quot; &quot;that&#39;s&quot; ## [101] &quot;who&#39;s&quot; &quot;what&#39;s&quot; &quot;here&#39;s&quot; &quot;there&#39;s&quot; &quot;when&#39;s&quot; ## [106] &quot;where&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; &quot;a&quot; &quot;an&quot; ## [111] &quot;the&quot; &quot;and&quot; &quot;but&quot; &quot;if&quot; &quot;or&quot; ## [116] &quot;because&quot; &quot;as&quot; &quot;until&quot; &quot;while&quot; &quot;of&quot; ## [121] &quot;at&quot; &quot;by&quot; &quot;for&quot; &quot;with&quot; &quot;about&quot; ## [126] &quot;against&quot; &quot;between&quot; &quot;into&quot; &quot;through&quot; &quot;during&quot; ## [131] &quot;before&quot; &quot;after&quot; &quot;above&quot; &quot;below&quot; &quot;to&quot; ## [136] &quot;from&quot; &quot;up&quot; &quot;down&quot; &quot;in&quot; &quot;out&quot; ## [141] &quot;on&quot; &quot;off&quot; &quot;over&quot; &quot;under&quot; &quot;again&quot; ## [146] &quot;further&quot; &quot;then&quot; &quot;once&quot; &quot;here&quot; &quot;there&quot; ## [151] &quot;when&quot; &quot;where&quot; &quot;why&quot; &quot;how&quot; &quot;all&quot; ## [156] &quot;any&quot; &quot;both&quot; &quot;each&quot; &quot;few&quot; &quot;more&quot; ## [161] &quot;most&quot; &quot;other&quot; &quot;some&quot; &quot;such&quot; &quot;no&quot; ## [166] &quot;nor&quot; &quot;not&quot; &quot;only&quot; &quot;own&quot; &quot;same&quot; ## [171] &quot;so&quot; &quot;than&quot; &quot;too&quot; &quot;very&quot; &quot;will&quot; Also, there are tokens that usually carry very limited semantic contents, such as numbers and punctuation. Numbers, symbols and punctuations are often treated differently in computational text analytics. When creating the dfm object, we can further specify a few parameters for the function dfm(): remove_punct = TRUE: remove all punctuation tokens remove = vector(): remove all words specified in the character vector here corp_us_dfm_stp &lt;- corp_us %&gt;% dfm(remove_punct = T, remove = stopwords(&quot;en&quot;)) corp_us_dfm_stp[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (60.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among ## 1789-Washington 1 1 2 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 ## 1801-Jefferson 2 0 0 0 1 ## 1805-Jefferson 0 0 0 0 7 ## 1809-Madison 1 0 0 0 0 ## features ## docs vicissitudes incident life event filled ## 1789-Washington 1 1 1 2 1 ## 1793-Washington 0 0 0 0 0 ## 1797-Adams 0 0 2 0 0 ## 1801-Jefferson 0 0 1 0 0 ## 1805-Jefferson 0 0 2 0 0 ## 1809-Madison 0 0 1 0 1 ## [ reached max_ndoc ... 4 more documents ] We can see that the number of features drops significantly after we remove stopwords: nfeat(corp_us_dfm_ngram) # bigram version ## [1] 63591 nfeat(corp_us_dfm) # default unigram version ## [1] 9360 nfeat(corp_us_dfm_stp) # unigram removing stopwords and punks ## [1] 9210 nfeat(corp_us_dfm_stem) # unigram stem version ## [1] 5544 12.8.3 Distributional Properties Depending on the granularity of the features you are considering, you may get a considerably large number (e.g., thousands of ngrams) of features in your dfm matrix. Another criteria relating to the distinctiveness of the features are their distributional properties. These can be very complicated but here we simplify these distributional criteria into three types: Frequency To make sure that the feature is important, we probably need to set a cut-off minimum frequency for a feature. For example, if the word occurs only once in the corpus (i.e., hapax legomenon), these words can be highly idiosyncratic usage, which is of little help in capturing the cross-document similarity. But on the other hand, we can also control the maximum frequency of the feature. If the word occurrs in all documents, they won’t help much as well. Dispersion If a word is more widely dispersed across different documents, they may be more informative in telling us the semantics of a group of documents. However, if a word occurs only in one particular document, this centralized distribution of the word may indicate that this feature is too domain-specific. Therefore, sometimes we can control the document frequency of the features (i.e., in how many different texts does the feature occur?) Other self-defined weights It is true that many other factors may have a great impact on the co-occurrence frequencies we discuss so far. For example, given a co-occurrence frequency n for a word w in a text d, the significance of this n may be connected to: the document size of d the total number of w Also sometimes the importance of features comes with the theoretical assumptions. For example, in modeling the semantics of the documents, it is probably intuitive to assume that content words should carry more semantic content than functional words. So in the dfm we can include only words belonging to the lexical categories, such as nouns and verbs. Exercise 12.2 Please get familar with the following functions, provided by quanteda for weighting of the document-feature matrix: dfm_weight(), dfm_tfidf(). In the following demo, we adopt a few simple distrubtional criteria: we use a simple unigram model we remove stopwords, punctuations, numbers, and symbols we remove words whose freqency &lt;= 10, docfreq &lt;= 3, docfreq = ndoc(CORPUS) corp_us_dfm_trimmed &lt;- corp_us %&gt;% dfm(remove = stopwords(&quot;en&quot;), remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% dfm_trim(min_termfreq = 10, termfreq_type = &quot;count&quot;, min_docfreq = 3, max_docfreq = ndoc(corp_us)-1, docfreq_type = &quot;count&quot;) nfeat(corp_us_dfm_trimmed) ## [1] 1387 corp_us_dfm_trimmed[1:10, 1:10] ## Document-feature matrix of: 10 documents, 10 features (52.0% sparse) and 4 docvars. ## features ## docs fellow-citizens senate house representatives among life event ## 1789-Washington 1 1 2 2 1 1 2 ## 1793-Washington 0 0 0 0 0 0 0 ## 1797-Adams 3 1 0 2 4 2 0 ## 1801-Jefferson 2 0 0 0 1 1 0 ## 1805-Jefferson 0 0 0 0 7 2 0 ## 1809-Madison 1 0 0 0 0 1 0 ## features ## docs greater order received ## 1789-Washington 1 2 1 ## 1793-Washington 0 0 0 ## 1797-Adams 0 4 0 ## 1801-Jefferson 1 1 0 ## 1805-Jefferson 0 3 0 ## 1809-Madison 0 0 0 ## [ reached max_ndoc ... 4 more documents ] 12.9 Exploratory Analysis of dfm We can check the top features in the current corpus: topfeatures(corp_us_dfm_trimmed) ## people government us can upon must great ## 575 564 478 471 371 366 340 ## may states shall ## 338 333 314 We can visualize the top features using a word cloud: require(RColorBrewer) set.seed(100) textplot_wordcloud(corp_us_dfm_trimmed, max_words = 200, random_order = FALSE, rotation = .25, color = c(&#39;red&#39;, &#39;pink&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;blue&#39;)) 12.10 Document Similarity As shown in 12.4, with the N-dimensional vector representation of each document, we can easily compute the mathematical distances/similarities between two documents. In Section 12.5, we introduced two important metrics: distance-based metric: Euclidean Distance similarity-based metric: Cosine Similarity quanteda provides useful functions to compute these metrics (as well as other alternatives): textstat_simil() and textstat_dist(): corp_us_euclidean &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme=&quot;prop&quot;) %&gt;% textstat_dist(method=&quot;euclidean&quot;) corp_us_cosine &lt;- corp_us_dfm_trimmed %&gt;% dfm_weight(scheme=&quot;prop&quot;) %&gt;% textstat_simil(method=&quot;cosine&quot;) corp_us_euclidean[1:5,1:5] ## 5 x 5 Matrix of class &quot;dspMatrix&quot; ## 1789-Washington 1793-Washington 1797-Adams 1801-Jefferson ## 1789-Washington 1.070008e-08 0.1524448 0.07003297 7.371303e-02 ## 1793-Washington 1.524448e-01 0.0000000 0.15290542 1.527390e-01 ## 1797-Adams 7.003297e-02 0.1529054 0.00000000 7.148902e-02 ## 1801-Jefferson 7.371303e-02 0.1527390 0.07148902 4.562530e-09 ## 1805-Jefferson 7.126417e-02 0.1530932 0.07194156 6.427144e-02 ## 1805-Jefferson ## 1789-Washington 0.07126417 ## 1793-Washington 0.15309316 ## 1797-Adams 0.07194156 ## 1801-Jefferson 0.06427144 ## 1805-Jefferson 0.00000000 corp_us_cosine[1:5, 1:5] ## 5 x 5 Matrix of class &quot;dspMatrix&quot; ## 1789-Washington 1793-Washington 1797-Adams 1801-Jefferson ## 1789-Washington 1.0000000 0.2958278 0.5558610 0.4748221 ## 1793-Washington 0.2958278 1.0000000 0.2865581 0.2763780 ## 1797-Adams 0.5558610 0.2865581 1.0000000 0.4959554 ## 1801-Jefferson 0.4748221 0.2763780 0.4959554 1.0000000 ## 1805-Jefferson 0.4950774 0.2655092 0.4738698 0.5440536 ## 1805-Jefferson ## 1789-Washington 0.4950774 ## 1793-Washington 0.2655092 ## 1797-Adams 0.4738698 ## 1801-Jefferson 0.5440536 ## 1805-Jefferson 1.0000000 Based on the distances/similarities, we can further examine how different documents may cluster together in terms of their features (lexical) distributions. Here we apply the hierarchical cluster analysis to examine the sub-groupings of the documents. # distance-based corp_us_hist_euclidean &lt;- corp_us_euclidean %&gt;% as.dist %&gt;% hclust plot(corp_us_hist_euclidean,hang = -1, cex = 0.7) # similarity corp_us_hist_cosine &lt;- (1 - corp_us_cosine) %&gt;% as.dist %&gt;% hclust plot(corp_us_hist_cosine,hang = -1, cex = 0.7) Please note that textstat_simil() gives us the similarity matrix. In other words, the numbers in the matrix indicate how similar the documents are. However, for hierarchical cluster analysis, the function hclust() expects a distance-based matrix, namely one indicating how dissimilar the documents are. That is the main reason why we use (1 - corp_us_cosine) in the cosine example. Cluster anlaysis is a very useful exploratory technique to examine the emerging structure of a large dataset. For more detail introduction to this statistical method, I would recommend Gries (2013) Ch 5.6. 12.11 Feature-Coocurrence Matrix (fcm) We can convert a document-feature matrix into a feature-cooccurrence matrix fmc, using the quanteda::fcm(). The fmc includes the co-occurrence frequencies of any two features within the same documents. # convert `dfm` to `fcm` corp_us_fcm &lt;- corp_us_dfm_trimmed %&gt;% fcm() corp_us_fcm[1:10, 1:10] ## Feature co-occurrence matrix of: 10 by 10 features. ## features ## features fellow-citizens senate house representatives among life event ## fellow-citizens 72 64 15 62 106 50 25 ## senate 0 13 16 29 36 30 2 ## house 0 0 3 11 11 32 6 ## representatives 0 0 0 8 45 33 11 ## among 0 0 0 0 155 240 26 ## life 0 0 0 0 0 208 21 ## event 0 0 0 0 0 0 9 ## greater 0 0 0 0 0 0 0 ## order 0 0 0 0 0 0 0 ## received 0 0 0 0 0 0 0 ## features ## features greater order received ## fellow-citizens 82 52 26 ## senate 35 19 6 ## house 14 14 3 ## representatives 28 29 9 ## among 144 209 21 ## life 136 182 16 ## event 16 15 12 ## greater 44 76 19 ## order 0 98 12 ## received 0 0 2 In quanteda, we can generate the fcm of a corpus, either directly from the corpus object or from the dfm object. The feature-cooccurrence matrix measures the co-occurrences of features within a user-defined context. If the input of fcm is a dfm object, the context is set to be documents. In other words, the counts in fcm refers to the number of co-occurrences the two features within the same document. If the input of fmc is a corpus object, we can specify the context to be a window size. The counts in fcm refers to the number of co-occurrences the two features within the window size. We can the structure of the fcm with a simple example: x &lt;- c(&quot;A B C A E F G&quot;, &quot;B C D E F G&quot;, &quot;B D A E F G&quot;) corpus(x) %&gt;% dfm %&gt;% fcm # fcm based on document context ## Feature co-occurrence matrix of: 7 by 7 features. ## features ## features a b c e f g d ## a 1 3 2 3 3 3 1 ## b 0 0 2 3 3 3 2 ## c 0 0 0 2 2 2 1 ## e 0 0 0 0 3 3 2 ## f 0 0 0 0 0 3 2 ## g 0 0 0 0 0 0 2 ## d 0 0 0 0 0 0 0 corpus(x) %&gt;% fcm(context = &quot;window&quot;, window = 2) # fcm based on window context ## Feature co-occurrence matrix of: 7 by 7 features. ## features ## features A B C E F G D ## A 0 3 2 2 2 0 1 ## B 0 0 2 0 0 0 2 ## C 0 0 0 2 0 0 1 ## E 0 0 0 0 3 3 2 ## F 0 0 0 0 0 3 1 ## G 0 0 0 0 0 0 0 ## D 0 0 0 0 0 0 0 corpus(x) %&gt;% fcm(context = &quot;document&quot;) # same as the first one ## Feature co-occurrence matrix of: 7 by 7 features. ## features ## features A B C E F G D ## A 1 3 2 3 3 3 1 ## B 0 0 2 3 3 3 2 ## C 0 0 0 2 2 2 1 ## E 0 0 0 0 3 3 2 ## F 0 0 0 0 0 3 2 ## G 0 0 0 0 0 0 2 ## D 0 0 0 0 0 0 0 fmc is an interesting structure because, similar to dfm, we can now examine the pairwise relationships between features. More specifically, we get to see how features (i.e., words, n-grams etc) are connected, or to what extent features are similar in their co-occurring contexts. # select top 30 features corp_us_topfeatures &lt;- names(topfeatures(corp_us_fcm, 50)) # plot network fcm_select(corp_us_fcm, pattern = corp_us_topfeatures) %&gt;% textplot_network(min_freq = 100) The dfm or fcm come with many potentials. Please refer to the quanteda documentation for more applications. Exercise 12.3 Please create a network of the top 30 bigrams based on the corpus corp_us. The criteria for bigrams selection are as follows: Include bigrams that consist of alphanumeric characters only (no punctuations) Include bigrams whose frequency &gt;= 10 and docfreq &gt;= 5 but &lt;= half number of the corpus References "],
["vector-space-representation-ii.html", "Chapter 13 Vector Space Representation II 13.1 Chinese Text Analytics Flowchart 13.2 A Quick View 13.3 Loading the Corpus 13.4 Semgentation 13.5 Corpus Metadata 13.6 Document-Feature Matrix 13.7 Top Features and Wordcloud 13.8 Document Similarity 13.9 Feature Similarity", " Chapter 13 Vector Space Representation II library(tidyverse) library(quanteda) library(tidytext) library(readtext) library(jiebaR) 13.1 Chinese Text Analytics Flowchart Figure 13.1: Chinese Text Analytics Flowchart In Chapter 12, we have demonstrated the potential of a vector representation of documents with the English data. Here, we would like to look at the Chinese data in more detail. In the corpus data processing flowchart, as repeated below (Figure 13.1), we need to deal with the word segmentation with the Chinese data. This prevents us from creating a dfm directly from a corpus object because the default internal word tokenization in quanteda is not optimized for non-English languages. In this chapter, we will be using the dataset demo_data/TaiwanPresidentalSpeech.zip. Please make sure that you have downloaded the dataset from demo_data. 13.2 A Quick View For Chinese data, the major preprocessing steps have been highlighted in Figure 13.1: First load the corpus using readtext() and create corpus tibble/data.frame object (with as_tibble()) Tokenize the text-based corpus data.frame using unnest_tokens() and self-defined word tokenizer from jiebaR::segment(). This will give you a token-based tibble/data.frame version of your corpus. Tokenize the texts column/vector from the text-based DF using jiebaR::segment() and convert the list output into a token object using as.tokens(). This will give you a token version of your corpus. A token object is defined in quanteda, with many similar functions as a corpus object. This is the most important trick with the Chinese data. When you utilize many different libaries in R for your tasks, one thing you need to keep in your mind is that you need to fully understand what kind of objects you are dealing with. That is, you need to keep track of every variable you create in your script in terms of their object type/class. A vector is different from a list; a list is different from a token. Also, some of the object classes are predefined in R (e.g., vector, data.frame) while others are defined in specific libararies (e.g., corpus, token, dfm). As a habit, always check your object class (i.e., class()). 13.3 Loading the Corpus corp_tw &lt;- readtext(file=&quot;demo_data/TW_President.tar.gz&quot;) %&gt;% as_tibble class(corp_tw) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; corp_tw %&gt;% mutate(text = str_sub(text, 1,20)) NB: readtext() creates a readtext or data.frame object. Following the tidy principle, we convert everything into tibble. When I load the corpus, I convert the readtext output directly into a tibble. If you need to process the corpus as a corpus object defined in quanteda, you can use corpus() to convert the readtext into corpus object. For now, we don’t need that. 13.4 Semgentation In order to create a vector representation of the Chinese documents, we need to create the dfm for our corpus. In Chapter 12, we suggest two ways to create the dfm: create the dfm from the corpus object create the dfm from the tokens object The second method is preferred for Chinese texts because we get to use our own word segmenter and self-defined dictionary for word segmentation. Therefore, with the text-based DF, now the next steps include: Initialize the jiebaR word segmenter, where a user dictionary is defined (Always use own dictionary to improve the performance of word segmentation) Subset the text column of corp_tw tokenize the texts and convert the output into a quanteda-compatible object, i.e., tokens # initialize segmenter chi_seg &lt;- worker(bylines = T, user = &quot;demo_data/dict-ch-user.txt&quot;, symbol = F) corp_tw_tokens &lt;- corp_tw$text %&gt;% # subset `text` column segment(jiebar = chi_seg) %&gt;% # tokenize the texts as.tokens # covert to `tokens` class(corp_tw_tokens) ## [1] &quot;tokens&quot; #str(corp_tw_tokens) corp_tw_tokens[[14]][1:10] ## [1] &quot;為&quot; &quot;年輕人&quot; &quot;打造&quot; &quot;一個&quot; &quot;更好&quot; &quot;的&quot; &quot;國家&quot; &quot;各位&quot; ## [9] &quot;友邦&quot; &quot;的&quot; Please note that when we initilize the segmenter chi_seg, we specify the argument work(…, symbol = F because symbols may not be semantically relevant in our later analysis of vector space representation. But you should be well aware of which tokens have been removed/kept in the preprocessing of your data. 13.5 Corpus Metadata When we subset the texts from the DF corp_tw for word segmentation, all the metadata connected to each text did not go with the texts. So the corp_tw_tokens did not have any metadata information. We can retrieve/add metadata information using quanteda::docvars() for corpus and tokens objects. Now the corp_tw_tokens does not have any metadata: docvars(corp_tw_tokens) So here we extract metadata information from the original filenames of each text stored in the corp_tw, and attach this metadata to corp_tw_tokens. corp_tw_meta &lt;-corp_tw %&gt;% dplyr::select(-text) %&gt;% separate(doc_id, into = c(&quot;YEAR&quot;,&quot;TERM&quot;,&quot;PRESIDENT&quot;),sep = &quot;_&quot;) %&gt;% mutate(PRESIDENT = str_replace(PRESIDENT, &quot;.txt&quot;,&quot;&quot;)) corp_tw_meta docvars(corp_tw_tokens) &lt;- corp_tw_meta docvars(corp_tw_tokens) 13.6 Document-Feature Matrix Now that we have a tokens version of our corpus, we can create dfm using the dfm(). Also, we can take care of the feature selection (cf. Chapter 12.8) using functions like dfm_trim(), dfm_select(). corp_tw_dfm &lt;- corp_tw_tokens %&gt;% dfm(remove = readLines(&quot;demo_data/stopwords-ch.txt&quot;), remove_punct = T, remove_numbers= T, remove_symbols = T) %&gt;% dfm_trim(min_termfreq = 5, termfreq_type = &quot;count&quot;, min_docfreq = 2, max_docfreq = ndoc(corp_tw_tokens), docfreq_type = &quot;count&quot;) nfeat(corp_tw_dfm) ## [1] 714 class(corp_tw_dfm) ## [1] &quot;dfm&quot; ## attr(,&quot;package&quot;) ## [1] &quot;quanteda&quot; corp_tw_dfm[1:5, 1:10] ## Document-feature matrix of: 5 documents, 10 features (14.0% sparse) and 3 docvars. ## features ## docs 中正 國民大會 憲法 選舉 中華民國 總統 國家 人民 公僕 就職 ## text1 5 1 12 1 5 1 18 17 1 2 ## text2 4 4 3 1 4 1 8 5 1 1 ## text3 5 1 0 1 0 2 3 6 0 1 ## text4 7 3 2 1 1 3 10 4 0 0 ## text5 5 1 1 0 3 2 2 1 1 0 13.7 Top Features and Wordcloud require(wordcloud2) top_features &lt;- corp_tw_dfm %&gt;% topfeatures(100) word_freq &lt;- data.frame(word = names(top_features), freq = top_features) word_freq %&gt;% wordcloud2(size= 0.8,minRotation = -pi/2, maxRotation = -pi/2) 13.8 Document Similarity corp_tw_cosine &lt;- corp_tw_dfm %&gt;% dfm_weight(scheme = &quot;prop&quot;) %&gt;% textstat_simil(method=&quot;cosine&quot;) corp_tw_hist &lt;- (1-corp_tw_cosine) %&gt;% as.dist %&gt;% hclust hist_labels &lt;- str_c(docvars(corp_tw_dfm,&quot;YEAR&quot;), docvars(corp_tw_dfm,&quot;PRESIDENT&quot;), sep=&quot;_&quot;) plot(corp_tw_hist, hang = -1, cex = 1.2, label = hist_labels) 13.9 Feature Similarity # convert `dfm` to `fcm` corp_tw_fcm &lt;- corp_tw_dfm %&gt;% fcm # select top 30 features corp_tw_topfeatures &lt;- names(topfeatures(corp_tw_fcm, 50)) # plot network fcm_select(x = corp_tw_fcm, pattern = corp_tw_topfeatures) %&gt;% textplot_network(min_freq = 0.5) -&gt;g ggsave(&quot;test.png&quot;, g) Exercise 13.1 Create the network of top 30 bigrams for the corpus corp_tw. The critera for bigrams selection are as follows: include bigrams whose frequency &gt;= 5 and docfreq &gt;= 2 Exercise 13.2 There is an interesting application. When we analyze the document similarity, we create the graph of a dendrogram using hierarchical cluster analysis. In fact, document relations can also be represented by a network as well, as we do with the features in Section 13.9. How could you make use of the function textplot_network() in quanteda to create a network of the presidents? Please create a similar president network as shown below. Hint: Transpose the dfm so that presidents become the features of the matrix. "],
["vector-space-representation-iii.html", "Chapter 14 Vector Space Representation III 14.1 Library 14.2 Text Collection 14.3 Tokenization and Vocabulary 14.4 Pruning 14.5 Term-Cooccurrence Matrix 14.6 Fitting Model 14.7 Averaging Word Vectors 14.8 Semantic Space 14.9 Visualizing Multi-dimensional Space", " Chapter 14 Vector Space Representation III In this chapter, we will discuss the idea of representing the semantic space of words via unsupervised learning. The word vectors learned from the large collection of text data are referred to as word embeddings. This tutorial is based on Dmitriy Selivanov’s website. 14.1 Library library(text2vec) 14.2 Text Collection library(text2vec) wikitext &lt;- &quot;demo_data/corp-wikipedia&quot; wiki &lt;- readLines(wikitext, n = 1, warn = FALSE) substr(wiki, 1, 400) ## [1] &quot; anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word a&quot; 14.3 Tokenization and Vocabulary # Create iterator over tokens tokens &lt;- space_tokenizer(wiki) # Create vocabulary. Terms will be unigrams (simple words). it &lt;- itoken(tokens, progressbar = FALSE) vocab &lt;- create_vocabulary(it) 14.4 Pruning vocab &lt;- prune_vocabulary(vocab, term_count_min = 5L) 14.5 Term-Cooccurrence Matrix # Use our filtered vocabulary vectorizer &lt;- vocab_vectorizer(vocab) # use window of 5 for context words tcm &lt;- create_tcm(it, vectorizer, skip_grams_window = 5L) 14.6 Fitting Model glove &lt;- GlobalVectors$new(rank = 50, x_max = 10) wv_main &lt;- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8) # main words vectors dim(wv_main) wv_context &lt;- glove$components dim(wv_context) 14.7 Averaging Word Vectors word_vectors &lt;- wv_main + t(wv_context) 14.8 Semantic Space berlin = word_vectors[&quot;paris&quot;, , drop = FALSE] - word_vectors[&quot;france&quot;, , drop = FALSE] + word_vectors[&quot;germany&quot;, , drop = FALSE] cos_sim = sim2(x = word_vectors, y = berlin, method = &quot;cosine&quot;, norm = &quot;l2&quot;) head(sort(cos_sim[,1], decreasing = TRUE), 5) ## berlin paris germany munich leipzig ## 0.7791436 0.7701525 0.6789727 0.6365748 0.6265751 14.9 Visualizing Multi-dimensional Space Please read How to Use t-SNE Effectively for more information. # TSNE library(Rtsne) library(dplyr) library(ggplot2) Visualizae the semantic distances of the top 100 content words in corpus Here I demonstrate how to visualize the semantic distances of the 2000 words included in the General Servise Word List using the word embeddings. The procedures are described as follows: We load the Google Analogy Dataset from the web. We clean up the data and retrieve a random sample set for visualization We use t-SNE for multidimensional scaling. We obtain two-dimensional cordinates from t-SNE for visualization require(stringr) google_analogy &lt;- readLines(&quot;http://download.tensorflow.org/data/questions-words.txt&quot;) google_analogy[1:10] ## [1] &quot;: capital-common-countries&quot; &quot;Athens Greece Baghdad Iraq&quot; ## [3] &quot;Athens Greece Bangkok Thailand&quot; &quot;Athens Greece Beijing China&quot; ## [5] &quot;Athens Greece Berlin Germany&quot; &quot;Athens Greece Bern Switzerland&quot; ## [7] &quot;Athens Greece Cairo Egypt&quot; &quot;Athens Greece Canberra Australia&quot; ## [9] &quot;Athens Greece Hanoi Vietnam&quot; &quot;Athens Greece Havana Cuba&quot; google_analogy[str_starts(google_analogy,&quot;\\\\:&quot;)] ## [1] &quot;: capital-common-countries&quot; &quot;: capital-world&quot; ## [3] &quot;: currency&quot; &quot;: city-in-state&quot; ## [5] &quot;: family&quot; &quot;: gram1-adjective-to-adverb&quot; ## [7] &quot;: gram2-opposite&quot; &quot;: gram3-comparative&quot; ## [9] &quot;: gram4-superlative&quot; &quot;: gram5-present-participle&quot; ## [11] &quot;: gram6-nationality-adjective&quot; &quot;: gram7-past-tense&quot; ## [13] &quot;: gram8-plural&quot; &quot;: gram9-plural-verbs&quot; google_analogy_df &lt;- data.frame(analogy = str_to_lower(google_analogy)) %&gt;% filter(str_detect(analogy,&quot;^[^\\\\:]&quot;)) %&gt;% tidyr::separate(analogy, into = c(&quot;w1&quot;,&quot;w2&quot;,&quot;w3&quot;,&quot;w4&quot;)) # random sample set.seed(12) words &lt;- google_analogy_df %&gt;% sample_n(20,replace = F) %&gt;% unlist %&gt;% as.vector # Include only seen cases words_seen &lt;-words[words %in% row.names(word_vectors)] %&gt;% unique tsne &lt;- Rtsne(word_vectors[words_seen,], perplexity = (length(words_seen)-1)/3 , pca = FALSE) # tsne_df &lt;- tsne$Y %&gt;% as.data.frame() %&gt;% mutate(word = words_seen) tsne_df tsne_df %&gt;% ggplot(aes(x = V1, y = V2, label = word)) + geom_point(size = 0.7, alpha = 0.7) + geom_text(size = 3, family = &quot;Arial Unicode MS&quot;, vjust = 1.4, angle = 0) + # #scale_x_continuous(expand= expansion(add=10)) + #scale_y_continuous(expand= expansion(add=50)) + scale_color_discrete(guide=F) + theme_bw()-&gt; tsne_plot tsne_plot An overview of tasks in Chinese NLP: See Chinese NLP Exercise 14.1 Use the word_vectors trained in this chapter and visualize the semantic distances of the words included in country below with the t-SNE multi-dimensional scaling techique. (Parameters: perplexity = 2.5) country &lt;- c(&quot;germany&quot;, &quot;berlin&quot;, &quot;france&quot;, &quot;paris&quot;, &quot;china&quot;, &quot;beijing&quot;, &quot;taiwan&quot;,&quot;taipei&quot;, &quot;england&quot;,&quot;london&quot;, &quot;netherlands&quot;,&quot;amsterdam&quot;) "],
["references.html", "Chapter 15 References", " Chapter 15 References "]
]
