<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Corpus Analysis: A Start | Corpus Linguistics</title>
  <meta name="description" content="ENC2036 Course material first edition" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Corpus Analysis: A Start | Corpus Linguistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="ENC2036 Course material first edition" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Corpus Analysis: A Start | Corpus Linguistics" />
  
  <meta name="twitter:description" content="ENC2036 Course material first edition" />
  

<meta name="author" content="Alvin Chen" />


<meta name="date" content="2020-03-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="creating-corpus.html"/>
<link rel="next" href="tokenization.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="libs/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.5/grViz.js"></script>
<link href="libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="libs/wordcloud2-0.0.1/hover.js"></script>
<script src="libs/wordcloud2-binding-0.2.1/wordcloud2.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://alvinntnu.github.io/NTNU_ENC2036/"><img src="images/alerts/home.svg" height = "20" width = "20"> Course Website</a></li>
<li><a href="./"><img src="images/alerts/book-solid.svg" height = "20" width = "20"> ENC 2036 Corpus Linguistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objective"><i class="fa fa-check"></i>Course Objective</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#textbook"><i class="fa fa-check"></i>Textbook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-website"><i class="fa fa-check"></i>Course Website</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-demo-data"><i class="fa fa-check"></i>Course Demo Data</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html"><i class="fa fa-check"></i><b>1</b> What is Corpus Linguistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#why-do-we-need-corpus-data"><i class="fa fa-check"></i><b>1.1</b> Why do we need corpus data?</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#what-is-corpus"><i class="fa fa-check"></i><b>1.2</b> What is corpus?</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#what-is-a-corpus-linguistic-study"><i class="fa fa-check"></i><b>1.3</b> What is a corpus linguistic study?</a></li>
<li class="chapter" data-level="1.4" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#additional-information-on-cl"><i class="fa fa-check"></i><b>1.4</b> Additional Information on CL</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>2</b> R Fundamentals</a><ul>
<li class="chapter" data-level="" data-path="r-fundamentals.html"><a href="r-fundamentals.html#a-quick-note"><i class="fa fa-check"></i>A Quick Note</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="creating-corpus.html"><a href="creating-corpus.html"><i class="fa fa-check"></i><b>3</b> Creating Corpus</a><ul>
<li class="chapter" data-level="3.1" data-path="creating-corpus.html"><a href="creating-corpus.html#html-structure"><i class="fa fa-check"></i><b>3.1</b> HTML Structure</a><ul>
<li class="chapter" data-level="3.1.1" data-path="creating-corpus.html"><a href="creating-corpus.html#html-syntax"><i class="fa fa-check"></i><b>3.1.1</b> HTML Syntax</a></li>
<li class="chapter" data-level="3.1.2" data-path="creating-corpus.html"><a href="creating-corpus.html#tags-and-attributes"><i class="fa fa-check"></i><b>3.1.2</b> Tags and Attributes</a></li>
<li class="chapter" data-level="3.1.3" data-path="creating-corpus.html"><a href="creating-corpus.html#css"><i class="fa fa-check"></i><b>3.1.3</b> CSS</a></li>
<li class="chapter" data-level="3.1.4" data-path="creating-corpus.html"><a href="creating-corpus.html#html-css-javascript"><i class="fa fa-check"></i><b>3.1.4</b> HTML + CSS ( + JavaScript)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="creating-corpus.html"><a href="creating-corpus.html#web-crawling"><i class="fa fa-check"></i><b>3.2</b> Web Crawling</a></li>
<li class="chapter" data-level="3.3" data-path="creating-corpus.html"><a href="creating-corpus.html#functional-programming"><i class="fa fa-check"></i><b>3.3</b> Functional Programming</a></li>
<li class="chapter" data-level="3.4" data-path="creating-corpus.html"><a href="creating-corpus.html#save-corpus"><i class="fa fa-check"></i><b>3.4</b> Save Corpus</a></li>
<li class="chapter" data-level="3.5" data-path="creating-corpus.html"><a href="creating-corpus.html#additional-resourcess"><i class="fa fa-check"></i><b>3.5</b> Additional Resourcess</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html"><i class="fa fa-check"></i><b>4</b> Corpus Analysis: A Start</a><ul>
<li class="chapter" data-level="4.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#installing-quanteda"><i class="fa fa-check"></i><b>4.1</b> Installing <code>quanteda</code></a></li>
<li class="chapter" data-level="4.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#building-a-corpus-from-character-vector"><i class="fa fa-check"></i><b>4.2</b> Building a <code>corpus</code> from character vector</a></li>
<li class="chapter" data-level="4.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#keyword-in-context-kwic"><i class="fa fa-check"></i><b>4.3</b> Keyword-in-Context (KWIC)</a></li>
<li class="chapter" data-level="4.4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#kwic-with-regular-expressions"><i class="fa fa-check"></i><b>4.4</b> KWIC with Regular Expressions</a></li>
<li class="chapter" data-level="4.5" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#tidy-text-format-of-the-corpus"><i class="fa fa-check"></i><b>4.5</b> Tidy Text Format of the Corpus</a></li>
<li class="chapter" data-level="4.6" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#frequency-lists"><i class="fa fa-check"></i><b>4.6</b> Frequency Lists</a></li>
<li class="chapter" data-level="4.7" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#word-cloud"><i class="fa fa-check"></i><b>4.7</b> Word Cloud</a></li>
<li class="chapter" data-level="4.8" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#collocations"><i class="fa fa-check"></i><b>4.8</b> Collocations</a><ul>
<li class="chapter" data-level="4.8.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#cooccurrence-table-and-observed-frequencies"><i class="fa fa-check"></i><b>4.8.1</b> Cooccurrence Table and Observed Frequencies</a></li>
<li class="chapter" data-level="4.8.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#expected-frequencies"><i class="fa fa-check"></i><b>4.8.2</b> Expected Frequencies</a></li>
<li class="chapter" data-level="4.8.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#association-measures"><i class="fa fa-check"></i><b>4.8.3</b> Association Measures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tokenization.html"><a href="tokenization.html"><i class="fa fa-check"></i><b>5</b> Tokenization</a><ul>
<li class="chapter" data-level="5.1" data-path="tokenization.html"><a href="tokenization.html#english-tokenization"><i class="fa fa-check"></i><b>5.1</b> English Tokenization</a></li>
<li class="chapter" data-level="5.2" data-path="tokenization.html"><a href="tokenization.html#text-analytics-pipeline"><i class="fa fa-check"></i><b>5.2</b> Text Analytics Pipeline</a></li>
<li class="chapter" data-level="5.3" data-path="tokenization.html"><a href="tokenization.html#proper-units-for-analysis"><i class="fa fa-check"></i><b>5.3</b> Proper Units for Analysis</a><ul>
<li class="chapter" data-level="5.3.1" data-path="tokenization.html"><a href="tokenization.html#sentence-tokenization"><i class="fa fa-check"></i><b>5.3.1</b> Sentence Tokenization</a></li>
<li class="chapter" data-level="5.3.2" data-path="tokenization.html"><a href="tokenization.html#words-tokenization"><i class="fa fa-check"></i><b>5.3.2</b> Words Tokenization</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tokenization.html"><a href="tokenization.html#lexical-bundles-n-grams"><i class="fa fa-check"></i><b>5.4</b> Lexical Bundles (n-grams)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html"><i class="fa fa-check"></i><b>6</b> Parts-of-Speech Tagging</a><ul>
<li class="chapter" data-level="6.1" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#parts-of-speech-tagging-1"><i class="fa fa-check"></i><b>6.1</b> Parts-of-Speech Tagging</a></li>
<li class="chapter" data-level="6.2" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#metalingusitic-analysis"><i class="fa fa-check"></i><b>6.2</b> Metalingusitic Analysis</a></li>
<li class="chapter" data-level="6.3" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#saving-pos-tagged-texts"><i class="fa fa-check"></i><b>6.3</b> Saving POS-tagged Texts</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="keyword-analysis.html"><a href="keyword-analysis.html"><i class="fa fa-check"></i><b>7</b> Keyword Analysis</a></li>
<li class="chapter" data-level="8" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html"><i class="fa fa-check"></i><b>8</b> Constructions and Idioms</a><ul>
<li class="chapter" data-level="8.1" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#chinese-four-character-idioms"><i class="fa fa-check"></i><b>8.1</b> Chinese Four-character Idioms</a></li>
<li class="chapter" data-level="8.2" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#dictionary-entries"><i class="fa fa-check"></i><b>8.2</b> Dictionary Entries</a></li>
<li class="chapter" data-level="8.3" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#case-study-x來y去"><i class="fa fa-check"></i><b>8.3</b> Case Study: <code>X來Y去</code></a></li>
<li class="chapter" data-level="8.4" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#exercises"><i class="fa fa-check"></i><b>8.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html"><i class="fa fa-check"></i><b>9</b> Chinese Text Processing</a><ul>
<li class="chapter" data-level="9.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#chinese-word-segmenter-jiebar"><i class="fa fa-check"></i><b>9.1</b> Chinese Word Segmenter <code>jiebaR</code></a><ul>
<li class="chapter" data-level="9.1.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#start"><i class="fa fa-check"></i><b>9.1.1</b> Start</a></li>
<li class="chapter" data-level="9.1.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#settings"><i class="fa fa-check"></i><b>9.1.2</b> Settings</a></li>
<li class="chapter" data-level="9.1.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#user-defined-dictionary"><i class="fa fa-check"></i><b>9.1.3</b> User-defined dictionary</a></li>
<li class="chapter" data-level="9.1.4" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#stopwords"><i class="fa fa-check"></i><b>9.1.4</b> Stopwords</a></li>
<li class="chapter" data-level="9.1.5" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#pos-tagging"><i class="fa fa-check"></i><b>9.1.5</b> POS Tagging</a></li>
<li class="chapter" data-level="9.1.6" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#default"><i class="fa fa-check"></i><b>9.1.6</b> Default</a></li>
<li class="chapter" data-level="9.1.7" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#reminder"><i class="fa fa-check"></i><b>9.1.7</b> Reminder</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#chinese-text-analytics-pipeline"><i class="fa fa-check"></i><b>9.2</b> Chinese Text Analytics Pipeline</a></li>
<li class="chapter" data-level="9.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-1-word-frequency-and-wordcloud"><i class="fa fa-check"></i><b>9.3</b> Case Study 1: Word Frequency and Wordcloud</a></li>
<li class="chapter" data-level="9.4" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-2-patterns"><i class="fa fa-check"></i><b>9.4</b> Case Study 2: Patterns</a></li>
<li class="chapter" data-level="9.5" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-3-lexical-bundles"><i class="fa fa-check"></i><b>9.5</b> Case Study 3: Lexical Bundles</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ckiptagger.html"><a href="ckiptagger.html"><i class="fa fa-check"></i><b>10</b> CKIP Tagger</a><ul>
<li class="chapter" data-level="10.1" data-path="ckiptagger.html"><a href="ckiptagger.html#installation"><i class="fa fa-check"></i><b>10.1</b> Installation</a></li>
<li class="chapter" data-level="10.2" data-path="ckiptagger.html"><a href="ckiptagger.html#download-the-model-files"><i class="fa fa-check"></i><b>10.2</b> Download the Model Files</a></li>
<li class="chapter" data-level="10.3" data-path="ckiptagger.html"><a href="ckiptagger.html#r-python-communication"><i class="fa fa-check"></i><b>10.3</b> R-Python Communication</a></li>
<li class="chapter" data-level="10.4" data-path="ckiptagger.html"><a href="ckiptagger.html#word-segmentation-in-r"><i class="fa fa-check"></i><b>10.4</b> Word Segmentation in R</a></li>
<li class="chapter" data-level="10.5" data-path="ckiptagger.html"><a href="ckiptagger.html#r-environment-setting"><i class="fa fa-check"></i><b>10.5</b> R Environment Setting</a></li>
<li class="chapter" data-level="10.6" data-path="ckiptagger.html"><a href="ckiptagger.html#loading-python-modules"><i class="fa fa-check"></i><b>10.6</b> Loading Python Modules</a></li>
<li class="chapter" data-level="10.7" data-path="ckiptagger.html"><a href="ckiptagger.html#segmenting-texts"><i class="fa fa-check"></i><b>10.7</b> Segmenting Texts</a></li>
<li class="chapter" data-level="10.8" data-path="ckiptagger.html"><a href="ckiptagger.html#define-own-dictionary"><i class="fa fa-check"></i><b>10.8</b> Define Own Dictionary</a></li>
<li class="chapter" data-level="10.9" data-path="ckiptagger.html"><a href="ckiptagger.html#beyond-word-boundaries"><i class="fa fa-check"></i><b>10.9</b> Beyond Word Boundaries</a></li>
<li class="chapter" data-level="10.10" data-path="ckiptagger.html"><a href="ckiptagger.html#tidy-up-the-results"><i class="fa fa-check"></i><b>10.10</b> Tidy Up the Results</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="structured-corpus.html"><a href="structured-corpus.html"><i class="fa fa-check"></i><b>11</b> Structured Corpus</a><ul>
<li class="chapter" data-level="11.1" data-path="structured-corpus.html"><a href="structured-corpus.html#nccu-spoken-mandarin"><i class="fa fa-check"></i><b>11.1</b> NCCU Spoken Mandarin</a><ul>
<li class="chapter" data-level="11.1.1" data-path="structured-corpus.html"><a href="structured-corpus.html#loading-the-corpus"><i class="fa fa-check"></i><b>11.1.1</b> Loading the Corpus</a></li>
<li class="chapter" data-level="11.1.2" data-path="structured-corpus.html"><a href="structured-corpus.html#line-segmentation"><i class="fa fa-check"></i><b>11.1.2</b> Line Segmentation</a></li>
<li class="chapter" data-level="11.1.3" data-path="structured-corpus.html"><a href="structured-corpus.html#metadata-vs.transcript"><i class="fa fa-check"></i><b>11.1.3</b> Metadata vs. Transcript</a></li>
<li class="chapter" data-level="11.1.4" data-path="structured-corpus.html"><a href="structured-corpus.html#word-tokenization"><i class="fa fa-check"></i><b>11.1.4</b> Word Tokenization</a></li>
<li class="chapter" data-level="11.1.5" data-path="structured-corpus.html"><a href="structured-corpus.html#word-frequencies-and-wordcloud"><i class="fa fa-check"></i><b>11.1.5</b> Word frequencies and Wordcloud</a></li>
<li class="chapter" data-level="11.1.6" data-path="structured-corpus.html"><a href="structured-corpus.html#concordances"><i class="fa fa-check"></i><b>11.1.6</b> Concordances</a></li>
<li class="chapter" data-level="11.1.7" data-path="structured-corpus.html"><a href="structured-corpus.html#n-grams-lexical-bundles"><i class="fa fa-check"></i><b>11.1.7</b> N-grams (Lexical Bundles)</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="structured-corpus.html"><a href="structured-corpus.html#connecting-spid-to-metadata"><i class="fa fa-check"></i><b>11.2</b> Connecting SPID to Metadata</a></li>
<li class="chapter" data-level="11.3" data-path="structured-corpus.html"><a href="structured-corpus.html#more-socialinguistic-analyses"><i class="fa fa-check"></i><b>11.3</b> More Socialinguistic Analyses</a><ul>
<li class="chapter" data-level="11.3.1" data-path="structured-corpus.html"><a href="structured-corpus.html#check-ngram-distribution-by-age-groups"><i class="fa fa-check"></i><b>11.3.1</b> Check Ngram Distribution By Age Groups</a></li>
<li class="chapter" data-level="11.3.2" data-path="structured-corpus.html"><a href="structured-corpus.html#check-word-distribution-of-different-genders"><i class="fa fa-check"></i><b>11.3.2</b> Check Word Distribution of different genders</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="xml.html"><a href="xml.html"><i class="fa fa-check"></i><b>12</b> XML</a><ul>
<li class="chapter" data-level="12.1" data-path="xml.html"><a href="xml.html#bnc-spoken-2014"><i class="fa fa-check"></i><b>12.1</b> BNC Spoken 2014</a></li>
<li class="chapter" data-level="12.2" data-path="xml.html"><a href="xml.html#process-the-whole-directory-of-bnc2014-sample"><i class="fa fa-check"></i><b>12.2</b> Process the Whole Directory of BNC2014 Sample</a><ul>
<li class="chapter" data-level="12.2.1" data-path="xml.html"><a href="xml.html#define-function"><i class="fa fa-check"></i><b>12.2.1</b> Define Function</a></li>
<li class="chapter" data-level="12.2.2" data-path="xml.html"><a href="xml.html#process-the-all-files-in-the-directory"><i class="fa fa-check"></i><b>12.2.2</b> Process the all files in the Directory</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="xml.html"><a href="xml.html#metadata"><i class="fa fa-check"></i><b>12.3</b> Metadata</a><ul>
<li class="chapter" data-level="12.3.1" data-path="xml.html"><a href="xml.html#text-metadata"><i class="fa fa-check"></i><b>12.3.1</b> Text Metadata</a></li>
<li class="chapter" data-level="12.3.2" data-path="xml.html"><a href="xml.html#speaker-metadata"><i class="fa fa-check"></i><b>12.3.2</b> Speaker Metadata</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="xml.html"><a href="xml.html#bnc2014-for-socialinguistic-variation"><i class="fa fa-check"></i><b>12.4</b> BNC2014 for Socialinguistic Variation</a><ul>
<li class="chapter" data-level="12.4.1" data-path="xml.html"><a href="xml.html#word-frequency-vs.gender"><i class="fa fa-check"></i><b>12.4.1</b> Word Frequency vs. Gender</a></li>
<li class="chapter" data-level="12.4.2" data-path="xml.html"><a href="xml.html#degree-adv-adj"><i class="fa fa-check"></i><b>12.4.2</b> Degree ADV + ADJ</a></li>
<li class="chapter" data-level="12.4.3" data-path="xml.html"><a href="xml.html#trigrams"><i class="fa fa-check"></i><b>12.4.3</b> Trigrams</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="vector-space-representation.html"><a href="vector-space-representation.html"><i class="fa fa-check"></i><b>13</b> Vector Space Representation</a><ul>
<li class="chapter" data-level="13.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#data-processing-flowchart"><i class="fa fa-check"></i><b>13.1</b> Data Processing Flowchart</a></li>
<li class="chapter" data-level="13.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-feature-matrix-dfm"><i class="fa fa-check"></i><b>13.2</b> Document-Feature Matrix (<code>dfm</code>)</a></li>
<li class="chapter" data-level="13.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#defining-feature-in-dfm"><i class="fa fa-check"></i><b>13.3</b> Defining <code>Feature</code> in <code>dfm</code></a></li>
<li class="chapter" data-level="13.4" data-path="vector-space-representation.html"><a href="vector-space-representation.html#feature-selection"><i class="fa fa-check"></i><b>13.4</b> Feature Selection</a><ul>
<li class="chapter" data-level="13.4.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#determining-linguistic-granularity"><i class="fa fa-check"></i><b>13.4.1</b> Determining Linguistic Granularity</a></li>
<li class="chapter" data-level="13.4.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#stopwords-1"><i class="fa fa-check"></i><b>13.4.2</b> Stopwords</a></li>
<li class="chapter" data-level="13.4.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#distributional-cut-offs-for-features"><i class="fa fa-check"></i><b>13.4.3</b> Distributional Cut-offs for Features</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="vector-space-representation.html"><a href="vector-space-representation.html#applying-dfm"><i class="fa fa-check"></i><b>13.5</b> Applying DFM</a><ul>
<li class="chapter" data-level="13.5.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#wordcloud"><i class="fa fa-check"></i><b>13.5.1</b> Wordcloud</a></li>
<li class="chapter" data-level="13.5.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-similarity"><i class="fa fa-check"></i><b>13.5.2</b> Document Similarity</a></li>
<li class="chapter" data-level="13.5.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#feature-similarity"><i class="fa fa-check"></i><b>13.5.3</b> Feature Similarity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html"><i class="fa fa-check"></i><b>14</b> Vector Space Representation II</a><ul>
<li class="chapter" data-level="14.1" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#a-quick-view"><i class="fa fa-check"></i><b>14.1</b> A Quick View</a></li>
<li class="chapter" data-level="14.2" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#loading-the-corpus-1"><i class="fa fa-check"></i><b>14.2</b> Loading the Corpus</a></li>
<li class="chapter" data-level="14.3" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#semgentation"><i class="fa fa-check"></i><b>14.3</b> Semgentation</a></li>
<li class="chapter" data-level="14.4" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#corpus-metadata"><i class="fa fa-check"></i><b>14.4</b> Corpus Metadata</a></li>
<li class="chapter" data-level="14.5" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#document-feature-matrix"><i class="fa fa-check"></i><b>14.5</b> Document-Feature Matrix</a></li>
<li class="chapter" data-level="14.6" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#wordcloud-1"><i class="fa fa-check"></i><b>14.6</b> Wordcloud</a></li>
<li class="chapter" data-level="14.7" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#document-similarity-1"><i class="fa fa-check"></i><b>14.7</b> Document Similarity</a></li>
<li class="chapter" data-level="14.8" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#feature-similarity-1"><i class="fa fa-check"></i><b>14.8</b> Feature Similarity</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html"><i class="fa fa-check"></i><b>15</b> Vector Space Representation III</a><ul>
<li class="chapter" data-level="15.1" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#library"><i class="fa fa-check"></i><b>15.1</b> Library</a></li>
<li class="chapter" data-level="15.2" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#text-collection"><i class="fa fa-check"></i><b>15.2</b> Text Collection</a></li>
<li class="chapter" data-level="15.3" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#tokenization-and-vocabulary"><i class="fa fa-check"></i><b>15.3</b> Tokenization and Vocabulary</a></li>
<li class="chapter" data-level="15.4" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#pruning"><i class="fa fa-check"></i><b>15.4</b> Pruning</a></li>
<li class="chapter" data-level="15.5" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#term-cooccurrence-matrix"><i class="fa fa-check"></i><b>15.5</b> Term-Cooccurrence Matrix</a></li>
<li class="chapter" data-level="15.6" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#fitting-model"><i class="fa fa-check"></i><b>15.6</b> Fitting Model</a></li>
<li class="chapter" data-level="15.7" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#averaging-word-vectors"><i class="fa fa-check"></i><b>15.7</b> Averaging Word Vectors</a></li>
<li class="chapter" data-level="15.8" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#semantic-space"><i class="fa fa-check"></i><b>15.8</b> Semantic Space</a></li>
<li class="chapter" data-level="15.9" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#visualizing-multi-dimensional-space"><i class="fa fa-check"></i><b>15.9</b> Visualizing Multi-dimensional Space</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>16</b> References</a></li>
<li class="divider"></li>
<li><a href="https://web.ntnu.edu.tw/~alvinchen" target ="blank">Alvin Chen </a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Corpus Linguistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="corpus-analysis-a-start" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Corpus Analysis: A Start</h1>
<p>In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data.</p>
<div id="installing-quanteda" class="section level2">
<h2><span class="header-section-number">4.1</span> Installing <code>quanteda</code></h2>
<p>To start with, this tutorial will use a powerful package, <code>quanteda</code>, for managing and analyzing textual data in R. You may refer to the <a href="https://quanteda.io/">official documentation of the package</a> for more detail.</p>
<p><code>quanteda</code> is not included in the default R installation. Please install the package if you haven’t done so.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;quanteda&quot;</span>)</a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="kw">install.packages</span>(<span class="st">&quot;readtext&quot;</span>)</a></code></pre></div>
<p>Also, as noted on the <code>quanteda</code> documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers.</p>
<ul>
<li>If you are using a Windows platform, this means you will need also to install the <a href="https://cran.r-project.org/bin/windows/Rtools/">Rtools</a> software available from CRAN.</li>
<li>If you are using macOS, you should install the <a href="https://cran.r-project.org/bin/macosx/tools/">macOS tools</a>.</li>
</ul>
<p>If you run into any installation errors, please go to the official documentation page for additional assistance.</p>
</div>
<div id="building-a-corpus-from-character-vector" class="section level2">
<h2><span class="header-section-number">4.2</span> Building a <code>corpus</code> from character vector</h2>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="kw">library</span>(quanteda)</a>
<a class="sourceLine" id="cb42-2" data-line-number="2"><span class="kw">library</span>(readtext)</a>
<a class="sourceLine" id="cb42-3" data-line-number="3"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb42-4" data-line-number="4"><span class="kw">library</span>(dplyr)</a></code></pre></div>
<p>To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the <code>quanteda</code> package, <code>data_corpus_inaugural</code>. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">data_corpus_inaugural</a></code></pre></div>
<pre><code>## Corpus consisting of 58 documents and 4 docvars.
## 1789-Washington :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;
## 
## 1793-Washington :
## &quot;Fellow citizens, I am again called upon by the voice of my c...&quot;
## 
## 1797-Adams :
## &quot;When it was first perceived, in early times, that no middle ...&quot;
## 
## 1801-Jefferson :
## &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot;
## 
## 1805-Jefferson :
## &quot;Proceeding, fellow citizens, to that qualification which the...&quot;
## 
## 1809-Madison :
## &quot;Unwilling to depart from examples of the most revered author...&quot;
## 
## [ reached max_ndoc ... 52 more documents ]</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="kw">class</span>(data_corpus_inaugural)</a></code></pre></div>
<pre><code>## [1] &quot;corpus&quot;</code></pre>
<p>We create a <code>corpus()</code> object with the pre-loaded <code>corpus</code> in <code>quanteda</code>– <code>data_corpus_inaugural</code>:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1">corp_us &lt;-<span class="st"> </span><span class="kw">corpus</span>(data_corpus_inaugural) <span class="co"># save the `corpus` to a short obj name</span></a>
<a class="sourceLine" id="cb47-2" data-line-number="2"><span class="kw">summary</span>(corp_us)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Text"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Types"],"name":[2],"type":["int"],"align":["right"]},{"label":["Tokens"],"name":[3],"type":["int"],"align":["right"]},{"label":["Sentences"],"name":[4],"type":["int"],"align":["right"]},{"label":["Year"],"name":[5],"type":["int"],"align":["right"]},{"label":["President"],"name":[6],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[7],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[8],"type":["fctr"],"align":["left"]}],"data":[{"1":"1789-Washington","2":"625","3":"1537","4":"23","5":"1789","6":"Washington","7":"George","8":"none","_rn_":"1"},{"1":"1793-Washington","2":"96","3":"147","4":"4","5":"1793","6":"Washington","7":"George","8":"none","_rn_":"2"},{"1":"1797-Adams","2":"826","3":"2577","4":"37","5":"1797","6":"Adams","7":"John","8":"Federalist","_rn_":"3"},{"1":"1801-Jefferson","2":"717","3":"1923","4":"41","5":"1801","6":"Jefferson","7":"Thomas","8":"Democratic-Republican","_rn_":"4"},{"1":"1805-Jefferson","2":"804","3":"2380","4":"45","5":"1805","6":"Jefferson","7":"Thomas","8":"Democratic-Republican","_rn_":"5"},{"1":"1809-Madison","2":"535","3":"1261","4":"21","5":"1809","6":"Madison","7":"James","8":"Democratic-Republican","_rn_":"6"},{"1":"1813-Madison","2":"541","3":"1302","4":"33","5":"1813","6":"Madison","7":"James","8":"Democratic-Republican","_rn_":"7"},{"1":"1817-Monroe","2":"1040","3":"3677","4":"121","5":"1817","6":"Monroe","7":"James","8":"Democratic-Republican","_rn_":"8"},{"1":"1821-Monroe","2":"1259","3":"4886","4":"129","5":"1821","6":"Monroe","7":"James","8":"Democratic-Republican","_rn_":"9"},{"1":"1825-Adams","2":"1003","3":"3147","4":"74","5":"1825","6":"Adams","7":"John Quincy","8":"Democratic-Republican","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After the <code>corpus</code> is loaded, we can use <code>summary()</code> to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="kw">require</span>(ggplot2)</a>
<a class="sourceLine" id="cb48-2" data-line-number="2"></a>
<a class="sourceLine" id="cb48-3" data-line-number="3">corp_us <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb48-4" data-line-number="4"><span class="st">  </span>summary <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb48-5" data-line-number="5"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Year, <span class="dt">y =</span> Tokens, <span class="dt">group =</span> <span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb48-6" data-line-number="6"><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb48-7" data-line-number="7"><span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb48-8" data-line-number="8"><span class="st">    </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-44" class="exercise"><strong>Exercise 4.1  </strong></span>Could you reproduce the above line plot and add information of <code>President</code> to the plot as labels of the dots?</p>
Hints: Please check <code>ggplot2::geom_text()</code> or more advanced one, <code>ggrepel::geom_text_repel()</code>
</div>

<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<hr />
</div>
<div id="keyword-in-context-kwic" class="section level2">
<h2><span class="header-section-number">4.3</span> Keyword-in-Context (KWIC)</h2>
<p>Keyword-in-Context (KWIC), or <em>concordances</em>, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context.</p>
<p>We can use <code>kwic()</code> to perform a search for a word and retrieve its concordances from the corpus:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="kw">kwic</span>(corp_us, <span class="st">&quot;terror&quot;</span>)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["docname"],"name":[1],"type":["chr"],"align":["left"]},{"label":["from"],"name":[2],"type":["int"],"align":["right"]},{"label":["to"],"name":[3],"type":["int"],"align":["right"]},{"label":["pre"],"name":[4],"type":["chr"],"align":["left"]},{"label":["keyword"],"name":[5],"type":["chr"],"align":["left"]},{"label":["post"],"name":[6],"type":["chr"],"align":["left"]},{"label":["pattern"],"name":[7],"type":["fctr"],"align":["left"]}],"data":[{"1":"1797-Adams","2":"1324","3":"1324","4":"fraud or violence , by","5":"terror","6":", intrigue , or venality","7":"terror","_rn_":"1"},{"1":"1933-Roosevelt","2":"111","3":"111","4":"nameless , unreasoning , unjustified","5":"terror","6":"which paralyzes needed efforts to","7":"terror","_rn_":"2"},{"1":"1941-Roosevelt","2":"285","3":"285","4":"seemed frozen by a fatalistic","5":"terror","6":", we proved that this","7":"terror","_rn_":"3"},{"1":"1961-Kennedy","2":"848","3":"848","4":"alter that uncertain balance of","5":"terror","6":"that stays the hand of","7":"terror","_rn_":"4"},{"1":"1981-Reagan","2":"811","3":"811","4":"freeing all Americans from the","5":"terror","6":"of runaway living costs .","7":"terror","_rn_":"5"},{"1":"1997-Clinton","2":"1047","3":"1047","4":"They fuel the fanaticism of","5":"terror","6":". And they torment the","7":"terror","_rn_":"6"},{"1":"1997-Clinton","2":"1647","3":"1647","4":"maintain a strong defense against","5":"terror","6":"and destruction . Our children","7":"terror","_rn_":"7"},{"1":"2009-Obama","2":"1619","3":"1619","4":"advance their aims by inducing","5":"terror","6":"and slaughtering innocents , we","7":"terror","_rn_":"8"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p><code>kwic()</code> returns a data frame, which can be easily output to a CSV file for later use.</p>
<div class="danger">
<p>
Please note that <code>kwic()</code>, when taking a <code>corpus</code> object as the argument, will automatically <em>tokenize</em>the corpus data and do the keyword-in-context search on a <em>word</em> basis. In other words, the pattern you look for cannot be a linguistic pattern across several words. We will talk about how to extract constructions later.
</p>
</div>
</div>
<div id="kwic-with-regular-expressions" class="section level2">
<h2><span class="header-section-number">4.4</span> KWIC with Regular Expressions</h2>
<p>For more complex searches, we can use regular expressions as well in <code>kwic()</code>. For example, if you want to include <code>terror</code> and all its other related word forms, such as <code>terrorist</code>, <code>terrorism</code>, <code>terrors</code>, you can do a regular expression search.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw">kwic</span>(corp_us, <span class="st">&quot;terror.*&quot;</span>, <span class="dt">valuetype =</span> <span class="st">&quot;regex&quot;</span>)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["docname"],"name":[1],"type":["chr"],"align":["left"]},{"label":["from"],"name":[2],"type":["int"],"align":["right"]},{"label":["to"],"name":[3],"type":["int"],"align":["right"]},{"label":["pre"],"name":[4],"type":["chr"],"align":["left"]},{"label":["keyword"],"name":[5],"type":["chr"],"align":["left"]},{"label":["post"],"name":[6],"type":["chr"],"align":["left"]},{"label":["pattern"],"name":[7],"type":["fctr"],"align":["left"]}],"data":[{"1":"1797-Adams","2":"1324","3":"1324","4":"fraud or violence , by","5":"terror","6":", intrigue , or venality","7":"terror.*","_rn_":"1"},{"1":"1933-Roosevelt","2":"111","3":"111","4":"nameless , unreasoning , unjustified","5":"terror","6":"which paralyzes needed efforts to","7":"terror.*","_rn_":"2"},{"1":"1941-Roosevelt","2":"285","3":"285","4":"seemed frozen by a fatalistic","5":"terror","6":", we proved that this","7":"terror.*","_rn_":"3"},{"1":"1961-Kennedy","2":"848","3":"848","4":"alter that uncertain balance of","5":"terror","6":"that stays the hand of","7":"terror.*","_rn_":"4"},{"1":"1961-Kennedy","2":"970","3":"970","4":"of science instead of its","5":"terrors","6":". Together let us explore","7":"terror.*","_rn_":"5"},{"1":"1981-Reagan","2":"811","3":"811","4":"freeing all Americans from the","5":"terror","6":"of runaway living costs .","7":"terror.*","_rn_":"6"},{"1":"1981-Reagan","2":"2186","3":"2186","4":"understood by those who practice","5":"terrorism","6":"and prey upon their neighbors","7":"terror.*","_rn_":"7"},{"1":"1997-Clinton","2":"1047","3":"1047","4":"They fuel the fanaticism of","5":"terror","6":". And they torment the","7":"terror.*","_rn_":"8"},{"1":"1997-Clinton","2":"1647","3":"1647","4":"maintain a strong defense against","5":"terror","6":"and destruction . Our children","7":"terror.*","_rn_":"9"},{"1":"2009-Obama","2":"1619","3":"1619","4":"advance their aims by inducing","5":"terror","6":"and slaughtering innocents , we","7":"terror.*","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>By default, the <code>kwic()</code> is word-based. If you like to look up a multiword combination, use <code>phrase()</code>:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="kw">kwic</span>(corp_us, <span class="kw">phrase</span>(<span class="st">&quot;our country&quot;</span>))</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["docname"],"name":[1],"type":["chr"],"align":["left"]},{"label":["from"],"name":[2],"type":["int"],"align":["right"]},{"label":["to"],"name":[3],"type":["int"],"align":["right"]},{"label":["pre"],"name":[4],"type":["chr"],"align":["left"]},{"label":["keyword"],"name":[5],"type":["chr"],"align":["left"]},{"label":["post"],"name":[6],"type":["chr"],"align":["left"]},{"label":["pattern"],"name":[7],"type":["fctr"],"align":["left"]}],"data":[{"1":"1801-Jefferson","2":"18","3":"19","4":"the first executive office of","5":"our country","6":", I avail myself of","7":"our country","_rn_":"1"},{"1":"1805-Jefferson","2":"1898","3":"1899","4":"course , I offer to","5":"our country","6":"sincere congratulations . With those","7":"our country","_rn_":"2"},{"1":"1809-Madison","2":"322","3":"323","4":"from this prosperous condition of","5":"our country","6":"to the scene which has","7":"our country","_rn_":"3"},{"1":"1813-Madison","2":"1033","3":"1034","4":"and an intelligent people .","5":"Our country","6":"abounds in the necessaries ,","7":"our country","_rn_":"4"},{"1":"1813-Madison","2":"1226","3":"1227","4":"arms now may long preserve","5":"our country","6":"from the necessity of another","7":"our country","_rn_":"5"},{"1":"1817-Monroe","2":"1217","3":"1218","4":"the highly favored condition of","5":"our country","6":", it is the interest","7":"our country","_rn_":"6"},{"1":"1817-Monroe","2":"2328","3":"2329","4":"among which the improvement of","5":"our country","6":"by roads and canals ,","7":"our country","_rn_":"7"},{"1":"1821-Monroe","2":"265","3":"266","4":", prosperity and happiness of","5":"our country","6":"will always be the object","7":"our country","_rn_":"8"},{"1":"1821-Monroe","2":"1203","3":"1204","4":"citizens from that destruction and","5":"our country","6":"from that devastation which are","7":"our country","_rn_":"9"},{"1":"1821-Monroe","2":"4028","3":"4029","4":"to the internal concerns of","5":"our country","6":", and more especially to","7":"our country","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>It should be noted that the output of <code>kwic</code> includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-50" class="exercise"><strong>Exercise 4.2  </strong></span>Please create a bar plot, showing the number of uses of the word <em>country</em> in each president’s address. Please include different variants of the word, e.g., <em>countries</em>, <em>Countries</em>, <em>Country</em>, in your <code>kwic()</code> search.
</div>

<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<hr />
</div>
<div id="tidy-text-format-of-the-corpus" class="section level2">
<h2><span class="header-section-number">4.5</span> Tidy Text Format of the Corpus</h2>
<p>So far our corpus is a <code>corpus</code> object defined in <code>quanteda</code>. In most of the R standard packages, people normally follow the <strong>using tidy data principles</strong> to make handling data easier and more effective. As described by Hadley Wickham <span class="citation">(Wickham and Grolemund <a href="#ref-hadley2017">2017</a>)</span>, tidy data has a specific structure:</p>
<ul>
<li>Each variable is a column</li>
<li>Each observation is a row</li>
<li>Each type of observational unit is a table</li>
</ul>
<p>With text data like a <code>corpus</code>, we can also define the <strong>tidy text format</strong> as being a <code>data.frame</code> with one-token-per-row. A token is a meaningful unit of text, such as a word that we are interested in using for analysis, and tokenization is the process of splitting text into tokens.</p>
<p>In computational text analytics, the token (i.e., each row in the data frame) is most often a single <em>word</em>, but can also be an <em>n-gram</em>, <em>sentence</em>, or <em>paragraph</em>. The <code>tidytext</code> package in R is made for the handling of the tidy text format of the corpus data.</p>
<p>Tidy datasets allow manipulation with a standard set of <em>tidy</em> tools, including popular packages such as <code>dplyr</code>, <code>tidyr</code>, and <code>ggplot2</code>.</p>
<p>The <code>tidytext</code> package includes functions to <code>tidy()</code> objects from <code>quanteda</code>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb52-2" data-line-number="2">corp_us_tidy &lt;-<span class="st"> </span><span class="kw">tidy</span>(corp_us) <span class="co"># convert `corpus` to `data.frame`</span></a>
<a class="sourceLine" id="cb52-3" data-line-number="3"><span class="kw">class</span>(corp_us_tidy)</a></code></pre></div>
<pre><code>## [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["text"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Year"],"name":[2],"type":["int"],"align":["right"]},{"label":["President"],"name":[3],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[4],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[5],"type":["fctr"],"align":["left"]}],"data":[{"1":"Fellow-Citizens of the Senate and of the House of ...","2":"1789","3":"Washington","4":"George","5":"none"},{"1":"Fellow citizens, I am again called upon by the voi...","2":"1793","3":"Washington","4":"George","5":"none"},{"1":"When it was first perceived, in early times, that ...","2":"1797","3":"Adams","4":"John","5":"Federalist"},{"1":"Friends and Fellow Citizens:\\n\\nCalled upon to under...","2":"1801","3":"Jefferson","4":"Thomas","5":"Democratic-Republican"},{"1":"Proceeding, fellow citizens, to that qualification...","2":"1805","3":"Jefferson","4":"Thomas","5":"Democratic-Republican"},{"1":"Unwilling to depart from examples of the most reve...","2":"1809","3":"Madison","4":"James","5":"Democratic-Republican"},{"1":"About to add the solemnity of an oath to the oblig...","2":"1813","3":"Madison","4":"James","5":"Democratic-Republican"},{"1":"I should be destitute of feeling if I was not deep...","2":"1817","3":"Monroe","4":"James","5":"Democratic-Republican"},{"1":"Fellow citizens, I shall not attempt to describe t...","2":"1821","3":"Monroe","4":"James","5":"Democratic-Republican"},{"1":"In compliance with an usage coeval with the existe...","2":"1825","3":"Adams","4":"John Quincy","5":"Democratic-Republican"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="frequency-lists" class="section level2">
<h2><span class="header-section-number">4.6</span> Frequency Lists</h2>
<p>To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages.</p>
<p>The <code>tidytext</code> provides a powerful function, <code>unnest_tokens()</code> to tokenize a data frame with larger linguistic units (e.g., <em>texts</em>) into one with smaller units (e.g., <em>words</em>).</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" data-line-number="1">corp_us_words &lt;-<span class="st"> </span>corp_us_tidy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb54-2" data-line-number="2"><span class="st">  </span><span class="kw">unnest_tokens</span>(<span class="dt">output =</span> word, <span class="dt">input =</span> text, <span class="dt">token =</span> <span class="st">&quot;words&quot;</span>) <span class="co"># tokenize the `text` column into `word`</span></a>
<a class="sourceLine" id="cb54-3" data-line-number="3"></a>
<a class="sourceLine" id="cb54-4" data-line-number="4">corp_us_words</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["word"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"fellow"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"citizens"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"senate"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"and"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"house"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="info">
<p>
The <code>unnest_tokens()</code> is optimized for English tokenization of other linguistic units, such as <em>words</em>, <em>ngrams</em>, <em>sentences</em>, <em>lines</em>, and <em>paragraphs</em> (check <code>?unnest_tokens()</code>). To handle Chinese data, however, we need to define own ways of tokenization <code>unnest_tokens(…, token = …)</code>. We will discuss the principles for Chinese text processing in a later <a href="chinese-text-processing.html#chinese-text-processing">chapter</a>.
</p>
</div>
<p>Now we can count the word frequencies:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1">corp_us_words_freq &lt;-<span class="st"> </span>corp_us_words <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb55-2" data-line-number="2"><span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb55-3" data-line-number="3"></a>
<a class="sourceLine" id="cb55-4" data-line-number="4">corp_us_words_freq</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["word"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"the","2":"10082"},{"1":"of","2":"7103"},{"1":"and","2":"5310"},{"1":"to","2":"4534"},{"1":"in","2":"2785"},{"1":"a","2":"2246"},{"1":"our","2":"2181"},{"1":"that","2":"1789"},{"1":"we","2":"1739"},{"1":"be","2":"1482"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Frequency lists can be generated for bigrams or any other multiword combinations as well:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1">corp_us_bigrams &lt;-<span class="st"> </span>corp_us_tidy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb56-2" data-line-number="2"><span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb56-3" data-line-number="3"></a>
<a class="sourceLine" id="cb56-4" data-line-number="4">corp_us_bigrams</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["bigram"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"fellow citizens"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"citizens of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the senate"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"senate and"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"and of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the house"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"house of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of representatives"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>To create bigram frequency list:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">corp_us_bigrams_freq &lt;-<span class="st"> </span>corp_us_bigrams <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb57-2" data-line-number="2"><span class="st">  </span><span class="kw">count</span>(bigram, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb57-3" data-line-number="3">corp_us_bigrams_freq</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["bigram"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"of the","2":"1766"},{"1":"in the","2":"812"},{"1":"to the","2":"720"},{"1":"of our","2":"619"},{"1":"and the","2":"471"},{"1":"it is","2":"323"},{"1":"by the","2":"318"},{"1":"for the","2":"312"},{"1":"to be","2":"311"},{"1":"the people","2":"265"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1"><span class="kw">sum</span>(corp_us_words_freq<span class="op">$</span>n) <span class="co"># size of unigrams</span></a></code></pre></div>
<pre><code>## [1] 135562</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1"><span class="kw">sum</span>(corp_us_bigrams_freq<span class="op">$</span>n) <span class="co"># size of bigrams</span></a></code></pre></div>
<pre><code>## [1] 135504</code></pre>
</div>
<div id="word-cloud" class="section level2">
<h2><span class="header-section-number">4.7</span> Word Cloud</h2>
<p>With frequency data, we can visualize important words in the corpus with a <strong>Word Cloud</strong>. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1"><span class="kw">library</span>(wordcloud)</a>
<a class="sourceLine" id="cb62-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb62-3" data-line-number="3"><span class="kw">with</span>(corp_us_words_freq, <span class="kw">wordcloud</span>(word, n, </a>
<a class="sourceLine" id="cb62-4" data-line-number="4">                                   <span class="dt">max.words =</span> <span class="dv">400</span>,</a>
<a class="sourceLine" id="cb62-5" data-line-number="5">                                   <span class="dt">min.freq =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb62-6" data-line-number="6">                                   <span class="dt">scale =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="fl">0.5</span>),</a>
<a class="sourceLine" id="cb62-7" data-line-number="7">                                   <span class="dt">color =</span> <span class="kw">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&quot;Dark2&quot;</span>),</a>
<a class="sourceLine" id="cb62-8" data-line-number="8">                                   <span class="dt">vfont=</span><span class="kw">c</span>(<span class="st">&quot;serif&quot;</span>,<span class="st">&quot;plain&quot;</span>)))</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<hr />

<div class="exercise">
<span id="exr:wordcloud1" class="exercise"><strong>Exercise 4.3  </strong></span>Word cloud would be more informative if we first remove functional words. In <code>tidytext</code>, there is a preloaded data frame, <code>stop_words</code>, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. (Criteria: Frequency &gt;= 10; Max Number of Words Plotted = 400)
</div>

<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1"><span class="kw">require</span>(tidytext)</a>
<a class="sourceLine" id="cb63-2" data-line-number="2">stop_words</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["word"],"name":[1],"type":["chr"],"align":["left"]},{"label":["lexicon"],"name":[2],"type":["chr"],"align":["left"]}],"data":[{"1":"a","2":"SMART"},{"1":"a's","2":"SMART"},{"1":"able","2":"SMART"},{"1":"about","2":"SMART"},{"1":"above","2":"SMART"},{"1":"according","2":"SMART"},{"1":"accordingly","2":"SMART"},{"1":"across","2":"SMART"},{"1":"actually","2":"SMART"},{"1":"after","2":"SMART"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-61-1.png" width="100%" /></p>

<div class="exercise">
<span id="exr:unnamed-chunk-62" class="exercise"><strong>Exercise 4.4  </strong></span>Get yourself familiar with another R package for creating word clouds, <code>wordcloud2</code>, and re-create a word cloud as Exercise <a href="corpus-analysis-a-start.html#exr:wordcloud1">4.3</a> but in a fancier format, i.e., a star-shaped one. (Criteria: Frequency &gt;= 15)
</div>

<p><img src="wc2.png" width="100%" /></p>
</div>
<div id="collocations" class="section level2">
<h2><span class="header-section-number">4.8</span> Collocations</h2>
<p>With unigram and bigram frequencies of the corpus, we can further examine the collocations within the corpus. Collocation refers to a frequent phenomenon where two words tend to co-occur very often in use. This co-occurrence is defined statistically by their <em>lexical associations</em>.</p>
<div id="cooccurrence-table-and-observed-frequencies" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Cooccurrence Table and Observed Frequencies</h3>
<p>Cooccurrence frequency data for a word pair, <em>w<sub>1</sub></em> and <em>w<sub>2</sub></em>, are often organized in a contingency table extracted from a corpus, as shown in Figure <a href="corpus-analysis-a-start.html#fig:collo1">4.1</a>. The cell counts of this contingency table are called the observed frequencies <em>O<sub>11</sub></em>, <em>O<sub>12</sub></em>, <em>O<sub>21</sub></em>, and <em>O<sub>22</sub></em>.</p>
<div class="figure"><span id="fig:collo1"></span>
<img src="images/collocation1.png" alt="Cooccurrence Freqeucny Table" width="80%" />
<p class="caption">
Figure 4.1: Cooccurrence Freqeucny Table
</p>
</div>
<p>The sum of all four observed frequencies (called the sample size <em>N</em>) is equal to the total number of bigrams extracted from the corpus. <em>R<sub>1</sub></em> and <em>R<sub>2</sub></em> are the row totals of the observed contingency table, while <em>C<sub>1</sub></em> and <em>C<sub>2</sub></em> are the corresponding column totals. The row and column totals are also called <strong>marginal frequencies</strong>, being written in the margins of the table, and <em>O<sub>11</sub></em> is called the <strong>joint frequency</strong>.</p>
</div>
<div id="expected-frequencies" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Expected Frequencies</h3>
<p>Equations for all association measures are given in terms of the observed frequencies, marginal frequencies, and the expected frequencies <em>E<sub>11</sub></em>, …, <em>E<sub>22</sub></em> (under the null hypothesis that <em>W<sub>1</sub></em> and <em>W<sub>2</sub></em> are statistically independent). The expected frequencies can easily be computed from the marginal frequencies as shown in Figure <a href="corpus-analysis-a-start.html#fig:collo2">4.2</a>.</p>
<div class="figure"><span id="fig:collo2"></span>
<img src="images/collocation2.png" alt="Computing Expected Frequencies" width="95%" />
<p class="caption">
Figure 4.2: Computing Expected Frequencies
</p>
</div>
</div>
<div id="association-measures" class="section level3">
<h3><span class="header-section-number">4.8.3</span> Association Measures</h3>
<p>The idea of lexical assoication is to measure how much the observed frequencies deviate from the expected. Some of the metrics (e.g., <em>t-statistic</em>, <em>MI</em>) consider only the joint frequency deviation (i.e., <em>O<sub>11</sub></em>), while others (e.g., <em>G<sup>2</sup></em>, a.k.a Log Likelihood Ratio)consider the deviations of ALL cells.</p>
<p>Here I would like to show you how we can compute the most common two asssociation metrics for all the bigrams found in the corpus–<strong>t-test</strong> statistic and <strong>Mutual Information</strong> (MI).</p>
<ul>
<li><span class="math inline">\(t = \frac{O_{11}-E_{11}}{\sqrt{E_{11}}}\)</span></li>
<li><span class="math inline">\(MI = log_2\frac{O_{11}}{E_{11}}\)</span></li>
<li><em>Log-Likelohood Ratio (LLR)</em> <span class="math inline">\(= 2 \sum_{ij}{O_{ij}log\frac{O_{ij}}{E_{ij}}}\)</span></li>
</ul>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">corp_us_collocations &lt;-<span class="st"> </span>corp_us_bigrams_freq <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb64-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># set bigram frequency cut-off</span></a>
<a class="sourceLine" id="cb64-3" data-line-number="3"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">O11 =</span> n) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb64-4" data-line-number="4"><span class="st">  </span>tidyr<span class="op">::</span><span class="kw">separate</span>(bigram, <span class="kw">c</span>(<span class="st">&quot;w1&quot;</span>, <span class="st">&quot;w2&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># split bigrams into two columns</span></a>
<a class="sourceLine" id="cb64-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">R1 =</span> corp_us_words_freq<span class="op">$</span>n[<span class="kw">match</span>(w1, corp_us_words_freq<span class="op">$</span>word)],</a>
<a class="sourceLine" id="cb64-6" data-line-number="6">         <span class="dt">C1 =</span> corp_us_words_freq<span class="op">$</span>n[<span class="kw">match</span>(w2, corp_us_words_freq<span class="op">$</span>word)]) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># retrieve w1 w2 unigram freq</span></a>
<a class="sourceLine" id="cb64-7" data-line-number="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">E11 =</span> (R1<span class="op">*</span>C1)<span class="op">/</span><span class="kw">sum</span>(O11)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># compute expected freq of bigrams</span></a>
<a class="sourceLine" id="cb64-8" data-line-number="8"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">MI =</span> <span class="kw">log2</span>(O11<span class="op">/</span>E11),</a>
<a class="sourceLine" id="cb64-9" data-line-number="9">         <span class="dt">t =</span> (O11 <span class="op">-</span><span class="st"> </span>E11)<span class="op">/</span><span class="kw">sqrt</span>(E11)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># compute associations</span></a>
<a class="sourceLine" id="cb64-10" data-line-number="10"><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(MI)) <span class="co"># sorting</span></a>
<a class="sourceLine" id="cb64-11" data-line-number="11"></a>
<a class="sourceLine" id="cb64-12" data-line-number="12">corp_us_collocations</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["w1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["O11"],"name":[3],"type":["int"],"align":["right"]},{"label":["R1"],"name":[4],"type":["int"],"align":["right"]},{"label":["C1"],"name":[5],"type":["int"],"align":["right"]},{"label":["E11"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["MI"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"indian","2":"tribes","3":"6","4":"8","5":"6","6":"0.0009108505","7":"12.685461","8":"198.77500"},{"1":"twenty","2":"five","3":"6","4":"16","5":"10","6":"0.0030361684","7":"10.948495","8":"108.83498"},{"1":"chief","2":"magistrate","3":"10","4":"36","5":"11","6":"0.0075145167","7":"10.378032","8":"115.27178"},{"1":"president","2":"bush","3":"6","4":"89","5":"6","6":"0.0101332119","7":"9.209727","8":"59.50365"},{"1":"move","2":"forward","3":"6","4":"15","5":"36","6":"0.0102470682","7":"9.193607","8":"59.17103"},{"1":"vice","2":"president","3":"16","4":"18","5":"89","6":"0.0303996357","7":"9.039802","8":"91.59249"},{"1":"two","2":"centuries","3":"7","4":"47","5":"15","6":"0.0133781168","7":"9.031336","8":"60.40456"},{"1":"interstate","2":"commerce","3":"6","4":"11","5":"64","6":"0.0133591408","7":"8.810991","8":"51.79573"},{"1":"domestic","2":"concerns","3":"7","4":"49","5":"17","6":"0.0158070515","7":"8.790643","8":"55.55086"},{"1":"god","2":"bless","3":"15","4":"92","5":"20","6":"0.0349159361","7":"8.746861","8":"80.08798"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="warning">
<p>
Please note that in the above example, we compute the lexical associations for bigrams whose frequency &gt; 5. This is necessary in collocation studies because bigrams of very low frequency would not be informative even though its association can be very strong.
</p>
</div>
<div class="info">
<p>
How to compute lexical assoications is a non-trivial issue. There are many more ways to compute the association strengths between two words. Please refer to <a href="http://www.collocations.de/index.html">Stefan Evert’s site</a> for a very comprehensive review of lexical assoication meaasures.
</p>
</div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-68" class="exercise"><strong>Exercise 4.5  </strong></span>Sort the collocation data frame <code>corp_us_collocations</code> according to the <code>t-score</code> and compare the results sorted by MI scores.
</div>

<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["w1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["O11"],"name":[3],"type":["int"],"align":["right"]},{"label":["R1"],"name":[4],"type":["int"],"align":["right"]},{"label":["C1"],"name":[5],"type":["int"],"align":["right"]},{"label":["E11"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["MI"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"indian","2":"tribes","3":"6","4":"8","5":"6","6":"0.0009108505","7":"12.685461","8":"198.77500"},{"1":"united","2":"states","3":"157","4":"202","5":"333","6":"1.2764431288","7":"6.942491","8":"137.83312"},{"1":"fellow","2":"citizens","3":"117","4":"152","5":"247","6":"0.7124369046","7":"7.359531","8":"137.77172"},{"1":"chief","2":"magistrate","3":"10","4":"36","5":"11","6":"0.0075145167","7":"10.378032","8":"115.27178"},{"1":"twenty","2":"five","3":"6","4":"16","5":"10","6":"0.0030361684","7":"10.948495","8":"108.83498"},{"1":"four","2":"years","3":"29","4":"36","5":"141","6":"0.0963224411","7":"8.233965","8":"93.12995"},{"1":"vice","2":"president","3":"16","4":"18","5":"89","6":"0.0303996357","7":"9.039802","8":"91.59249"},{"1":"let","2":"us","3":"97","4":"154","5":"478","6":"1.3968651562","7":"6.117720","8":"80.89001"},{"1":"god","2":"bless","3":"15","4":"92","5":"20","6":"0.0349159361","7":"8.746861","8":"80.08798"},{"1":"those","2":"who","3":"123","4":"328","5":"370","6":"2.3029336977","7":"5.739042","8":"79.53458"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />

<div class="exercise">
<span id="exr:exllr" class="exercise"><strong>Exercise 4.6  </strong></span>Based on the formula provided above, please create a new column for <code>corp_us_collocations</code>, which gives the Log-Likelihood Ratios of all the bigrams.
</div>

<div class="danger">
<p>
When you do the above exercise, you may run into a couple of problems:
</p>
<ul>
<li>
Some of the bigrams have <code>NaN</code> values in their LLR. This may be due to the issue of <code>NAs produced by integer overflow</code>. Please solve this.
</li>
<li>
After solving the above overflow issue, you may still have a few bigrams with <code>NaN</code> in their LLR, which may be due to the computation of the <code>log</code> value. In Math, <code>log(1/0)= Inf</code> and <code>log(0/1) = -Inf</code>. Do you know when you would get an undefined value <code>NaN</code> in the computation of <code>log()</code>?
</li>
</ul>
</div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["w1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["MI"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["LLR"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["O11"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["O12"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["O21"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["O22"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["E11"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["E12"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["E21"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["E22"],"name":[13],"type":["dbl"],"align":["right"]}],"data":[{"1":"united","2":"states","3":"6.942491","4":"137.83312","5":"1465.2545","6":"157","7":"45","8":"176","9":"52320","10":"1.2764431","11":"200.7236","12":"331.7236","13":"52164.28"},{"1":"fellow","2":"citizens","3":"7.359531","4":"137.77172","5":"1157.9238","6":"117","7":"35","8":"130","9":"52416","10":"0.7124369","11":"151.2876","12":"246.2876","13":"52299.71"},{"1":"and","2":"of","3":"-3.090931","4":"-23.61307","5":"1056.9342","6":"84","7":"5226","8":"7019","9":"40369","10":"715.7184333","11":"4594.2816","12":"6387.2816","13":"41000.72"},{"1":"has","2":"been","3":"4.965806","4":"72.60057","5":"1018.2445","6":"180","7":"442","8":"308","9":"51768","10":"5.7599150","11":"616.2401","12":"482.2401","13":"51593.76"},{"1":"have","2":"been","3":"4.431367","4":"62.95832","5":"986.7757","6":"202","7":"809","8":"286","9":"51401","10":"9.3621769","11":"1001.6378","12":"478.6378","13":"51208.36"},{"1":"to","2":"and","3":"-6.250642","4":"-21.09354","5":"943.1952","6":"6","7":"4528","8":"5304","9":"42860","10":"456.8587043","11":"4077.1413","12":"4853.1413","13":"43310.86"},{"1":"it","2":"is","3":"3.067292","4":"45.82485","5":"931.8411","6":"323","7":"1066","8":"1139","9":"50170","10":"38.5350108","11":"1350.4650","12":"1423.4650","13":"49885.54"},{"1":"those","2":"who","3":"5.739042","4":"79.53458","5":"833.9012","6":"123","7":"205","8":"247","9":"52123","10":"2.3029337","11":"325.6971","12":"367.6971","13":"52002.30"},{"1":"let","2":"us","3":"6.117720","4":"80.89001","5":"731.1687","6":"97","7":"57","8":"381","9":"52163","10":"1.3968652","11":"152.6031","12":"476.6031","13":"52067.40"},{"1":"we","2":"have","3":"2.934205","4":"38.37207","5":"679.6418","6":"255","7":"1484","8":"756","9":"50203","10":"33.3623477","11":"1705.6377","12":"977.6377","13":"49981.36"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-72" class="exercise"><strong>Exercise 4.7  </strong></span></p>
<ol style="list-style-type: upper-alpha">
<li><p>Find the top FIVE bigrams ranked according to MI values for each president. The result would be a data frame as shown below.</p></li>
<li>Create a plot as shown below to visualize your results.
</div></li>
</ol>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year_President"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w1"],"name":[2],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[3],"type":["chr"],"align":["left"]},{"label":["n"],"name":[4],"type":["int"],"align":["right"]},{"label":["w1freq"],"name":[5],"type":["int"],"align":["right"]},{"label":["w2freq"],"name":[6],"type":["int"],"align":["right"]},{"label":["w12freq_exp"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["MI"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[9],"type":["dbl"],"align":["right"]}],"data":[{"1":"1789_Washington","2":"assure","3":"myself","4":"2","5":"12","6":"33","7":"1.061349e-02","8":"7.5579568","9":"1.4067087"},{"1":"1789_Washington","2":"executive","3":"department","4":"2","5":"97","6":"24","7":"6.239447e-02","8":"5.0024381","9":"1.3700940"},{"1":"1789_Washington","2":"how","3":"far","4":"2","5":"45","6":"87","7":"1.049288e-01","8":"4.2525168","9":"1.3400177"},{"1":"1789_Washington","2":"retreat","3":"which","4":"2","5":"8","6":"1006","7":"2.157005e-01","8":"3.2128988","9":"1.2616903"},{"1":"1789_Washington","2":"i","3":"dare","4":"2","5":"838","6":"10","7":"2.245986e-01","8":"3.1545789","9":"1.2553983"},{"1":"1793_Washington","2":"i","3":"am","4":"2","5":"838","6":"65","7":"1.459891e+00","8":"0.4541392","9":"0.3819146"},{"1":"1793_Washington","2":"it","3":"shall","4":"2","5":"1389","6":"314","7":"1.168947e+01","8":"-2.5471382","9":"-6.8514934"},{"1":"1793_Washington","2":"by","3":"the","4":"2","5":"1083","6":"10082","7":"2.926431e+02","8":"-7.1929984","9":"-205.5156936"},{"1":"1793_Washington","2":"of","3":"the","4":"4","5":"7103","6":"10082","7":"1.919339e+03","8":"-8.9063936","9":"-957.6693468"},{"1":"1793_Washington","2":"to","3":"the","4":"2","5":"4534","6":"10082","7":"1.225156e+03","8":"-9.2587495","9":"-864.9017596"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-73-1.png" width="110%" /></p>

</div>
</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-hadley2017">
<p>Wickham, Hadley, and Garrett Grolemund. 2017. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. 1st ed. O’Reilly Media, Inc.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https:\/\/alvinntnu.github.io" + location.pathname;  // Replace PAGE_URL with your page's canonical URL variable
/*
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
*/
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://enc2036.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="creating-corpus.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tokenization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
