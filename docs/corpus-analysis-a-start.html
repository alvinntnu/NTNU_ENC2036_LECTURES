<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Corpus Analysis: A Start | Corpus Linguistics</title>
  <meta name="description" content="ENC2036 Course material first edition" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Corpus Analysis: A Start | Corpus Linguistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="ENC2036 Course material first edition" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Corpus Analysis: A Start | Corpus Linguistics" />
  
  <meta name="twitter:description" content="ENC2036 Course material first edition" />
  

<meta name="author" content="Alvin Chen" />


<meta name="date" content="2020-03-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="creating-corpus.html"/>
<link rel="next" href="tokenization.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://alvinntnu.github.io/NTNU_ENC2036/"><img src="images/alerts/home.svg" height = "20" width = "20"> Course Website</a></li>
<li><a href="./"><img src="images/alerts/book-solid.svg" height = "20" width = "20"> ENC 2036 Corpus Linguistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-objective"><i class="fa fa-check"></i>Course Objective</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#textbook"><i class="fa fa-check"></i>Textbook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-website"><i class="fa fa-check"></i>Course Website</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-demo-data"><i class="fa fa-check"></i>Course Demo Data</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html"><i class="fa fa-check"></i><b>1</b> What is Corpus Linguistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#why-do-we-need-corpus-data"><i class="fa fa-check"></i><b>1.1</b> Why do we need corpus data?</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#what-is-corpus"><i class="fa fa-check"></i><b>1.2</b> What is corpus?</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#what-is-a-corpus-linguistic-study"><i class="fa fa-check"></i><b>1.3</b> What is a corpus linguistic study?</a></li>
<li class="chapter" data-level="1.4" data-path="what-is-corpus-linguistics.html"><a href="what-is-corpus-linguistics.html#additional-information-on-cl"><i class="fa fa-check"></i><b>1.4</b> Additional Information on CL</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>2</b> R Fundamentals</a><ul>
<li class="chapter" data-level="" data-path="r-fundamentals.html"><a href="r-fundamentals.html#a-quick-note"><i class="fa fa-check"></i>A Quick Note</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="creating-corpus.html"><a href="creating-corpus.html"><i class="fa fa-check"></i><b>3</b> Creating Corpus</a><ul>
<li class="chapter" data-level="3.1" data-path="creating-corpus.html"><a href="creating-corpus.html#html-structure"><i class="fa fa-check"></i><b>3.1</b> HTML Structure</a><ul>
<li class="chapter" data-level="3.1.1" data-path="creating-corpus.html"><a href="creating-corpus.html#html-syntax"><i class="fa fa-check"></i><b>3.1.1</b> HTML Syntax</a></li>
<li class="chapter" data-level="3.1.2" data-path="creating-corpus.html"><a href="creating-corpus.html#tags-and-attributes"><i class="fa fa-check"></i><b>3.1.2</b> Tags and Attributes</a></li>
<li class="chapter" data-level="3.1.3" data-path="creating-corpus.html"><a href="creating-corpus.html#css"><i class="fa fa-check"></i><b>3.1.3</b> CSS</a></li>
<li class="chapter" data-level="3.1.4" data-path="creating-corpus.html"><a href="creating-corpus.html#html-css-javascript"><i class="fa fa-check"></i><b>3.1.4</b> HTML + CSS ( + JavaScript)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="creating-corpus.html"><a href="creating-corpus.html#web-crawling"><i class="fa fa-check"></i><b>3.2</b> Web Crawling</a></li>
<li class="chapter" data-level="3.3" data-path="creating-corpus.html"><a href="creating-corpus.html#functional-programming"><i class="fa fa-check"></i><b>3.3</b> Functional Programming</a></li>
<li class="chapter" data-level="3.4" data-path="creating-corpus.html"><a href="creating-corpus.html#save-corpus"><i class="fa fa-check"></i><b>3.4</b> Save Corpus</a></li>
<li class="chapter" data-level="3.5" data-path="creating-corpus.html"><a href="creating-corpus.html#additional-resourcess"><i class="fa fa-check"></i><b>3.5</b> Additional Resourcess</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html"><i class="fa fa-check"></i><b>4</b> Corpus Analysis: A Start</a><ul>
<li class="chapter" data-level="4.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#installing-quanteda"><i class="fa fa-check"></i><b>4.1</b> Installing <code>quanteda</code></a></li>
<li class="chapter" data-level="4.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#building-a-corpus-from-character-vector"><i class="fa fa-check"></i><b>4.2</b> Building a <code>corpus</code> from character vector</a></li>
<li class="chapter" data-level="4.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#keyword-in-context-kwic"><i class="fa fa-check"></i><b>4.3</b> Keyword-in-Context (KWIC)</a></li>
<li class="chapter" data-level="4.4" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#kwic-with-regular-expressions"><i class="fa fa-check"></i><b>4.4</b> KWIC with Regular Expressions</a></li>
<li class="chapter" data-level="4.5" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#tidy-text-format-of-the-corpus"><i class="fa fa-check"></i><b>4.5</b> Tidy Text Format of the Corpus</a></li>
<li class="chapter" data-level="4.6" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#frequency-lists"><i class="fa fa-check"></i><b>4.6</b> Frequency Lists</a></li>
<li class="chapter" data-level="4.7" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#word-cloud"><i class="fa fa-check"></i><b>4.7</b> Word Cloud</a></li>
<li class="chapter" data-level="4.8" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#collocations"><i class="fa fa-check"></i><b>4.8</b> Collocations</a><ul>
<li class="chapter" data-level="4.8.1" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#cooccurrence-table-and-observed-frequencies"><i class="fa fa-check"></i><b>4.8.1</b> Cooccurrence Table and Observed Frequencies</a></li>
<li class="chapter" data-level="4.8.2" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#expected-frequencies"><i class="fa fa-check"></i><b>4.8.2</b> Expected Frequencies</a></li>
<li class="chapter" data-level="4.8.3" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#association-measures"><i class="fa fa-check"></i><b>4.8.3</b> Association Measures</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="corpus-analysis-a-start.html"><a href="corpus-analysis-a-start.html#constructions"><i class="fa fa-check"></i><b>4.9</b> Constructions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tokenization.html"><a href="tokenization.html"><i class="fa fa-check"></i><b>5</b> Tokenization</a><ul>
<li class="chapter" data-level="5.1" data-path="tokenization.html"><a href="tokenization.html#english-tokenization"><i class="fa fa-check"></i><b>5.1</b> English Tokenization</a></li>
<li class="chapter" data-level="5.2" data-path="tokenization.html"><a href="tokenization.html#text-analytics-pipeline"><i class="fa fa-check"></i><b>5.2</b> Text Analytics Pipeline</a></li>
<li class="chapter" data-level="5.3" data-path="tokenization.html"><a href="tokenization.html#proper-units-for-analysis"><i class="fa fa-check"></i><b>5.3</b> Proper Units for Analysis</a><ul>
<li class="chapter" data-level="5.3.1" data-path="tokenization.html"><a href="tokenization.html#sentence-tokenization"><i class="fa fa-check"></i><b>5.3.1</b> Sentence Tokenization</a></li>
<li class="chapter" data-level="5.3.2" data-path="tokenization.html"><a href="tokenization.html#words-tokenization"><i class="fa fa-check"></i><b>5.3.2</b> Words Tokenization</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tokenization.html"><a href="tokenization.html#lexical-bundles-n-grams"><i class="fa fa-check"></i><b>5.4</b> Lexical Bundles (n-grams)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html"><i class="fa fa-check"></i><b>6</b> Parts-of-Speech Tagging</a><ul>
<li class="chapter" data-level="6.1" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#parts-of-speech-tagging-1"><i class="fa fa-check"></i><b>6.1</b> Parts-of-Speech Tagging</a></li>
<li class="chapter" data-level="6.2" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#metalingusitic-analysis"><i class="fa fa-check"></i><b>6.2</b> Metalingusitic Analysis</a></li>
<li class="chapter" data-level="6.3" data-path="parts-of-speech-tagging.html"><a href="parts-of-speech-tagging.html#saving-pos-tagged-texts"><i class="fa fa-check"></i><b>6.3</b> Saving POS-tagged Texts</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="keyword-analysis.html"><a href="keyword-analysis.html"><i class="fa fa-check"></i><b>7</b> Keyword Analysis</a></li>
<li class="chapter" data-level="8" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html"><i class="fa fa-check"></i><b>8</b> Constructions and Idioms</a><ul>
<li class="chapter" data-level="8.1" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#chinese-four-character-idioms"><i class="fa fa-check"></i><b>8.1</b> Chinese Four-character Idioms</a></li>
<li class="chapter" data-level="8.2" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#dictionary-entries"><i class="fa fa-check"></i><b>8.2</b> Dictionary Entries</a></li>
<li class="chapter" data-level="8.3" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#case-study-x來y去"><i class="fa fa-check"></i><b>8.3</b> Case Study: <code>X來Y去</code></a></li>
<li class="chapter" data-level="8.4" data-path="constructions-and-idioms.html"><a href="constructions-and-idioms.html#exercises"><i class="fa fa-check"></i><b>8.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html"><i class="fa fa-check"></i><b>9</b> Chinese Text Processing</a><ul>
<li class="chapter" data-level="9.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#chinese-word-segmenter-jiebar"><i class="fa fa-check"></i><b>9.1</b> Chinese Word Segmenter <code>jiebaR</code></a><ul>
<li class="chapter" data-level="9.1.1" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#start"><i class="fa fa-check"></i><b>9.1.1</b> Start</a></li>
<li class="chapter" data-level="9.1.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#settings"><i class="fa fa-check"></i><b>9.1.2</b> Settings</a></li>
<li class="chapter" data-level="9.1.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#user-defined-dictionary"><i class="fa fa-check"></i><b>9.1.3</b> User-defined dictionary</a></li>
<li class="chapter" data-level="9.1.4" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#stopwords"><i class="fa fa-check"></i><b>9.1.4</b> Stopwords</a></li>
<li class="chapter" data-level="9.1.5" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#pos-tagging"><i class="fa fa-check"></i><b>9.1.5</b> POS Tagging</a></li>
<li class="chapter" data-level="9.1.6" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#default"><i class="fa fa-check"></i><b>9.1.6</b> Default</a></li>
<li class="chapter" data-level="9.1.7" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#reminder"><i class="fa fa-check"></i><b>9.1.7</b> Reminder</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#chinese-text-analytics-pipeline"><i class="fa fa-check"></i><b>9.2</b> Chinese Text Analytics Pipeline</a></li>
<li class="chapter" data-level="9.3" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-1-word-frequency-and-wordcloud"><i class="fa fa-check"></i><b>9.3</b> Case Study 1: Word Frequency and Wordcloud</a></li>
<li class="chapter" data-level="9.4" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-2-patterns"><i class="fa fa-check"></i><b>9.4</b> Case Study 2: Patterns</a></li>
<li class="chapter" data-level="9.5" data-path="chinese-text-processing.html"><a href="chinese-text-processing.html#case-study-3-lexical-bundles"><i class="fa fa-check"></i><b>9.5</b> Case Study 3: Lexical Bundles</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ckiptagger.html"><a href="ckiptagger.html"><i class="fa fa-check"></i><b>10</b> CKIP Tagger</a><ul>
<li class="chapter" data-level="10.1" data-path="ckiptagger.html"><a href="ckiptagger.html#installation"><i class="fa fa-check"></i><b>10.1</b> Installation</a></li>
<li class="chapter" data-level="10.2" data-path="ckiptagger.html"><a href="ckiptagger.html#download-the-model-files"><i class="fa fa-check"></i><b>10.2</b> Download the Model Files</a></li>
<li class="chapter" data-level="10.3" data-path="ckiptagger.html"><a href="ckiptagger.html#r-python-communication"><i class="fa fa-check"></i><b>10.3</b> R-Python Communication</a></li>
<li class="chapter" data-level="10.4" data-path="ckiptagger.html"><a href="ckiptagger.html#word-segmentation-in-r"><i class="fa fa-check"></i><b>10.4</b> Word Segmentation in R</a></li>
<li class="chapter" data-level="10.5" data-path="ckiptagger.html"><a href="ckiptagger.html#r-environment-setting"><i class="fa fa-check"></i><b>10.5</b> R Environment Setting</a></li>
<li class="chapter" data-level="10.6" data-path="ckiptagger.html"><a href="ckiptagger.html#loading-python-modules"><i class="fa fa-check"></i><b>10.6</b> Loading Python Modules</a></li>
<li class="chapter" data-level="10.7" data-path="ckiptagger.html"><a href="ckiptagger.html#segmenting-texts"><i class="fa fa-check"></i><b>10.7</b> Segmenting Texts</a></li>
<li class="chapter" data-level="10.8" data-path="ckiptagger.html"><a href="ckiptagger.html#define-own-dictionary"><i class="fa fa-check"></i><b>10.8</b> Define Own Dictionary</a></li>
<li class="chapter" data-level="10.9" data-path="ckiptagger.html"><a href="ckiptagger.html#beyond-word-boundaries"><i class="fa fa-check"></i><b>10.9</b> Beyond Word Boundaries</a></li>
<li class="chapter" data-level="10.10" data-path="ckiptagger.html"><a href="ckiptagger.html#tidy-up-the-results"><i class="fa fa-check"></i><b>10.10</b> Tidy Up the Results</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="structured-corpus.html"><a href="structured-corpus.html"><i class="fa fa-check"></i><b>11</b> Structured Corpus</a><ul>
<li class="chapter" data-level="11.1" data-path="structured-corpus.html"><a href="structured-corpus.html#nccu-spoken-mandarin"><i class="fa fa-check"></i><b>11.1</b> NCCU Spoken Mandarin</a><ul>
<li class="chapter" data-level="11.1.1" data-path="structured-corpus.html"><a href="structured-corpus.html#loading-the-corpus"><i class="fa fa-check"></i><b>11.1.1</b> Loading the Corpus</a></li>
<li class="chapter" data-level="11.1.2" data-path="structured-corpus.html"><a href="structured-corpus.html#line-segmentation"><i class="fa fa-check"></i><b>11.1.2</b> Line Segmentation</a></li>
<li class="chapter" data-level="11.1.3" data-path="structured-corpus.html"><a href="structured-corpus.html#metadata-vs.transcript"><i class="fa fa-check"></i><b>11.1.3</b> Metadata vs. Transcript</a></li>
<li class="chapter" data-level="11.1.4" data-path="structured-corpus.html"><a href="structured-corpus.html#word-tokenization"><i class="fa fa-check"></i><b>11.1.4</b> Word Tokenization</a></li>
<li class="chapter" data-level="11.1.5" data-path="structured-corpus.html"><a href="structured-corpus.html#word-frequencies-and-wordcloud"><i class="fa fa-check"></i><b>11.1.5</b> Word frequencies and Wordcloud</a></li>
<li class="chapter" data-level="11.1.6" data-path="structured-corpus.html"><a href="structured-corpus.html#concordances"><i class="fa fa-check"></i><b>11.1.6</b> Concordances</a></li>
<li class="chapter" data-level="11.1.7" data-path="structured-corpus.html"><a href="structured-corpus.html#n-grams-lexical-bundles"><i class="fa fa-check"></i><b>11.1.7</b> N-grams (Lexical Bundles)</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="structured-corpus.html"><a href="structured-corpus.html#connecting-spid-to-metadata"><i class="fa fa-check"></i><b>11.2</b> Connecting SPID to Metadata</a></li>
<li class="chapter" data-level="11.3" data-path="structured-corpus.html"><a href="structured-corpus.html#more-socialinguistic-analyses"><i class="fa fa-check"></i><b>11.3</b> More Socialinguistic Analyses</a><ul>
<li class="chapter" data-level="11.3.1" data-path="structured-corpus.html"><a href="structured-corpus.html#check-ngram-distribution-by-age-groups"><i class="fa fa-check"></i><b>11.3.1</b> Check Ngram Distribution By Age Groups</a></li>
<li class="chapter" data-level="11.3.2" data-path="structured-corpus.html"><a href="structured-corpus.html#check-word-distribution-of-different-genders"><i class="fa fa-check"></i><b>11.3.2</b> Check Word Distribution of different genders</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="xml.html"><a href="xml.html"><i class="fa fa-check"></i><b>12</b> XML</a><ul>
<li class="chapter" data-level="12.1" data-path="xml.html"><a href="xml.html#bnc-spoken-2014"><i class="fa fa-check"></i><b>12.1</b> BNC Spoken 2014</a></li>
<li class="chapter" data-level="12.2" data-path="xml.html"><a href="xml.html#process-the-whole-directory-of-bnc2014-sample"><i class="fa fa-check"></i><b>12.2</b> Process the Whole Directory of BNC2014 Sample</a><ul>
<li class="chapter" data-level="12.2.1" data-path="xml.html"><a href="xml.html#define-function"><i class="fa fa-check"></i><b>12.2.1</b> Define Function</a></li>
<li class="chapter" data-level="12.2.2" data-path="xml.html"><a href="xml.html#process-the-all-files-in-the-directory"><i class="fa fa-check"></i><b>12.2.2</b> Process the all files in the Directory</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="xml.html"><a href="xml.html#metadata"><i class="fa fa-check"></i><b>12.3</b> Metadata</a><ul>
<li class="chapter" data-level="12.3.1" data-path="xml.html"><a href="xml.html#text-metadata"><i class="fa fa-check"></i><b>12.3.1</b> Text Metadata</a></li>
<li class="chapter" data-level="12.3.2" data-path="xml.html"><a href="xml.html#speaker-metadata"><i class="fa fa-check"></i><b>12.3.2</b> Speaker Metadata</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="xml.html"><a href="xml.html#bnc2014-for-socialinguistic-variation"><i class="fa fa-check"></i><b>12.4</b> BNC2014 for Socialinguistic Variation</a><ul>
<li class="chapter" data-level="12.4.1" data-path="xml.html"><a href="xml.html#word-frequency-vs.gender"><i class="fa fa-check"></i><b>12.4.1</b> Word Frequency vs. Gender</a></li>
<li class="chapter" data-level="12.4.2" data-path="xml.html"><a href="xml.html#degree-adv-adj"><i class="fa fa-check"></i><b>12.4.2</b> Degree ADV + ADJ</a></li>
<li class="chapter" data-level="12.4.3" data-path="xml.html"><a href="xml.html#trigrams"><i class="fa fa-check"></i><b>12.4.3</b> Trigrams</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="vector-space-representation.html"><a href="vector-space-representation.html"><i class="fa fa-check"></i><b>13</b> Vector Space Representation</a><ul>
<li class="chapter" data-level="13.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#data-processing-flowchart"><i class="fa fa-check"></i><b>13.1</b> Data Processing Flowchart</a></li>
<li class="chapter" data-level="13.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-feature-matrix-dfm"><i class="fa fa-check"></i><b>13.2</b> Document-Feature Matrix (<code>dfm</code>)</a></li>
<li class="chapter" data-level="13.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#defining-feature-in-dfm"><i class="fa fa-check"></i><b>13.3</b> Defining <code>Feature</code> in <code>dfm</code></a></li>
<li class="chapter" data-level="13.4" data-path="vector-space-representation.html"><a href="vector-space-representation.html#feature-selection"><i class="fa fa-check"></i><b>13.4</b> Feature Selection</a><ul>
<li class="chapter" data-level="13.4.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#determining-linguistic-granularity"><i class="fa fa-check"></i><b>13.4.1</b> Determining Linguistic Granularity</a></li>
<li class="chapter" data-level="13.4.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#stopwords-1"><i class="fa fa-check"></i><b>13.4.2</b> Stopwords</a></li>
<li class="chapter" data-level="13.4.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#distributional-cut-offs-for-features"><i class="fa fa-check"></i><b>13.4.3</b> Distributional Cut-offs for Features</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="vector-space-representation.html"><a href="vector-space-representation.html#applying-dfm"><i class="fa fa-check"></i><b>13.5</b> Applying DFM</a><ul>
<li class="chapter" data-level="13.5.1" data-path="vector-space-representation.html"><a href="vector-space-representation.html#wordcloud"><i class="fa fa-check"></i><b>13.5.1</b> Wordcloud</a></li>
<li class="chapter" data-level="13.5.2" data-path="vector-space-representation.html"><a href="vector-space-representation.html#document-similarity"><i class="fa fa-check"></i><b>13.5.2</b> Document Similarity</a></li>
<li class="chapter" data-level="13.5.3" data-path="vector-space-representation.html"><a href="vector-space-representation.html#feature-similarity"><i class="fa fa-check"></i><b>13.5.3</b> Feature Similarity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html"><i class="fa fa-check"></i><b>14</b> Vector Space Representation II</a><ul>
<li class="chapter" data-level="14.1" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#a-quick-view"><i class="fa fa-check"></i><b>14.1</b> A Quick View</a></li>
<li class="chapter" data-level="14.2" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#loading-the-corpus-1"><i class="fa fa-check"></i><b>14.2</b> Loading the Corpus</a></li>
<li class="chapter" data-level="14.3" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#semgentation"><i class="fa fa-check"></i><b>14.3</b> Semgentation</a></li>
<li class="chapter" data-level="14.4" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#corpus-metadata"><i class="fa fa-check"></i><b>14.4</b> Corpus Metadata</a></li>
<li class="chapter" data-level="14.5" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#document-feature-matrix"><i class="fa fa-check"></i><b>14.5</b> Document-Feature Matrix</a></li>
<li class="chapter" data-level="14.6" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#wordcloud-1"><i class="fa fa-check"></i><b>14.6</b> Wordcloud</a></li>
<li class="chapter" data-level="14.7" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#document-similarity-1"><i class="fa fa-check"></i><b>14.7</b> Document Similarity</a></li>
<li class="chapter" data-level="14.8" data-path="vector-space-representation-ii.html"><a href="vector-space-representation-ii.html#feature-similarity-1"><i class="fa fa-check"></i><b>14.8</b> Feature Similarity</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html"><i class="fa fa-check"></i><b>15</b> Vector Space Representation III</a><ul>
<li class="chapter" data-level="15.1" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#library"><i class="fa fa-check"></i><b>15.1</b> Library</a></li>
<li class="chapter" data-level="15.2" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#text-collection"><i class="fa fa-check"></i><b>15.2</b> Text Collection</a></li>
<li class="chapter" data-level="15.3" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#tokenization-and-vocabulary"><i class="fa fa-check"></i><b>15.3</b> Tokenization and Vocabulary</a></li>
<li class="chapter" data-level="15.4" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#pruning"><i class="fa fa-check"></i><b>15.4</b> Pruning</a></li>
<li class="chapter" data-level="15.5" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#term-cooccurrence-matrix"><i class="fa fa-check"></i><b>15.5</b> Term-Cooccurrence Matrix</a></li>
<li class="chapter" data-level="15.6" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#fitting-model"><i class="fa fa-check"></i><b>15.6</b> Fitting Model</a></li>
<li class="chapter" data-level="15.7" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#averaging-word-vectors"><i class="fa fa-check"></i><b>15.7</b> Averaging Word Vectors</a></li>
<li class="chapter" data-level="15.8" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#semantic-space"><i class="fa fa-check"></i><b>15.8</b> Semantic Space</a></li>
<li class="chapter" data-level="15.9" data-path="vector-space-representation-iii.html"><a href="vector-space-representation-iii.html#visualizing-multi-dimensional-space"><i class="fa fa-check"></i><b>15.9</b> Visualizing Multi-dimensional Space</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>16</b> References</a></li>
<li class="divider"></li>
<li><a href="https://web.ntnu.edu.tw/~alvinchen" target ="blank">Alvin Chen </a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Corpus Linguistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="corpus-analysis-a-start" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Corpus Analysis: A Start</h1>
<p>In this chapter, I will demonstrate how to do basic corpus analysis after you have collected data. I will show you some of the most common ways that people work with the text data.</p>
<div id="installing-quanteda" class="section level2">
<h2><span class="header-section-number">4.1</span> Installing <code>quanteda</code></h2>
<p>To start with, this tutorial will use a powerful package, <code>quanteda</code>, for managing and analyzing textual data in R. You may refer to the <a href="https://quanteda.io/">official documentation of the package</a> for more detail.</p>
<p><code>quanteda</code> is not included in the default R installation. Please install the package if you haven’t done so.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;quanteda&quot;</span>)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">install.packages</span>(<span class="st">&quot;readtext&quot;</span>)</a></code></pre></div>
<p>Also, as noted on the <code>quanteda</code> documentation, because this library compiles some C++ and Fortran source code, you will need to have installed the appropriate compilers.</p>
<ul>
<li>If you are using a Windows platform, this means you will need also to install the <a href="https://cran.r-project.org/bin/windows/Rtools/">Rtools</a> software available from CRAN.</li>
<li>If you are using macOS, you should install the <a href="https://cran.r-project.org/bin/macosx/tools/">macOS tools</a>.</li>
</ul>
<p>If you run into any installation errors, please go to the official documentation page for additional assistance.</p>
</div>
<div id="building-a-corpus-from-character-vector" class="section level2">
<h2><span class="header-section-number">4.2</span> Building a <code>corpus</code> from character vector</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">library</span>(quanteda)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">library</span>(readtext)</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="kw">library</span>(dplyr)</a></code></pre></div>
<p>To demonstrate a typical corpus analytic example with texts, I will be using a pre-loaded corpus that comes with the <code>quanteda</code> package, <code>data_corpus_inaugural</code>. This is a corpus of US presidential inaugural address texts, and metadata for the corpus from 1789 to present.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">data_corpus_inaugural</a></code></pre></div>
<pre><code>## Corpus consisting of 58 documents and 4 docvars.
## 1789-Washington :
## &quot;Fellow-Citizens of the Senate and of the House of Representa...&quot;
## 
## 1793-Washington :
## &quot;Fellow citizens, I am again called upon by the voice of my c...&quot;
## 
## 1797-Adams :
## &quot;When it was first perceived, in early times, that no middle ...&quot;
## 
## 1801-Jefferson :
## &quot;Friends and Fellow Citizens: Called upon to undertake the du...&quot;
## 
## 1805-Jefferson :
## &quot;Proceeding, fellow citizens, to that qualification which the...&quot;
## 
## 1809-Madison :
## &quot;Unwilling to depart from examples of the most revered author...&quot;
## 
## [ reached max_ndoc ... 52 more documents ]</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">class</span>(data_corpus_inaugural)</a></code></pre></div>
<pre><code>## [1] &quot;corpus&quot;</code></pre>
<p>We create a <code>corpus()</code> object with the pre-loaded <code>corpus</code> in <code>quanteda</code>– <code>data_corpus_inaugural</code>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">corp_us &lt;-<span class="st"> </span><span class="kw">corpus</span>(data_corpus_inaugural) <span class="co"># save the `corpus` to a short obj name</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">summary</span>(corp_us)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Text"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Types"],"name":[2],"type":["int"],"align":["right"]},{"label":["Tokens"],"name":[3],"type":["int"],"align":["right"]},{"label":["Sentences"],"name":[4],"type":["int"],"align":["right"]},{"label":["Year"],"name":[5],"type":["int"],"align":["right"]},{"label":["President"],"name":[6],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[7],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[8],"type":["fctr"],"align":["left"]}],"data":[{"1":"1789-Washington","2":"625","3":"1537","4":"23","5":"1789","6":"Washington","7":"George","8":"none","_rn_":"1"},{"1":"1793-Washington","2":"96","3":"147","4":"4","5":"1793","6":"Washington","7":"George","8":"none","_rn_":"2"},{"1":"1797-Adams","2":"826","3":"2577","4":"37","5":"1797","6":"Adams","7":"John","8":"Federalist","_rn_":"3"},{"1":"1801-Jefferson","2":"717","3":"1923","4":"41","5":"1801","6":"Jefferson","7":"Thomas","8":"Democratic-Republican","_rn_":"4"},{"1":"1805-Jefferson","2":"804","3":"2380","4":"45","5":"1805","6":"Jefferson","7":"Thomas","8":"Democratic-Republican","_rn_":"5"},{"1":"1809-Madison","2":"535","3":"1261","4":"21","5":"1809","6":"Madison","7":"James","8":"Democratic-Republican","_rn_":"6"},{"1":"1813-Madison","2":"541","3":"1302","4":"33","5":"1813","6":"Madison","7":"James","8":"Democratic-Republican","_rn_":"7"},{"1":"1817-Monroe","2":"1040","3":"3677","4":"121","5":"1817","6":"Monroe","7":"James","8":"Democratic-Republican","_rn_":"8"},{"1":"1821-Monroe","2":"1259","3":"4886","4":"129","5":"1821","6":"Monroe","7":"James","8":"Democratic-Republican","_rn_":"9"},{"1":"1825-Adams","2":"1003","3":"3147","4":"74","5":"1825","6":"Adams","7":"John Quincy","8":"Democratic-Republican","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After the <code>corpus</code> is loaded, we can use <code>summary()</code> to get the metadata of each text in the corpus, including word types and tokens as well. This allows us to have a quick look at the size of the addressess made by all presidents.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">require</span>(ggplot2)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">corp_us <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="st">  </span>summary <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb8-5" data-line-number="5"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Year, <span class="dt">y =</span> Tokens, <span class="dt">group =</span> <span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb8-6" data-line-number="6"><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb8-7" data-line-number="7"><span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="st">    </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-7" class="exercise"><strong>Exercise 4.1  </strong></span>Could you reproduce the above line plot and add information of <code>President</code> to the plot as labels of the dots?</p>
Hints: Please check <code>ggplot2::geom_text()</code> or more advanced one, <code>ggrepel::geom_text_repel()</code>
</div>

<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<hr />
</div>
<div id="keyword-in-context-kwic" class="section level2">
<h2><span class="header-section-number">4.3</span> Keyword-in-Context (KWIC)</h2>
<p>Keyword-in-Context (KWIC), or <em>concordances</em>, are the most frequently used method in corpus linguistics. The idea is very intuitive: we get to know more about the semantics of a word by examing how it is being used in a wider context.</p>
<p>We can use <code>kwic()</code> to perform a search for a word and retrieve its concordances from the corpus:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">kwic</span>(corp_us, <span class="st">&quot;terror&quot;</span>)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["docname"],"name":[1],"type":["chr"],"align":["left"]},{"label":["from"],"name":[2],"type":["int"],"align":["right"]},{"label":["to"],"name":[3],"type":["int"],"align":["right"]},{"label":["pre"],"name":[4],"type":["chr"],"align":["left"]},{"label":["keyword"],"name":[5],"type":["chr"],"align":["left"]},{"label":["post"],"name":[6],"type":["chr"],"align":["left"]},{"label":["pattern"],"name":[7],"type":["fctr"],"align":["left"]}],"data":[{"1":"1797-Adams","2":"1324","3":"1324","4":"fraud or violence , by","5":"terror","6":", intrigue , or venality","7":"terror","_rn_":"1"},{"1":"1933-Roosevelt","2":"111","3":"111","4":"nameless , unreasoning , unjustified","5":"terror","6":"which paralyzes needed efforts to","7":"terror","_rn_":"2"},{"1":"1941-Roosevelt","2":"285","3":"285","4":"seemed frozen by a fatalistic","5":"terror","6":", we proved that this","7":"terror","_rn_":"3"},{"1":"1961-Kennedy","2":"848","3":"848","4":"alter that uncertain balance of","5":"terror","6":"that stays the hand of","7":"terror","_rn_":"4"},{"1":"1981-Reagan","2":"811","3":"811","4":"freeing all Americans from the","5":"terror","6":"of runaway living costs .","7":"terror","_rn_":"5"},{"1":"1997-Clinton","2":"1047","3":"1047","4":"They fuel the fanaticism of","5":"terror","6":". And they torment the","7":"terror","_rn_":"6"},{"1":"1997-Clinton","2":"1647","3":"1647","4":"maintain a strong defense against","5":"terror","6":"and destruction . Our children","7":"terror","_rn_":"7"},{"1":"2009-Obama","2":"1619","3":"1619","4":"advance their aims by inducing","5":"terror","6":"and slaughtering innocents , we","7":"terror","_rn_":"8"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p><code>kwic()</code> returns a data frame, which can be easily output to a CSV file for later use.</p>
<div class="danger">
<p>
Please note that <code>kwic()</code>, when taking a <code>corpus</code> object as the argument, will automatically <em>tokenize</em>the corpus data and do the keyword-in-context search on a <em>word</em> basis. In other words, the pattern you look for cannot be a linguistic pattern across several words. We will talk about how to extract constructions later.
</p>
</div>
</div>
<div id="kwic-with-regular-expressions" class="section level2">
<h2><span class="header-section-number">4.4</span> KWIC with Regular Expressions</h2>
<p>For more complex searches, we can use regular expressions as well in <code>kwic()</code>. For example, if you want to include <code>terror</code> and all its other related word forms, such as <code>terrorist</code>, <code>terrorism</code>, <code>terrors</code>, you can do a regular expression search.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">kwic</span>(corp_us, <span class="st">&quot;terror.*&quot;</span>, <span class="dt">valuetype =</span> <span class="st">&quot;regex&quot;</span>)</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["docname"],"name":[1],"type":["chr"],"align":["left"]},{"label":["from"],"name":[2],"type":["int"],"align":["right"]},{"label":["to"],"name":[3],"type":["int"],"align":["right"]},{"label":["pre"],"name":[4],"type":["chr"],"align":["left"]},{"label":["keyword"],"name":[5],"type":["chr"],"align":["left"]},{"label":["post"],"name":[6],"type":["chr"],"align":["left"]},{"label":["pattern"],"name":[7],"type":["fctr"],"align":["left"]}],"data":[{"1":"1797-Adams","2":"1324","3":"1324","4":"fraud or violence , by","5":"terror","6":", intrigue , or venality","7":"terror.*","_rn_":"1"},{"1":"1933-Roosevelt","2":"111","3":"111","4":"nameless , unreasoning , unjustified","5":"terror","6":"which paralyzes needed efforts to","7":"terror.*","_rn_":"2"},{"1":"1941-Roosevelt","2":"285","3":"285","4":"seemed frozen by a fatalistic","5":"terror","6":", we proved that this","7":"terror.*","_rn_":"3"},{"1":"1961-Kennedy","2":"848","3":"848","4":"alter that uncertain balance of","5":"terror","6":"that stays the hand of","7":"terror.*","_rn_":"4"},{"1":"1961-Kennedy","2":"970","3":"970","4":"of science instead of its","5":"terrors","6":". Together let us explore","7":"terror.*","_rn_":"5"},{"1":"1981-Reagan","2":"811","3":"811","4":"freeing all Americans from the","5":"terror","6":"of runaway living costs .","7":"terror.*","_rn_":"6"},{"1":"1981-Reagan","2":"2186","3":"2186","4":"understood by those who practice","5":"terrorism","6":"and prey upon their neighbors","7":"terror.*","_rn_":"7"},{"1":"1997-Clinton","2":"1047","3":"1047","4":"They fuel the fanaticism of","5":"terror","6":". And they torment the","7":"terror.*","_rn_":"8"},{"1":"1997-Clinton","2":"1647","3":"1647","4":"maintain a strong defense against","5":"terror","6":"and destruction . Our children","7":"terror.*","_rn_":"9"},{"1":"2009-Obama","2":"1619","3":"1619","4":"advance their aims by inducing","5":"terror","6":"and slaughtering innocents , we","7":"terror.*","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>By default, the <code>kwic()</code> is word-based. If you like to look up a multiword combination, use <code>phrase()</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">kwic</span>(corp_us, <span class="kw">phrase</span>(<span class="st">&quot;our country&quot;</span>))</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["docname"],"name":[1],"type":["chr"],"align":["left"]},{"label":["from"],"name":[2],"type":["int"],"align":["right"]},{"label":["to"],"name":[3],"type":["int"],"align":["right"]},{"label":["pre"],"name":[4],"type":["chr"],"align":["left"]},{"label":["keyword"],"name":[5],"type":["chr"],"align":["left"]},{"label":["post"],"name":[6],"type":["chr"],"align":["left"]},{"label":["pattern"],"name":[7],"type":["fctr"],"align":["left"]}],"data":[{"1":"1801-Jefferson","2":"18","3":"19","4":"the first executive office of","5":"our country","6":", I avail myself of","7":"our country","_rn_":"1"},{"1":"1805-Jefferson","2":"1898","3":"1899","4":"course , I offer to","5":"our country","6":"sincere congratulations . With those","7":"our country","_rn_":"2"},{"1":"1809-Madison","2":"322","3":"323","4":"from this prosperous condition of","5":"our country","6":"to the scene which has","7":"our country","_rn_":"3"},{"1":"1813-Madison","2":"1033","3":"1034","4":"and an intelligent people .","5":"Our country","6":"abounds in the necessaries ,","7":"our country","_rn_":"4"},{"1":"1813-Madison","2":"1226","3":"1227","4":"arms now may long preserve","5":"our country","6":"from the necessity of another","7":"our country","_rn_":"5"},{"1":"1817-Monroe","2":"1217","3":"1218","4":"the highly favored condition of","5":"our country","6":", it is the interest","7":"our country","_rn_":"6"},{"1":"1817-Monroe","2":"2328","3":"2329","4":"among which the improvement of","5":"our country","6":"by roads and canals ,","7":"our country","_rn_":"7"},{"1":"1821-Monroe","2":"265","3":"266","4":", prosperity and happiness of","5":"our country","6":"will always be the object","7":"our country","_rn_":"8"},{"1":"1821-Monroe","2":"1203","3":"1204","4":"citizens from that destruction and","5":"our country","6":"from that devastation which are","7":"our country","_rn_":"9"},{"1":"1821-Monroe","2":"4028","3":"4029","4":"to the internal concerns of","5":"our country","6":", and more especially to","7":"our country","_rn_":"10"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>It should be noted that the output of <code>kwic</code> includes not only the concordances (i.e., preceding/subsequent co-texts + the keyword), but also the sources of the texts for each concordance line. This would be extremely convenient if you need to refer back to the original discourse context of the concordance line.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-13" class="exercise"><strong>Exercise 4.2  </strong></span>Please create a bar plot, showing the number of uses of the word <em>country</em> in each president’s address. Please include different variants of the word, e.g., <em>countries</em>, <em>Countries</em>, <em>Country</em>, in your <code>kwic()</code> search.
</div>

<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<hr />
</div>
<div id="tidy-text-format-of-the-corpus" class="section level2">
<h2><span class="header-section-number">4.5</span> Tidy Text Format of the Corpus</h2>
<p>So far our corpus is a <code>corpus</code> object defined in <code>quanteda</code>. In most of the R standard packages, people normally follow the <strong>using tidy data principles</strong> to make handling data easier and more effective. As described by Hadley Wickham <span class="citation">(Wickham and Grolemund <a href="#ref-hadley2017">2017</a>)</span>, tidy data has a specific structure:</p>
<ul>
<li>Each variable is a column</li>
<li>Each observation is a row</li>
<li>Each type of observational unit is a table</li>
</ul>
<p>With text data like a <code>corpus</code>, we can also define the <strong>tidy text format</strong> as being a <code>data.frame</code> with one-token-per-row. A token is a meaningful unit of text, such as a word that we are interested in using for analysis, and tokenization is the process of splitting text into tokens.</p>
<p>In computational text analytics, the token (i.e., each row in the data frame) is most often a single <em>word</em>, but can also be an <em>n-gram</em>, <em>sentence</em>, or <em>paragraph</em>. The <code>tidytext</code> package in R is made for the handling of the tidy text format of the corpus data.</p>
<p>Tidy datasets allow manipulation with a standard set of <em>tidy</em> tools, including popular packages such as <code>dplyr</code>, <code>tidyr</code>, and <code>ggplot2</code>.</p>
<p>The <code>tidytext</code> package includes functions to <code>tidy()</code> objects from <code>quanteda</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">corp_us_tidy &lt;-<span class="st"> </span><span class="kw">tidy</span>(corp_us) <span class="co"># convert `corpus` to `data.frame`</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3"><span class="kw">class</span>(corp_us_tidy)</a></code></pre></div>
<pre><code>## [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["text"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Year"],"name":[2],"type":["int"],"align":["right"]},{"label":["President"],"name":[3],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[4],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[5],"type":["fctr"],"align":["left"]}],"data":[{"1":"Fellow-Citizens of the Senate and of the House of ...","2":"1789","3":"Washington","4":"George","5":"none"},{"1":"Fellow citizens, I am again called upon by the voi...","2":"1793","3":"Washington","4":"George","5":"none"},{"1":"When it was first perceived, in early times, that ...","2":"1797","3":"Adams","4":"John","5":"Federalist"},{"1":"Friends and Fellow Citizens:\\n\\nCalled upon to under...","2":"1801","3":"Jefferson","4":"Thomas","5":"Democratic-Republican"},{"1":"Proceeding, fellow citizens, to that qualification...","2":"1805","3":"Jefferson","4":"Thomas","5":"Democratic-Republican"},{"1":"Unwilling to depart from examples of the most reve...","2":"1809","3":"Madison","4":"James","5":"Democratic-Republican"},{"1":"About to add the solemnity of an oath to the oblig...","2":"1813","3":"Madison","4":"James","5":"Democratic-Republican"},{"1":"I should be destitute of feeling if I was not deep...","2":"1817","3":"Monroe","4":"James","5":"Democratic-Republican"},{"1":"Fellow citizens, I shall not attempt to describe t...","2":"1821","3":"Monroe","4":"James","5":"Democratic-Republican"},{"1":"In compliance with an usage coeval with the existe...","2":"1825","3":"Adams","4":"John Quincy","5":"Democratic-Republican"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="frequency-lists" class="section level2">
<h2><span class="header-section-number">4.6</span> Frequency Lists</h2>
<p>To get a frequency list of words, word tokenization is an important step for corpus analysis because words are a meaningful linguistic unit in language. Also, word frequency lists are often indicative of many important messages.</p>
<p>The <code>tidytext</code> provides a powerful function, <code>unnest_tokens()</code> to tokenize a data frame with larger linguistic units (e.g., <em>texts</em>) into one with smaller units (e.g., <em>words</em>).</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">corp_us_words &lt;-<span class="st"> </span>corp_us_tidy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="st">  </span><span class="kw">unnest_tokens</span>(<span class="dt">output =</span> word, <span class="dt">input =</span> text, <span class="dt">token =</span> <span class="st">&quot;words&quot;</span>) <span class="co"># tokenize the `text` column into `word`</span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3"></a>
<a class="sourceLine" id="cb14-4" data-line-number="4">corp_us_words</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["word"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"fellow"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"citizens"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"senate"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"and"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"house"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="info">
<p>
The <code>unnest_tokens()</code> is optimized for English tokenization of other linguistic units, such as <em>words</em>, <em>ngrams</em>, <em>sentences</em>, <em>lines</em>, and <em>paragraphs</em> (check <code>?unnest_tokens()</code>). To handle Chinese data, however, we need to define own ways of tokenization <code>unnest_tokens(…, token = …)</code>. We will discuss the principles for Chinese text processing in a later <a href="chinese-text-processing.html#chinese-text-processing">chapter</a>.
</p>
</div>
<p>Now we can count the word frequencies:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">corp_us_words_freq &lt;-<span class="st"> </span>corp_us_words <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="st">  </span><span class="kw">count</span>(word, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3"></a>
<a class="sourceLine" id="cb15-4" data-line-number="4">corp_us_words_freq</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["word"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"the","2":"10082"},{"1":"of","2":"7103"},{"1":"and","2":"5310"},{"1":"to","2":"4534"},{"1":"in","2":"2785"},{"1":"a","2":"2246"},{"1":"our","2":"2181"},{"1":"that","2":"1789"},{"1":"we","2":"1739"},{"1":"be","2":"1482"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Frequency lists can be generated for bigrams or any other multiword combinations as well:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">corp_us_bigrams &lt;-<span class="st"> </span>corp_us_tidy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"><span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3"></a>
<a class="sourceLine" id="cb16-4" data-line-number="4">corp_us_bigrams</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["bigram"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"fellow citizens"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"citizens of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the senate"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"senate and"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"and of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of the"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"the house"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"house of"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"of representatives"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>To create bigram frequency list:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">corp_us_bigrams_freq &lt;-<span class="st"> </span>corp_us_bigrams <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2"><span class="st">  </span><span class="kw">count</span>(bigram, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">corp_us_bigrams_freq</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["bigram"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"of the","2":"1766"},{"1":"in the","2":"812"},{"1":"to the","2":"720"},{"1":"of our","2":"619"},{"1":"and the","2":"471"},{"1":"it is","2":"323"},{"1":"by the","2":"318"},{"1":"for the","2":"312"},{"1":"to be","2":"311"},{"1":"the people","2":"265"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">sum</span>(corp_us_words_freq<span class="op">$</span>n) <span class="co"># size of unigrams</span></a></code></pre></div>
<pre><code>## [1] 135562</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">sum</span>(corp_us_bigrams_freq<span class="op">$</span>n) <span class="co"># size of bigrams</span></a></code></pre></div>
<pre><code>## [1] 135504</code></pre>
</div>
<div id="word-cloud" class="section level2">
<h2><span class="header-section-number">4.7</span> Word Cloud</h2>
<p>With frequency data, we can visualize important words in the corpus with a <strong>Word Cloud</strong>. It is a novel but intuitive visual representation of text data. It allows us to quickly perceive the most prominent words from a large collection of texts.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">library</span>(wordcloud)</a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="kw">with</span>(corp_us_words_freq, <span class="kw">wordcloud</span>(word, n, </a>
<a class="sourceLine" id="cb22-4" data-line-number="4">                                   <span class="dt">max.words =</span> <span class="dv">400</span>,</a>
<a class="sourceLine" id="cb22-5" data-line-number="5">                                   <span class="dt">min.freq =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb22-6" data-line-number="6">                                   <span class="dt">scale =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="fl">0.5</span>),</a>
<a class="sourceLine" id="cb22-7" data-line-number="7">                                   <span class="dt">color =</span> <span class="kw">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&quot;Dark2&quot;</span>),</a>
<a class="sourceLine" id="cb22-8" data-line-number="8">                                   <span class="dt">vfont=</span><span class="kw">c</span>(<span class="st">&quot;serif&quot;</span>,<span class="st">&quot;plain&quot;</span>)))</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<hr />

<div class="exercise">
<span id="exr:wordcloud1" class="exercise"><strong>Exercise 4.3  </strong></span>Word cloud would be more informative if we first remove functional words. In <code>tidytext</code>, there is a preloaded data frame, <code>stop_words</code>, which contains common English stop words. Please make use of this data frame and try to re-create a word cloud with all stopwords removed. (Criteria: Frequency &gt;= 10; Max Number of Words Plotted = 400)
</div>

<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="kw">require</span>(tidytext)</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">stop_words</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["word"],"name":[1],"type":["chr"],"align":["left"]},{"label":["lexicon"],"name":[2],"type":["chr"],"align":["left"]}],"data":[{"1":"a","2":"SMART"},{"1":"a's","2":"SMART"},{"1":"able","2":"SMART"},{"1":"about","2":"SMART"},{"1":"above","2":"SMART"},{"1":"according","2":"SMART"},{"1":"accordingly","2":"SMART"},{"1":"across","2":"SMART"},{"1":"actually","2":"SMART"},{"1":"after","2":"SMART"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-24-1.png" width="100%" /></p>

<div class="exercise">
<span id="exr:unnamed-chunk-25" class="exercise"><strong>Exercise 4.4  </strong></span>Get yourself familiar with another R package for creating word clouds, <code>wordcloud2</code>, and re-create a word cloud as Exercise <a href="corpus-analysis-a-start.html#exr:wordcloud1">4.3</a> but in a fancier format, i.e., a star-shaped one. (Criteria: Frequency &gt;= 15)
</div>

<p><img src="wc2.png" width="100%" /></p>
</div>
<div id="collocations" class="section level2">
<h2><span class="header-section-number">4.8</span> Collocations</h2>
<p>With unigram and bigram frequencies of the corpus, we can further examine the collocations within the corpus. Collocation refers to a frequent phenomenon where two words tend to co-occur very often in use. This co-occurrence is defined statistically by their <em>lexical associations</em>.</p>
<div id="cooccurrence-table-and-observed-frequencies" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Cooccurrence Table and Observed Frequencies</h3>
<p>Cooccurrence frequency data for a word pair, <em>w<sub>1</sub></em> and <em>w<sub>2</sub></em>, are often organized in a contingency table extracted from a corpus, as shown in Figure <a href="corpus-analysis-a-start.html#fig:collo1">4.1</a>. The cell counts of this contingency table are called the observed frequencies <em>O<sub>11</sub></em>, <em>O<sub>12</sub></em>, <em>O<sub>21</sub></em>, and <em>O<sub>22</sub></em>.</p>
<div class="figure"><span id="fig:collo1"></span>
<img src="images/collocation1.png" alt="Cooccurrence Freqeucny Table" width="80%" />
<p class="caption">
Figure 4.1: Cooccurrence Freqeucny Table
</p>
</div>
<p>The sum of all four observed frequencies (called the sample size <em>N</em>) is equal to the total number of bigrams extracted from the corpus. <em>R<sub>1</sub></em> and <em>R<sub>2</sub></em> are the row totals of the observed contingency table, while <em>C<sub>1</sub></em> and <em>C<sub>2</sub></em> are the corresponding column totals. The row and column totals are also called <strong>marginal frequencies</strong>, being written in the margins of the table, and <em>O<sub>11</sub></em> is called the <strong>joint frequency</strong>.</p>
</div>
<div id="expected-frequencies" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Expected Frequencies</h3>
<p>Equations for all association measures are given in terms of the observed frequencies, marginal frequencies, and the expected frequencies <em>E<sub>11</sub></em>, …, <em>E<sub>22</sub></em> (under the null hypothesis that <em>W<sub>1</sub></em> and <em>W<sub>2</sub></em> are statistically independent). The expected frequencies can easily be computed from the marginal frequencies as shown in Figure <a href="corpus-analysis-a-start.html#fig:collo2">4.2</a>.</p>
<div class="figure"><span id="fig:collo2"></span>
<img src="images/collocation2.png" alt="Computing Expected Frequencies" width="95%" />
<p class="caption">
Figure 4.2: Computing Expected Frequencies
</p>
</div>
</div>
<div id="association-measures" class="section level3">
<h3><span class="header-section-number">4.8.3</span> Association Measures</h3>
<p>The idea of lexical assoication is to measure how much the observed frequencies deviate from the expected. Some of the metrics (e.g., <em>t-statistic</em>, <em>MI</em>) consider only the joint frequency deviation (i.e., <em>O<sub>11</sub></em>), while others (e.g., <em>G<sup>2</sup></em>, a.k.a Log Likelihood Ratio)consider the deviations of ALL cells.</p>
<p>Here I would like to show you how we can compute the most common two asssociation metrics for all the bigrams found in the corpus–<strong>t-test</strong> statistic and <strong>Mutual Information</strong> (MI).</p>
<ul>
<li><span class="math inline">\(t = \frac{O_{11}-E_{11}}{\sqrt{E_{11}}}\)</span></li>
<li><span class="math inline">\(MI = log_2\frac{O_{11}}{E_{11}}\)</span></li>
<li><em>Log-Likelohood Ratio (LLR)</em> <span class="math inline">\(= 2 \sum_{ij}{O_{ij}log\frac{O_{ij}}{E_{ij}}}\)</span></li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">corp_us_collocations &lt;-<span class="st"> </span>corp_us_bigrams_freq <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># set bigram frequency cut-off</span></a>
<a class="sourceLine" id="cb24-3" data-line-number="3"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">O11 =</span> n) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb24-4" data-line-number="4"><span class="st">  </span>tidyr<span class="op">::</span><span class="kw">separate</span>(bigram, <span class="kw">c</span>(<span class="st">&quot;w1&quot;</span>, <span class="st">&quot;w2&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># split bigrams into two columns</span></a>
<a class="sourceLine" id="cb24-5" data-line-number="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">R1 =</span> corp_us_words_freq<span class="op">$</span>n[<span class="kw">match</span>(w1, corp_us_words_freq<span class="op">$</span>word)],</a>
<a class="sourceLine" id="cb24-6" data-line-number="6">         <span class="dt">C1 =</span> corp_us_words_freq<span class="op">$</span>n[<span class="kw">match</span>(w2, corp_us_words_freq<span class="op">$</span>word)]) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># retrieve w1 w2 unigram freq</span></a>
<a class="sourceLine" id="cb24-7" data-line-number="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">E11 =</span> (R1<span class="op">*</span>C1)<span class="op">/</span><span class="kw">sum</span>(O11)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># compute expected freq of bigrams</span></a>
<a class="sourceLine" id="cb24-8" data-line-number="8"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">MI =</span> <span class="kw">log2</span>(O11<span class="op">/</span>E11),</a>
<a class="sourceLine" id="cb24-9" data-line-number="9">         <span class="dt">t =</span> (O11 <span class="op">-</span><span class="st"> </span>E11)<span class="op">/</span><span class="kw">sqrt</span>(E11)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># compute associations</span></a>
<a class="sourceLine" id="cb24-10" data-line-number="10"><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(MI)) <span class="co"># sorting</span></a>
<a class="sourceLine" id="cb24-11" data-line-number="11"></a>
<a class="sourceLine" id="cb24-12" data-line-number="12">corp_us_collocations</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["w1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["O11"],"name":[3],"type":["int"],"align":["right"]},{"label":["R1"],"name":[4],"type":["int"],"align":["right"]},{"label":["C1"],"name":[5],"type":["int"],"align":["right"]},{"label":["E11"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["MI"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"indian","2":"tribes","3":"6","4":"8","5":"6","6":"0.0009108505","7":"12.685461","8":"198.77500"},{"1":"twenty","2":"five","3":"6","4":"16","5":"10","6":"0.0030361684","7":"10.948495","8":"108.83498"},{"1":"chief","2":"magistrate","3":"10","4":"36","5":"11","6":"0.0075145167","7":"10.378032","8":"115.27178"},{"1":"president","2":"bush","3":"6","4":"89","5":"6","6":"0.0101332119","7":"9.209727","8":"59.50365"},{"1":"move","2":"forward","3":"6","4":"15","5":"36","6":"0.0102470682","7":"9.193607","8":"59.17103"},{"1":"vice","2":"president","3":"16","4":"18","5":"89","6":"0.0303996357","7":"9.039802","8":"91.59249"},{"1":"two","2":"centuries","3":"7","4":"47","5":"15","6":"0.0133781168","7":"9.031336","8":"60.40456"},{"1":"interstate","2":"commerce","3":"6","4":"11","5":"64","6":"0.0133591408","7":"8.810991","8":"51.79573"},{"1":"domestic","2":"concerns","3":"7","4":"49","5":"17","6":"0.0158070515","7":"8.790643","8":"55.55086"},{"1":"god","2":"bless","3":"15","4":"92","5":"20","6":"0.0349159361","7":"8.746861","8":"80.08798"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="warning">
<p>
Please note that in the above example, we compute the lexical associations for bigrams whose frequency &gt; 5. This is necessary in collocation studies because bigrams of very low frequency would not be informative even though its association can be very strong.
</p>
</div>
<div class="info">
<p>
How to compute lexical assoications is a non-trivial issue. There are many more ways to compute the association strengths between two words. Please refer to <a href="http://www.collocations.de/index.html">Stefan Evert’s site</a> for a very comprehensive review of lexical assoication meaasures.
</p>
</div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-31" class="exercise"><strong>Exercise 4.5  </strong></span>Sort the collocation data frame <code>corp_us_collocations</code> according to the <code>t-score</code> and compare the results sorted by MI scores.
</div>

<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["w1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["O11"],"name":[3],"type":["int"],"align":["right"]},{"label":["R1"],"name":[4],"type":["int"],"align":["right"]},{"label":["C1"],"name":[5],"type":["int"],"align":["right"]},{"label":["E11"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["MI"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"indian","2":"tribes","3":"6","4":"8","5":"6","6":"0.0009108505","7":"12.685461","8":"198.77500"},{"1":"united","2":"states","3":"157","4":"202","5":"333","6":"1.2764431288","7":"6.942491","8":"137.83312"},{"1":"fellow","2":"citizens","3":"117","4":"152","5":"247","6":"0.7124369046","7":"7.359531","8":"137.77172"},{"1":"chief","2":"magistrate","3":"10","4":"36","5":"11","6":"0.0075145167","7":"10.378032","8":"115.27178"},{"1":"twenty","2":"five","3":"6","4":"16","5":"10","6":"0.0030361684","7":"10.948495","8":"108.83498"},{"1":"four","2":"years","3":"29","4":"36","5":"141","6":"0.0963224411","7":"8.233965","8":"93.12995"},{"1":"vice","2":"president","3":"16","4":"18","5":"89","6":"0.0303996357","7":"9.039802","8":"91.59249"},{"1":"let","2":"us","3":"97","4":"154","5":"478","6":"1.3968651562","7":"6.117720","8":"80.89001"},{"1":"god","2":"bless","3":"15","4":"92","5":"20","6":"0.0349159361","7":"8.746861","8":"80.08798"},{"1":"those","2":"who","3":"123","4":"328","5":"370","6":"2.3029336977","7":"5.739042","8":"79.53458"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />

<div class="exercise">
<span id="exr:exllr" class="exercise"><strong>Exercise 4.6  </strong></span>Based on the formula provided above, please create a new column for <code>corp_us_collocations</code>, which gives the Log-Likelihood Ratios of all the bigrams.
</div>

<div class="danger">
<p>
When you do the above exercise, you may run into a couple of problems:
</p>
<ul>
<li>
Some of the bigrams have <code>NaN</code> values in their LLR. This may be due to the issue of <code>NAs produced by integer overflow</code>. Please solve this.
</li>
<li>
After solving the above overflow issue, you may still have a few bigrams with <code>NaN</code> in their LLR, which may be due to the computation of the <code>log</code> value. In Math, <code>log(1/0)= Inf</code> and <code>log(0/1) = -Inf</code>. Do you know when you would get an undefined value <code>NaN</code> in the computation of <code>log()</code>?
</li>
</ul>
</div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["w1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["MI"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["LLR"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["O11"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["O12"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["O21"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["O22"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["E11"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["E12"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["E21"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["E22"],"name":[13],"type":["dbl"],"align":["right"]}],"data":[{"1":"united","2":"states","3":"6.942491","4":"137.83312","5":"1465.2545","6":"157","7":"45","8":"176","9":"52320","10":"1.2764431","11":"200.7236","12":"331.7236","13":"52164.28"},{"1":"fellow","2":"citizens","3":"7.359531","4":"137.77172","5":"1157.9238","6":"117","7":"35","8":"130","9":"52416","10":"0.7124369","11":"151.2876","12":"246.2876","13":"52299.71"},{"1":"and","2":"of","3":"-3.090931","4":"-23.61307","5":"1056.9342","6":"84","7":"5226","8":"7019","9":"40369","10":"715.7184333","11":"4594.2816","12":"6387.2816","13":"41000.72"},{"1":"has","2":"been","3":"4.965806","4":"72.60057","5":"1018.2445","6":"180","7":"442","8":"308","9":"51768","10":"5.7599150","11":"616.2401","12":"482.2401","13":"51593.76"},{"1":"have","2":"been","3":"4.431367","4":"62.95832","5":"986.7757","6":"202","7":"809","8":"286","9":"51401","10":"9.3621769","11":"1001.6378","12":"478.6378","13":"51208.36"},{"1":"to","2":"and","3":"-6.250642","4":"-21.09354","5":"943.1952","6":"6","7":"4528","8":"5304","9":"42860","10":"456.8587043","11":"4077.1413","12":"4853.1413","13":"43310.86"},{"1":"it","2":"is","3":"3.067292","4":"45.82485","5":"931.8411","6":"323","7":"1066","8":"1139","9":"50170","10":"38.5350108","11":"1350.4650","12":"1423.4650","13":"49885.54"},{"1":"those","2":"who","3":"5.739042","4":"79.53458","5":"833.9012","6":"123","7":"205","8":"247","9":"52123","10":"2.3029337","11":"325.6971","12":"367.6971","13":"52002.30"},{"1":"let","2":"us","3":"6.117720","4":"80.89001","5":"731.1687","6":"97","7":"57","8":"381","9":"52163","10":"1.3968652","11":"152.6031","12":"476.6031","13":"52067.40"},{"1":"we","2":"have","3":"2.934205","4":"38.37207","5":"679.6418","6":"255","7":"1484","8":"756","9":"50203","10":"33.3623477","11":"1705.6377","12":"977.6377","13":"49981.36"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-35" class="exercise"><strong>Exercise 4.7  </strong></span></p>
<ol style="list-style-type: upper-alpha">
<li><p>Find the top FIVE bigrams ranked according to MI values for each president. The result would be a data frame as shown below.</p></li>
<li>Create a plot as shown below to visualize your results.
</div></li>
</ol>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year_President"],"name":[1],"type":["chr"],"align":["left"]},{"label":["w1"],"name":[2],"type":["chr"],"align":["left"]},{"label":["w2"],"name":[3],"type":["chr"],"align":["left"]},{"label":["n"],"name":[4],"type":["int"],"align":["right"]},{"label":["w1freq"],"name":[5],"type":["int"],"align":["right"]},{"label":["w2freq"],"name":[6],"type":["int"],"align":["right"]},{"label":["w12freq_exp"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["MI"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[9],"type":["dbl"],"align":["right"]}],"data":[{"1":"1789_Washington","2":"assure","3":"myself","4":"2","5":"12","6":"33","7":"1.061349e-02","8":"7.5579568","9":"1.4067087"},{"1":"1789_Washington","2":"executive","3":"department","4":"2","5":"97","6":"24","7":"6.239447e-02","8":"5.0024381","9":"1.3700940"},{"1":"1789_Washington","2":"how","3":"far","4":"2","5":"45","6":"87","7":"1.049288e-01","8":"4.2525168","9":"1.3400177"},{"1":"1789_Washington","2":"retreat","3":"which","4":"2","5":"8","6":"1006","7":"2.157005e-01","8":"3.2128988","9":"1.2616903"},{"1":"1789_Washington","2":"i","3":"dare","4":"2","5":"838","6":"10","7":"2.245986e-01","8":"3.1545789","9":"1.2553983"},{"1":"1793_Washington","2":"i","3":"am","4":"2","5":"838","6":"65","7":"1.459891e+00","8":"0.4541392","9":"0.3819146"},{"1":"1793_Washington","2":"it","3":"shall","4":"2","5":"1389","6":"314","7":"1.168947e+01","8":"-2.5471382","9":"-6.8514934"},{"1":"1793_Washington","2":"by","3":"the","4":"2","5":"1083","6":"10082","7":"2.926431e+02","8":"-7.1929984","9":"-205.5156936"},{"1":"1793_Washington","2":"of","3":"the","4":"4","5":"7103","6":"10082","7":"1.919339e+03","8":"-8.9063936","9":"-957.6693468"},{"1":"1793_Washington","2":"to","3":"the","4":"2","5":"4534","6":"10082","7":"1.225156e+03","8":"-9.2587495","9":"-864.9017596"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-36-1.png" width="110%" /></p>
</div>
</div>
<div id="constructions" class="section level2">
<h2><span class="header-section-number">4.9</span> Constructions</h2>
<p>We are often interested in the use of linguistic patterns, which are beyond the lexical boundaries. My experience is that usually it is better to work with the corpus on a sentential level.</p>
<p>We can use the same tokenization function, <code>unnest_tokens()</code> to convert our text-based corpus data frame, <code>corpus_us_tidy</code>, into a sentence-based tidy structure:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">corp_us_sents &lt;-<span class="st"> </span>corp_us_tidy <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb25-2" data-line-number="2"><span class="st">  </span><span class="kw">unnest_tokens</span>(<span class="dt">output =</span> sentence, <span class="dt">input =</span> text, <span class="dt">token =</span> <span class="st">&quot;sentences&quot;</span>) <span class="co"># tokenize the `text` column into `sentence`</span></a>
<a class="sourceLine" id="cb25-3" data-line-number="3">corp_us_sents</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["sentence"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"fellow-citizens of the senate and of the house of representatives:  among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"on the one hand, i was summoned by my country, whose voice i can never hear but with veneration and love, from a retreat which i had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"on the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"in this conflict of emotions all i dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"all i dare hope is that if, in executing this task, i have been too much swayed by a grateful remembrance of former instances, or by an affectionate sensibility to this transcendent proof of the confidence of my fellow citizens, and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead me, and its consequences be judged by my country with some share of the partiality in which they originated."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"such being the impressions under which i have, in obedience to the public summons, repaired to the present station, it would be peculiarly improper to omit in this first official act my fervent supplications to that almighty being who rules over the universe, who presides in the councils of nations, and whose providential aids can supply every human defect, that his benediction may consecrate to the liberties and happiness of the people of the united states a government instituted by themselves for these essential purposes, and may enable every instrument employed in its administration to execute with success the functions allotted to his charge."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"in tendering this homage to the great author of every public and private good, i assure myself that it expresses your sentiments not less than my own, nor those of my fellow citizens at large less than either."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"no people can be bound to acknowledge and adore the invisible hand which conducts the affairs of men more than those of the united states."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"every step by which they have advanced to the character of an independent nation seems to have been distinguished by some token of providential agency; and in the important revolution just accomplished in the system of their united government the tranquil deliberations and voluntary consent of so many distinct communities from which the event has resulted can not be compared with the means by which most governments have been established without some return of pious gratitude, along with an humble anticipation of the future blessings which the past seem to presage."},{"1":"1789","2":"Washington","3":"George","4":"none","5":"these reflections, arising out of the present crisis, have forced themselves too strongly on my mind to be suppressed."}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>With each sentence, we can investigate particular constructions in more detail. Let’s assume that we are interested in the use of <em>Perfect</em> aspect in English by different presidents. We can try to extract Perfect constructions (including Present/Past Perfect) from each sentence using the regular expression.</p>
<p>Here we make a simple naive assumption: Perfect constructions include all <code>have/has/had + VERB-en/ed</code> tokens from the sentences.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">require</span>(stringr)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2"><span class="co"># Perfect</span></a>
<a class="sourceLine" id="cb26-3" data-line-number="3">corp_us_sents <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pattern =</span> <span class="kw">str_extract_all</span>(sentence, <span class="st">&quot;ha[d|ve|s] </span><span class="ch">\\</span><span class="st">w+(en|ed)&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-5" data-line-number="5"><span class="st">  </span><span class="kw">unnest_tokens</span>(perfect, pattern,<span class="dt">token =</span> c) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-6" data-line-number="6"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>sentence) -&gt;<span class="st"> </span>result_perfect</a>
<a class="sourceLine" id="cb26-7" data-line-number="7">result_perfect</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["perfect"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"had chosen"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"has been"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"has resulted"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"has ordained"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"has given"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"has been"},{"1":"1793","2":"Washington","3":"George","4":"none","5":"has been"},{"1":"1797","2":"Adams","3":"John","4":"Federalist","5":"had contributed"},{"1":"1797","2":"Adams","3":"John","4":"Federalist","5":"has been"},{"1":"1797","2":"Adams","3":"John","4":"Federalist","5":"has equaled"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>And of course we can do an exploratory analysis of the frequencies of Perfect constructions by different presidents:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="kw">require</span>(tidyr)</a>
<a class="sourceLine" id="cb27-2" data-line-number="2"><span class="co"># table</span></a>
<a class="sourceLine" id="cb27-3" data-line-number="3">result_perfect <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb27-4" data-line-number="4"><span class="st">  </span><span class="kw">group_by</span>(President) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb27-5" data-line-number="5"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">TOKEN_FREQ =</span> <span class="kw">n</span>(),</a>
<a class="sourceLine" id="cb27-6" data-line-number="6">            <span class="dt">TYPE_FREQ =</span> <span class="kw">n_distinct</span>(perfect))</a></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["President"],"name":[1],"type":["chr"],"align":["left"]},{"label":["TOKEN_FREQ"],"name":[2],"type":["int"],"align":["right"]},{"label":["TYPE_FREQ"],"name":[3],"type":["int"],"align":["right"]}],"data":[{"1":"Adams","2":"26","3":"14"},{"1":"Buchanan","2":"13","3":"9"},{"1":"Bush","2":"10","3":"8"},{"1":"Carter","2":"7","3":"7"},{"1":"Cleveland","2":"2","3":"2"},{"1":"Clinton","2":"5","3":"3"},{"1":"Coolidge","2":"16","3":"10"},{"1":"Eisenhower","2":"5","3":"5"},{"1":"Garfield","2":"18","3":"13"},{"1":"Grant","2":"4","3":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># graph</span></a>
<a class="sourceLine" id="cb28-2" data-line-number="2">result_perfect <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb28-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(President) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb28-4" data-line-number="4"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">TOKEN_FREQ =</span> <span class="kw">n</span>(),</a>
<a class="sourceLine" id="cb28-5" data-line-number="5">            <span class="dt">TYPE_FREQ =</span> <span class="kw">n_distinct</span>(perfect)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb28-6" data-line-number="6"><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="kw">c</span>(<span class="st">&quot;TOKEN_FREQ&quot;</span>, <span class="st">&quot;TYPE_FREQ&quot;</span>), <span class="dt">names_to =</span> <span class="st">&quot;STATISTIC&quot;</span>, <span class="dt">values_to =</span> <span class="st">&quot;NUMBER&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb28-7" data-line-number="7"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(President, NUMBER, <span class="dt">fill =</span> STATISTIC)) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-8" data-line-number="8"><span class="st">           </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>,<span class="dt">position =</span> <span class="kw">position_dodge</span>()) <span class="op">+</span></a>
<a class="sourceLine" id="cb28-9" data-line-number="9"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle=</span><span class="dv">90</span>))</a></code></pre></div>
<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>There are quite a few things we need to take care of more thoroughly:</p>
<ol style="list-style-type: decimal">
<li><p>The auxilliary <em>HAVE</em> and the past participle do not necessarily have to stand next to each other for Perfect constructions.</p></li>
<li><p>We now lose track of one important information: from which sentence of the Presidental addressess did we collect each Perfect constructional token?</p></li>
</ol>
<p>Any ideas how to solve all these issues?</p>

<div class="exercise">
<p><span id="exr:unnamed-chunk-40" class="exercise"><strong>Exercise 4.8  </strong></span>Please create a better regular expression to retrieve more tokens of English Perfect constructions, where the auxilliary and participle may not stand together.</p>
</div>

<div id="htmlwidget-26ec631b2698538f700c" style="width:960px;height:100%;" class="str_view html-widget"></div>
<script type="application/json" data-for="htmlwidget-26ec631b2698538f700c">{"x":{"html":"<ul>\n  <li>When it was first perceived, in early times, that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims, men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country. Relying, however, on the purity of their intentions, the justice of their cause, and the integrity and intelligence of the people, under an overruling Providence which <span class='match'>had so signally protected<\/span> this country from the first, the representatives of this nation, then consisting of little more than half its present number, not only broke to pieces the chains which were forging and the rod of iron that was lifted up, but frankly cut asunder the ties which had bound them, and launched into an ocean of uncertainty.\n\nThe zeal and ardor of the people during the Revolutionary war, supplying the place of government, commanded a degree of order sufficient at least for the temporary preservation of society. The Confederation which was early felt to be necessary was prepared from the models of the Batavian and Helvetic confederacies, the only examples which remain with any detail and precision in history, and certainly the only ones which the people at large <span class='match'>had ever considered<\/span>. But reflecting on the striking difference in so many particulars between this country and those where a courier may go from the seat of government to the frontier in a single day, it was then certainly foreseen by some who assisted in Congress at the formation of it that it could not be durable.\n\nNegligence of its regulations, inattention to its recommendations, if not disobedience to its authority, not only in individuals but in States, soon appeared with their melancholy consequences -- universal languor, jealousies and rivalries of States, decline of navigation and commerce, discouragement of necessary manufactures, universal fall in the value of lands and their produce, contempt of public and private faith, loss of consideration and credit with foreign nations, and at length in discontents, animosities, combinations, partial conventions, and insurrection, threatening some great national calamity.\n\nIn this dangerous crisis the people of America were not abandoned by their usual good sense, presence of mind, resolution, or integrity. Measures were pursued to concert a plan to form a more perfect union, establish justice, insure domestic tranquillity, provide for the common defense, promote the general welfare, and secure the blessings of liberty. The public disquisitions, discussions, and deliberations issued in the present happy Constitution of Government.\n\nEmployed in the service of my country abroad during the whole course of these transactions, I first saw the Constitution of the United States in a foreign country. Irritated by no literary altercation, animated by no public debate, heated by no party animosity, I read it with great satisfaction, as the result of good heads prompted by good hearts, as an experiment better adapted to the genius, character, situation, and relations of this nation and country than any which <span class='match'>had ever been proposed<\/span> or suggested. In its general principles and great outlines it was conformable to such a system of government as I <span class='match'>had ever most esteemed<\/span>, and in some States, my own native State in particular, had contributed to establish. Claiming a right of suffrage, in common with my fellow-citizens, in the adoption or rejection of a constitution which was to rule me and my posterity, as well as them and theirs, I did not hesitate to express my approbation of it on all occasions, in public and in private. It was not then, nor has been since, any objection to it in my mind that the Executive and Senate were not more permanent. Nor have I ever entertained a thought of promoting any alteration in it but such as the people themselves, in the course of their experience, should see and feel to be necessary or expedient, and by their representatives in Congress and the State legislatures, according to the Constitution itself, adopt and ordain.\n\nReturning to the bosom of my country after a painful separation from it for ten years, I had the honor to be elected to a station under the new order of things, and I have repeatedly laid myself under the most serious obligations to support the Constitution. The operation of it has equaled the most sanguine expectations of its friends, and from an habitual attention to it, satisfaction in its administration, and delight in its effects upon the peace, order, prosperity, and happiness of the nation I have acquired an habitual attachment to it and veneration for it.\n\nWhat other form of government, indeed, can so well deserve our esteem and love?\n\nThere may be little solidity in an ancient idea that congregations of men into cities and nations are the most pleasing objects in the sight of superior intelligences, but this is very certain, that to a benevolent human mind there can be no spectacle presented by any nation more pleasing, more noble, majestic, or august, than an assembly like that which <span class='match'>has so often been seen<\/span> in this and the other Chamber of Congress, of a Government in which the Executive authority, as well as that of all the branches of the Legislature, are exercised by citizens selected at regular periods by their neighbors to make and execute laws for the general good. Can anything essential, anything more than mere ornament and decoration, be added to this by robes and diamonds? Can authority be more amiable and respectable when it descends from accidents or institutions established in remote antiquity than when it springs fresh from the hearts and judgments of an honest and enlightened people? For it is the people only that are represented. It is their power and majesty that is reflected, and only for their good, in every legitimate government, under whatever form it may appear. The existence of such a government as ours for any length of time is a full proof of a general dissemination of knowledge and virtue throughout the whole body of the people. And what object or consideration more pleasing than this can be presented to the human mind? If national pride is ever justifiable or excusable it is when it springs, not from power or riches, grandeur or glory, but from conviction of national innocence, information, and benevolence.\n\nIn the midst of these pleasing ideas we should be unfaithful to ourselves if we should ever lose sight of the danger to our liberties if anything partial or extraneous should infect the purity of our free, fair, virtuous, and independent elections. If an election is to be determined by a majority of a single vote, and that can be procured by a party through artifice or corruption, the Government may be the choice of a party for its own ends, not of the nation for the national good. If that solitary suffrage can be obtained by foreign nations by flattery or menaces, by fraud or violence, by terror, intrigue, or venality, the Government may not be the choice of the American people, but of foreign nations. It may be foreign nations who govern us, and not we, the people, who govern ourselves; and candid men will acknowledge that in such cases choice would have little advantage to boast of over lot or chance.\n\nSuch is the amiable and interesting system of government (and such are some of the abuses to which it may be exposed) which the people of America have exhibited to the admiration and anxiety of the wise and virtuous of all nations for eight years under the administration of a citizen who, by a long course of great actions, regulated by prudence, justice, temperance, and fortitude, conducting a people inspired with the same virtues and animated with the same ardent patriotism and love of liberty to indep<\/li>\n<\/ul>"},"evals":[],"jsHooks":[]}</script>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-42" class="exercise"><strong>Exercise 4.9  </strong></span>Re-generate a <code>result_perfect</code> data frame, where you can keep track of:</p>
<ul>
<li>From the N-th sentence of the address did the Perfect come?</li>
<li>From which address did the Perfect come?</li>
</ul>
You may have a data frame as shown below.
</div>

<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Year"],"name":[1],"type":["int"],"align":["right"]},{"label":["President"],"name":[2],"type":["chr"],"align":["left"]},{"label":["FirstName"],"name":[3],"type":["chr"],"align":["left"]},{"label":["Party"],"name":[4],"type":["fctr"],"align":["left"]},{"label":["INDEX"],"name":[5],"type":["chr"],"align":["left"]},{"label":["SENT_ID"],"name":[6],"type":["int"],"align":["right"]},{"label":["perfect"],"name":[7],"type":["chr"],"align":["left"]}],"data":[{"1":"1789","2":"Washington","3":"George","4":"none","5":"1789-Washington","6":"2","7":"had chosen"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"1789-Washington","6":"4","7":"has been"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"1789-Washington","6":"9","7":"has resulted"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"1789-Washington","6":"16","7":"has ordained"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"1789-Washington","6":"17","7":"has given"},{"1":"1789","2":"Washington","3":"George","4":"none","5":"1789-Washington","6":"23","7":"has been"},{"1":"1793","2":"Washington","3":"George","4":"none","5":"1793-Washington","6":"2","7":"has been"},{"1":"1797","2":"Adams","3":"John","4":"Federalist","5":"1797-Adams","6":"12","7":"had contributed"},{"1":"1797","2":"Adams","3":"John","4":"Federalist","5":"1797-Adams","6":"14","7":"has been"},{"1":"1797","2":"Adams","3":"John","4":"Federalist","5":"1797-Adams","6":"17","7":"has equaled"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>

<div class="exercise">
<span id="exr:unnamed-chunk-44" class="exercise"><strong>Exercise 4.10  </strong></span>Re-create the above bar plot in a way that the type and token frequencies are computed based on each address and the x axis should be arranged accordingly. Your resulting graph should look similar to the one below.
</div>

<p><img src="CorpusLinguistics_bookdown_files/figure-html/unnamed-chunk-45-1.png" width="960" /></p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-hadley2017">
<p>Wickham, Hadley, and Garrett Grolemund. 2017. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. 1st ed. O’Reilly Media, Inc.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https:\/\/alvinntnu.github.io" + location.pathname;  // Replace PAGE_URL with your page's canonical URL variable
/*
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
*/
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://enc2036.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="creating-corpus.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tokenization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
